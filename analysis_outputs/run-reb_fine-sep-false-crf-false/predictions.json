{"abstract_id": 0, "sentences": ["Thanks for your feedback.", "We discuss each comment in the following:", "- The experiments are not large scale", "We respectfully disagree with the reviewer's main comment that the experiments are not large scale.", "One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4).", "Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches).", "Sure, this is not the scale of 80 million tiny images; but one wouldn\u2019t ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances.", "Representation learning, the topic of this conference, has many facets.", "Learning representations from \u201cbig data\u201d (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side.", "Both are valuable in different circumstances.", "- No substantiate insight with respect to NP-hard problems", "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.", "To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives.", "These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard.", "Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees.", "Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML.", "Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem.", "So powerful non-convex solvers might be of a significant advantage over convex relaxations.", "Our paper simply shows ONE example for this.", "- It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly?", "It would not be possible to set the input dimension the same as the embedding dimension.", "Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error.", "The size of the embedding dimension can be too low to achieve this.", "One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation.", "However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding.", "- Methods, where items have no representation, are questionable", "Items having no representation is a caveat of the data available rather than that of the method.", "The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework.", "- How to generalize to unseen items", "First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage.", "We believe that in our case, generalization is realizable.", "One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items.", "The network can be trained with extra batches of triplets which involves the new items.", "- The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.", "We don\u2019t really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 1, "sentences": ["Overall:", "We thank you for your time and appreciating the strengths of our work.", "Based on your guidance and suggestions, we have tried to do additional experiments to improve the quality of our work.", "Concern 1: Comparison to Meta-RevGrad", "Meta-RevGrad tries to achieve feature invariance at the embedding level.", "Achieving such feature invariance for high-dimensional feature mapping can be a very weak constraint [1], causing limitation in performance.", "Recently, generative approaches, following image-to-image translation have been shown achieve better domain adaptation, as this constrains the feature embedding to generate the data in a new domain.", "Being a non-GAN based approach, concepts such idt (encouraging the styling network to behave as an identity when given a target domain instance as input) and revMap (constructing source instance back from generated target instance) are not applicable in this scenario, as no instances or images are being generated from a feature embedding.", "Concern 1, 2, 3: Experiments", "Thank you for these suggestions.", "Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.", "Specifically:", "\u201cDomain Adaptation Baselines\u201d,", "We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.", "\u201cSimple baseline \u2013 combining a subset of a new domain as training set\u201d", "We see the merit of this baseline, but there are several challenges in executing this.", "Designing it in a fair way is tricky.", "Using some labelled data in target domain maybe unfair, as we are not allowed to see meta-test data.", "Moreover, this is likely to not work, as the meta-train data would be too large, and would dominate, and we do not have a clear way to set the weights.", "\u201cDramatic Domain Shift, Omniglot to Fashion-MNIST\u201d", "This could be an interesting setting, but we don\u2019t think this will work very well, as the tasks are themselves completely different. We would not expect a character recognition model to transfer to a object recognition task, as the visual features are very different.", "Minor:", "Thanks for this; we have updated the draft to make the presentation clearer.", "Unlabelled data refers to only the domain of the meta-test data, but the meta-test data is never used in meta-training.", "L_da is essentially the sum of L_gan and L_cycle.", "[1] Shu, R., Bui, H.H., Narui, H. and Ermon, S. A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR 2018"], "labels": ["rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 2, "sentences": ["We hope that the reviewer will change his opinion once we clarify the goal of our paper and explain how it relates to prior work, as we believe we are fundamentally on the same page.", "We are well aware of SIFT, HOG, the results of Olshausen and Field on learning image filters from a few example images (some of us are sufficiently old to have implemented all such methods from scratch as grad students!) and no annotations, as well as Mallat\u2019s Scattering nets [1].", "In fact, we discuss and evaluate Oyallon\u2019s 2017 implementation [2] of this at page 5 and table 2 in the paper.", "However, the existence of these methods does not detract from the message of this paper.", "Our goal is to provide \u201ccritical analysis\u201d of current self-supervision methods because these *specific* tools are now very heavily researched.", "Our paper sends a cautionary message: current self-supervised learning techniques cannot improve on what can be obtained from a single image plus transformations for early layers in a network, and only improves in a limited manner for deeper layers, despite ingesting millions of images (which is touted as their key advantage).", "In particular, the claims are not limited to the first few layers as we show that one image recovers two thirds of the performance of deeper layers as well.", "This message, which is a partially negative result, stands on its own, regardless of whether good low-level features can be obtained in some other ways (e.g. manually) and, we hope the reviewer will agree, should be known by the community.", "Nevertheless, we also agree with the reviewer that it is interesting to put these findings in a broader context, so we are happy to expand the discussion of prior feature learning/design work further.", "However, please note that none of this literature makes our specific findings on the limits of self-supervision obvious.", "Furthermore, although this is a little besides the point, in the paper we do show in Table 2 that scattering transforms works as well as conv1, but that from conv2 onwards self-supervision on a single image does better, so even the claim that handcrafted features are equivalent to the first few layers in deep networks is not proven.", "Also, the fact that Olshausens\u2019s filters resemble conv1 does not mean that they are equivalent to conv1 in recognition performance.", "\u2014", "[1] J. Bruna and S. Mallat. \"Invariant scattering convolution networks.\" TPAMI 2013", "[2] E. Oyallon, et al. \"Scaling the scattering transform: Deep hybrid networks.\" ICCV 2017"], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 3, "sentences": ["Thank you for your supportive review.", "We answer the specific queries below and have also added them to the revised version of the paper.", "1.         We found that the entropy regularizer produces an ME score that stays constant across training, at the cost of the model being less confident about predictions made for seen classes.", "We added details regarding this condition to the manuscript.", "2.         The base rate is the probability of observing a new word in the target at that particular point in training.", "We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word.", "Thus, the base rate at time t in training is defined as:", "$$P(\\text{new in target at t}) = \\frac{ \\text{# of unseen sentences in target with new words}} {\\text{# of unseen sentences}}$$", "3.", "In Section 4.2, we use \u201cnew\u201d to refer to the set of all the unseen classes at a particular timepoint t.", "For the classifier, P(N|t) is calculated by adding the probabilities the model assigns to all the \u201cnew\u201d classes when iterating through the remaining corpus (similar to Equation 1 in our paper).", "For the dataset, we compute P(N|t) by sampling all unseen images in the corpus and compute the proportion from \u201cnew\u201d classes given their ground truth labels.", "4.         We will release our code and data with the publication of the paper.", "Most of our experiments are easy to replicate as they use standard datasets, models, loss functions and optimizers.", "We sincerely hope that our challenge and these resources will stimulate progress in this area.", "Please also see above where we write a general response to all reviews."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 4, "sentences": ["Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "Here we respond to your specific comments.", "\"(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.\"", ">>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation.", "The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples.", "While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A).", "This leads to better generalization ability for test tasks.", "In Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as \"Label Propagation\").", "\"", "(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.", "Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.", "For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.", "This is a major concern.\"", ">>> At first, we want to clarify the few-shot network architecture setting.", "Currently, there are two common network architectures: 4-layer ConvNets (e.g., [1][2][3]) and 12-layer ResNet (e.g., [4][5][6][7]).", "Our method belongs to the first one, which contains much fewer layers than the ResNet setting.", "Thus, it is more reasonable to compare TADAM with ResNet version of our method.", "To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:", "Method", "1-shot    5-shot", "SNAIL [4]", "55.71     68.88", "adaResNet [5]                        56.88     71.94", "Discriminative k-shot [6]", "56.30     73.90", "TADAM [7]", "58.50     76.70", "--------------------------------------------------------", "Ours", "59.46     75.65", "--------------------------------------------------------", "It can be seen that we beat TADAM for 1-shot setting.", "For 5-shot, we outperform all other recent high-performance methods except for TADAM.", ">>> We want to clarify that \"Label Propagation\" in Table 1 and Table 2 is a strong baseline.", "It combines label propagation method [8] with episodic meta-learning.", "The usage of transductive inference makes this baseline outperform most published state-of-the-art methods.", "Moreover, the performance of TPN over label propagation is not very small.", "For example, in miniImagenet, TPN outperforms label propagation with 1.44% and 1.25% for 1-shot and 5-shot respectively, but this advantage grows to 3.20% and 1.68% with \"Higher Shot\" training.", "The improvements are even larger for tieredImagenet with 4.68% and 2.87%.", "We believe in few-shot learning, this is a large improvement.", "[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML. 2017.", "[2] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.", "[3] Yang, Flood Sung Yongxin et al. \"Learning to compare: Relation network for few-shot learning.\" CVPR. 2018.", "[4] Mishra, Nikhil et al. \"A simple neural attentive meta-learner.\" ICLR. 2018.", "[5] Munkhdalai, Tsendsuren et al. \"Rapid adaptation with conditionally shifted neurons.\" ICML. 2018.", "[6] Bauer, Matthias et al. \"Discriminative k-shot learning using probabilistic models.\" arXiv. 2017.", "[7] Oreshkin, B.N., Lacoste, A. and Rodriguez, P., 2018. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS. 2018.", "[8] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 5, "sentences": ["Thank you very much for the positive comments.", "We added the more experimental data of runtime analysis to address the Reviewer's main concern.", "Q1. My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.", "Compressability is evaluated, but that was already present in the previous work.", "Therefore the novel contribution of this paper over [1] is not clearly outlined.", "We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [1].", "We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles.", "The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.", "We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [1].", "The proposed method outperformed both baseline method and [1] in all simulation results.", "Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.", "While preparing for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.", "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.", "Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.", "Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.", "In the revision, we added the more precise mathematical description of the input and output of each block in Figure 1 and showed the change of the exact weight representation at each process.", "We first prune weights in a neural network with the Viterbi-based pruning scheme [1], then we quantize the pruned weights with the alternating quantization method [2].", "Our main contribution is the third process, which includes encoding each weight with the Viterbi algorithm, and retraining for the recovery of accuracy.", "With our proposed method, the sparse and encoded weights are reconstructed to a dense matrix as described in Figure 2.", "Figure 2 illustrates the purpose of our proposed scheme, which is the parallelization of the whole sparse-to-dense conversion process with the VDs while maintaining the high compression rate.", "Q3. Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)", "Thanks very much for the suggestions.", "We tried to fix grammatical mistakes as much as possible in the revision.", "Reference", "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "[2] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 6, "sentences": ["We thank Reviewer 2 for the constructive feedback.", "Here is our point-to-point response to the comments and questions raised in the review:", "1. \u201cIt is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.\u201d", "We do not claim that our method is more efficient than Miyato et al.\u2019s method, which uses the spectral norm of the convolution kernel matrix to approximate the spectral norm of the convolution operation.", "In fact, our proposed method is computationally more expensive than their approximate scheme because each power iteration in our method requires a conv/deconv operation rather than a simple division used by Miyato et al.\u2019s.", "We introduce our new spectral normalization scheme for convolutional layers because there exist examples where the true spectral norm of a convolution operation can be arbitrarily larger than Miyato et al.\u2019s approximation.", "Therefore, Miyato et al.\u2019s normalization scheme is not guaranteed to control the spectral norm of convolutional layers which is critical for controlling a DNN\u2019s generalization performance (please see our generalization bounds in Section 3).", "To further support our argument, we performed additional experiments demonstrating how our proposed method better controls the spectral norm of convolution layers, resulting in better generalization and test performance.", "The results are presented in Appendix A.1.", "Furthermore, we run several experiments to show that our method is not significantly slower than Miyato et al.\u2019s method, and we report the results in Appendix A.1, Table 3.", "2. \u201cFig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing\u201d", "We relabel the axes and add a more thorough explanation in the caption.", "We note that the text explaining Figure 3 mentions how the margin normalization is performed (paragraph 3 in section 5.1): the margin normalization factor is exactly the capacity norm \\Phi described in Theorems 1-4.", "We clarify that we divide the obtained margins by the values of \\Phi estimated on the dataset.", "3. \u201cThe epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\u201d", "Yes, the epsilons are chosen to be different depending on whether we are looking at norm_inf attacks or norm_2 attacks.", "This is because the two norms can behave very differently in adversarial attack experiments.", "For example, a norm_inf attack of 0.5 implies that all pixels can be changed by 0.5.", "On the other hand, a norm_2 attack of 0.5 means the overall Euclidean norm of perturbation across all pixels is bounded by 0.5, resulting in a much less powerful attack.", "Based on this comment, we update the plots with the same attack-norm to have the same scale.", "4. \"Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.", "However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.\"", "We redo the visualization in Figure 6 to make the gains provided by SN clearer.", "We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.", "5. \"The baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).", "It is thus unclear whether the advantage can be maintained after applying these standard regularisers.\"", "We did not originally discuss weight decay, dropout, and batch normalization as none of these methods were motivated by the theory we introduced in section 3.", "However, due to the reviewers\u2019 concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay, dropout, or batch norm in Appendix A.2.", "In our experiments, the SN-regularized network still performs better in terms of test accuracy."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 7, "sentences": ["Thank you to Reviewer 3 for your thoughtful critique and we are happy that you share our enthusiasm for the motivation behind our approach.", "We share your curiosity on the qualitative behavior of such systems, and as documented in this response we have augmented the paper to address that and other of your suggestions.", "Re: \"- no qualitative analysis on how modulation is actually use by the systems.", "E.g., when is modulation strong and when is it not used \"", "Following the reviewer\u2019s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).", "This figure shows that while neuromodulation clearly reacts to reward, this reaction is complex and varies both within each episode and between runs.", "Re: \"- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\".", "Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.", "Furthermore PTB is not a \"challenging\" LM benchmark.\"", "We agree that, while the differences are statistically significant, they are minor.", "We were using that word technically, but do not want to give the wrong impression.", "We have thus modified the text to make it clear that we mean \u201cstatistically significant\u201d only.", "We also removed the adjective \u201cchallenging\u201d as regards PTB.", "We agree that, ideally, a comparison with SOTA architectures would be desirable.", "As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.", "We will keep trying to investigate such massive architectures in the future.", "Importantly, our purpose in this task is to show that, **all other things being equal**, a neuromodulated plastic LSTM can outperform a standard LSTM in realistic settings.", "We believe that outperforming standard LSTMs (again, all else being equal) on their \u201cworkhorse\u201d task domain (language processing) is worthy of notice, especially given the ease of implementation of our method which requires only adding a few lines of codes (<10) to a standard LSTM implementation and can then be used as a drop-in replacement to standard LSTM."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 8, "sentences": ["First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers,  and apologize for the late response due to additional experiments and modifications of the paper.", "Remark 1. Expression and detail", "A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.", "Remark 2.", "What is \"Selection Network\"?", "A : It is a module that estimates the confidence of the softmax output according to the inputs of the classification network.", "The selection network is trained with sigmoid and binary cross-entropy in a supervised manner.", "And the threshold is not 0.5 but high because selection network is learned with many \u20191\u2019 labels with close to 100 % training accuracy.", "The selection network has advantages in out of class unlabeled data.", "Since softmax output is a relative value, the softmax output can be high for some out of class unlabeled data.", "In our original paper (in table 10), there already exist results of softmax output for in or out of class unlabeled data with 0.9999 thresholds.", "Further, we experimented with the same threshold in table 4 of the new version and the results have shown that out of class unlabeled data are added even with an extremely small threshold such as 0.99999 (epsilon = 10^-5).", "Remark 3. \"As the base classifier is different for various baselines, it is hard to compare the methods.\"", "A : SST has a network structure similar to other papers.", "The difference of structure was that the selection network is added and Gaussian noise and the mean only batch norm are not used.", "As mentioned in the paper (4. Experiments), our supervised learning performs slightly better than conventional SSL algorithms because of different settings such as learning rate and Gaussian noise on the input layer.", "(When SST uses Gaussian noise, ours are also degraded.)", "Remark 4.", "Experiments Detail ( data setting, threshold, number of iterations, animal vs nonanimal)", "A :", "==> Data setting", "The purpose of experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.", "Therefore, we experimented with the popular setting.", "We have added a detailed description on the data setting to Section 6.3 of the supplementary material.", "==> Iterations & Threshold", "We have missed out on a detailed description of how to set up some hyper-parameters.", "We set parameters as follows.", "The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.", "In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.", "While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.", "Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added.", "The growth rate of epsilon is determined according to when the validation accuracy saturates.", "The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.", "If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.", "If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.", "We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.", "(In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.", "However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.", "In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.", "In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.", "Other details are the same as those of the first experiment.", "(In previous versions, the training iterations of fixed mode had been fixed.", "Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)", "=", "=> Animal vs non-animal", "The citation of that part is obscure and has been modified.", "We experimented similar to the [1] and they categorized according to the animal.", "Our approach is similar but not identical.", "Their unlabeled data came from only in 4 classes, however, we selected unlabeled data in all classes.", "[1] Odena, Augustus, et al. \"Realistic Evaluation of Semi-Supervised Learning Algorithms.\" (2018)", "Some Questions and comments", "Remark 5. \"The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?\"", "A : To the best of our knowledge, the main purpose of transfer learning is to improve the performance on the target domain by effectively utilizing the knowledge of the source domain.", "However, in our case, there is no separated source and target domains.", "We focus on the single classification task.", "We think that the goal of our method and that of transfer learning are quite different.", "Remark 6. \"What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?\"", "A \" We have modified that expression and we wanted to address that \"if one class dominates the dataset, the performances are degraded by the imbalanced distribution.", "(Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches, 2013)\"", "Remark 7. \"Figure 3: wouldn\u2019t the plot of accuracy vs amount of data be more suitable here?\"", "A : I agree that your suggestion is more suitable for the figure.", "However, it is difficult to show the figure you want because the number of selected samples is different every time.", "Remark 8. \"Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?\"", "A : The performance depends on the initial points, therefore sometimes the performance is not good. Since the inputs are the x and y coordinate values, it can be very easy to add to the training set.", "(ex.. class 1 : (-1, 0), (1, 0) , class 2 : (0.5, -0.5), (1.5, -0.5) , then decision boundary could be (:, -0.25) then class 2 unlabeled data (0, 0.5) is classified as class 1 and can have a very high selection score.)", "Remark 9. Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?", "A : In fixed mode, we ensemble the selection scores, which makes the prediction more consistent.", "Also, for a more reliable selection score, we do not add unlabeled data to the new training set and train with labeled data only for 5 iterations.", "Remark 10. \"How was it possible to improve the performance in the experiment of section 4.2 with 100% of irrelevant classes?\"", "A : We suspect that this performance improvement is due to re-initializing learning rate.", "After constructing a new training dataset, we retrain our model with the learning rate of the initial value.", "In decay mode (Figure 2, Figure 3 (a) and (b) of the original manuscript), the accuracy is slightly increased and gets saturated while unlabeled data is not being added.", "However, the accuracy begins to increase or decrease relatively more after adding selected data to the new training dataset.", "In fixed mode (Figure 3 (c) and (d) of the original manuscript), the improvement with the 100% of irrelevant classes seems to be due to re-initializing learning rate.", "However, SST algorithm with other ratios of out-of-class samples results in performance improvement compared to the 100% because out-of-class samples are not selected."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 9, "sentences": ["Thanks for the insightful comments.", "We\u2019ve tried to improve our paper based on your feedback.", "Most significantly, we\u2019ve performed additional ablation studies to confirm that our modeling choices improve performance, and we provide further empirical insight on what the coreference operations do.", "We\u2019ve also updated the model description and the notation in Section 4 to clarify modeling mechanisms and choices.", "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "Below we address your concerns point-by-point.", "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "This is especially the case in a few places involving coreference:", "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.", "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.", "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.", "======", "Based on your comments, we\u2019ve performed additional ablations to measure the impact of the co-reference mechanisms.", "We find that removing any of them leads to a decrease in performance (Rows 2, 3, 4 of Table 5).", "To provide more than just this quantitative insight, we\u2019ll expand here on how KG-MRC handles coreference to better motivate the modeling choices:", "The construction of graph G_t from G_{t-1} uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies.", "We perform coreference disambiguation between location nodes of G_t and G_{t-1} via Eq. 1 (call this inter-graph coreference) and between the location nodes in the same graph Gt (call this intra-graph coreference) via Eq. 2.", "The inter-graph coreference yields new, intermediate representations for the nodes in G_t.", "These are further updated via the intra-graph coreference step.", "Inter-graph Co-ref: One way to think about this is that we construct a new graph G_t at every time step.", "Now the graph G_{t-1} might contain some location nodes which are predicted again at time step \u2018t\u2019 (e.g., in Figure 2, leaf node already existed in G_{t-1}).", "Instead of replacing an old node with an entirely new node at \u2018t\u2019, we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step \u2018t\u2019.", "Intra-graph Co-ref: Inter-graph co-ref isn\u2019t enough since the MRC module makes its span predictions independently.", "This means that, at time step t, the model could predict the same span/location for multiple entities and add all these duplicates to the graph.", "Moreover, a single location might have the same surface form but be from different parts of the paragraph (e.g. \u201cleaf\u201d in the 1st and the 5th sentence of the para in figure 2).", "The operations in Eq. 2 resolve this by performing self-attention (i.e., the predicted locations of all entities are compared to each other).", "=====", "Response continued from above.", "Why does the graph update require coreference pooling again?", "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?", "=====", "We agree that the coreference pooling in the graph update seems repetitive at first glance.", "We have further clarified the explanation given in the text and included another ablation experiment  (row 4 of Table 5) to confirm its usefulness.", "This step does indeed repeat Eq. 2.", "In a nutshell, this is necessary because, after the recurrent and residual graph updates (Eqs 3.1 - 3.3) that propagate information across edges, we may end up with different representations for location nodes corresponding to the same location.", "We don\u2019t want these representations to diverge from each other because of information propagation.", "To give you more detail:", "The graph update step ensures information propagation between entities and location representations.", "Specifically if the current location of entity \u201ce_t\u201d is predicted as \u201c\\lambda_t\u201d, the graph update steps ensures that both the entity and location representation gets the same update (via eq 3.2 and 3.3).", "This would have been sufficient if every entity had a unique location.", "But, multiple entities can actually exist in the same location.", "Let\u2019s consider this small graph below", "Water - -> leaf", "CO_2 --> leaf", "Here both water and CO_2 exist in the same location, leaf.", "But let\u2019s say that the MRC model picked the \u201cleaf\u201d span from sentence 1 (of the text in Fig 2) for \u201cWater\u201d and from sentence 4 for CO_2.", "In reality, they refer to the same location entity \u201cleaf\u201d.", "Now, due to eq. 3.3, the two embeddings of leaf will get two different residual updates (one would be corresponding to Water and other would be because of CO_2).", "Because of the different updates, the two representations of the same entity might diverge.", "To remedy this, we re-use the coreference matrix \u201cU\u201d we create in eq. (2), which should already have a high attention score corresponding to the two leaf locations.", "Thus we perform a similar operation to the intra-graph update.", "====", "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?", "====", "The \u201cprefixes\u201d that our model reads at each time step comprise all sentences up to and including the current sentence s_t.", "The motivation for this modeling choice was empirical.", "In our preliminary experiments we evaluated alternative strategies, such as (a) only considering the current sentence s_t, and (b) considering the entire paragraph at every time step.", "We found that operating on prefixes performed best.", "This is in line with the findings of Dalvi et al., 2018, where the Pro-Global model (which uses prefixes) performs better than the Pro-Local model (which operates on single sentences)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 10, "sentences": ["We agree with most of the comments."], "labels": ["rebuttal_concede-criticism"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_concede-criticism_label"]}
{"abstract_id": 11, "sentences": ["Thank you Reviewer 2 for your positive appraisal of our results and presentation.", "As documented below, we do our best to address your questions, which have helped us improve the paper.", "Re: \"The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.\"", "We have added \u201csimilar to\u201d in order to emphasize that we adapted and re-ran their architecture (we still use some of their code, which we believe might warrant citation; we are happy to drop it altogether if it is found confusing).", "Re: \"One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017.\"", "We agree that, ideally, a comparison with SOTA architectures would be desirable.", "As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.", "We will keep trying to investigate such massive architectures in the future.", "Re: \"Why were zero-sequences necessary in Experiment 1? [...] Perhaps the authors could clarify on what a confounding \"time-locked scheduling strategy\" would look like in this task?\"", "The random zero-inputs make the timing of the cues unpredictable, forcing the network to be driven specifically by the stimuli - as opposed to learning a pre-programmed strategy at each given time step.", "This is merely a convenient choice to make the task more challenging.", "Re: \"Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?\"", "Again, this simply makes the task more challenging.", "Non-target cues operate as distractors and having pairs of stimuli shown before each response increases the uncertainty in reward credit assignment (i.e. when receiving a reward, the network must still find out which of the two stimuli is the target).", "To better describe the task, we have added a schema of an episode to Figure 1.", "We hope this may facilitate understanding.", "Re: \"Why is non-plastic rnn left out of Figure 2b?\"", "As documented in Miconi et al 2018, non-plastic networks are terrible at this task.", "We are happy to run this experiment and include it if the reviewer finds it useful.", "Typos: \"However, in Nature,\" -- no caps", "in appendix: \"(see Figure A.4)\" -- the figure is labeled \"Figure 3\"\"", "We thank the reviewer for noticing these typos and have fixed them in the text."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 12, "sentences": ["Thanks a lot for your review and for pointing us to the reference, we will add and discuss this work in our paper.", "The referenced paper mimics the Choquet integral to fuse different neural networks such as CaffeNet, GoogLeNet, and ResNet50 that have been pre-trained for classification problems and can be viewed as ensemble method for multiple noisy classifiers.", "Contrary, we are interested in regression problems that have inherent non-additive effect such as automatic summarization.", "Furthermore, the referenced paper is much closer to the Choquet as we intent to be.", "As we describe in the paper, the proposed architectures are only inspired by the Choquet integral.", "This idea can be found in both of our architectures.", "In Figure 1c, u_i and in Figure 1d g_i * u_i model these meaningful intermediate values.", "We do not claim that we obtain any theoretical guarantees or properties of the Choquet integral.", "\"How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral?\"", "As described above, the proposed approaches are inspired by the way Choquet integrals handle non-additive utility aggregations.", "We do not claim that we obtain any theoretical guarantees or properties of the Choquet integral.", "Furthermore, the main idea of this work is to not learn a representation.", "Instead, we propose to predict many meaningful intermediate values that can simply be summed to obtain a set utility.", "\"What is your loss or your algorithm?\"", "We describe in Section 3.3 that we use mean squared error (MSE) and mean absolute error (MAE) in our experiments.", "We use MSE because it is usually used in regression problems.", "We were also interested in the mean absolute error because minimizing this loss might be more appropriate in a task such as automatic summarization, in which we don't want to punish a model strong if it makes a few severe mistakes compared to making many small mistakes.", "We also describe in Section 3.3. that we use Adam as optimizer.", "\"According to the illustration, it seems that you first obtain \u201cfeatures/representations\u201d. Then the representations are fed to the four architectures you listed in figure one.\"", "This is correct.", "\"RNN-based approaches are with better \u201ccomplexity\u201d comparing to your sum baseline and \u201cDeepset\u201d approach.\"", "We also compare against an RNN-based approach (abbreviated with \"RNN\" in the paper).", "The RCN approach is the smallest modification one can make to implement our idea into a standard RNN.", "Hence, we think that the comparison is fair and meaningful.", "Furthermore, we demonstrate in the extrapolation experiments that standard RNNs tend to overfit.", "The simple sum baselines and deepsets perform better in this experiments.", "Hence, a \"better\" complexity turns out to be prone to overfitting, which shows that larger models are not necessarily better."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 13, "sentences": ["We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper.", "1. Title of the paper", "- We agree that the main highest-level task that we show is VQA, even though our method is more general.", "Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA.", "Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks.", "2. Description of variables", "- Thanks for the feedback.", "Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables.", "We edited the text to address variables more gently and to explain the arrow sign.", "3. Query for the relationship module", "- The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training.", "When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores.", "This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients.", "4. CIDEr score of captioning", "- That may be true to some extent.", "However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size.", "5 and 6.", "Comparison with SOTA models for counting and relationship detection", "- To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering.", "Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018).", "Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling.", "This shows that additional modules help.", "Kim et al. (2018) which is concurrent to our work shows similar performance.", "For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult.", "7. Table 4, accuracies are from Zhang et al. 2018", "- Yes, the numbers are from their paper.", "One possible explanation for this could be their use of high regularization for a single model instead of ensembling.", "Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller.", "(Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering", "(Kim et al. 2018) Bilinear Attention Networks", "(Lu et al. 2016) Visual Relationship Detection with Language Priors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 14, "sentences": ["Thank you for your insightful comments. We have incorporated your suggestions into the revised version of the paper.", "Q: Relationship to optimizers with adaptive learning rates, and comparison between Adam and Adam-APO.", "While Adam and Adagrad are often described as having \u201cadaptive learning rates,\u201d they still have a global learning rate that is just as critical to tune as for SGD.", "In our experiments, we consider tuning the learning rate for RMSprop, which also maintains adaptive learning rates for each parameter, and is closely related to Adam/Adagrad.", "Adam is essentially RMSprop with momentum; APO can be applied to Adam by applying momentum on top of the updates computed by APO.", "To address your question about Adam, we added experiments for tuning the global learning rate of Adam with APO in appendix Section G, Figure 14, where Adam-APO achieves better performance than Adam with a fixed global learning rate, and achieves comparable performance as Adam with a manual schedule.", "Q: Comparison with population-based training (PBT)", "We have added a comparison between APO and PBT in appendix Section H, Figure 15.", "For population-based training, one must carefully select many hyperparameters, including the size of the population, the perturbation strategy (e.g., randomly perturb the learning rate by multiplying it by 1.2 or 0.8), the exploration interval (e.g., the number of training iterations to run before exploiting other members of the population).", "We used PBT and APO to tune the learning rate of RMSprop while training a ResNet34 model on CIFAR-10.", "For PBT, we used a population of size 4, and chose to exploit/explore after each epoch of training.", "We tried multiple exploration strategies, and found that it was critical to set the probability of resampling a learning rate from an underlying distribution to be 0; otherwise, the learning rates could jump from small to large values, and yield unstable training.", "In contrast, APO only requires a simple grid search over lambda, and all other hyperparameters can be kept at their default settings.", "We found that  APO substantially outperformed PBT, achieving a lower final training loss and equal test accuracy in much less wall-clock time; this shows the advantage of gradient-based methods for tuning learning rates, such as APO, compared to evolutionary methods based on random perturbations such as PBT.", "Q: The convergence results appear to rely on strong convexity of the loss.", "How is this a reasonable assumption?", "Note that we assume strong convexity of the loss as a function of the output units, not as a function of the weights.", "Hence, our assumption is fairly realistic in the neural net setting.", "The loss function on top of the network output is usually defined as a simple convex function; for instance, in regression, a common choice of loss function is the quadratic loss (i.e, the squared distance between the network output and the true label), which is strongly convex.", "In fact, even without assuming that the loss function is strongly convex and that the output manifold is dense, we are still able to show a fast convergence rate.", "In the updated version of the paper, we show that our algorithm with an oracle converges to stationary point globally with a fast rate, which provides insight into why APO works well.", "Q: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?", "APO is robust to the initial learning rate of the base optimizer, using the default meta learning rate suggested in our updated paper.", "We have added a section to the appendix in which we include RMSprop-APO experiments on Rosenbrock, MNIST, and CIFAR-10 to show that the training loss, test accuracy, and learning rate trajectories are nearly identical when starting with initial learning rates {1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7}, spanning 5 orders of magnitude.", "Note that 1e-2 is quite a large initial learning rate for RMSprop."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 15, "sentences": ["We thank the reviewer for the valuable feedback!", "The suggestion comments were very helpful and led to a clear improvement of our manuscript.", "We reply to the answers and comments in the order they were raised:", "(1) While indeed we need more samples of weight matrices than e.g. for applying VI for BNNs for due to the input dependency, we do not believe this makes our method unscalable to real world scenarios.", "Note, that input dependent samples are also needed in the variational training of VAEs (where the number of hidden variables is of course much smaller than the number of weight parameters in our setting).", "While we present the training algorithm naively in an online version for clearness in Algorithm 1, in practice mini-batching can be done efficiently, due to the availability of batched linear algebra operations, at least in the framework we use (PyTorch), e.g. torch.bmm, broadcasting semantics, etc.", "For convolution layers, we can simply use a different type of mixing distribution, e.g. a fully-factorized multivariate normal instead of matrix-variate normal.", "(2) Thank you very much for the pointer to VIB!", "We have added a section in the updated manuscript to compare the objective of CDNs with that used in VIB and VI for Bayesian neural networks (see new Section 4).", "Furthermore, while we  always used 1 sample during training in the original submission (which indeed makes the CDN an instance of VIB) we now added experiments using 10 samples (see Section 6.4) in an experimental analysis of the different objectives.", "The results show that the CDN objective produces superior results compared to VI and VIB.", "(3) Of course! We have moved the test accuracy (which previously was only given in the Appendix and thus hard to find) to the legends of the plots to make it more easily accessible.", "CDNs give better uncertainty estimates while still having similar predictive power compared to the baselines.", "(4) Thank you for the great suggestion.", "We performed the following 2 experiments for the revised version: First, we picked a weight of a CDN trained on a toy regression experiment (with heteroscedastic  noise) at random and visualized its conditional distributions given different values of x. We found that the means and variances vary for different x.  Furthermore, we picked a weight of a CDN trained on a toy classification dataset (created by sampling x ~ 1/2*N(-3, 1) + 1/2*N(3, 1), and assign y=0 if x comes from the first Gaussian and y=1, otherwise) at random and visualized its marginal distributions.", "We found that CDNs indeed capable of learning multimodal weight distribution and to learn input specific mixing distributions.. We detail this in Appendix G.", "(5) We found that the regularization term has a significant impact on the quality of the prediction and the uncertainty estimate (we found that the uncertainty estimates are worse with small \\lambda).", "It makes sure that the variance of \\theta is not shrinking too much, i.e. encouraging the mixing distribution to be close to the prior implies it should have similar variance to the prior (which was chosen to be large).", "Naturally, the coefficient \\lambda controls this behavior: as \\lambda increases the validation accuracy is decreasing while the uncertainty is increasing (and vice versa).", "This gives rise to the selection heuristic for \\lambda we applied: pick the highest \\lambda that still gives high accuracy on the validation set (e.g. > 0.97 in MNIST).", "We found that this works very well in the experiments we did (on OOD and adversarial examples).", "Furthermore, indeed CDNs are rather designed to capture the (heteroscedastic) aleatoric uncertainty.", "We have revised the toy experiments to better account for that.", "However, curiously, CDNs also work well in tasks that are usually shown as prime examples of epistemic uncertainty, e.g. OOD classification and adversarial attack.", "(6) Thank you for this feedback.", "You are right! We have revised the baseline experiments with Bayesian models so that they either use \\lambda = 1 or the settings that the original authors recommended, i.e. we only tune \\tau in KFLA and set \\tau = 0.01 in noisy-KFAC as these are the settings suggested in their respective publications.", "Note, that the conclusions keep unchanged.", "References:", "[1] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016.", "[2] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Variational dropout and the local reparameterization trick.\" Advances in Neural Information Processing Systems. 2015"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 16, "sentences": ["Thanks for your comments.", "We are sorry to say that we miss some previous works, especially the ECCV one. And we will provide more literature review in our updated paper.", "We admit that the computation process of F-pooling and the ECCV method is the same.", "However, we defend the novelty of our work.", "The values of our work are not how the output of F-pooling is computed.", "Instead, the values are the strict definition of shift-equivalence and the theoretical properties of F-pooling.", "In previous works, they even don\u2019t give an operable definition shift-equivalence when down sampling involved.", "Please refer to our general response for more of F-pooling\u2019s values.", "Moreover, we discuss some practical problems of F-pooling.", "Such as how to deal with the imaginary part and the zero-padding of convolutions.", "With suitable settings, the shift consistency of F-pooling is much better."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 17, "sentences": ["Thank you for your thoughtful review. We will address your concerns in turn.", "Q1: The degradation function F is challenging to obtain in real world scenarios.", "A1: Many state-of-the-art approach, including SRCNN and SRGAN, has their own implicitly defined degradation function.", "They use their function F to generate training samples during their training process, while we use our explicitly defined function F during the inference process.", "If the assumed degradation function F is not exactly the function in real scenarios, both these state-of-the-art approach and our method will suffer.", "So it is unfair to criticize our motivation just because we explicitly write out the degradation function F.", "Q2: TV can also be applied for different restoration tasks, and it is easier to be optimized.", "A2: We think you underestimate the difficulty of those restoration problems.", "Please check the degraded images in Table 3.", "These images are damaged so badly that TV cannot recover any meaningful thing.", "As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 18, "sentences": ["We thank the reviewer for the detailed review.", "Below we address the main concerns.", "--------------------------------------------------------------------------------------------------------------------------------", "Q:\u201dso", "framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail\u201d", "A: We agree with the reviewer and will change our wording accordingly.", "--------------------------------------------------------------------------------------------------------------------------------", "Q:\u201dI would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from\u201d.", "\u201cthe two results coincide for the exchangeable case\u201d", "A: We agree with the reviewer that such a discussion will be helpful to the reader. We will add such a discussion (in addition to the short discussion at the end of Appendix 1).", "--------------------------------------------------------------------------------------------------------------------------------", "Q: Comparison to popular graph convolution methods (GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.).", "A: As discussed in our response to Reviewer 2, We will add a theoretical result that shows that our model is at least as powerful in terms of universality as [Kipf & Welling ICLR 2017]."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_none", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label"]}
{"abstract_id": 19, "sentences": ["Thank you for your comments!"], "labels": ["rebuttal_social"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 20, "sentences": ["Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments.", "Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.", "> i) \u201cThe proposed architecture is mainly adopted from the graph attention networks (Veli\u010dkovi\u0107  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).", "Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.\u201d", "We concede that the modifications to the existing models is a minor contribution.", "We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.", "We plan to make our implementation public to aid research in the area.", "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs.", "We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the paper\u2019s contribution.", "> ii) \u201cIn table 2, I don\u2019t really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse.\u201d", "We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way.", "This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT.", "These results will be included in the new manuscript.", "We also feel that some of the results being \u201csignificantly worse\u201d is one of the main contributions of our paper.", "Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart.", "This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].", "The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.", "On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.", "> iii) \u201cCould you explain why your MUTAG is now a single graph and is cast as node classification problem?\u201d", "The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description.", "There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2].", "In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form", "d1 -> hasAtom -> d1_1", "d1 -> hasBond -> bond1", "d1- > hasStructure -> ring_size_6-1", "where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on.", "There are many more types than this, and are viewable in the .owl located at [2].", "Nodes correspond to entities from the point of view of RDF.", "Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation).", "This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled).", "Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules.", "If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective.", "[1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis", "[2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 21, "sentences": ["1- The idea of using concepts to represent a problem is simple, but using it along with neural network based embedding gives us the opportunity to gain concept continuity as discussed on the last paragraph on page 7 and table 2, which is an active field of research in education.", "The focus of this work is on problem embedding and its application in a recommendation system that uses problem embedding to project students\u2019 performance for the problems they solved onto the problems that they have not solved yet.", "Using the evaluation on unseen problems, a problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time we cover all the concepts necessary for them to learn.", "In the meanwhile, we got the interesting idea of negative pre-training on training with imbalanced training data and tested our hypothesis and included in the paper.", "Due to space limit, we did not include the literature review and comparison of other methods in terms of memory use and training complexity, but you can find them in the response of a previous comment below titled \u201cResponse to Question on Negative Pre-Training\u201d on this page to see the comparison.", "We can include the literature review for training on imbalanced data sets as well as comparison of other methods with negative pre-training in terms of memory use and training complexity in the final version.", "In summary, a) oversampling extremely suffers from over-fitting, b) SMOTE method that generates synthetic data sample is not feasible in word space, so the generated synthetic data (that are mathematical problems) are not of use for our training purpose, c) borderline-SMOTE both suffers from the same issue as SMOTE and its high complexity for finding the pairwise distance between all data samples, which is a burden in high dimensional data, and d) hybrid methods need m >> 1 weak learners in contrast to negative pre-training that uses a single learner.", "Memory use and training time is an issue for hybrid method when the weak learners are deep neural networks with too many parameters.", "We are currently running a broader experiment for negative pre-training on other data sets to gain more insight on it, but for the purpose of the task proposed in this work, it outperforms one-shot learning, which cannot be said that is the state-of-the art, but is a common practice. There is no notion of state-of-the-art in training on imbalanced data sets since due to our best knowledge, there is no method that outperforms all the other ones, and the performance of different methods depends more on the nature of the data set.", "2- The data set being small", "is the nature of the application since creating mathematical problems is a creative process, so it is hard to have a very big data set.", "The Prob2Vec method is performing well on this not relatively big data set, which is our goal, but if we have a bigger data set (as we have right now with more than 2400 problems), Prob2Vec may even have a better performance since with more data we can have a more precise concept and problem embedding.", "3- Thanks for your suggestion.", "4- It is difficult for humans to determine a similarity score consistent across a large enough training set, so it is not feasible to simply apply supervised methods to learn a similarity score for problems.", "Even if problem-problem similarity annotation is feasible, a lot of effort should go into the annotation, which is not scalable."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 22, "sentences": ["We thank the reviewer for the comments on our paper.", "- We have included the result concerning a noisy oracle for the F_p moment estimation problem in the paper.", "- We like the question of minimizing the number of oracle calls.", "This is an interesting open problem and we intend to explore it in future work."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 23, "sentences": ["Thanks so much for your valuable review comments!", "Following your suggestion, we evaluated the Byzantine settings Multi-Krum (Blanchard et al 2017) and Bulyan (El Mhamdi et al 2018 ICML).", "For both DBA and centralized attack we use the aggregation rule that can tolerate f Byzantine workers among n workers", "(Blanchard et al 2017)", ".", "For centralized attack there is 1 attacker and n-1 non-Byzantine workers.", "For DBA there are f distributed attackers and n-f non-Byzantine workers.", "The total number of poisoned pixel amounts are kept the same.", "1. Multi-Krum", "- To meet the assumption that 2f + 2 < n, we set  (n=10, f=3) for loan and (n=12, f=4) for image datasets.", "The Multi-Krum parameter m is set to m=n-f.", "For Tiny-imagenet we decrease the poison ratio to 5/64 for both attacks.", "Other parameters are the same as described in the paper.", "- For CIFAR and Tiny-imagenet, we find that DBA is more effective.", "- For LOAN and MNIST, both attacks don\u2019t behave well.", "We believe the reason can be explained by the fact that Loan and MNIST are simpler tasks and benign clients quickly agree on the correct gradient direction, so malicious updates are more difficult to succeed.", "2. Bulyan", "- We use Bulyan", "based on the Byzantine\u2013resilient aggregation rule Krum", ".", "To meet the assumption that 4f + 3 <= n, we set  (n=15, f=3) for loan and (n=20, f=4) for image datasets.", "- For CIFAR, DBA is more effective.", "- For other datasets, both attacks fail.", "However, we note that our distributed and centralized backdoor attacks are not optimized for Byzantine setting.", "We believe it\u2019s worthwhile to explore the distributed version of other new attack algorithms, e.g. A Little Is Enough (Baruch et al 2019) that manipulates its update to mitigate Krum and Bulyan defenses.", "In summary, Multi-Krum and Bulyan have stricter assumptions on the proportion of attackers than RFA and FoolsGold.", "In addition, while RFA and FoolsGold still assign potential outliers with extreme low weights, Krum (Multi-Krum, Krum-based Bulyan) directly removes them, making it impossible to inject backdoors if the malicious updates are obviously far from the benign updates.", "The centralized attack for four datasets totally fails under Multi-Krum and Bulyan while DBA can still succeed in some cases.", "We have included these results in Appendix A.6 of the revised version."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 24, "sentences": ["We very much appreciate your valuable comments, efforts and times on our paper.", "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "Q1. Comparison with [1, 2, 3, 4].", "The main difference between our method and [1, 2] is that we do not directly train the Gaussian mixture model, i.e., generative classifier but we post-process it on hidden feature spaces of pre-trained deep models.", "In addition, we study a robust inference method to handle noisy labels in training samples, while they did not.", "Next, [3,4] also assume clean training labels, and aim for detecting abnormal test samples after \u2019clean\u2019 training.", "Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.", "We clarified this in Section 2.1 of the revised draft.", "Q2. Computational cost.", "As you expect, estimating the parameters of LDA is very cheap compared to training original deep models like ResNet and DenseNet, since it requires only one forward pass to extract the hidden features.", "Q3. Version of backward/forward losses.", "As mentioned in Appendix B of the previous draft, we use the estimated noise transition matrices for backward/forward losses.", "We clarified more details of experimental setups in Appendix B of the revised draft.", "Q4. Updated abstract and performance evaluation.", "As AnonReviewer 3 mentioned, our main contribution is developing a new inference method which can be used under any pre-trained deep model.", "In other words, our goal is not outperforming the performance of prior training methods and complementary to them, i.e., our inference method can improve the performance of any prior training methods (see our common response to all reviewers).", "Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.", "In the abstract of the revised draft, we report our improvement over Co-teaching [5] which is the most recent and state-of-the-art training method.", "Q5. Evaluation on adversarial attacks.", "In the revised draft, we also consider optimization-based adaptive attacks against our method under the black-box setup (see Table 5) and the white-box setup (see Table 10).", "In both setups, our inference method is shown to be more robust compared to the softmax inference.", "We further show that our method further improves the robustness of deep models optimized by adversarial training (see Table 6 and 11).", "Such experimental results support our claim that the proposed generative classifier can improve the robustness against adversarial attacks as it utilizes multiple hidden features (i.e., harder to attack all of them).", "We very much appreciate your valuable comments again.", "[1] Wen, Y., Zhang, K., Li, Z. and Qiao, Y., A discriminative feature learning approach for deep face recognition. In ECCV, 2016.", "[2] Wan, W., Zhong, Y., Li, T. and Chen, J., Rethinking feature distribution for loss functions in image classification. In CVPR, 2018.", "[3] Lee, K., Lee, K., Lee, H. and Shin, J., A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. In NIPS, 2018.", "[4] Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Houle, M.E., Schoenebeck, G., Song, D. and Bailey, J. Characterizing adversarial subspaces using local intrinsic dimensionality. In ICLR, 2018.", "[5] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: robust training deep neural networks with extremely noisy labels. In NIPS, 2018.", "Thanks a lot,", "Authors", "Dear AnonReviewer2,", "We hope that you found our rebuttal/revision for you and other reviewers in common.", "If you have any remaining questions/concerns, please do not hesitate to let us know and we would be happy to answer.", "Thank you very much,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_summary", "rebuttal_answer", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 25, "sentences": ["We thank the reviewer for their review.", "\u201cThe paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. [...] It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.\u201d", "We agree with the reviewer that our analysis of capacity in section 3 does not take into account the magnitude of the weights, nor the dependence on the depth of the network.", "Our objective in this section was to provide a empirical lower-bound on the capacity by designing a setup where we can vary the quantity of information contained in a dataset (in our case, N choose n), and evaluate empirically the effect of data augmentation.", "In relation to section 5, we aim at seeing how much a network can remember if it is explicitly trained to remember a given set of images.", "We understand the limitations of MacKay's analysis, which was presented to give a rough theoretical comparison point to our empirical evaluation.", "We will clarify this in the paper and improve the discussion along the lines discussed by the reviewer.", "\u201cThere is a slight oxymoron in the premise of the first set of experiments.", "The network is forced to memorize a set of positive examples relative to the negative set it sees during training. What is memorized I presume depends a lot on the negative set used for training (its diversity, closeness to the positive set and how frequently each negative example is seen during training).", "[...] Is there a training task which would allow one to more explicitly memorize the image (some sort of reconstruction task) as opposed to an in/out classification task?\u201d", "In these experiments, the set of positive and negatives is fixed (when varying data augmentation and architectures).", "During training, we feed to the network all positives and an equal number of negatives during each epoch.", "The performance does indeed depend on the closeness of the positive and the negatives, but this is similar to the membership inference problem presented in section 5, where it is difficult for a network to tell apart a seen image from an unseen, very similar image.", "A reconstruction task would suffer the same problems: the reconstruction is only approximate so we would need to evaluate the distance between our reconstruction, positives and negatives, which also depends on the closeness between positives and negatives.", "Also, the reconstruction task would need to remember the values of all pixels which requires more capacity.", "We agree that this specific deserves a short discussion and will add it to the paper.", "\u201cThis paper is a slightly difficult read [...] because there is not one main coherent argument or goal for the paper.[...]. Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.\u201d", "The general goal of the paper is to empirically assess memorization in neural networks, and in particular the important question of implicit memorization, which is important for privacy: does a network trained for classification remember an image, or a set of images ?", "This aspect is empirically evaluated in sections 4 and 5, and section 3 is a preliminary study of the memorization capabilities for systems explicitly trained to memorize (this serves as a qualitative upper-bound for implicit memorization).", "We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.", "We decided to move it to an appendix after reading the feedback from the three reviewers.", "\u201cThe conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author?\u201d", "We do not consider the conclusions of the experiments from Section 3  to be more important than those of the other sections, in fact quite the opposite.", "As mentioned above, we will move it to appendices.", "\u201c[...]In section 3 is a perturbed positive image considered a positive training image? And in the testing phase are only unperturbed versions of the positive images given to the ConvNet as input?\u201d", "When data augmentation is used, we consider that perturbed positive images are also positive images.", "In the testing phase, perturbed versions of the positive images are given to the ConvNet.", "\u201cLast paragraph page 4: \"when the accuracy gets over 60\\% and at 90\\%\". Is this training or validation accuracy?\u201d", "We decrease the learning rate when the training accuracy reaches these thresholds.", "We thank the reviewer for reporting typos, we will correct them in the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 26, "sentences": ["Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!", "We answer your questions and concerns in the following.", "> \"However, I do not understand how are the *discrete* output y is handled.\"", "For this toy problem, we represent labels y by standard one-hot encoding, and we directly regress one-hot vectors using squared loss instead of softmax.", "This allows us to input one-hot vectors into the inverted network to generate conditional x-samples.", "> \"I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments.\"", "We included this experiment to demonstrate that we are able to find multi-modal posteriors in a second real-world setting relevant to natural science.", "> \"INN outperforms other methods [...] over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better\"", "We indeed consider the calibration errors (reported in Sec. 4.2 (\u201cQuantitative results\u201d) and Appendix Sec. 6) the most meaningful of these comparisons, because they directly measure the quality of the estimated posterior distributions, and INNs have a clear lead here.", "We will add these numbers to Table 1 to emphasize their importance.", "> \"However, the real-world experiments are not necessarily the easiest to read.\"", "We understand, although we tried our best to condense the complicated nature of these applications.", "For the astrophysics setting, we provide more information in the appendix, Sec. 5, and for the medical application we refer to [1] for full details.", "[1] Wirkert et al.: Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression. International Journal of Computer Assisted Radiology and Surgery, 2016.", "(https://link.springer.com/article/10.1007/s11548-016-1376-5 )", "We have uploaded a revised version of the paper, thank you again for your suggestions.", "The changes and additions are highlighted in red font for convenience.", "Please also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.", "If this presents a problem, we can attempt shorten the paper accordingly."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 27, "sentences": ["We thank reviewer #2 for the useful feedback.", "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added theorems and more formal statements in the main text,  compressed the appendix and enhanced the description of the experimental setup.", "We have updated the paper and kindly ask the reviewer to take another look.", "Robustness of the curvature sampling method: we provide confidence intervals in Table 2 of our results.", "We learn curvatures for each of the component spaces and show learned values together with confidence intervals in Appendix E. It can be seen that these variances are in the low regime."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 28, "sentences": ["Thank you for the comments!", "To review\u2019s feedback:", "- We pay attention to the term \u201cskill discovery\u201d and made it more clear about the connection between prior works and the current work in the revised version.", "Our method can also be combined with DIAYN to learn the skill-conditioned policy as mentioned in the paper.", "- We added both a theoretical connection and new experimental results to compare MISC and the empowerment method in the revised version.", "In the navigation tasks, we show that our method outperforms the empowerment method.", "- An intuition for why I(s_c, s_i) could be superior to I(a, s_i) is that in robotic tasks, the mutual information between the robotic sates, s_c, and the object states, s_i, could be easier to be estimated than the mutual information between the action, a, and the object states, s_i, as shown in Figure 4 in the paper.", "Therefore, the agent receives a higher MI reward more easily and learns to control s_i more efficiently.", "The context states can be seen as the summary information of the agent\u2019s action and the transition model of the environment, which could be more relevant in terms of estimating the object states in comparison to the agent\u2019s actions.", "- VIME and PER are used as described in their original papers.", "- We have added an appendix to provide more information about experiment details.", "- We also newly evaluated our method on gazebo-based robotic simulations, including the cases when there is no object, a single object of interest, and multiple objects of interests.", "A video showing new experimental results is available at https://youtu.be/l5KaYJWWu70?t=104", "In this experiment, we also compare MISC with two additional baselines, including ICM and empowerment (with state of interest), see Figure 4 in the paper.", "- We now mention that the states of interests vs context are given in the revised paper.", "However, when they are not given. They can also be automatically learned/selected by iterating over all possible combinations", ".", "Afterwards, an optimal combination can be chosen by the user via testing in the task at hand."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 29, "sentences": ["> I found the paper interesting to read and well written.", "The main contribution (backpropagating analytic gradients through imagined trajectories?) could potentially be highlighted more but otherwise the paper was clear.", "Thank you.", "Correct, our main contribution is to learn long-horizon behaviors by propagating analytic value gradients through imagined trajectories.", "Moreover, we show that this yields a scalable algorithm that solves control tasks of higher difficulty than was previously possible using model-based agents.", "> I wonder if the authors ever looked at how much the size of the latent vector determines the performance of the system? Is there an optional latent vector size across domains or is that optimal size task dependent?", "For our experiments, we used the same hyper parameters across all tasks, including the state size.", "We conducted an additional experiment where we trained Dreamer with latent states of 100, 200, 300, 400, 500 deterministic units and 10, 20, 30, 50, 100 stochastic units.", "We find that all sizes equal to or larger than the 200 and 30 used in our main experiments yield very similar performance, while smaller sizes result in suboptimal scores on some of the tasks, hinting at insufficient model capacity.", "> Additionally, how much variance is there in the imagined trajectories from a certain starting state? In other words, are the endpoints of most imagined trajectories similar or very different?", "We have not studied this quantitatively.", "Qualitatively, we find more diversity in the stochastic multi-step predictions near states that are more challenging to predict.", "For example, this includes collisions with the ground, the unstable equilibrium of an upright balanced pendulum that could either rotate left or right, or cheetah balancing on its front feet which might flip over on its back or fall back on its feet.", "> There is actually not too much for me to critique and I would suggest this paper should be accepted.", "Thank you."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_accept-praise"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 30, "sentences": ["We thank the reviewer for the positive comments.", "Below we address the main concerns.", "Q: \u201dApplying the model of Hartford et al. to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation... Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?\u201d", "A: Our goal in performing the synthetic experiments was to quantify the expressive power that is  gained by adding our basis elements to [Hartford et al. 18].", "We felt it is an informative experiment since [Hartford et al. 18] also discuss applying their model in the jointly exchangeable setting (page 3, second column, top paragraph).", "Having said that, we agree with the reviewer that [Hartford et al. 18] probably cannot handle such tasks by construction. As we mentioned in our response to Reviewer1 we will change the wording of this section to better reflect that this is *not* a failure of Hartford et al. but merely a setting outside their scope due to a different assumption on the symmetry group of the data.", "If the reviewers feel strongly about this experiment, we are open to replace it with a discussion.", "--------------------------------------------------------------------------------------------------------------------------------", "Q: \u201cSome of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al\u201918 and references in there) are missing\u201d.", "A: We did our best to survey and compare to the most related works on the dataset collection introduced in [Yanardag & Vishwanathan 2015].", "These datasets contain graphs from multiple origins, where some of them consist of highly varying graph sizes (within the same dataset).", "In any case we will make the code available as soon as possible.", "--------------------------------------------------------------------------------------------------------------------------------"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_by-cr", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label"]}
{"abstract_id": 31, "sentences": ["Glad to know that you like our paper!", "1) Difference from parameter noise except for memory consumption:", "As stated in Section 3.3, we believe NADPEx is a generalization of parameter noise, with not only flexible memory consumption but also lower variance in gradients.", "This theory is examined in Section 4.2, where NADPEx shows faster convergence and lower variance in performance with different random seeds.", "Besides, comparing with [1], our work provides a theoretical modeling for the idea \"a hierarchy of stochasticity for exploration\".", "We model the NADPEx policy as a joint distribution of dropout random variables and actions, such that it could be combined seamlessly with existing on-policy policy gradient methods.", "One example is the policy space constraint stated in Section 3.2.", "We also provide another distribution i.e. Bernoulli distribution for stochasticity at high level, for which we derive gradient alignment and policy space constraint, as well as empirical results.", "As a minor point, in [1], the stochasticity at the high level i.e. the variance of parameter noise, is adjusted in a heuristic manner.", "NADPEx, in contrast, aligns the stochasticity throughout the hierarchy with end-to-end gradient update.", "2) Other good side effects:", "The robustness of the NADPEx policy is orthogonal to our current work, but will be an interesting direction for the future.", "Currently we only have some preliminary results.", "For example, it is more robust to adversarial neural attacks.", "In the future we will investigate how robust NADPEx policies could be when the environment is perturbed, e.g. agents are dragged slightly by humans as in [2, 3].", "That temporally consistent exploration is fairly important for physical robots is one of our motivations for this whole project.", "In the next step we will look for simulator environments with more authentic actuators to see how NADPEx could help solve that.", "Our ultimate goal is to find a safer and more efficient way for on-policy exploration on physical robots.", "We believe the application of NADPEx to off-policy exploration is straightforward.", "However, as stated in Section 1, off-policy methods benefit from stronger flexibility for experience sampler.", "This makes the gradient alignment and policy space constraint not as important as in the on-policy methods.", "As off-policy methods have the potential to be much more data-efficient, we will compare in the future how NADPEx performs comparing with auto-correlated noise in [4] and separate sampler in [5].", "[1] Plappert et al., \"Parameter Space Noise for Exploration\", ICLR 2018.", "[2] Tassa et al., \"Synthesis and stabilization of complex behaviors through online trajectory optimization\", IROS 2012.", "[3] Clavera et al., \"Learning to Adapt: Meta-Learning for Model-based Control\", arXiv 2018.", "[4] Lillicrap et al., \"Continuous control with deep reinforcement learning\", ICLR 2016.", "[5] Xu et al., \"Learning to explore via meta-policy gradient\", ICML 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_future", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 32, "sentences": ["We thank reviewer 1 for the detailed feedback.", "In this response, we clarify the accuracy-realism trade-off, revise the accuracy metrics, indicate reruns and new experiments, and address the individual questions.", "We updated Section 4.4 to indicate that it is to be expected that, although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG).", "A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.", "In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.", "Although the SVG generator is simpler than ours, ours is just a simple variation from Ebert et al. (2017).", "Since proposing a strong generator architecture is not the goal of this paper,", "any video generator (including the one from Denton & Fergus (2018)) could be used with our losses.", "We added this clarification to Section 3.4.", "Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).", "It's also worth noting that with a simpler feed-forward posterior and a unit Gaussian prior, our VAE ablation and SVG achieve similar performance on various metrics.", "We added Section 3.5 to point out the differences between the VAE component of our model and prior work.", "We have included a revised plot in Figure 14 (note that this temporary plot will be incorporated into Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the LPIPS metric (Zhang et al., 2018).", "LPIPS linearly calibrates AlexNet feature space to better match human perceptual similarity judgements.", "Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS.", "After examining the KTH results further, we realized that our results are likely weaker than they should have been, because we did not use the same preprocessing as prior work.", "The experiments from our original submission cropped the videos into a square before resizing, and thus discarded information from the sides of the video.", "We are currently rerunning the KTH experiments and we plan to update the results in the paper.", "We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.", "Although the combination of VAEs and GANs have been explored recently for conditional image generation (Zhang et al. 2018), the video prediction task is substantially different, with unique challenges, due to spatiotemporal relationships and inherent compounding uncertainty of the future.", "Furthermore, while the individual components have indeed been known for video prediction, their combination is novel and not present in prior work, and we demonstrate that this produces state-of-the-art results in terms of diversity and realism.", "In addition, this work provides a detailed comparison of the effect of the losses on the various metrics.", "Furthermore, we are currently running experiments for various weightings of the KL loss and the adversarial loss, and we plan to include additional results that illustrate the trade-offs based on these hyperparameters.", "Although MoCoGAN performs well for videos with a single frame-centered actor, it struggles with multiple simultaneously moving entities.", "The authors of MoCoGAN also mentioned in personal correspondence that the conditional version (i.e. video prediction) was significantly harder to train.", "We noticed the same in earlier iterations of our model.", "In our case, we found that the model would degenerate to static videos or videos with a cyclic flickering artifact, which are issues that aren't a problem in conditional image generation.", "We added details to Section 3.4 describing the importance of a few components, such as spectral normalization and not conditioning the discriminator in the ground-truth context frames.", "The purpose of adding adversarial losses to a pure VAE is to improve on blurry predictions where the latent variables alone cannot capture the uncertainty of the data.", "However, that is typically not the case of synthetic datasets.", "In early experiments, we trained our pure VAE model on the stochastic shape movement dataset from Babaeizadeh et al. (2018), and our pure VAE was able to model the dataset without any blur and with perfect separation of the possible futures.", "We agree that plausibility is indeed important, and that's what our human subject studies try to capture.", "Since we provide predictions of the whole sequence to the human evaluator, we are not only evaluating for image realism but also for plausibility of the dynamics.", "Unlike the VAE models that implausibly erase the small objects that are being pushed in the BAIR dataset, our SAVP model moves those objects in a more plausible way.", "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077", "[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. 2018 PIRM Challenge on Perceptual Image Super-resolution. In Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018.", "https://arxiv.org/abs/1809.07517"], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 33, "sentences": ["We thank the reviewer for their helpful comments.", "Please could the reviewer clarify why they felt our work muddies the debate regarding large-batch training?", "We demonstrate that one can initially increase the batch size with no loss in test accuracy by simultaneously increasing the learning rate.", "However for very large batch sizes the test accuracy degrades under both constant epoch and constant step budgets.", "We agree that some of our observations under constant epoch budgets in sections 2 and 3 have been made in previous work.", "However there are also several important differences:", "1.", "Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).", "As we show in sections 4 and 5, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.", "A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.", "To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).", "2. Zhang et al. argued that Momentum only helps in the large batch limit.", "However, their analysis is based on the noisy quadratic model, which cannot explain the results we observed on the test set in sections 4 and 5.", "These experiments clearly demonstrate that, unlike the SDE perspective, the noisy quadratic model is not an appropriate model for predicting test set performance in deep learning.", "Their work also does not clarify the assumptions under which linear scaling of the learning rate should arise.", "3. Our empirical results in section 3 are similar to Shallue et al., however their work argues that there is no reliable relationship between learning rate and batch size.", "We draw a very different conclusion: the learning rate usually obeys linear scaling, but linear scaling only holds theoretically when the assumptions we specify are satisfied.", "Linear scaling may not hold in cases where these assumptions break down (e.g., language modelling).", "4. The observation that the test accuracy is independent of batch size in the noise dominated regime is a natural consequence of the SDE analogy, since any two training runs which integrate the same SDE should sample final parameters from the same probability distribution.", "We will clarify this in the updated text.", "Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.", "We apologise for this.", "It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.", "Turning to our generalization experiments in sections 4 and 5.", "It is true that a number of papers in recent years have claimed that SGD noise enhances generalization.", "However Shallue et al. recently argued no previous work had provided convincing empirical evidence for this claim.", "Indeed in their abstract, they state \u2018We find no evidence that larger batch sizes degrade out-of-sample performance\u2019.", "In another recent paper, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets.", "Crucially, to establish that SGD noise enhances generalization, one must show that small batch sizes generalize better than large batch sizes under constant step budgets, with realistic learning rate decay schedules, and one must independently tune the learning rate at each batch size.", "In section 4, we are the first authors to perform this experiment and confirm that the final test accuracy of SGD does degrade for very large batch sizes under both constant epoch and constant step budgets, contradicting the claims of both Shallue et al and Zhang et al.", "Furthermore, we show in section 5 that the optimal SGD temperature which maximizes the test accuracy is almost independent of the epoch budget.", "These results provide the first convincing empirical evidence that SGD noise does enhance generalization in well-tuned networks with learning rate decay schedules.", "We believe this is an important contribution."], "labels": ["rebuttal_social", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 34, "sentences": ["We thank the reviewer for the comments and suggestions.", "As the reviewer points out, the community currently lacks a strong theoretical understanding of minmax optimization, and we believe our work helps to fill this gap.", "We comment on the practical implications of our work below:", "1) While the exact form of the sufficiently bilinear condition may be somewhat unwieldy, the result gives concrete evidence that having higher bilinearity can aid convergence for certain algorithms, even for settings that are not purely bilinear.", "This indicates that one should pay attention to the magnitude and condition number of the off-diagonal of the Jacobian when constructing a min-max problem and choosing an algorithm to solve the problem.", "2) In non-convex-concave settings, HGD will converge to all types of stationary points, as the reviewer points out.", "We propose some modifications to HGD to allow it to work in non-convex settings in Appendix A, which essentially amount to explicitly determining the local curvature of the problem and running a modified algorithm, such as Hamiltonian Gradient Ascent, near undesirable critical points.", "This would allow us to show similar local convergence guarantees to those proven by other works in the area (see Appendix A).", "However, as the reviewer points out as well, the HGD analysis is also useful because it implies similar convergence results for CO, which is a practical algorithm.", "3) Our result for CO shows that as long as $\\gamma \\ge 4L_g/\\alpha$, then CO will converge in sufficiently bilinear settings (currently it\u2019s written as $\\gamma = 4L_g/\\alpha$ but we will change this in the final version).", "This indicates that increasing $\\gamma$ may speed up convergence when we are in a sufficiently bilinear region (and in particular, the algorithm may not converge if $\\gamma$ is too small and the region has a very large bilinear term).", "If $\\gamma$ is too large, CO will converge to stationary points that are not local min-maxes, so these two phenomena must be traded off.", "One could potentially detect which regime one is in by computing a few eigenvalues of the Jacobian (using a logarithmic number of Hessian-vector products) during or after training.", "Other comments:", "-We thank the reviewer for pointing out Azizian et al. 2019.", "This work was released concurrently to ours on Arxiv and indeed seems to have some similar findings.", "We will include a reference to it in our revised version.", "-We thank the reviewer for the comment on notation and will incorporate it into the final version.", "References:", "Azizian, Wa\u00efss, et al. \"A Tight and Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games.\" arXiv preprint arXiv:1906.05945 (2019)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 35, "sentences": ["Many thanks for the detailed review!", "Main comments:", "1/ The DIP approach critically relies on regularization in order to make the method work (both by adding random noise in each optimization step to the input, as well as early stopping).", "As the first reviewer noted ``In fact, the DIP of Ulyanov et al. can hardly be considered \"a model\" (or a prior, for that matter), and instead should be considered \"an algorithm\", since it relies on the early stopping of a specific optimization algorithm''.", "However we follow the reviewers' suggestion and made clear that the idea to use a deep network without learning as an image model is not new and rewrote the item to ``The network itself acts as a natural data model.  Not only does the network require no training (just as the DIP); it also does not critically rely on regularization, for example by early stopping (in contrast to the DIP).''", "Before that, in the introduction, in the original and revised version, we have a paragraph devoted to the DIP explaining that Ulyanov et al. introduced the idea of using a deep neural network without learning as an image model.", "2/ Regarding the theoretical contribution: We fully agree that a limitation of the theorem is that it pertains to a one layered version of the decoder.", "We are currently extending this to the multilayer case, but still have to address a technical difficulty in counting the number of different sign pattern matrices.", "Regarding the assumptions: The proposition uses the assumption that k^2 log(n_0)  / n <= 1/32.", "Here, the constant 1/32 is not optimal.", "k^2 is essentially the number of parameters of the model, and n is the output dimension.", "The proposition is only interesting if k^2 log(n_0)  / n <= 1/20 even without this assumption (due to the right hand side of the lower bound) therefore this assumption is not restrictive.", "The bound is applicable if the number of parameters, k^2 is smaller than a logarithmic term times the number of output parameters, i.e., it allows the number of parameters to scale almost linearly in the output dimension.", "This is the regime in which the deep decoder operates throughout the paper.", "We agree that many natural noise patterns have structure, and that those can be better approximated with deep models, and are thus more difficult to remove.", "3/ We have added the sentence ``In the default architectures with $d=6$ and $k=64$ or $k=128$, we have that N = 25,536 (for k=64) and N = 100,224 (k=128)", "out of an RGB image space of dimensionality 512\\times512\\times3=786,432 parameters.'' to specify the number of parameters.", "Thanks for the suggestion to try second order method like LBFGS; we have tried LBFGS as a response to the reviewer's comment.", "It converges in significantly fewer iterations, but each iterations is so much more expensive that overall it optimizes slower than ADAM or gradient descent.", "Minor comments:", "1/ Figure 4: We have added labels and the sentence ``Early stopping can mildly enhance the performance of DD; to see this note that in panel (a), the minimum is obtained at around 5000 iterations and not at 50,000.'' in the caption to clarify.", "Also, we have added the sentence ``Models are fitted independently for the noisy image, the noiseless image, and the noise.'', and rewrote the paragraph", "Thanks for pointing this out!", "We agree that here we present only results for one image, but we did carry out simulations for many images, and those plots are qualitatively the same for all the images considered.", "Thus our conclusions about the model do not only hold for one image.", "2/ Normalization is applied channel wise.", "Let z{ij} be the j-th column in the i-th layer.", "Then z{ij} is normalized independently of any of the other channels.", "3/ We have reworded the corresponding paragraphs to make clear that while we do not use convolutions, and thus this is not strictly speaking a convolutional neural network, it shares many structural similarities with a conventional neural network, as pointed out by the reviewer.", "4/ The equation is correct in that the parameter choices in the paper are such that the deep decoder has much fewer model parameters N than its output dimension. Thus N is much less than n.", "5/ We agree that it is not optimal to use unintroduced notation at this point, but we made this compromise so that we can illustrate the performance of the deep decoder without introducing its details, but wanted to give a reader the chance to later see exactly what parameters we used.", "6/ Unfortunately choosing k=6 is too small to have a small representation error, i.e., to represent the image well.", "We have, however not hand-selected the 8 images shown out of the 64, and the other 64-8 images look very similar.", "We have all the images in the jupyter notebook that comes with the paper.", "7/ Great question, it is faster to optimize the deep decoder since the adam/SGD steps are cheaper, but it indeed seems to require slightly more iterations for best performance than the DIP."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_reject-request", "rebuttal_concede-criticism", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label"]}
{"abstract_id": 36, "sentences": ["We would like to thank the reviewer for providing valuable and detailed feedback.", "We have addressed the clarity concerns in the updated paper.", "Figure captions, metrics used in the table, etc, as mentioned in the presentation section of the review have been carefully examined and updated in the paper.", "We will reorganize the experiment section to better present the comparisons under different experimental settings.", "(1) Factorized Latent Variables:", "The factorization of latent space with respect to the modalities provides a way to differentiate observed and unobserved modalities.", "Therefore, VSAE is capable of handling partially-observed data where the missing modalities can be arbitrary.", "In addition, the embeddings are intuitively more meaningful as input to unimodal encoders is now limited to only observed modalities, eliminating the effect of missing modalities.", "When performing imputation/generation, however, we want to capture the dependencies between modalities.", "In other words, unobserved modalities should be imputed based on the information extracted from observed modalities.", "For experiments, we design this by conditioning decoders on all latent variables, essentially accessing information from all observed modalities.", "This is not in contradiction to the factorized latent variable assumption.", "Instead, the encoders try to embed each modalities individually, while decoders learn the dependencies between different modalities.", "(2) Multimodal Experiments:", "We apologize for unclear description of experimental settings.", "In general, we believe multi-modal data is more general than conventional image-text or video-text pairs.", "By unifying tabular data also as multi-modal (with each attribute as one modality), we show that VSAE provides us a principled way for imputation, capable of generalizing to more data families.", "Specifically, we conducted experiments on two types of data:", "(1) low-dimensional tabular data, and (2) high-dimensional data (pixel or text) as \"multimodal\" to better define the overall task of learning from partially-observed data.", "Upon request, we have included more extensive experiments following [1] on MNIST/FashionMNIST, and [2] on CMU-MOSI/ICT-MMMO.", "Results are reported in Table 10 and Table 11 (Appendix C.5).", "As shown, VSAE consistently outperforms baseline models across the added experiments as well.", "(3) Discussions on Comparison with Upper Bound Methods:", "Models trained with fully-observed data in theory should have better performance, thus we treat them as upper bound methods.", "However, it is very interesting to observe that in some cases, VSAE have superior performances.", "One possible explanation is that missing modalities introduces extra noise into the model as regularizer, thereby, increasing the generalization ability.", "However, detailed experiments and more discussions need to be carried out to back up this explanation.", "[1] Wu et al. Multimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018.", "[2] Tsai et al. Learning Factorized Multimodal Representation, ICLR 2019."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_done", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 37, "sentences": ["We thank the reviewer for the comments.", "As correctly observed by the reviewer, Arora et. al. 2015 suffers from a bias in estimation both in the analysis and in the empirical evaluations.", "The source of this bias term is an irreducible error in the coefficient estimate (formed using the hard-thresholding step).", "NOODL overcomes this issue by introducing a iterative hard-thresholding (IHT)-based coefficient update step, which removes the dependence of the error in estimated coefficient on this irreducible error, and ultimately the dictionary estimate.", "Intuitively, this approach highlights the symbiotic relationship between the two unknown factors \u2014 the dictionary and the coefficients.", "In other words, to make progress on one, it is imperative to make progress on the other.", "To this end, in Theorem 1 we first show that the coefficient error only depends on the dictionary error (given an appropriate number of IHT iterations R), i.e. we remove the dependence on x_0 which is the source of bias in Arora et. al. 2015.", "We have added the intuition corresponding to this in the revised paper after the statement of Theorem 1 in Section 3.", "Analysis of Computational Time \u2014 We have added the average per iteration time taken by various algorithms considered in our analysis in Table~4 and Appendix E.", "The primary takeaway is that although NOODL takes marginally more time per iteration as compared to other methods when accounting for just one (Lasso-based) sparse recovery for coefficient update, it (a) is in fact faster per iteration since it does not involve any computationally expensive tuning procedure to scan across regularization parameters; owing to its geometric convergence property (b) achieves orders of magnitude superior error at convergence, and as a result, (c) overall takes significantly less time to reach such a solution; see Appendix E for details.", "We would like to add that since NOODL involves simple separable update steps, this computation time can be further lowered by distributing the processing of individual samples across cores of a GPU (e.g. via TensorFlow) by utilizing the architecture shown in Fig. 1.", "We plan to release all the relevant code as a package in the future.", "In this revision, we have added comparison to Mairal '09, a popular online DL algorithm.", "Further, we have also added a proof map, in addition to the Table 3, for easier navigation of the results."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 38, "sentences": ["Thank you for your thoughtful feedback!"], "labels": ["rebuttal_social"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 39, "sentences": ["We thank the reviewer for their insightful comments.", "Q1 \"all the experiments except the last row of Table 2 concern adaptation between two domains. Given the paper title, the reviewer would have expected more experiments in a multiple domain context.\"", "A1 A main difference between domain adaptation and MDL is the fact that the former aims to minimize the target error, while the latter aims to minimize the average error.", "In this sense, our goal (and the validation experiments on Cell) are focused on MDL.", "Q2", "\"Although the paper introduces the generalization bound for MDL, it does not give new formulation or algorithm to handle MDL\" [...] \"There is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.\"", "A2 This issue is related to the above: the new generalization bound extends that of Ben David et al. in the sense that it considers all pairs of domains involved, thus bounding the *average* risk; and this bound is the one underlying the proposed algorithm and its MDL experiments.", "We have clarified this in the manuscript.", "Q3 \"the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset. \"", "A3: We added 3 domain experiments for Office, which are now displayed in Appendix E.1 table 6.", "As discussed in [3], we also find that the addition of a second source is not necessarily beneficial to target accuracy.", "Q4: Comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.\"", "A4: ADDA, an unsupervised DA method, proceeds by training sequentially a classifier on Source, then learning the Target feature space by making it indistinguishable from the Source one.", "However this is not applicable to the semi-supervised setting: either target labels would not be used in the first training step, or they would be used but without any domain loss to account for the fact that two domains are being used at the same time.", "Thus, the classifier would actually learn two sub-classifiers: one for each domain, which would turn counter-productive in the second step where this strong distinction between source and target would have to be un-learned.", "We are re-programming DSN and experimental results will be added.", "We thank the reviewer for the suggestion.", "Q5: \"The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.\"", "A5 Zhao et al. [3] consider the multiple source context; they define a weighted scheme where the weight of a source depends on its H-divergence with the target, plus its own classification error.", "The feature extractor is trained either from the best source only (in the sense of this weight), or from a weighted sum of the sources.", "When interested in multi-domain learning (thus aiming to minimize the average risk), it seems that there are two possibilities: a single feature extractor; or a feature extractor per domain.", "In the former case, the feature extractor might be overly conservative; in both cases, scalability w.r.t. the number of domains might be an issue.", "Hoffman et al. [4] also consider the multiple source context, assuming that the target is a unknown mixture of the sources (or not too far thereof in terms of Renyi divergence).", "Their experiments follow this assumption (using as target a mixture of sources Amazon, Webcam and DSLR).", "In our case this assumption does not hold, e.g. the joint distribution of England(x,y) is *not* a mixture of Texas(x,y) and California(x,y) (as can be seen by eye, and confirmed by experiments).", "The adversarial change of representation only enforces the merge of the marginals.", "Q6 \"The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier. Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples\"", "A6", "As the reviewer suggests, there are indeed misclassifications of samples using their entropy ranking in early training stages.", "We mention this section 4.3.", "This misclassification is the reason why it is better that hyper-parameter p slightly underestimates p* than is equal to it, as can be seen in Fig. 1, right (except when p*=1 as one could expect).", "Q7", "\"Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).\"", "A7 L is the cardinal of the union of classes with labeled examples in at least one domain."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 40, "sentences": ["1- There are two reasons that concept and problem embedding are performed in this work.", "Considering concept continuity is an important matter in education.", "Having concept embedding, concept continuity can be reached as is discussed in the last paragraph on page 7 and some other examples are given in table 2.", "By just having the most sophisticated concept extractor, the concept continuity cannot be retrieved.", "Furthermore, problem embedding is used by the recommender system to project the performance of students on the problems they solved onto other problems that they have not solved.", "This way, we have an idea of what problems should be recommended to them and which problems should not by having an evaluation of their ability to solve unseen problems and recommend problems in the boundary of their capacity, not way beyond, and to recommend problems in a way that covers all concepts necessary for students to learn.", "We have observed interesting patterns, e.g. similar problems are more likely to be solved correctly at the same time or wrong at the same time.", "Note that by just having the concepts of problems that are not in numerical form, performance projection may not be feasible and there is a need for using other methods like embedding.", "2- The data size being small", "is just the nature of the application.", "Creating new problems is a creative process and is not easy, given that with the insight we have on the application, the data size seems to suffice.", "Furthermore, since Prob2Vec is performing well for not a relatively big data set, it would definitely do well for big data sets since the more data we have, the more precise the concept and problem embedding are.", "The easy-tough-to-beat method proposed by Arora et al. is the state of the art in unsupervised sentence embedding that we compared our algorithm with.", "Please let us know if we missed anything.", "Pre-training is a common practice in transfer learning (one-shot learning).", "The objective function does not differ from the objective function used for post training.", "Training on only negative samples with lower training epochs than the training epochs in post training just adjusts the weights of the neural network to a better starting point.", "If the training epochs in pre-training is relatively smaller than the training epochs in post training, due to curse of dimensionality, the warm start for post training results in better performance for NN classifier.", "To make it more clear what it means to train the neural network on a pure set of negative data samples, think about batch training.", "It's not likely, but possible, that a batch only has negative or positive samples.", "In the pre-training phase of our method, we intentionally used a pure set of negative samples (with fewer training epochs) to have a warm start for post training.", "As table 3 shows, our proposed method outperforms one-shot learning.", "Please look at part 1 of our response to reviewer2 and part 2 of comment titled \"Response to Question on Negative Pre-Training\" below."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 41, "sentences": ["First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers,  and apologize for the late response due to additional experiments and modifications of the paper.", "Remark 1. \"the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).\"", "A : The purpose of our experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.", "Therefore, we experimented with the popular setting.", "The following is the experimented dataset in other papers.", "- Temporal ensembling & \u03a0 model [1]: CIFAR-10 (4k), SVHN (500, 1k), CIFAR-100 (10k)", "- VAT [2] : CIFAR-10 (4k), SVHN (1k), CIFAR-100 (no experiment)", "- Mean Teacher [3]: CIFAR-10 (1k, 2k, 4k), SVHN (250, 500, 1k), CIFAR-100 (10k)", "We took the reviewer\u2019s comment judiciously and have added CIFAR-10 (1k, 2k) experiments in Section 6.5 of the supplementary material and their accuracies are comparable with those of the conventional SSL algorithms. (It took a long time to perform 5 runs of test for all additional experiments.)", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "The result of CIFAR-10 (1k, 2k) with 5 runs", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "(error     / standard deviation) |", "1k        |        2k", "supervised                       | ( 38.71 / 0.47 ) | ( 26.99 / 0.79 )", "\u03a0 model [1]                      | ( 27.36 / 1.2 )  | ( 18.02 / 0.60 )", "mean teacher [3]", "| ( 21.55 / 1.48 ) | ( 15.73 / 0.31 )", "SST                              | ( 23.15 / 0.61 ) | ( 15.72 / 0.50 )", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "[1] Laine, Samuli, and Timo Aila. \"Temporal ensembling for semi-supervised learning.\" arXiv preprint arXiv:1610.02242 (2016).", "[2]Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.", "[3] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep", "Remark 2. \"Why does the SST algorithm use different epsilon policies for synthetic vs organic datasets?\"", "A : There are different network structures in the synthetic and organic dataset (table 4-5 in the supplementary materials in the new version).", "And, because there are only 12 initial points in the synthetic, it needs much higher confidence than organic datasets.", "(Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added in the organic dataset.", "However, in synthetic data, unlabeled data is added when epsilon begins at (10^(\u22125)).", "Therefore, We have changed the epsilon value so that no data is added at the beginning of the iteration.)", "Remark 3.", "What is the problem in SVHN (balance problem or dataset or both)?", "A : We have experimented with SVHN with data balancing.", "In SVHN, 1,000 images are used as the labeled data and 45,000 balanced unlabeled images are used.", "As a result, the SST is still worse than other algorithm [1].", "Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.", "Remark 4. \"You should also show the performance of regular SSL methods in the setup on Table 4.\"", "A : We have performed experiments of self-training without threshold and SST with softmax output.", "Although the experimental setting is a bit different from [1], the setting of 100% of non-animal unlabeled data is the same.", "They have shown that the performance degraded when the unlabeled dataset contained 100% of non-animal data in figure 2 in [4].", "The approximate score in Figure 2 of [4]", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "100% out-of-class (error)", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "supervised learning : about 23.5%", "\u03a0 model : about 26.3 %", "Mean Teacher : about 26.3 %", "VAT : about 26%", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "[4] Odena, Augustus, et al. \"Realistic Evaluation of Semi-Supervised Learning Algorithms.\" (2018). ( https://arxiv.org/abs/1804.09170 )", "Remark 5. combining SST and other SSL", "A : Combining and the additional cost is expensive. Therefore, we have modified our expressions."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_none", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_summary_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 42, "sentences": ["Thanks for your feedback.", "We reply to all reviewers jointly in our comments above."], "labels": ["rebuttal_social", "rebuttal_summary"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label"]}
{"abstract_id": 43, "sentences": ["We thank the reviewer for their comprehensive review.", "We updated the paper with better results over more tasks, either matching or outperforming the human baseline in terms of final accuracy, and outperforming the model-free baseline in all cases.", "We also included results over multiple runs of all experiments, showing the minimum, maximum and mean accuracy.", "1. While it is true that the manually-tuned baseline we provided is simple, it is a standard practice in the field to adjust the learning rate during training and keep the rest of the hyperparameters constant.", "Adjusting all of them requires significantly more effort and is infeasible in many cases.", "2. Due to time constraints, we have not benchmarked our method against more hyperparameter-tuning baselines yet.", "We agree that it would be a very valuable comparison and leave that for future work.", "Nevertheless, please note that the human baselines we use for Transformer have been tuned by researchers using auto-tuners among other tools.", "3. [1] successfully use PPO with an LSTM policy on a challenging, partially-observable environment.", "It is equally principled to use a Transformer policy, since both would operate on the same sequence of observations.", "The SimPLe algorithm runs PPO on an MDP approximated by a powerful model that handles stochasticity well, which is also a valid approach.", "4. We updated the paper with a justification of our action discretization scheme.", "Such a discretization has a number of benefits, including multi-modality, which cannot be achieved using a parameterized Gaussian policy.", "[2] show that discretization of the action space improves the average performance, stability and robustness to hyperparameters of reinforcement learning agents on a range of continuous control tasks.", "5. While we have not included such transfer experiments in our current work, we do believe that a model trained on enough architectures and tasks will generalize to new ones.", "For instance, in the updated version of the paper, we show that the learned policy employs similar learning rate and weight decay rate adjustment schemes across very different tasks.", "Substantiating this claim in the general case will likely require a large-scale study, which we plan to perform in the future.", "[1] OpenAI et al. \u201cLearning Dexterous In-Hand Manipulation\u201d, arXiv preprint arXiv:1808.00177 (2018)", "[2] Tang et al. \u201cDiscretizing Continuous Action Space for On-Policy Optimization\u201d, arXiv preprint 1901.10500 (2019)"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 44, "sentences": ["We very much appreciate your valuable comments, efforts and times on our paper.", "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "Q1. Updated proof.", "To address your concerns, we provided more detailed explanations of our proof arguments in the revised draft (see Appendix F).", "We also re-organized our proof completely for better understanding.", "Q2. Relation to Tandem approach.", "As you pointed out, our method is somewhat related to Tandem approach [1] in that both post-process a generative model on top of hidden features extracted by DNNs.", "However, the main purpose of Tandem is not for handling noisy labels.", "In particular, the Tandem approaches utilize the EM algorithm that should be highly influenced by outliers, while our method is specialized to be robust against them.", "We clarified this in Section 2 of the revised draft.", "[1]  Hermansky, H., Ellis, D.P. and Sharma, S., Tandem connectionist feature extraction for conventional HMM systems. In IEEE ICASSP, 2000.", "Thanks a lot,", "Authors", "Dear AnonReviewer1,", "We hope that you found our rebuttal/revision for you and other reviewers in common.", "If you have any remaining questions/concerns, please do not hesitate to let us know and we would be happy to answer.", "Thank you very much,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 45, "sentences": ["Thanks for your valuable comments and feedback.", "We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.", "New experiments have also been added.", "Please find bellow our answers to your specific remarks.", "R: \"In the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, ...  Beyond this simplification, I am not clear if that is actually intended by the authors.\"", "A: You are totally right, eq.10 simplifies to $\\log p(D) \\geq", "\\mathbb{E}_{I \\sim", "q^D}", "\\left[ \\sum\\limits_{i=1}^{|D|-1} \\log p(D_i |D_{<i},I_{<i})  + \\sum\\limits_{v \\not\\in U^D} \\log p(v \\not\\in U^D| D_{\\leq |D|-1}, I)\\right]$. We were aware of this but for simplicity and the conciseness of presentation we chose to not give this final derivation.", "Also, this was because $P(D_i,I_i|D_{<i},I_{<i})$ is much easier to compute (and more efficient) than $P(D_i |D_{<i},I_{<i})$, given that the computation of $P(I_i| D_{\\leq i},I_{<i})$ is in both case required for sampling ($P(D_i,I_i|D_{<i},I_{<i})$ involves a simple product while $P(D_i|D_{<i},I_{<i})$ involves a sum of products).", "But we agree this was not a good choice, since this simplification appears obvious to the reader.", "We hesitated a lot on this, our decision was taken to present things as closely as possible to our implementation (by the way there was a mistake in the algorithm 2 resulting from this hesitation - the implementation for our experiments was correct however).", "We agree that this may appear confusing.", "Particularly since it gives the feeling that parents of infections are not involved anymore in the computation.", "But there are actually, since there remains $P(I_i|D_{<i},I_{<i})$ terms in the expectation.", "The process learns parameters that tend to give high probabilities to the most likely parent vectors regarding infections from the episode.", "In the revision of the paper, we gave the final derivation you suggested, since it is better for comprehension (It also allowed us to discuss more precisely on what is optimized by considering the given gradient update), and we discuss about its implementation in the appendix as an explanation for the algorithm 2 (that only slightly changed to correct the aforementioned mistake).", "Thanks for this remark that helped us to much improve the paper.", "R: \"The authors explain how they trained their own model but there is no mention on how they trained benchmark models\"", "A: You are totally right, it is missing.", "Baseline models are trained on the same training set as our model following the methods proposed in their original paper.", "Our model and the baselines were tuned by a grid search process on a validation set for each dataset (whose size is given in the description of the datasets),", "although the best hyper-parameters obtained for Arti1 remained near optimal for the other ones.", "For every model with an embedding space (i.e., all except CTIC), we set its dimension to $d=50$ (larger dimensions induce a more difficult convergence of the learning without significant gain in accuracy).", "We added this explanation in the new version of the paper."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 46, "sentences": ["Thank you for your fruitful comments.", ">>", "What would make it stronger imo is to address the issue of how much is gained from a discrete vs. continuous representation.", "Discrete representations by themselves are not better than continuous ones (cf. Table 1, wav2vec vs. vq-wav2vec).", "However, discretization enables the application of existing algorithms from the NLP literature which were designed for discrete inputs.", "We show that the BERT model can be directly applied to discretized speech.", "BERT can better model context than (vq-)wav2vec.", ">> The authors take it as a given that discrete is good because it allows us to leverage work in NLP. That makes sense -- but at what cost?", "Chaining vq-wav2vec and BERT requires more computational effort than just wav2vec, however, it does improve accuracy as our results show (cf. Table 1).", "Running BERT requires roughly as much computational overhead as just vq-wav2vec.", ">> The state of the art on LibriSpeech is not Mohamed at al. 2019.", "See e.g. Irie et al. Interspeech 2019 for better result.", "Thanks for pointing this out, we fixed this in the updated version of the paper we just posted.", ">> The Conclusion is very sparse.", "We broadened conclusion and delineated additional future work."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 47, "sentences": ["We thank the reviewer for the great feedback.", "We have simplified the architecture described in the paper by combining the networks $\\sigma$ and $\\omega$, and included the results from this architecture as well, producing a more robust architecture that performs better for multiple rewrite steps (while keeping the original, more complicated solution as one of the baselines).", "As suggested, we have added further analysis of failure cases.", "We also corrected the typos and clarified the definitions of True, Pred (One Step) and Pred (Multi Step) variants.", "We are very grateful for the review that helped to improve the paper significantly."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 48, "sentences": ["Thank you for your comments and suggestions.", "We will address the issues you mentioned.", "1.", "ARTNet is not very much related to our V4D.", "Basically, ARTNet is an alternative for 3D CNNs by replacing 3D convolution layers with SMART blocks.", "The SMART blocks are two branch units, with one branch for learning static appearance features and one branch for learning motion features.", "ARTNet is a clip-based method for learning short-term representations while our V4D is a video-level method for learning both short-term and long-term representations.", "2.\tDuring training, we uniformly divide the whole video into U sections and randomly select one action unit from each section.", "So there are no overlaps for training.", "For testing, there might be overlapping during sampling due to the limit length of video.", "However, our V4D inference algorithm in section 3.4 guarantees that only the non-overlapping action units will interact with each other during testing.", "3.", "Sure.", "We train all the models with 8 GPUs of GTX 1080 with memory capacity of 11178MB.", "For the inference speed, our V4D ResNet18 takes 0.67s per video and V4D ResNet50 takes 1.22s per video.", "In addition, we also reported the GFLOPs of V4D and compared it with other typical methods in Table 2(b).", "For training on Mini-Kinetics, V4D ResNet18 takes a bit more than 1 day while V4D ResNet50 takes a bit more than 3 days.", "4.", "We can only find the results of ARTNet ResNet18 in the published paper.", "After communication with the authors of ARTNet, we confirm that there are no results published for ARTNet ResNet50.", "So instead we implement ARTNet ResNet50 by ourselves and the top1 accuracy on Kinetics-400 is 74.3%.", "This is still lower than our V4D ResNet50 whose top1 on Kinetics-400 is 77.4%.", "Also, ARTNet ResNet18 reports an average metric of 81.4%, which is the average of top1 and top5 accuracy.", "While our V4D yields an average score of 85.3%.", "5.", "Thank you for providing this related work and we now cite this paper in the second version.", "This paper utilizes 4D CNN to process videos of point cloud so that their input is 4D data.", "Instead, our V4D processes videos of RGB frames so that our input is 3D data.", "This basically makes the methods and tasks quite different.", "Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.", "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 49, "sentences": ["We thank reviewer for his insightful comments.", "1.", "We agree with the reviewer that sinkhorn iteration is a way to obtain an upper bound on Wasserstein distance.", "However, based on the original paper, they solve the Sinkhorn divergen with T iterations, later when they solve the generator based on the estimated distance, the gradient has to backpropagate through those T iterations, which is expensive and infeasible.", "We also note that there is new work, IPOT (Xie et al., 2018), which can get rid of backpropagating through the T iterations as what we adopted (Bertsekas, 1985) in the paper.", "Combining PC-GAN with IPOP or other future works could be an interesting future work.", "2.  The variance of the sandwiched estimator can be higher, but we are more concerned about bias in this work, which can be treated as a bias-variance trade-off.", "3. The 20:1 mixture used in practice do not directly correspond to s in theory, because the distances we compute are not scaled.", "For example, if the f_\\phi, the discriminator of GAN, is k-Lipschitz, the lower bound estimate should be divided by k.", "However, k is unknown in practice.", "Therefore, we just numerically did a coarse grid search and find the best mixture ratio.", "Also, we try different ratios as we replied to R2 above.", "Ratio                   D2F (Distance to Face)", "Coverage", "1:0", "6.03E+00                        3.36E-01", "40:1", "6.06E+00                       3.41E-01", "20:1", "5.77E+00                       3.47E-01", "10:1", "6.85E+00                       3.56E-01", "0 :1", "9.19E+00                       3.67E-01", "4. We do not consider W_s to be very close from W_U. As can be seen from Figure 6, for the aeroplane examples, W_U fails to capture aeroplane tires while W_s can.", "Similarly for Chair example, W_s recovers better legs than W_U. Quantitatively, we highlight that W_s outperforms W_U consistently as shown in Table 1.", "Thus, we consider both W_U and W_L is needed to generate good quality point clouds."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_none", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 50, "sentences": ["Thank you for your positive review, we address your reasonable concern for the applications of the proposed method in below:", "Q1:  It would be nice to see a better case made for spherical convolutions within the experimental section.", "A1", ": We believe the best case is the non-rigid shape classification and retrieval.", "Our method achieves a state-of-the-art classification and retrieval performance on Shrec\u201911.", "The good performance on non-rigid shape is mainly contributed by the use of bijective spherical parameterization method, which obtains the input spherical image without topological information losses.", "When using spherical projection method to represent 3D shape, there will be information loss if the object is non-convex.", "The lossy input affect the performance of rigid shape analysis to some extent.", "Q2: The experiments on SHREC17 show all three spherical methods under-performing other approaches.", "It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.", "Is there a task that this representation significantly outperforms other spherical methods and non-spherical method?", "A2: The experiments on SHREC17 does show all three spherical methods slightly under-perform some other state-of-the art approaches, We believe this is mainly due to the information losses introduced in the spherical projection process which retains only the convex portion geometric information.", "Future improvements can be added by using:  (1) less lossy input in spherical projection methods (e.g. on top of SEF, and EDF, we can also add other statistic information such as the minimum distance of intersection, mean distance of intersections or standard deviation of the intersection and so on to reduce the information losses); (2) extend the spherical parameterization method on to the general 3D shapes.", "Currently, the spherical parameterization method only works for genus-0 closed object.", "The 3D models presented on ModelNet and Shrec\u201917 are of arbitrary genus which prevents us from using spherical parameterization method.", "Generalization of spherical parameterization methods to objects with arbitrary topology will be one of the future work.", "Compared to non-spherical method, spherical image is one of the most compact representation for 3D shape analysis, the spherical convolution methods rely on no data augmentation (for Type I and Type II) or reduced data augmentation (for Type III, only SO(2) rotation augmentation is required).", "Other non-spherical method (such as volumetric or multi-view based methods)  can only be generalized into unknown orientations using SO(3) rotation augmentations, their representation of 3D shapes are either too sparse (voxel model) or too redundant (multi-view projections).", "Compared to SO(3) spherical convolution method (Cohen et al. 2018), our network is computationally efficient (in terms of network model), but we have to admit that this is at the price of required data augmentation", ".", "Another advantage is that our network allows local filters and local-to-global multi-level spherical features extraction.", "The other two spherical convolution methods (Cohen et al 2018 and Esteves et al 2018) use a lat-lon grid and conduct convolution in the Fourier space, which has a common disadvantage: the Fourier transform does not support local spherical filters.", "Another disadvantage is the use of lat-lon grid which introduces unevenness of the perception field (due to the high resolution near the poles, and low resolution at the equator).", "Experimentally, we compared the three spherical convolution methods in Table 3 using Shrec\u201917 perturbed shape retrieval experiment.", "Cohen et al 2018 and ours obtain similar performances because both of the methods used anisotropic filters, the former achieves rotational invariant using SO(3) rotations for filters, while ours achieves rotational invariant using alt-az rotation of filter and SO(2) rotation augmentation of input shapes.", "As expected, anisotropic filter perform  better than the isotropic filter proposed in Esteves et al 2018 which limits the model capacity.", "Q3:  Is there a specific useful application where spherical methods in general outperform other approaches?", "A3: As mentioned in Cohen et al 2018, perhaps the most exciting future application of the Spherical CNN is in omnidirectional vision.", "Although very little omnidirectional image data is currently available in public repositories, the increasing prevalence of omnidirectional sensors in drones, robots, and autonomous cars makes this a very compelling application of our work.", "Omnidirectional vision is a better application to show the strength of the spherical convolution method."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 51, "sentences": ["Thank you for your suggestion of increasing the discussion of the results.", "We\u2019ve expanded the discussion of the results as much as possible.", "For now we would prefer to keep the actual bar plot of individual module performance in the appendix in the interest of space, and keep the dataset description in the main part, as this was appreciated by the other two reviewers.", "As you say, the ability to generalize is very important in mathematics.", "The paper contains an extrapolation test set to do exactly this - these include generalization tests on larger numbers, longer sequences, more function compositions (which is similar to having more variables), etc (see Appendix B for more details).", "We haven\u2019t attempted to be exhaustive in types of generalization, but the extrapolation test set can be extended in the future to allow for this.", "None of the modules currently include \u201cunsolvable\u201d as an answer, but this is something that would definitely fit within the framework.", "(As an aside: there would be no need to have a special character; we could simply select some consistent word like \u201cUnsolvable\u201d; neural models trained so far seem to have no problem outputting \u201cTrue\u201d or \u201cFalse\u201d.) More generally, there are many further types of problems, that could be included in the dataset - but we hope for now that the current range is comprehensive in types of reasoning required for school-level mathematics.", "We always welcome contributions to the dataset that extend the range of questions in a consistent manner."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_future", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 52, "sentences": ["Thanks for your careful review! As mentioned in the intro, we are trying to give some principled insight into benefits of BN, which has proved tricky.", "Also, it is noted in the paper that BN probably has many desirable properties, of which auto-rate tuning is just one.", "(i) Speed of SGD vs GD:", "Note that \u201ctime\u201d here refers to number of iterations, not epochs.", "We are not aware of results establishing SGD is faster in this measure.", "(As noted on p2,  we are working within the standard paradigm of convergence rates in optimization.", "The only new part is the automatic rate tuning  behavior shown for most parameters when BN is used.)", "(ii) \u201cusually training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.\u201d", "We\u2019re assuming training proceeds until gradient is small (stationary point).", "We are not aware of any prior analysis of speed of convergence that deviates from this assumption.", "Perhaps the reviewer is thinking of early stopping in context of better generalization?", "(iii) \u201cclarify difference from Wu et al. (2018)\u201d", "Wu et al. 2018 introduces a *new* algorithm inspired by weight normalization (WN) and studies its convergence rate to stationary point.", "This algorithm can be seen as an explicit way to tune the learning rate (thus it is conceptually analogous Adagrad).", "They don't have any results about WN or BN itself.", "Their analysis could be adapted to GD on one-neuron network with WN or BN without scale-variant parameters (gamma and beta).", "Even this adaptation is not immediate because the goal of this work is to find a stationary point on the unit sphere rather than R^d.", "Finally, they prove no results for SGD, whereas our paper does.", "(iv) \u201csingle learning rate doesn\u2019t apply for all parameters\u201d", "Correct.", "The algorithm can use a single learning rate for scale-invariant parameters but needs a tuned rate for the scale-variant ones.", "In feedforward nets, the number of scale-variant parameters scales as the number of nodes and the number of scale-invariant parameters scales as the number of edges (up to weight sharing).", "Thus the vast majority of parameters are scale-invariant.", "(v) \u201cRelation between original loss and loss using BN.\u201d", "Our results hold for the loss of batch-normalized network (\u201cBN-loss\u201d)  which is different from the loss of the original network (\u201cBN-less loss\u201d).", "Probably the reshaping of loss function due to BN is very important but currently hard to analyse theoretically because we lack a good mathematical understanding of the loss landscape (even BN-less)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_followup", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 53, "sentences": ["Dear Reviewer 2, thank you for your constructive feedback and for taking the time to review our work.", "Your specific points are addressed below.", "> \u201cThe results are not strong. And, unfortunately, the model contribution currently is too modest.\u201d", "Indeed, the model is a minor contribution and, especially in light of a more thorough evaluation of RGCN, the sum attention RGAT results do not improve on those in Schlichtkrull et al. (2017).", "However, we would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.", "We plan to make our implementation public to aid future research in the area.", "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs", "(Zhang et al. 2018)", ".", "We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance model contributions of the paper.", "We also consider the poor results on MUTAG to be significant and informative for the rest of the community in developing more powerful models that can be applied to relational graphs.", "> \u201cWe should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)\u201d", "We agree that including experiments on the other data sets presented in Wu et al. (2018) would be a valuable addition to the paper.", "Unfortunately, we are unable to perform the required thorough hyperparameter exploration required to draw meaningful conclusions within the remaining time of the rebuttal period.", "This is something we will investigate in the future.", "> \u201cMy introduction suggestion: do not talk about Convolutional neural networks (CNNs).\u201d", "Thank you for this stylistic critique.", "We see how the approach taken did not provide an intuition about the problem as well as it could have.", "We agree with your point regarding the wealth of graph neural network studies.", "We feel that the field of geometric deep learning, from which part of the direction of this work originated, is important to keep as part of the development of graph-based machine learning models.", "Some recent publications in the area of graph based ML have put less emphasis on geometric deep learning, the generalisations of convolutions from grids to graph, and the modifications of convolutions to achieve non-basis dependent methods.", "We felt that it is important to keep these concepts associated with the field of graph based ML.", "This introduction could, however, talk less in detail about CNNs themselves, and deal more with graphs - the main focus of the paper.", "We will produce a reworked introduction where graphs play a larger role.", "This should provide a more intuitive introduction to our work, whilst maintaining cornerstone concepts."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_by-cr", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_future_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label"]}
{"abstract_id": 54, "sentences": ["We thank reviewer #3 for the extensive feedback.", "We have incorporated it as stated below.", "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work and added the suggested new interesting references, added theorems and more formal statements in the main text, compressed the appendix and enhanced the description of the experimental setup.", "Proofs that the space \"interpolates smoothly with curvature\": we added formal proofs (see theorems 2 and 3) that all the operations are differentiable, i.e. the gradients are equal from both the left and right at 0, w.r.t. curvature, for the chosen models of hyperbolic and spherical geometry.", "k-addition definiteness in the spherical setting: we have added the formal condition that the k-addition be well-defined, and a proof that for two points this condition indeed recovers x != y / (k ||y||^2) - see Theorem 1.", "Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.", "This is a desirable property for Riemannian vector averaging.", "Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.", "Other comments:", "-Synthetic tree: contains |V| - 1 edges.", "We corrected this mistake.", "-Curvatures: are learned as we state in the paper section 4 and appendix F.", "-Citations: fixed.", "-Working with the Poincare ball as opposed to the hyperboloid model: this allows us to use gyrovector spaces which are defined either for the Poincare ball or the Klein model, as well as to connect those with the Riemannian geometry of the space.", "Moreover, as we show in the paper, we can now smoothly interpolate between all constant curvature spaces which is beneficial for learning curvatures without a priori deciding on their signs.", "-Statement about \u201cgeneral class of trees\u201d replaced by \u201call weighted or unweighted trees\u201d.", "[1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips\u201919", "[2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips\u201919"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 55, "sentences": ["We appreciate your constructive feedback.", "Specifically, your comments about our motivation and development of our idea greatly help us to improve the quality of our paper.", "If we correctly understand reviewer 2\u2019s concerns, the concerns can be divided into two folds:", "1. Our suggestion to mitigate the catastrophic forgetting looks a naive combination of well-known concepts. Thus, it is more system engineering than science.", "2. Each component described in Figure 1 is not explained enough.", "Also, there is no description of the complete task.", "[Response for 1]", "As we explained at the common response, we started our research from clear open questions.", "Our first open question was that why other GR-based algorithms [1, 2] assume unit Gaussian priors even though they integrate classification loss into their VAE formulation.", "Since they do not consider the conflict between the unit Gaussian prior and discriminative loss for the latent variable z, their models generate ambiguous samples that negatively affect the performance of incremental learning, which is discussed in section 4.1 in our paper.", "This leads us to a more theoretical formulation for classification-regularized VAE.", "By introducing class conditional priors induced by the mutual information maximization, DiVA yields class-wise discriminative one mode Gaussians for latent variable z.", "Naturally, DiVA can conduct both class prediction and class conditional sample generation with one integrated model.", "The second open question was that why GR-based algorithms suffer from serious catastrophic forgetting in natural image datasets, even though generated samples are not completely noisy.", "We assumed that this is due to the vulnerability of neural networks [3] triggered by different distributions of pixel values between real and generated images.", "Thus, we defined the two domains: real domain and sample domain.", "To narrowing the distribution gap, we needed a solution that satisfies two conditions (also described in section 5):", "1. We should translate only the style (a global pattern of a specific domain) as keeping outline patterns of given images.", "2. We should consider an unpaired domain translation between real and generated images because the generated images are sampled randomly.", "Fortunately, we were able to find an existing solution that satisfies the requirements: CycleGAN. Any other domain translators that satisfy the conditions can be used or newly studied.", "With the solution, we could make a breakthrough for GR-based methods.", "To the best of our knowledge, this is the first successful approach for a GR-based algorithm to start to resist the catastrophic forgetting problem with a natural image dataset.", "[Response for 2]", "Figure 1 is a conceptual description of our proposed model, DiVA.", "Each component is explained in section 4, below Equation 2, and justified in section 4.1.", "Also, for an easy understanding of the whole CL process with DiVA, we added another figure in Appendix E.", "[References]", "[1] van de Ven, Gido M., and Andreas S. Tolias. \"Generative replay with feedback connections as a general strategy for continual learning.\" arXiv preprint arXiv:1809.10635 (2018).", "[2] Mundt, Martin, et al. \"Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition.\" arXiv preprint arXiv:1905.12019 (2019).", "[3] Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. \"One pixel attack for fooling deep neural networks.\" IEEE Transactions on Evolutionary Computation (2019)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 56, "sentences": ["We are grateful to the reviewer for the comments.", "In this revision, we have corrected the minor typos, added additional comparisons, and added a proof map for easier navigation of the results.", "Specific comments are addressed below.", "1. Regarding exact recovery guarantees \u2014 NOODL converges geometrically to the true factors.", "Therefore, the error drops exponentially with iterations t. In other words, as t \u2014> infinity A_i \u2014> A^*_i for i in [1,m] and x_j \u2014> x^*_j for j in [1,m], where x_j is in R^m.", "We have added this clarification in Section 1.1.", "2. On tuning parameters \u2014 There are primarily three tuning parameters, namely eta_x (step-size for the IHT step), tau (threshold for the IHT step), and eta_A (step-size for the dictionary update step.) Our main result prescribes the theoretical values of these as shown in assumptions A.5 and A.6.", "Here, eta_x = Omega_tilde(k/sqrt(n)), tau = Omega_tilde(k\u02c62/n), and eta_A = Theta(m/k).", "We have updated A.6. to include the order of these parameters.", "The specific choices of these parameters, like other similar problems, depend on some a priori unknown parameters (e.g. the sparsity k, and the incoherence mu) which makes some level of tuning unavoidable.", "This is true for Arora '15 and Mairal '09, as well, where tuning is required for the choice of step-size for dictionary update, and for choice of regularization parameter and the step-size for coefficient estimation via FISTA.", "Note that, in our experiments we fix the step-size for FISTA as 1/L, where L is the estimate of the Lipschitz constant (since A is not known exactly).", "Alternately, since NOODL involves gradient-based updates for the coefficients and the dictionary, tuning (the step-sizes and the threshold) is relatively straightforward in practice, since it is based on a gradient descent strategy.", "In fact, to compile the experiments presented in this paper, we fixed step-size, eta_x, and threshold, tau, and tuned the step-size parameter eta_A only (Theta(m/k)).", "The choices of eta_A are 30 for k = 10,20 and eta_A = 15 for k =50,100, as shown in Fig.2.", ", eta_A mostly effects the convergence rate as long as it is chosen in Theta(m/k).", "Also, as shown in Table 4 (Appendix E), the tuning process for l1-based algorithms (i.e. FISTA) takes more time, since one needs to scan over the range of the regularization parameter to find one that works.", "This (a) adds to the computational time, and (b) since the dictionary is not known exactly, may guarantee recovery of coefficients only in terms of closeness in l2-norm sense, due to the error-in-variables (EIV) model for the dictionary.", "In this sense, NOODL is (a) simple to tune, (b) assures guaranteed recovery of both factors, and (c) is fast due to its geometric convergence properties.", "These factors highlight its applicability in practical DL problems.", "3. Definition of Hard Thresholding (HT) \u2014 As per the recommendation of the reviewer, we have repeated the definition of hard-thresholding (HT) initially presented in the \"Notation\" sub-section, in Section 2 for clarity.", "4. Comparison to other Online DL algorithms \u2014 As correctly observed by the reviewer, the overall structure of NOODL is similar to successful online DL algorithms.", "These successful algorithms (such as Mairal '09) leverage the progress made on both factors for convergence, however, do not guarantee recovery of the factors.", "On the other hand, the state-of-the-art provable DL algorithms focus on the progress made on only one of factors (the dictionary), and do not have good performance in practice, since they incur a non-negligible bias; see Section 5 and Appendix E.", "NOODL bridges the gap between these two.", "In addition to our main theoretical result, which establishes conditions for exact recovery of both factors at a geometric rate, NOODL also has superior empirical performance, leading to a neurally-plausible practical online DL algorithm with strong guarantees; see Section 3 and 4.", "Our work also paves way for the development and analysis of related alternating optimization-based techniques.", "On reviewer's recommendation, we compare the performance of NOODL with one of the most popular alternating minimization-based online DL algorithm used in practice -- Mairal `09 -- in Fig. 2 and Table 4 (Appendix E).", "In this work, the authors show that alternating between a l1-based sparse approximation step and dictionary update based on block co-ordinate descent converges to a stationary point.", "The other comparable techniques shown in Table 1, are not ``online\u2019\u2019 and/or require stringent initializations, in terms of closeness to the true dictionary, as compared to NOODL.", "Our experiments show that due to the geometric convergence to the true factors, NOODL outperforms competing state-of-the-art provable online DL techniques both in terms of overall computational time, and convergence performance.", "These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 57, "sentences": ["We thank our second reviewer for his comments.", "We first refer to your main comments and then answer each point in part.", "The work of Lakshminarayanan et al. indeed showed that deterministic ensembles can improve on the performance of MC-dropout techniques and provides a foundation for ours.", "And as Beluch et al. (2018) showed, this can be valuable in an active learning setting.", "However, our work differs in two major ways:", "i) We focus on showing the uncertainty representation in these methods suffer from overconfident predictions and that combining the two methods into a stochastic ensemble can be of great benefit and improve on the quality of the uncertainty.", "ii) We believe the true novelty to be in applying them in an active learning setting, and in particular on a small dataset problem (i.e. the size of the final dataset acquired during AL is only a small fraction of the entire available unlabelled dataset).", "As you mentioned, data is notoriously scarce and deep learning methods rarely work on small dataset problems.", "We thank the reviewer for pointing us to the work of Huand et al. Indeed this is an interesting method that would allow us to most likely achieve similar or better results with less computational overhead.", "This is definitely something we will consider for future work, but it is somehow out of the main scope of the paper, which was to show the power of combining MC-dropout with ensembles in the active learning setting.", "Taking into account more advanced ensemble methods is definitely of interest.", "In terms of the Bayesian Optimization literature, this is definitely of interest if we are to focus on hyper-parameter tuning for our models, but we fail to see the connection of the work you mentioned to our active learning examples.", "Our focus was not on fine-tuning our models.", "In relation to your specific points, we answer these below:", "1) Gal has already showed in his PhD thesis that MC-Dropout almost always performs best in terms of prediction accuracy and uncertainty quality assessment when compared to alternative Bayesian neural network approaches such as Probabilistic Back Prop and other variants of stochastic gradient MCMC methods.", "The aim of our paper was to improve upon MC-Dropout in the context of active learning, which would invariably translate into better performance w.r.t. other Bayesian NN approaches.", "2) Beluch et al. (2018) showed that going beyond 3 networks in their deterministic ensemble method does not add any significant improvements in terms of performance.", "Therefore, we used this number when benchmarking against their method.", "3) The aim of the paper was to improve upon the state-of-the-art in active learning for the image classification task.", "We specifically chose this task due to its relevance to the real world especially in the medical imaging industry.", "We agree that a more comprehensive study could be done in order to asses the viability of our method for ML tasks other than image classification.", "As for other neural network architectures, we chose the one used in the benchmarked methods.", "4) Results are averaged over 5 multiple independent runs.", "We will include both this and confidence scores in a revised version of our paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 58, "sentences": ["We thank the referee for finding our paper clearly written and in an interesting domain.", "1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.", "We have shown in our experiments (see Fig.\u00a03) that using a single RBM as a pre-processor will not result in increased adversarial resistance.", "A comparison of mean field training vs. constrastive divergence for RBMs has been made by [III,IV].", "Nevertheless, we have added a comparison between our method and the exact solution for a small Boltzmann machine of 16 units in the appendix.", "While we don't claim that this explains the performance of the method under all circumstances, it gives an indication of how well it works.", "We agree that a more thorough study of the training method would be desirable but in this article we are concentrating on reporting the results on increased adversarial resistance.", "We believe that these results should be interesting to the community even if some doubts about the training procedure remain.", "Lastly, we want to obtain a differentiable building block that can be used in standard neural nets.", "The unrolling of the mean field iterations (see Fig.\u00a01) provides a straightforward to achieve this.", "Propagating a gradient through a sampling based building block, while possible, would be considerably harder.", "2. In [III,IV] the authors have succesfully used a mean field approximation beyond the trivial first order to train RBMs.", "In [II] a first order approximation is used.", "In our approach we use the approximation up to fourth order in the coupling J.", "3. The approach of the authors in [III,IV] and also our approach is based on free energy.", "The systematic expansion of the free energy in the coupling constant forms the basis of our approach.", "4. D is calculated as the relative entropy over a batch of 10000 examples.", "The reference probabilities are taken to be uniform and the model probabilities are calculated according to the procedure outlined in section 2.", "Due to the approximations involved it is possible that the sum of the model probabilities exceeds one.", "In this case we rescale the unclamped free energy to limit the total probability to one.", "We have added a note to that effect to the article.", "5. We have included a comparison to the, to our knowledge, currently most adversarially resistant model on MNIST.", "Since our results are derived for MNIST we can only compare to methods in the literature that are evaluated on MNIST.", "We acknowledge that other methods perform very well on more sophisticated tasks and have added a reference.", "6. While we have not explicitely labelled white box and black box attacks we are using a strong white box attack (gradient based) and the, to our knowledge, strongest black box attack (boundary method).", "We have added the labels in the text and Table 1 to make this more clear.", "7. We have added some more attacks on the most robust model.", "We completely agree that more experiments on other datasets are needed to show the universality of the method.", "Due to the computational complexity this will need to remain a topic for a follow up article.", "[I] Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, Oncel Tuzel: Divide, Denoise, and Defend against Adversarial Attacks. arXiv 1802.06806 (2018).", "[II] Ruslan Salakhutdinov, Geoffrey Hinton: Deep Boltzmann Machines. Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, 448-455, (2009)", "[III] Pawe\u0142 Budzianowski. Training Restricted Boltzmann Machines Using High-Temperature Expansions. Master\u2019s thesis, University of Cambridge, 2016.", "[IV] Marylou Gabri\u00e9, Eric W. Tramel, and Florent Krzakala. Training restricted boltzmann machines via the Thouless-Anderson-Palmer free energy. CoRR, abs/1506.02914, 2015."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_reject-request", "rebuttal_future", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 59, "sentences": ["We would like to thank Reviewer 1 for their review and constructive suggestions.", "Our responses inline:", ">Can you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance?", "-The primary reason is to ensure that training is invariant to the per-device batch size.", "When scaling from resolution 128x128 to 256x256, we increase the number of devices but maintain the same overall batch size, reducing the per-device batch size.", "Cross-replica BatchNorm ensures that the smaller per-device batch size does not affect training.", "Switching to per-device BatchNorm at 128x128 results in a performance drop, albeit not a crippling one: for a model which would otherwise get an IS of 92 and an FID of 9.5, switching to per-device BatchNorm results in an IS of 78 and FID of 13.", ">It is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs.", "Providing such analysis would be also helpful for the community.", "-The goal of this work is to explore GANs at large scale; the exploration of small or medium scale models would indeed be interesting for another study.", "Having said that, we do evaluate BigGAN on conditional CIFAR-10 (mentioned briefly in Appendix C.2) and obtain an IS of 9.22 and an FID of 14.73 without truncation, which to our knowledge are better than any published results.", ">How do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?", "-Any of the proposed techniques could be applied to standard GANs for text or other sequential data in principle, but we have not experimented with these applications ourselves."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 60, "sentences": ["1. We apologize for typos and if any term is not defined at the appropriate places.", "We  fixed all the typos and define the abbreviation for IPM at the first occurrence.", "Please check the revision.", "P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3, is defined in the subsequent two sentences in the same paragraph.", "By the notation G_theta(u) \\sim p(theta), we mean that we want to train the generator G_theta such that when fed a random variable u \\sim p(u), the distribution of G_theta(u) matches that of p(theta).", "Sorry if it is confusing, but G_theta is not parameterized by theta, it just indicates that its the generator for theta. (Like G_x indicates that it is the generator for x).", "2. The training of G_theta is described in the subsection titled \u201cHierarchical Sampling\u201d.", "As correctly pointed out by the reviewer, that G_theta does not appear in the objective function (4).", "Using (4), we train G_x and Q networks.", "After training G_x and Q, we use trained Q to collect inferred Q(X), for each point cloud X.", "Then we train the generator G_theta using ordinary WGAN formulation to produce samples from same distribution as that of the samples Q(X) for each point cloud X. In addition to such two step training, a joint training also works, but is slower computationally, thus we report only the two step training in the paper.", "3. Quantitative evaluation of generative modeling performance is unfortunately very hard for real world problems like point clouds, which is the probable cause for it being missing from much of GAN literature.", "Thus, to provide some quantitative results for generation, we resorted to the toy problem.", "In the toy problem, we can accurately gauge the generation capabilities as can be seen from Figure 5.", "(We did not explicitly provide numbers like KL divergence, as it is evident from the Figure that PC-GAN would be significantly better than AAEs if we evaluate the numbers.) The same protocol can be extended for measuring the quality of the final hierarchical sampling.", "4. To showcase the effect of varying s, we chose the reasonable sized ModelNet10 dataset and ran for s=0, s=1, and three values s_1<s_2<s_3 in between.", "The results are as follows:", "D2F (Distance to Face)       Coverage", "s=0                        6.03E+00                     3.36E-01", "s1", "6.06E+00                     3.41E-01", "s2", "5.77E+00                     3.47E-01", "s3", "6.85E+00                     3.56E-01", "s=1", "9.19E+00                     3.67E-01", "4. Yes the model nicely captures simple topological features of the object, like presence of holes versus being one solid object. Even in the latent space, objects with hole group together."], "labels": ["rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_answer_label"]}
{"abstract_id": 61, "sentences": ["We thank the reviewer for the valuable feedback! We reply to the answers and comments in the order they were raised.", "Note, that regarding the methodology there were some misunderstandings (which we try to avoid for future readers in the revised version).", "(1) The equations are indeed very related.", "Note however, that In a standard Bayesian neural network (BNN), one would assume that \\theta is a global random variable (i.e. does not depend on input x), whereas in the CDN, we assume that \\theta depends on x and is thus a local random variable.", "Furthermore, in a Bayesian setting p(theta|...) would play the role of a approximate posterior, which would require variational inference (VI), and thus a different objective,  to estimate it.", "(2) In Equation (4) we followed with  p(D | \\psi) a standard notation for \\sum_n p(y_n | x_n; \\psi) (i.e. summation of Equation (3) wrt all data in D) which also can be found e.g. in the work of Graves (2011) [4] and Blundell et al. (2015) [5].", "The objective we introduce for CDNs differs from the ELBO-based objective in VI in the way the logarithm is placed in the first term of the objective: in the ELBO we have a logarithm inside the expectation, while the logarithm is outside the expectation in the CDN objective (note however, that the sample-based approximations get equivalent if only one sample is used).", "Furthermore, in the ELBO we have a fixed value of \\lambda = 1.", "We added a new Section 4 in the revised version of the paper discussing these differences.", "Moreover, we investigated the impact of the different objectives empirically and found that the CDN-based objective led to significantly better results, as shown in the newly added Section 6.4 in the revised manuscript.", "(3) Indeed we need the probabilistic version of hypernetworks to implement the model we described in Equation (3).", "We just wanted to point out that this is in contrast to the vanilla  hypernetworks proposed by Ha et al. (2016) [1] and Jia et al.", "(2016) [2] which would produce a point estimate for \\theta.", "(4) We used a matrix-variate normal (MVN) to reduce the parameters of the model.", "Using a diagonal MVN for X \\in R^{p x q} one needs pq+p+q parameters.", "In contrast a fully-factorized diagonal Gaussian needs pq+pq.", "But you are right, we could easily extend our approach to account for more flexible distributions by using a \"diagonal plus rank-one\" structure diag(a)+uu^T, with vectors a and u, as noted by Louizos and Welling ( 2016) [3] (the increase of parameters is negligible: adding additional vector u).", "We will investigate the benefits of more flexible mixing distributions in future work.", "(5) Thanks for this valuable comment!", "We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).", "It shows that CDNs are able to quantify the heteroscedastic aleatoric uncertainty.", "However, we kept the OOD experiments on MNIST and notMNIST in the paper as well, since we consider it as very interesting that the CDN, while being designed for modelling aleatoric uncertainty, is very competitive on this task.", "Moreover, we investigate the mixing distribution learned in Appendix G.", "(6) Of course, you are right! Previously we showed the test accuracy in Appendix F.  To make it more directly accessible, we have now added the test accuracy achieved by the different models into the legends of the plots, showing that CDN achieves similar predictive power as the baselines.", "We now present the validation accuracy instead in Appendix F.", "(7) Sorry, for this unfortunate formulation!", "We observed that generally as \\lambda increases, the uncertainty is increasing, while the accuracy is decreasing.", "Therefore a simple and effective heuristic for choosing \\lambda is to look at the validation set of MNIST and choose the highest \\lambda that still results in high accuracy (e.g. >. 0.97).", "We have made this procedure clear in the revised manuscript.", "References:", "[1] Ha, David, Andrew Dai, and Quoc V. Le. \"Hypernetworks.\" arXiv preprint arXiv:1609.09106 (2016).", "[2] Jia, Xu, et al. \"Dynamic filter networks.\" Advances in Neural Information Processing Systems. 2016.", "[3] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016.", "[4] Graves, A. (2011). Practical variational inference for neural networks. In Advances in neural information processing systems (pp. 2348-2356).", "[5] Blundell, C., Cornebise, J., Kavukcuoglu, K. & Wierstra, D.. (2015). Weight Uncertainty in Neural Network. Proceedings of the 32nd International Conference on Machine Learning, in PMLR 37:1613-1622"], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 62, "sentences": ["We thank the reviewer for a detailed review.", "We agree with the reviewer that a uniqueness result based on the axioms is desirable, but we don\u2019t have it.", "While we\u2019re able to show that the paths at the input and at the hidden layer must be coupled (i.e. non-oblivious), we just don\u2019t understand the space of non-oblivious methods that well.", "Mathematically, we don\u2019t have a handle on how the path at the hidden layer can vary as the network below the hidden layer is changed.", "Partition consistency is the only axiom about the network below the hidden layer, but it is not applicable to all networks.", "We probably need another axiom to prove uniqueness.", "Another key observation made by the reviewer is the interpretability vs importance of neurons.", "While those are not the same, we demonstrate that conductance can give us some insights about the network (Sections 5.1 and 6.1)."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 63, "sentences": ["Regarding comment 1: Thanks for recognizing the merit of our idea.", "As also mentioned by Reviewer #2, the proposed MAD implicitly assumes that classifiers in the competition are reasonably accurate.", "Otherwise, the selected counterexamples may be less meaningful.", "We will make this assumption explicit in the revised manuscript.", "From this perspective (and other reasons mentioned in the discussion section), MAD should be viewed as complementary to, rather than a replacement for, the conventional accuracy comparison for image classification.", "When two classifiers perform at a reasonable level and achieve very close accuracy numbers (e.g., VGG16BN and ResNet34 on ImageNet validation set), MAD provides the most efficient way of differentiating the two models by maximizing their discrepancies over a large-scale image set.", "We want to emphasize that MAD is especially useful on image classification tasks where most cutting-edge classifiers achieve very close performance.", "In these situations, the MAD competition ranking, which is obtained by evaluating on corner examples searched from web-scale unlabeled dataset, is more convincing than something like 1% accuracy advantage on the validation set.", "For problem domains where there are few sufficiently accurate models, we may still apply the underlying principle behind MAD to create adaptive test sets such that the strengths and weaknesses of the models are most easily revealed.", "In those scenarios, we conjecture that we need increase k to a reasonably larger number, thus at the cost of efficiency.", "Regarding comment 2: Thanks for the comment.", "As long as two (or multiple) models differ (even in slightly different ways), MAD provides the highly efficient way of spotting such differences by exploring a large-scale unlabelled dataset.", "However, these differences are less likely to be revealed using a fixed and small test set (i.e., they will probably have the same accuracy numbers as models to be compared are very similar and are biased in similar ways).", "For the extreme case that two models are exactly the same (i.e, they are biased in identical ways and make identical prediction errors), both MAD and traditional accuracy-based methods will draw the same conclusion - the two models have the same performance.", "Accuracy-based evaluation methods arrive at this conclusion by comparing model predictions with ground truth labels and outputting the same accuracy numbers.", "In contrast, MAD arrives at the same conclusion without any human labeling since the set S for subjective testing is empty.", "So in this extreme case, both MAD and accuracy fail to compare those two models.", "In summary, to reliably compare the relative performance of computational models, all evaluation methodologies (including MAD) rely on the assumption that the models to be compared should be diverse to a certain extent, and the proposed MAD makes this assumption more explicit.", "In fact, MAD makes the best use of model discrepancies (even if models are biased in very similar but not identical ways) to rank the model performance.", "As a matter of fact, based on our experiments, we find that state-of-the-art ImageNet classifiers do have their own biases. (See figure 8 in the appendix.)", "Regarding comment 3: Thanks for the excellent question.", "We believe that the parameter k is task-dependent.", "For problem domains where there are reasonably accurate models (e.g., imageNet classification in our example), we may obtain a stable ranking with a relative small k (e.g., k=15 in the imageNet classification example).", "For problem domains where there are no good models, we may increase k to the limit of human labeling budget in order to obtain reasonable performance comparison.", "Regarding comment 4: Thanks for the comment.", "In our current setting, we restrict the dataset D to the domain of interest that contain natural images of mainly 200 classes.", "However, as the construction of D is noisy and coarse, D contains plenty of open-set images, which do not belong to any class of interest.", "Since we do not perform any manually data screening at this stage, some of the open-set images may even be selected to construct the dataset S.", "This means that although the selected open-set image is out of the domain of interest, the associated two classifiers make different, high-confident (with threshold set to 0.8), but incorrect predictions (Case III).", "As a result, we consider it as a strong counterexample of the two classifiers.", "Note that this situation rarely happens, at least in our experiments because the competing classifiers tend to give open-set images low-confidence, and therefore are automatically filtered out.", "We agree with the reviewer that selecting images that contain only one salient object requires a lot of human effort.", "So we did not eliminate Case I. It turns out that keeping Case I does not seem to affect the comparison and analysis of competing models."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label"]}
{"abstract_id": 64, "sentences": ["Thank you very much for your constructive comments.", "First, with respect to baselines, we have updated the results tables to include the additional baselines mentioned, as well as runs for VAT(+EntMin) with lower numbers of labels on CIFAR-10.", "After updating these baselines, we note that our method still achieves state-of-the-art performance in the regime where 1000 and 2000 labels are used for training on CIFAR-10, with and without data augmentation.", "We have also updated the text to tone down the claims.", "In addition, we note that the highest performance in many of the mentioned baselines (and with VAT) are obtained with a combination of multiple approaches.", "When our method is compared head-to-head against the proposed method in the mentioned papers, it is competitive and sometimes outperforms them, for instance, in experiments on CIFAR-10 with 4000 labels", "With augmentation:", "Adversarial Dropout [1] (11.32) vs ours (11.79 +/- 0.25)", "Without augmentation:", "Improved GAN + SNTG [2] (14.93) vs ours (14.34 +/- 0.17)", "Defining the best combination of techniques to achieve the highest performance is an interesting direction of future work; our preliminary experiments combining Mean Teacher with manifold regularization have shown some improvements and we will include the results in the final version of the paper.", "Second, with respect to novelty, we would like to re-iterate our contributions since they may not have been clear.", "First, while manifold regularization has been explored in (Kumar et al 2017) and (Qi et al 2018), we proposed an efficient and effective approximation of manifold regularization that is far easier to compute than the involved method in (Kumar et al 2017).", "Moreover, we point out issues with the standard finite difference approximation to the Jacobian regularization and propose a solution to this problem by ignoring the magnitude of the gradient and using only the direction information.", "Moreover, we showed manifold regularization provides significant improvements to image quality and linked it to gradient penalties used for stabilizing GAN training, which were not shown by (Qi et al 2018).", "We did try to use spectral normalization but did not observe any gains for semi-supervised learning.", "Finally we would like to emphasize the conceptual differences between our method and other smoothing methods like spectral normalization - such methods perform isotropic regularization, whilst ours performs anisotropic smoothing along the manifold directions of generated data-points.", "We showed through experiments using (isotropic) ambient regularization that anisotropic regularization is more beneficial in the case of semi-supervised learning."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 65, "sentences": ["We would like to thank Reviewer 2 for their review and constructive suggestions.", "Our responses inline:", ">Discussions sometimes lack depth or are absent.", "-We have added an additional section (Appendix G) expanding on our discussion and providing additional insight into the observed instabilities.", ">For example, it is unclear to me why some larger models are not amenable to truncation.", "Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?", "-Truncation introduces a train-test disparity in G\u2019s inputs--at sampling time, G is given a distribution it has effectively never seen in training.", "The observation that imposing orthogonality constraints improves amenability to truncation is empirical.", "Our suspicion is that if G is not encouraged to be \u201csmooth\u201d in some sense, then it is likely that G will only properly generate images given points from the untruncated distribution.", "We hypothesize that models which are not amenable end up learning mappings which, when given truncated noise, either attenuate or amplify certain activation pathways, leading to extreme output values (hence the observed saturation artifacts).", "We speculate that encouraging G\u2019s filters to have minimum pairwise cosine similarity means that, when exposed to distribution shift, the network\u2019s features are less correlated and less likely to align and amplify an activation path it would otherwise have learned to scale properly.", ">Were samples from those networks better without using truncation? Why would this be?", "-Samples from those networks without truncation do not have measurably different quality, and their training metrics (losses, singular values) show no differences.", "Aside from empirically testing each network individually for amenability to truncation, we found no other way to check for that amenability.", "> Authors report how wider networks perform best, and how deeper networks degrade performance.", "Again, discussions are lacking, and it doesn\u2019t seem the authors tried to understand why such behaviors were shown.", "Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.", "-We are wary of explanations for which we do not have evidence.", "For each of the modifications introduced in Section 3, we offer a succinct conjecture as to why that change improves performance, but we are not aware of any existing reliable, informative metric which we could employ to understand or trace the source of each observed behavior, particularly with respect to GAN stability or performance.", "Regarding depth vs width: This paper is empirical, and we only briefly experimented with increasing depth analogously to increasing width.", "While increasing width provided an immediate measurable benefit, increasing depth did not.", "We felt that it was better to report the results of this brief investigation than to omit it for a lack of investigatory depth.", "> In Section 3.1 : \u201cAcross runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.\u201d For me, this is not particularly clear.", "Is this something the reader should understand from Table 1?", "-This means that of all the models we trained for the study presented in Table 1 which did not use Orthogonal Regularization, only 16% were amenable to truncation.", "Of all the models which we trained for the study presented in Table 2 which did use Orthogonal Regularization, 60% were amenable to truncation.", "This is not reflected in Table 1, which is merely a presentation of how the introduced modifications impact performance.", ">I question the choice of sections chosen to be in the main paper/appendices.", "I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.", "However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.", "In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.", "However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.", "With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.", "-We appreciate this suggestion.", "While we recognize that this paper generally has a strong focus on implementation details, we felt that this instability was one of the most salient behaviors we observed, and that future work would be best served by presenting our investigations and attempts to understand its source, even if these methods did not improve performance.", "The information in Appendix B and C is intended to be of interest to those who want to reproduce our experiments, so it largely comprises hyperparameters and architectural details that we felt were not necessary to understand the main results of the paper.", ">In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says \u201closses\u201d.", "-Thanks! This was indeed an error, which we\u2019ve corrected in the updated draft.", ">I would also be curious to see the proposed techniques applied on simpler datasets. Can this be useful for someone having less compute power and working on something similar to CelebA?", "-The goal of this work is to explore GANs at large scale; the exploration of small or medium scale models would indeed be interesting for another study.", "Having said that, we do evaluate BigGAN on conditional CIFAR-10 (mentioned briefly in Appendix C.2) and obtain an IS of 9.22 and an FID of 14.73 without truncation, which to our knowledge are better than any published results."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 66, "sentences": ["Thank you for your appreciation of our work!"], "labels": ["rebuttal_social"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 67, "sentences": ["We thank the reviewer for their helpful comments.", "We agree that our most surprising results are for SGD under constant step budgets or unlimited epoch budgets.", "However the behaviour of SGD under constant epoch budgets has generated a lot of debate in the literature in recent years, and we felt it was important to address this simple case first.", "We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:", "1. Ma, Bassily and Belkin also introduced the notion of two regimes, however their theory holds for convex losses in the interpolating regime.", "We will discuss their contribution explicitly in the updated text.", "Our discussion in section 2 clarifies why the two regimes arise in practical deep learning models for which these conditions may not hold.", "2.", "Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).", "As we show in later sections, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.", "A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.", "To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).", "3. We clarify the differences to some other recent papers in our reply to reviewer 1.", "Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.", "We apologise for this.", "It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.", "Turning to our generalization experiments in sections 4 and 5.", "We agree that many authors have proposed that SGD noise enhances generalization.", "Most notably, Keskar et al. argued that large minibatches perform worse than small minibatches on the test set, even when both achieve similar performance on the training set.", "However their experiments do not provide convincing evidence for this claim, because they tuned the learning rate with small batches and then used the same learning rate value with large batches.", "A convincing experiment should independently tune the learning rate at all batch sizes under a constant step budget, and it should use a realistic learning rate decay schedule.", "Indeed, Shallue et al. recently argued that no existing paper has provided convincing evidence that small batch sizes generalize better than large batch sizes under constant step budgets, and they state in their abstract \u2018We find no evidence that larger batch sizes degrade out-of-sample performance\u2019.", "Meanwhile, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets.", "To our knowledge, our experimental results in section 4 are the first to provide convincing evidence that very large minibatches do perform worse than small batch sizes on the test set, even under constant step budgets and when the learning rate is independently tuned.", "We believe this is an important contribution.", "Meanwhile, our results in section 5 suggest that SGD has an optimal temperature early in training which promotes generalization and is independent of the epoch budget.", "In response to the reviewer\u2019s specific comments:", "1) Looking at Figure 1c, while the optimal learning rate at 8k with Momentum is 4, the error bars at this batch size range from 4 to 32.", "These error bars can be very large in the curvature regime, precisely because the optimal learning rate is close to instability.", "2) Yes, Momentum will help under constant step budgets if the batch size is large, since it enables us to achieve larger effective learning rates which are beneficial for generalization.", "We will add additional experiments to the text to clarify this.", "3) We will clarify the meaning of warm up, epoch budget and step budget as requested."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_by-cr_label"]}
{"abstract_id": 68, "sentences": ["We very much appreciate your valuable comments, efforts and times on our paper.", "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "Q1. More related works", "We updated the introduction by including more recent works [1, 2, 3, 4, 5] related to deep learning with noisy labels.", "In the previous draft, we only included the relevant literature which involves a single network/classifier.", "The updated related works utilize multiple networks, e.g., an ensemble of classifiers or meta-learning model.", "We also added new experimental results for them in Table 4 of the revised draft, as we mentioned in our common response to all reviewers.", "Thank you very much for the suggestions.", "Q2. Comparison with VAT [6].", "We remark that a targeted setting of VAT [6] is different from ours in that it is designed for improving the performance on semi-supervised learning, while our main goal is handling noisy labels in the training dataset.", "Due to this, we skip the comparison with VAT.", "Instead, as we mentioned in our common response to all reviewers, we consider more training baselines (such as MentorNet [2] and Co-teaching [3]) focusing on handling noisy labels, and show that our inference method can improve all of them.", "Q3. L-FBGS adversarial attacks [8].", "We remark that L-FBGS [8] is known to fail easily due to the near-zero gradient of loss function [7].", "Instead, we consider CW attack [7] which is known to be much stronger.", "[1] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.", "[2] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing very deep neural networks on corrupted labels. In ICML, 2018.", "[3] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: robust training deep neural networks with extremely noisy labels. In NIPS, 2018.", "[4] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.", "[5] Eran Malach and Shai Shalev-Shwartz. Decoupling\u201d when to update\u201d from\u201d how to update\u201d. In NIPS, 2017.", "[6] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.", "[7] C. Nicholas and W. David. Towards evaluating the robustness of neural networks. In IEEE Symposium on SP, 2017.", "[8] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2013.", "Thanks a lot,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 69, "sentences": ["We thank the reviewer for the valuable review comments and suggestions! Please find our point-by-point response as follows.", "Q1: Is there only 1 adversarial party?", "A1: There is only one adversarial party in centralized attack.", "But we make sure that the total injected triggers (e.g., modified pixels) of DBA attackers is close to and even less than that of the centralized attacker.", "We stressed this setup in Section 3.2.", "That is, the ratio of the global trigger of DBA pixels to the centralized is 0.992 for LOAN, 0.964 for MNIST, 0.990 for CIFAR and 0.991 for Tiny-imagenet.", "Q2: What\u2019s the result for centralized attacks with the same number of scaling times as DBA, but each update includes 1/4 number of poisoning samples?", "A2: Following your suggestion, we conducted two sets of new experiments.", "1. Change the poison ratio into 1/4: We decrease the fraction of backdoored samples added per training batch into 1/4.", "2. Change the data size into 1/4: We divide the local dataset into 4 parts and use 1/4 dataset for each update and keep the poison ratio unchanged.", "We have included the results and discussion in Appendix A.4 of the revised version.", "Q3: If the decomposition is also useful for trigger patterns that are not necessarily regular shapes?", "A3: It\u2019s also useful for irregular shape triggers.", "1. We study the irregular pixel logo \u2018ICLR\u2019 for three image datasets.", "Specifically, we use \u2018ICLR\u2019 as the global trigger pattern and decompose it into \u2018I\u2019, \u2018C\u2019, \u2018L\u2019, \u2018R\u2019 for local triggers.", "2. We also use the physical trigger glasses (Chen et al.,2017) on Tiny-imagenet and decomposed the pattern into four parts.", "The results are in Appendix A.3 of our revised version.", "DBA is also more effective and this conclusion is consistent in different colors of glasses.", "Q4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.", "A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;", "We have also provided more details about LOAN dataset and how we attack in Appendix A.1."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 70, "sentences": ["We thank the reviewer for their positive evaluation of our study.", "We agree that the prediction from CSLB features is particularly interesting, and we are currently working on improving this further by interpolating to other objects in a semi-supervised manner (similar to what was proposed by the reviewer).", "We also strongly agree that testing additional embeddings would be very interesting!", "For the present work, we focused on synset embeddings because they represent a closer match to the meaning of each individual object than word embeddings would and provide a one-to-one match for the meanings.", "For example, our list contains four different meanings for the object named by the word \u201cbaton\u201d, referring to (1) an item in relay races, (2) in twirling, (3) a weapon used by police, and (4) an item used by a musical conductor.", "Due to the novelty of this line of research, to our knowledge there are no other synset embeddings available than the ones we used, and we included both a 50d dense and a 300d dense version.", "In addition, we would have liked to include sparse positive synset embeddings as a reference, however those are currently not available; for that reason, we included NNSE word embeddings instead.", "In the future, we would like to add sparse positive synset embeddings and test their interpretability relative to our similarity embedding.", "We hope this will underline the unique contribution of a behavior-based similarity embedding presented here.", "In addition, we would like to thank the reviewer for their idea on how to extend the embedding.", "Indeed, we are currently working on predicting similarities for other concepts and images from pretrained synset vectors and activations in deep convolutional neural networks.", "However, this effort is still in its early stages and beyond the scope of the present work."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 71, "sentences": ["We are thankful for the reviewer 2 to provide us with his/her feedback,", "The reviewer mentioned: \"the interpolation mechanism is also too simple\":", "We would like to highlight that despite the simplicity of interpolated samples, there has been demonstrated the effectiveness of using such samples on developing more regularized and generalized neural networks (Zhang et al, 2018) as well as on making them more secure", "(Zhao et. al. 2018)", ".", "Thus, we believe that simplicity does not necessarily lead to ineffectiveness.", "The reviewer mentioned \u201cmany hidden assumptions on the images source or the base classier\u201d:", "As this statement is not clear for us, we would appreciate if the reviewer could elaborate more on it.", "The only assumption we made is on the fact that the out-distribution samples should be statistically and semantically different than the in-distribution samples.", "Then among such out-distribution sets, we propose a measurement for identifying the most representative one among those available.", "The reviewer stated \"There exists more principled approaches for *selecting* out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors\":", "While there are the approaches that aim to detect out-distribution sets, they have not been designed for the selection purposes as we do.", "By mentioning this statement, if the reviewer means the missing of some principled approaches like ODIN in our comparisons, we would like to inform the inclusion of ODIN results in the revised version of the paper.", "Reference:", "- Zhao, Jake, and Kyunghyun Cho. \"Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples.\" arXiv preprint arXiv:1802.09502 (2018).", "- Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017).", "mixup: Beyond empirical risk minimization.", "ICLR 2018 (arXiv preprint arXiv:1710.09412)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_reject-criticism_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 72, "sentences": ["We thank the reviewer for the effort, however we believe there is a mis-understanding.", "As for the synthetic curves experiment, we updated the paper with a justification.", "This task, while simple, showcases the ability of Transformer to model a distribution over curves of similar shape to real training curves with varying speeds of convergence.", "It has been designed so it is easy to quantify the diversity of generated curves and the fit between the distribution generated by the model and the real one.", "Furthermore, we included two additional tasks, attesting to the ability of Transformer to model a wide range of distributions over training curves.", "We also updated the citation of the paper you mentioned with an arxiv URL.", "We still believe that while focusing on the synthetic task the reviewer might have missed the main point of the paper, namely that time-series forecasting with Transformer works really well, at least in the context of modeling deep learning dynamics.", "The general problem has been studied in the community for many decades and we believe that we made significant progress, so we kindly encourage the reviewer to reconsider their assessment of our contributions."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 73, "sentences": ["We thank Reviewer 1 (R1) for their review and for asking interesting questions that helped us to understand where our paper may have been unclear.", "In our response below we will try our best to better explain our motivation for building and using SQOOP, as well as address R1\u2019s other questions and concerns.", "A key concern that R1 expressed in their review is that we perform our study on the new SQOOP dataset, instead of using an available one (for example CLEVR or Abstract Scenes VQA).", "Though we appreciate the concern (it has spurred us to rethink and rephrase how we justify SQOOP) we still believe that the SQOOP dataset is the best choice for precisely testing our ideas.", "We kindly invite R1 to consider the following arguments in favor of doing so:", "The goal of our study was to perform a thorough investigation of systematic generalization of language understanding models.", "To that end, we wanted a setup that is as simple as possible, while still being challenging by testing the ability to extend the relational reasoning learned to unseen combinations of seen words.", "We therefore choose to focus on simplest relational questions of the form XRY, as they also allow us to factor out challenges of discrete optimization in choosing the right module layout (required for Stochastic N2NMN).", "The simplicity is also useful because most models get to 100% accuracy on the training set of SQOOP, which allowed us to put aside any remaining optimization challenges and just focus our study on systematic generalization.", "In contrast, we find that the popular CLEVR dataset does not satisfy our requirements and if we did modify it sufficiently, we believe that it would only differ from SQOOP in the actual rendering and would not affect our conclusions.", "Though visually more complex, CLEVR has only 3 object types: cylinder, sphere and cube.", "Therefore, it would only allow for 3x4x3=36 different XRY relational questions.", "This is arguably not enough to sufficiently represent real world situations, and would definitely hinder our experiments.", "Specifically, we would not be able to sufficiently vary the difficulty of our generalization challenge when allowing 1,2,4,8 or 18 possible right hand-side objects in the questions (we clarify why splits with lower #rhs/lhs are more difficult than those with higher #rhs/lhs later in this response).", "Hence, we did not find the original CLEVR readily appropriate for our study.", "We could, in theory, introduce new object types to CLEVR and rerender a new dataset in 3D using Blender (the renderer that was used to create CLEVR) with different lighting conditions and partial occlusions.", "Though enticing, we believe that such a 3D version of SQOOP would lead to exactly same conclusions, because the vision required to recognize the objects in the scene would still be rather trivial.", "The Ying and Yang dataset is clearly a valuable resource (and we thank the reviewer for the pointer), but we do not think it is readily suitable for the kind of study that we aim to perform.", "The dataset, to the best of our understanding, uses crowd-sourced questions (as the questions are taken from Abstract VQA dataset, whose captions were entered by a human, according to the original VQA paper https://arxiv.org/pdf/1505.00468v6.pdf).", "Using crowd-sourced questions would not allow us to control our experiments at the level of precision that we wanted to achieve (e.g. we would not know the ground-truth layouts, it would be harder to construct splits of varying difficulty, etc.).", "As well, Abstract VQA contains only 50k scenes, and from our experience with SQOOP we know that this number would be not sufficient to rule out overfitting to training images as a factor.", "We thank R1 for their constructive suggestion to consider NMNs that form a DAG.", "We are currently investigating a chain-structured NMN with shortcuts from the output of the stem to each of the modules, and we will soon report these additional results in the upcoming revision of the paper.", "We hope that these results, combined with further qualitative investigations we are conducting, will answer the legitimate question of R1 as to why Chain-NMN performs so much worse than Tree-NMN.", "We acknowledge that the text of the paper can be improved to explain better why splits with lower #rhs/lhs are generally harder than those with higher #rhs/lhs, and we thank R1 for pointing this", "out", ".", "Our reasoning is that lower #rhs/lhs are harder because the training admits more spurious solutions in them.", "In such spurious regimes models adapt to the specific lhs-rhs combinations from the training and can not generalize to unseen lhs-rhs combinations (i.e. generalizing from questions about \u201cA\u201d in relation with \u201cB\u201d to \u201cA\u201d in relation to \u201cD\u201d (as in #rhs/lhs=1) is more difficult than generalizing from questions about \u201cA\u201d in relation to \u201cB\u201d and \u201cC\u201d to the same \u201cA\u201d in relation to \u201cD\u201d (as in #rhs/lhs=2).", "We will update the paper to be more explicit in explaining these considerations.", "We would like to conclude our response by replying to the higher-level concern of R1 that the findings of our study may not \u201cgeneralize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more\u201d.", "While we fully agree that more complex datasets with more complex questions would bring new challenges, these are ones we purposely put aside (such as the general unavailability of ground-truth layouts for vanilla NMN, the need to consider an exponentially large set of possible layouts for Stochastic N2NMN, etc.) We believe that it is highly valuable for the research community to know what happens in the simple ideal case of SQOOP, where we can precisely test our specific generalization criterion.", "This knowledge (e.g. the superiority of trees to chains, the sensitivity of layout induction to initialization, the emergence of spurious parameterization in end-to-end learning), will guide researchers in choosing, designing and troubleshooting their models, as they now know what to expect modulo the optimization challenges that they may face.", "The field of language understanding with deep learning is not easily amenable to mathematical theoretical investigations and, with that in mind, rigorous minimalistic studies like ours are arguably very important.", "To some extent, they play the role of the former: they inform researcher intuition and lay a solid foundation for scientific dialogue.", "We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.", "We believe that the total of our results makes a complete conference paper.", "All that said, we would welcome specific suggestions of additional experiments that we could carry out in order to better validate our claims.", "We hope that this response has clarified to R1 what our paper was insufficiently clear about. A new revision with additional experiments and fixed typos will soon be uploaded to OpenReview, and we hope that R1 takes this response and the changes that we will make to the paper into account."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_by-cr_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 74, "sentences": ["We thank the reviewer for the evaluation.", "Please see our detailed response to several recurring issues at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X.", "In that response we address the following issues:", "(1) We emphasize fundamental differences between Cakewalk and CE.", "(2) How the sampling distribution should not be considered as a part of Cakewalk, and that it is mostly provided as an example, and a basis for the reported experiments.", "(3) The experiments include results two tasks.", "Nonetheless, it appears the paper doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "Next, we\u2019ll try to provide some intuition as to why a sampling distribution that assumes independence between the different dimensions can be useful in some cases.", "The simple explanation is that in some problems the conditional expectation of the objective given that some x_i=j is much better than for other values x_i=k. In such cases, for each dimension the algorithm will tend to sample values which are useful to many possible solutions.", "In the clique problem for example, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.", "In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.", "Lastly, we note that these kind of factorized distributions have a long history of being useful  in machine learning.", "In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.", "In different contexts, such distributions have also been used as naive mean field approximations in variational inference."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 75, "sentences": ["We thank the reviewer for the feedback.", "In the following, we address the comments individually:", "-  Comparison with conventional triplet methods using images and their corresponding RGB images", "We did not consider comparisons with conventional triplet approaches: the message of our paper was not to demonstrate the utility of ordinal embedding approaches over conventional (representation-based) triplet approaches.", "It was rather to show that when input representations are NOT available, we provide a scalable approach to solve the ordinal embedding problem.", "We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.", "-  More synthetic experiments comparing the various ordinal embedding approaches", "There is a large literature that compares existing ordinal embedding approaches, and in order to not overload the figures, we had decided to just compare against the most popular traditional algorithms. But we can definitely add more comparisons in the revision of the paper.", "-", "The \u201cclaim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems\u201d", "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 76, "sentences": ["We thank you for your constructive feedback.", "We reply to all reviews in a general response above."], "labels": ["rebuttal_social", "rebuttal_structuring"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label"]}
{"abstract_id": 77, "sentences": ["Thank you for the instructive review!", "Our algorithm 1 minimizes the empirical learnable noise risk (Eq. 4), which does not assume that X_{t-1}^{(j)} follows a diagonal gaussian distribution.", "Originally, to justify the I^u=1/2 \\sum_l log(1+Var(X^(j)_{t-1,l})/ \\eta_{j,l}^2) term used in our experiments for estimating mutual information, we used diagonal Gaussian assumption for X_{t-1}^(j) in the experiment.", "In fact, a better way to justify this is to note that I^u provides an upper bound for the mutual information subject to the constraint of known variance of marginal distributions of X^(j)_{t-1}, and the upper bound is reached with the diagonal Gaussian distribution, as is proved in Appendix C in the revision.", "Therefore, the assumption of diagonal Gaussian assumption is dropped for the experiments in the revision.", "Practitioners can choose to optimize an upper bound of the learnable noise risk for better efficiency (as is also used in the experiments in this paper), or use differentiable estimate of mutual information for better accuracy, as has also been pointed out in the paper.", "In the revision, we have also added a more detailed comparison with other methods in sections 4.2 and 4.3, showing the strength of our method.", "For example, in section 4.2, our method correctly identifies important causal arrows, while the four other comparison methods either have more false positives and false negatives, or completely fail to discover causal arrows.", "In section 4.3", ", we compare with the results in previous literature.", "We note that although all compared methods correctly identify the causal relations, our method have the advantage that the inferred causal strength does not decay with increasing history length (we also analyzed that in the original submission)."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 78, "sentences": ["Regarding your first concern on the comparison with CW: In short, MarginAttack is able to achieve a higher attack success rate than CW AND a shorter running time.", "The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference.", "To show this point clearly, we would like to refer you to the results in our response to reviewer 3, where we scanned through the number of binary search steps and measure the success rate and running time.", "As can be seen, MarginAttack has a higher success rate than all the versions of CW.", "There is a success-rate-efficiency tradeoff in CW, as a smaller binary search step number leads to a lower success rate.", "However, even with 10 binary search steps, CW is still unable to outperform MarginAttack in terms of success rate.", "On the other hand, with very small numbers of binary search steps, CW still runs slower than MarginAttack.", "Hope these results will clarify your major concern.", "Regarding your minor concern:", "In the theorem, we did not assume convexity.", "The assumption with the name 'convexity' is saying that the constraint set should not be 'too concave'.", "Please check the following figure where we listed what decision boundaries are permitted by our theorem and what not.", "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf", "As can be seen, the convexity assumption permits a wide variety of decision boundaries.", "Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does.", "In this case, the critical point becomes a local maximum rather than a local minimum."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 79, "sentences": ["Thank you for the review and accurate summary of our submission!", "> I am reluctant to give a higher score due to its incremental contribution.", "Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.", "SVG clearly differs from Dreamer in that it only considers 1-step model predictions in SVG(1) or multi-step predictions without value function in SVG(\u221e).", "SVG(0) does not use a dynamics model.", "In addition, Dreamer propagates gradients through transitions in a learned features, making it effective for high-dimensional control tasks.", "> Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.", "Besides the important technical difference described above, we highlight the empirical performance of Dreamer.", "A conclusion of the SVG paper was that the model did not yield substantial practical benefits beyond 1-step predictions.", "We found it important to revisit this topic in the light of recent substantial improvements to dynamics models (see below).", "> Effectiveness on very long horizon trajectories: Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.", "This is an open issue in model-based RL.", "While current dynamics models still cannot accurately predict full episodes, this is rarely needed in practice.", "Recent works successfully use learned dynamics for control from both proprioceptive inputs (Chua et al. 2018, Shyam et al. 2019, Wang & Ba 2019) and from images (Hafner et al. 2019, Zhang et al. 2019).", "Dreamer shows that the relatively short model predictions (H=20) yield high-quality policy gradients, and that an additional value function in the latent space is effective for solving tasks that require longer-term credit assignment (e.g. with sparse rewards).", "Our experiments provide evidence that combination is effective in practice.", "> However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).", "This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).", "I think this point should be discussed in the paper.", "That is, the issue still exists, and Dreamer is less effective with very long horizon.", "We address the challenge of long horizons not using long-term model predictions but by learning a value function that estimates the infinite sum of discounted future rewards.", "Figure 4 in our submission shows that this gives Dreamer robustness to the imagination horizon compared to two baselines.", "> Inapplicability to discrete controls:  One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.", "This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.", "Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.", "This restriction should be noted in the paper.", "We applied Dreamer to environments with discrete actions using the DiCE estimator (Foerster et al. 2018) locally for the da/d\u03bc and da/d\u03c3 derivatives.", "This was a drop-in replacement for the reparameterization estimator and slightly outperformed a Gumble-softmax actor.", "We find that with this 1 line change, Dreamer solves discrete action tasks of the Atari suite and a 3D DMLab environment.", "> There is no mention about variance of policy gradient estimates.", "Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.", "Dreamer uses reparamterization gradients that already have low variance (Kingma & Welling 2013, Rezende et al. 2014); although see Miller et al. (2017).", "Learning baselines for variance reduction is common for Reinforce estimators as used in A3C and PPO (Mnih et al. 2016, Schulman et al. 2017) but not for reparameterization estimators as used in Dreamer, SVG, and SAC (Heess et al. 2015, Haarnoja et al. 2018)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 80, "sentences": ["Thank you for your thoughtful review. We will address your concerns in turn.", "Q1: So you provide the general framework where somebody has to specify only the F?", "A1: Yes, and that is the motivation of this work, to avoid training new models for slightly different situations.", "Q2: The efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.", "A2: Yes, so we use WGAN-GP, a strong and elegant implementation, as our trained GAN.", "Q3: Is parameter Omega estimated individually for each degraded image?", "A3: Yes.", "Thank you again for your positive reviews which give me some confidence, I really appreciate it."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 81, "sentences": ["We thank the reviewer for their time and their clear understanding of the key aspects of the paper.", "We address the reviewer\u2019s questions in the following:", ">  How much does the image matter for the single-image data set?", "The reviewer raises an important point about the tested single images.", "Less crowded images could lead to many patches having no gradients (e.g. showing only the sky), leading to a failure of at least RotNet, if not also BiGAN on many samples of the augmented dataset.", "Our image choices were thus motivated by striving for simplicity and not further adding a pipeline that would, for example, extract only patches with sufficiently large image gradients.", "We are training DeepCluster now on a significantly less busy image and will report results in the coming days.", ">  How general is the proposed approach?", "We believe that this method will work well for pretext tasks that rely on learning via detecting and learning invariances, such as Exemplar [1], Colorization [2], and Noise-as-targets [3].", "Methods such as Context [4] and Jigsaw [5] could potentially work less well as they would potentially easily find a way to cheat given the limited amount of original data of one image.", "However, as the authors note in the paper cited by the reviewer, the accuracy of a pretext task does not translate to downstream task performances, so even a method that is simple on one image\u2019s patches does not necessarily fail.", "This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.", "> [1] found that the network architecture for self-supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved.", "Indeed, the paper mentioned by the reviewer shows that the performance of various self-supervised methods for ResNets does not degrade with the depth as it does for VGG and AlexNets due to the skip-connections.", "However, as ResNets have not been originally used to train the methods analyzed in our paper, we have stayed in the bounds that are required for fair comparisons and only used AlexNet.", "We agree with the reviewer that it would be good to check if ResNets, in general, can also be trained in such a manner (e.g. could global pooling destroy the signal?), so we are running an experiment on a ResNet-18 and will report results in the upcoming days.", "> Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?", "MonoGAN trained without any exploding gradients or other problems frequently encountered by GANs.", "As we have suggested in the paper, this might be due to the fact that image-patches from one image follow a simpler distribution than in-the-wild images of a complete dataset.", "\u2014", "[1] A. Dosovitskiy et al. \"Discriminative unsupervised feature learning with exemplar convolutional neural networks.\" TPAMI 2015", "[2] R. Zhang et al. \"Colorful image colorization.\" ECCV 2016.", "[3] P. Bojanowski et al. \"Unsupervised learning by predicting noise.\" ICML 2017.", "[4] D. Pathak et al. \u201cContext Encoders: Feature Learning by Inpainting\". CVPR 2016.", "[5] M. Noroozi \"Unsupervised learning for visual representations by solving jigsaw puzzles.\" ECCV 2016"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 82, "sentences": ["We are aware of the related work you mention.", "Please note that unfortunately the \u201cSemantically Conditioned LSTM\u2026\u201d is not directly comparable because, as they state in their paper, \u201cthe generator is further conditioned on a control vector d, a 1-hot representation of the dialogue act (DA) type and its slot-value pairs\u201d.", "Our goal is to work with arbitrarily complex questions that map to correspondingly arbitrarily complex logical forms and not a very restricted set of logical forms that could be represented in a one-hot fashion.", "Please do note that we ran 2 sets of human evaluations (Adequacy and Fluency), as is standard in Machine translation in order to deal with the evaluation bias problem you describe - we took this into account when conducting experiments and will make it more clear in a revised version.", "We also observe significant improvements in both human evaluations, suggesting that the improvement comes from our method and not from evaluation bias.", "Our dataset only contains a single logical form for each question and vice-versa, making it impossible to evaluate quantitative metrics (bleu, rouge, meteor) in the multi-reference setting you describe.", "Please also note that metrics like bleu and rouge have been commonly used in a non multi-reference setting by significant work in the natural language processing community.", "We thank the reviewer for their comments and will take them into account in a revised version."], "labels": ["rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 83, "sentences": ["The main topic of the paper is how to evaluate models that use confidence thresholding.", "The primary purpose is to compare *defenses*. However, to justify the attack strategy that we propose to use, we also compare *attacks*. Specifically, we provide an experiment demonstrating that our attack actually is stronger than the baseline.", "However, it is not really necessary to provide multiple experiments demonstrating that MaxConfidence is more powerful because the superiority of MaxConfidence is theoretically guaranteed."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 84, "sentences": ["Thank you for the fruitful comments!", "We addressed your main concern and updated Section 1 of the paper to better situate it in the existing literature.", ">> Would it be possible to train the vq-wav2vec model jointly with BERT, i.e. as one model? [...] Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end-to-end model, e.g. by randomly masking some of the continuous input speech?", "The focus of this paper is a quantization approach for audio.", "Replacing the two-step training process by an adaptation of BERT to continuous data (using a wav2vec/CPC-like objective function instead of the cross entropy) is an interesting direction for future work (and we amended the future work section accordingly).", "However, our current paper is a proof of concept that a pre-training scheme based on masked inputs (BERT) can improve over previous methods in the speech domain.", ">> What exactly does \"mode collapse\" refer to in this context?", "In several configurations (especially for one and two groups) considerably less codewords than theoretically possible are used.", "We loosely refer to mode collapse as the phenomenon when very few codewords per group are used (cf. Appendix A).", "We updated the paper to also refer to the appendix where we outline the number of codewords that the model uses.", "We observed that in the \u201cfew group regime\u201d (G=1...4), only a few of the available centroids per group are used and refer to this phenomenon as mode collapse \u2014 for BERT training, this is actually favorable e.g. in the G=2, V=320 setting as it yields a codebook of acceptable size for NLP model training (13.5k/23k).", "Mode collapse could potentially be circumvented by strategies like embedding re-initialization used in classical k-means and this is an interesting avenue for future work.", ">> [...] BERT is required on top of the vq-wav2vec discrete symbols.", "Is it possible that the output acoustic model is simply better-matched to continuous rather than discrete input (direct vq-wav2vec gives discrete while BERT gives continuous)? Would it make sense to train the wav2vec acoustic model on top of the vqvae codebook entries (e) instead of directly on the symbols?", "We actually did what you suggest: when we train acoustic models on top of vq-wav2vec, we input the dense embedding vectors corresponding to the discrete codewords.", "On the other hand, we also trained an NLP sequence to sequence (Section 6.3) which takes the quantized audio codes as input and then generates the transcriptions.", "This gives reasonable accuracy and suggests that the discrete codes by themselves, and without the learned continuous representations, are useful.", "We clarified this in the updated version of the paper.", "We believe the reason the dense embeddings for the discrete codewords work less well", "is because they do not encode as much detailed context information as a representation built by wav2vec or BERT.", "The information in the codebook is ultimately less detailed than a context vector specific to the current input sequence."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 85, "sentences": ["Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!", "We answer your questions and concerns in the following.", "> \"The advantage of INN is not crystal clear to me versus other generative methods such as GAN and VAE.\"", "It is indeed possible to adapt other network types to the task of predicting conditional posteriors.", "We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper.", "In the present paper, we focus on demonstrating that high-quality posteriors can actually be learned using bi-directional training as facilitated by INNs.", "Concerning the comments/questions:", "1.", "> \"could the authors elaborate on the comparison against cGAN\"", "cGAN generators are at an inherent disadvantage relative to INNs, because they never see ground-truth pairs (x,y) directly -- they are only informed about them indirectly via discriminator gradients.", "This it not a problem for simple relationships, e.g. between images x and attributes y, and cGANs work very well there.", "However, it makes learning of complicated forward processes much harder and may cause the resulting posteriors to be inaccurate.", "Moreover, INNs are forced to embed every training point x somewhere in the latent space, whereas cGAN generators may fail to allocate latent space for some x, because this is never explicitly penalized.", "This can lead to mode collapse and insufficient diversity.", "> \"Can cGAN be used to estimate the density of X (posterior or not)?\"", "cGANs can in principle do this by choosing a generator architecture with tractable Jacobian (using e.g. coupling layers or autoregressive flow), but we are not aware of published results about this possibility.", "2.", "> \"For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)?\"", "Yes, the weights of the losses are considered as hyperparameters, because the magnitude of MMD-based losses depends on the chosen kernel function.", "Hyperparameter optimization suggested an up-weighting of MMD-based losses by a factor of 5, to give them approximately equal impact as the supervised loss.", "For the iterations, we accumulated gradients over one forward and one inverse network execution before each parameter update.", "We also tried alternating parameter updates after each forward and backward pass, which resulted in equal accuracy, but was a bit slower.", "We did not experiment with other ratios than 1:1.", "3.", "> \"Is this to effectively increase the intermediate network dimensions?\"", "This is precisely the reason: It improves the representational power of the INN, as mentioned in Sec. 3.2 and discussed in our response to reviewer 1.", "At present, we find this is only necessary for the toy problem in Fig. 2.", "> \"It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).\"", "This is correct.", "We explicitly prevent information from being hidden in the padding dimensions in the following way:", "A squared loss ensures that the amplitudes are close to zero.", "In an additional inverse training pass, we overwrite the padding dimensions with noise of the same amplitude, and minimize their effect via a reconstruction loss.", "We will add this to the relevant paragraph in the paper.", "4.", "> \"I am curious if this model could succeed on higher dimensional data\"", "Works such as [1, 2, 3] (also cited in our paper) have shown that the coupling layer architecture in general works well with images.", "These works use maximum likelihood training, i.e. exploit the tractable Jacobians to maximize the likelihood of the data embedding in latent space.", "To scale-up our approach, we may need to replace MMD loss with maximum likelihood as well, and first experiments with this show promising results, see", "https://i.imgur.com/ft09Pk9.png .", "[1] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv:1605.08803, 2016.", "[2] Diederik P Kingma and Prafulla Dhariwal.", "Glow: Generative flow with invertible 1x1 convolutions. arXiv:1807.03039, 2018", "[3] Schirrmeister, Robin Tibor, et al. \"Generative Reversible Networks.\" arXiv:1806.01610, 2018", "We have uploaded a revised version of the paper, thank you again for your suggestions.", "The changes and additions are highlighted in red font for convenience.", "Please also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.", "If this presents a problem, we can attempt shorten the paper accordingly."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 86, "sentences": ["Except for the learning rate, all the hyper-parameters were chosen according to the values suggested by the authors of AdaGrad, and Adam.", "The learning rate was chosen as 1/K, with K=100 being the number of examples used to estimate the CDF.", "As our stated goal is to present an algorithm which can be blindly applied with some fixed set of hyper-parameters to any possible objective, one of the goals of the experiments is to show that in such a setting some methods will work, while others will fail.", "Thus, as a controlled experiment for this hypothesis, we first fixed the set of all hyper-parameters for all methods, and then proceeded to apply them to various problems.", "In this setting therefore, tuning the learning rate or any other hyper-parameter for that matter will compromise the validity of our results.", "Regarding table 3, we accept the reviewer\u2019s suggestion, this is a good point. We particularly like the suggestion of writing NA or some such value, and we will use it to correct the paper.", "We thank the reviewer for the evaluation.", "Please see our detailed response to several recurring issues at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X.", "In that response we address the following issues:", "(1) We emphasize fundamental differences between Cakewalk and CE.", "These go beyond the differences the reviewer mentions.", "(2) How the sampling distribution should not be considered as a part of Cakewalk, and that it is mostly provided as an example, and a basis for the reported experiments.", "(3) The experiments include results two tasks.", "Nonetheless, it appears the paper doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "Next, we try to answer the specific issues the reviewer mentions.", "First, we address the suggestion of introducing Cakewalk as a generalization of CE.", "While we were writing the paper we in fact considered presenting Cakewalk as the reviewer suggests.", "We eventually decided against this approach as CE is a method for adapting an importance sampler, and its convergence guarantees only apply when it is treated as such.", "The convergence guarantees of REINFORCE on the other hand still apply under our surrogate objective framework.", "This property allows us to explore various surrogates, where one such construction allows us to interpret CE as a policy gradient method, and another makes the basis for Cakewalk.", "Second, we address the issue of using a sampling distribution that assumes independence between the different dimensions.", "As the author correctly states, such a distribution will not always be useful, and one can design a problem for which this distribution will lead to a poor local optimum.", "Note however that a global maximizer for the objective suggested by the reviewer can be easily found just by random sampling: sampling such a maximizer has the same probably as sampling an odd integer - half.", "Nonetheless, for the clique problem such a distribution can be effective.", "Intuitively, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.", "In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.", "A similar reasoning applies for the k-medoids problem.", "We note that these kind of factorized distributions have a long history of being useful in machine learning.", "In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.", "In different contexts, such distributions have also been used as naive mean field approximations in variational inference.", "Next, we address the question regarding the gradient update types.", "One intuitive explanation for why an algorithm that maintains a \u2018memory\u2019 of previous gradient updates like AdaGrad or Adam is required", "is that they protect against sampling biases.", "Consider for example the case when the execution is at the start, and the sampling distribution still has maximum entropy.", "Due to the combinatorial nature of the solution space, the examples that have been sampled thus far create a distorted representation of the solution space.", "In this case we could get that some x_i=j will occur few times, while some other x_k will not receive the value j at all.", "Now if we apply vanilla gradient updates this can skew the sampling distribution in random directions.", "Gradient updates such as those of AdaGrad and Adam on the other hand will lessen the impact of such deviations as the importance of each case is inversely proportional to the number of previous observations.", "As such deviations will inevitably occur whenever we rely on polynomially sized samples to represent a combinatorial solution space, without such corrections a gradient based adaptive sampling algorithm will almost surely fail.", "Indeed, as can be seen in tables 1,2 and 4, SGA almost never leads to a locally optimal solution.", "Furthermore, this reasoning explains why AdaGrad is superior to Adam: AdaGrad corrects against sampling biases that entail all the examples that have been encountered, while Adam does this only within some exponentially moving time window.", "Indeed, this phenomenon is studied in detail in the AdaGrad paper (though without assuming a data distribution), and sparse data like ours (one can say our data points are N indicator vectors of length M) is the first motivating example in their paper."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 87, "sentences": ["Overall", "We thank you for your kind comments, in acknowledging that the work is well motivated, and the problem is an important one, currently not studied under the meta-learning paradigm.", "We have made substantial improvements based on your suggestions, and other reviewers\u2019 comments, and hopefully we are able to address most of your concerns.", "Concern 1: Reproducibility", "Our code is built on top of existing code (Prototypical Networks and Image-to-Image Translation from CycleGAN).", "Thus, we adopt the same hyperparameters and architectures as the prior work, and as a result our work is fairly easy to reproduce.", "We will of course release the code.", "As suggested, we have utilized the appendices to give detailed information about the experimental setup.", "Concern 2: Size of unlabelled test set, data-split information", "We did provide some details on the first version (in the appendix).", "In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.", "Concern 3: Jointly learning vs Freezing GAN", "Training in joint manner can be very tricky, and may often cause stability issues.", "You are right in your suggestion, that it is similar to first styling, and then applying meta-learning.", "Having said that, this is a common strategy in several state of the art domain adaptation techniques, where the GAN-based domain adaptation and task-specific classifier are trained in multiple steps.", "For example, see training protocol in [1,2,3, etc.].", "[1] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alyosha Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018", "[2] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 4, 2017.", "[3] Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., & Krishnan, D. Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017"], "labels": ["rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 88, "sentences": ["Thank you for your feedback and additional references! To address the comments:", "- As we explained in Thm.1, \\eta_{\\mathcal{H}} is a constant measuring how well the model family \\mathcal{H} can fit the true models from both domains.", "Estimating this term requires *labelled* target samples, which is usually unavailable in domain adaptation.", "However, when we have access to a handful of labelled target data, we can certainly estimate this term and perform model selection (e.g., choosing neural network models \\mathcal{H}) better, meaning that we can find better values for alphas, and so achieve even better adaptation performance.", "- Yes, there are many methods that conduct single-source to single-target adaption in the literature.", "However, our main focus is *multi-source* to single-target adaptation.", "This is why our comparisons focus on similar methods, that also use multiple sources at the same time.", "They are generally more competitive than single-source methods.", "Following Reviewer #1's suggestions, we added one additional state-of-the-art competitor, Moment Matching for Multi-Source Domain Adaptation (M3SDA) (Peng et al., 2019), to our experiments.", "Moreover, we also add the challenging Office-Home dataset as you suggested[D]; results can be found in the new Section 5.3.", "The results with the new competitor, and on both the earlier datasets and the new one, show that our method outperforms the competition, especially on the Office-Home dataset, in which we achieve state-of-the-art performance.", "If you have any comments or concerns, feel free to leave a message here and we can discuss further. Thank you!"], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 89, "sentences": ["Thanks for the review.", "There are two significant inaccuracies:", "1. GGT does not take the view of a low-rank *approximation*. This is a central point of the paper.", "2. Re: iterative methods: the preconditioner is a -1/2 power of the Gram matrix, not the inverse.", "More details below:", "@Inverse square root: We are fully aware of the distinction.", "- Note that iterative solvers like conjugate gradient do not immediately apply here, as we are solving a linear system in M^{1/2}, not M.", "- Krylov subspace iterative solvers suffer from a condition number dependence, incurring a hard tradeoff between iteration complexity and \\eps. [1]", "- We actually *did* try polynomial approximations to M^{-1/2} as an alternative to our proposed small-SVD step.", "We saw worse approximation (the condition number dependence kicks in) and worse GPU performance (parallel computation time scales with polynomial degree).", "@Full-matrix terminology: The use of \u201cfull-matrix\u201d to distinguish from \u201cdiagonal-matrix\u201d is standard, and taken directly from [2].", "@Full-matrix vs. full-rank: Note that we do not consider the windowed Gram matrix to be an \u201capproximation\u201d of the \u201cfull\u201d gram matrix.", "The window is for the purpose of forgetting gradients from the distant past, motivated by (1) our theory, (2) the small-scale synthetic experiments, and (3) the extreme ubiquity of Adam and RMSprop, which do the same.", "Note that we do no approximation on the windowed Gram matrix, the fact that it is low rank is a feature.", "@Location of \\mu definition: Is the reviewer\u2019s suggestion simply to move this definition into the intro?", "@Comparison with second-order methods", ": Please refer to our response to Reviewer 1 for some additional comments.", "@Tweaks: We don\u2019t believe that any of the tweaks should be so controversial.", "- The \\eps parameters are present in *every* adaptive optimizer, for stability.", "The interpolation with SGD is just another take on this.", "- The exponential smoothing of the first moment estimator is a subtler point.", "As we point out in Appendix A.2, in the theory for Adam/AMSgrad [3,4], \\beta_1 *degrades* the moment estimation, yet everyone uses momentum in practice.", "Even if this is unconvincing, the performance gap upon removing this tweak is minor, and our empirical results hold without this tweak.", "We are simply offering a heuristic that we have observed to help training unconditionally, just like momentum in Adam.", "@Informal main theorem: By \u201cinformal\u201d we truly mean that we are suppressing the smoothness constants (L, M) for readability and space constraints. We are simply adopting the widespread practice of deferring the non-asymptotic mathematical statement to the appendix.", "[1] Tight complexity bounds for optimizing composite objectives. Blake E Woodworth, Nati Srebro. NIPS 2016.", "[2] Adaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer. JMLR 2012.", "[3] Adam: A Method for Stochastic Optimization. D.Kingma,J. Ba. ICLR 2015.", "[4] On the Convergence of Adam and Beyond. S. Reddi, S. Kale, S. Kumar. ICLR 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_followup", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 90, "sentences": ["We\u2019re glad that you found the paper interesting and well-written. To address your comments and questions:", "1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN.", "Also, NPN can probably be modified to output spans of a sentence.", "I will be curious to know how it performs.", "The NPN model requires a pre-defined lexicon of action types (i.e., verbs), such as cut, bake, boil, etc.", "For the recipes dataset, the action types and their causal effects were manually collected and defined.", "Since the ProPara dataset does not have these annotations, we would have to manually identify action types to apply NPN to it.", "Also, NPN treats the state change as a classification problem (of about 260 classes that are also manually defined).", "In contrast, KG-MRC finds the state-describing span in the text directly, which we believe is a more generic approach.", "2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper.", "We agree that more detail would help readers to understand the model better.", "We\u2019ve made some hopefully significant updates to Section 4 (model description and notation) to improve clarity, and we hope you\u2019ll take the time to read the new manuscript.", "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "3. What are the results when using the whole training set of Recipes ?", "We\u2019ve completed an experiment on the full Recipes dataset and updated the paper to describe the result (this experiment did not finish in time for the initial submission).", "The model\u2019s F1 score improves from 51.64 on the partial data to 54.27 on the full data, surpassing the previous state of the art by a more significant margin."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 91, "sentences": ["We thank the referee for their review and the summary of our results", "1. We have included some more attacks on the most robust model (a transfer attack and a Gaussian random noise attack).", "2.", "(a) We have evaluated the adversarial resistance when training a Boltzmann machine with 256 fully connected latent variables directly on the 8x8 patches.", "The version with only 128 hidden units was not able to reduce the relative entropy to the values of the larger, stacked machine.", "We find that the model without stacking is not able to increase the adversarial resistance.", "It is possible that we are unable to complete the training due to the approximations involved.", "For a small machine (16 units) of full hidden connectivity we can observe the noise rejection behaviour, as shown in appendix D.", "(b) We have trained a machine with the same connectivity as the stacked machine directly on the 8x8 patches.", "This training gives similar results to the training in stages.", "(c) From the result in (b) we conclude that the particular manner of the pre-training does not matter.", "Therefore also the choice of first training set (98% coverage or full coverage) does not influence adversarial resistance.", "(d) There are a total of 28800 parameters in the Boltzmann machine.", "We will gladly provide files with the trained weights and also fully trained neural networks on request.", "3. We currently do not have a full explanation for the large adversarial resistance, but noise resistance must play a part in it.", "The very strong rejection of Gaussian noise and the observations in Fig.\u00a04 point in this direction."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_none", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 92, "sentences": ["With all our due respect to Reviewer #3\u2019s valuable time and effort in reviewing our manuscript, we must admit that we are a bit upset by this last late review, due to the apparent lack of understanding before placing comments, and several factual errors that make the current comments at least poorly grounded.", "We understand that the idea of \u201cmodel falsification as model comparison\u201d might not be trivial to understand for people primarily from practical deep learning backgrounds.", "The idea is deeply rooted in a successful series of studies from image perceptual assessment research: a basic introduction can be found in (Wang & Simoncelli (2008)).", "We notice that Reviewer #2 also kindly points out another interdisciplinary foundation of MAD in software differential testing.", "We hope Reviewer #3 can carefully read the below explanation, and reconsider the rating to a more serious and appropriate one.", "Q1:", "One of the main advantages is that it can select a sample set from an arbitrarily large unlabeled images.", "However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods.", "Response:", "Our method is very efficient in terms of human annotation budget compared with traditional methods, which is one of the main claims we elaborated in our paper.", "We are disappointed that this major important point was not well understood.", "In fact, MAD provides the very first and efficient solution (in the context of image classification) to exploit a large-scale image set under the constraint of the very limited budget for human labeling.", "We have noticed that the other two reviewers agree with us and appreciate this point.", "For example, quote Reviewer #2: \u201cBecause of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload\u201d.", "To evaluate the relative performance of two ImageNet classifiers, traditional evaluation methods compute accuracy on a fixed test set.", "For ImageNet validation set, human annotations for 50,000 images need to be provided.", "This number is large in terms of human labeling effort, but is extremely small compared to the set of all natural images (the natural image manifold).", "As also mentioned by the reviewer, annotation for each image is a 1000-class classification task, which makes the labeling task more difficult compared to a binary classification problem.", "In contrast, rather than comparing fixed test sets which are typically small, the proposed MAD adaptively samples a test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers, measured by the distance over WordNet hierarchy.", "Human labeling is only required on the resulting small and model-dependent image sets, which contains only k=30 images (for each pair of classifiers) on the ImageNet experiment as reported in our paper.", "Our experiments show that the MAD ranking stabilizes at around k>15 (see figure 5) and successfully tracks the recent progress in image classification .", "For comparing 11 classifiers, the total labeled images needed are 1,650 (see page 6): it is obviously smaller than 50,000 and leaves much room to compare more classifiers (before it reaches 50, 000).", "In conclusion, our method is apparently much more efficient in terms of human annotation budget compared with traditional methods.", "In addition, despite the fact that the selected set by MAD is small (as a way of maximizing the efficiency of human labeling), it provides the strongest examples to let classifiers compete with one another.", "Quote Reviewer #2: \u201cThe proposed MAD competition distinguishes classifiers by finding their respective counterexamples. It is therefore an \"error spotting\" mechanism\u201d.", "Their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix), which sheds light on potential ways to improve the classifiers or combine them into a better one.", "Those gains are way beyond the scope of collecting random image samples.", "Q2: Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.", "Response:", "We agree with the reviewer that k is a critical parameter in MAD.", "We want to however draw the reviewer\u2019s attention to the ablation study and figure 5, if they were accidentally missed in the first reading.", "Based on them, we cannot concur with the statement \u201cif k is relatively small the method seems very sensitive to selected examples\u201d.", "When we apply MAD to compare imageNet classifiers, we find that the MAD ranking stabilizes very quickly when around k>15.", "We would like to also emphasize that despite the small size of labeled images, MAD successfully tracks the steady progress in image classification, as verified by a reasonable Spearman rank-order correlation coefficient (SRCC) of 0.89 between the accuracy rank on ImageNet validation set and the MAD rank on our test set.", "As also pointed out by Review #2, the selected top-k images provide the strongest examples to let classifiers compete with one another.", "Through this process, their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix).", "Q3: The authors invite five volunteer graduate students to annotate the selected example.", "However, for many categories, it\u2019s nor easy for normal people to distinguish.", "So the experiments in this paper is also not convincing.", "Response:", "As veterans in performing subjective studies, we understand and agree with the reviewer that querying ground truth labels for a 200-class classification problem is difficult. That is exactly why we have carefully designed our subjective experiment.", "Given an image x, which is associated with two classifiers f_i and f_j , we pick two binary questions for human annotators: \u201cDoes x contain an f_i(x)?\u201d and \u201cDoes x contain an f_j (x)?\u201d.", "For each question, we follow  the original ImageNet instructions and include the definition of f_i(x) (or f_j(x))  with a link to a corresponding Wikipedia page.", "We also show several example images of f_i(x) (or f_j(x)) sampled from the ImageNet validation set.", "Moreover, if more than three of our five human annotators find difficulty in labeling x, it is discarded and replaced.", "When both answers to the two binary questions are false (corresponding to Case III), we cease to source the ground-truth label of x for reasons mentioned by the reviewer, and treat x as a strong counterexample for both f_i and f_j.", "Based on the above, we cannot concur with the judgement \u201cthe experiments in this paper is (are) also not convincing\u201d."], "labels": ["rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 93, "sentences": ["Thank you very much for the instructive and detailed review!", "For the first and second comments, we appreciate the detailed example you proposed.", "Specifically, we agree with the 1) and 2) of your analysis.", "For 3), although x^(1)_{t-2} is useful for predicting x^3_{t}, due to the causal chain and the presence of independent noise in the response function Eq. (1), x^(2)_{t-1} is even more useful for predicting x^(3)_{t}. When Eq. (2) is minimized w.r.t. both f_\\theta and all \\eta, with appropriate \\lambda, it is likely that \\eta_1 will go to infinity and \\eta_2 will be finite, leading to the correct conclusion that X^(1)_{t-1} does not directly structurally cause x^(3)_t.", "For example, in the new Appendix B.3, we show analytically and numerically that for a linear Gaussian system, the global minimum of the learnable noise risk lies on I(x^(1)_{t-2}; \\tilde{x}^(1)_{t-2})=0, i.e. \\eta_1->\\infty, for a wide range of \\lambda.", "To study the general landscape and global minimum of the learnable noise risk, we first carefully inspect Theorem 2, and find that its original statement is not true in general.", "We have replaced the original Theorem 2 with a detailed analysis of the loss landscape of the learnable noise risk.", "Specifically, there are four properties that the minimum MSE (MMSE, the first term of the learnable noise risk after minimizing w.r.t. f_\\theta) must obey, as demonstrated in the new Appendix B. In particular, we prove that the MMSE based only on the uncorrupted variables that directly structurally cause x^(i)_t is the minimum among all MMSE based on any set of uncorrupted variables.", "These properties will likely lead to nonzero mutual information for the variables that directly structurally cause x^(i)_t, at the global minimum of the learnable noise risk, as we ramp down \\lambda from infinity.", "In a sense, the learnable noise risk behaves similarly as an L1 regularized risk.", "Whereas L1 encourages sparsity of the parameters of the model, the mutual information term in the learnable noise risk encourages sparsity of the influence of the inputs, where the strength of sparsity inducing depends on \\lambda.", "As also pointed out in the \u201crelated works\u201d in the revision, the learnable noise risk is invariant to model structure change (keeping the function mapping unchanged) and rescaling of inputs, while L1 or group L1 do not, making learnable noise risk suitable for causal discovery where the scale of data may span orders of magnitude and the model structure may vary.", "For the third and fourth comment, thanks for pointing out and we have added the constraint-based methods in the related works section, and stressed that we are dealing with \u201ccausality in mean\u201d in section 3.1."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 94, "sentences": ["We thank the reviewer for their comments.", "Although our primary contributions are empirical, we also provided a detailed theoretical discussion in section 2, where we give a clear and simple account of why the two regimes arise.", "Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.", "We would also like to emphasize that we make a significant contribution to the debate regarding SGD and generalization.", "While many papers have proposed that small batches may generalize better than large minibatches, it was recently pointed out by Shallue et al. that none of these experiments provide convincing evidence for this claim, because no experiment to date has compared small and large batch training under a constant step budget with a realistic learning rate decay schedule while independently tuning the learning rate at each batch size.", "We are the first to run this experiment and conclusively establish that SGD noise does enhance generalization in popular models/datasets.", "We believe this is an important contribution.", "We also provide intriguing results as we vary the epoch budget, which demonstrate that the optimal learning rate which maximizes the test accuracy does not decrease as the epoch budget rises.", "This supports the notion that SGD has an optimal \u201ctemperature\u201d which biases it towards solutions that generalize well.", "Additional experiments in the appendix G go further and study how the optimal learning rate schedule changes as we increase the epoch budget."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 95, "sentences": ["We would like to thank the reviewer for the time and useful feedback.", "Our response is given below.", "- Interpretation of self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture.", "Overall, self-modulation appears to yield the most consistent improvement for the deeper ResNet architecture, than the shallower, more poorly performing, SNDC architecture.", "Self-modulation doesn\u2019t help in the SNDC/Spectral Norm setting on the Bedroom data, where the SNDC architecture appears to perform very poorly compared to ResNet.", "For the other three datasets, self-modulation helps in this setting though.", "- The ablation study shows that the impact is highest when modulation is applied to the last layer (if only one layer is modulated).", "It seems modulation on layer 4 comes in as a close second.", "I am curious about why that might be.", "Figure 4 in the Appendix contains the equivalent of Figure 2(c) for all datasets.", "Considering all datasets: (1) Adding self-modulation to all layers performs best.", "(2) In terms of median performance, adding it to the layer farthest from the input is the most effective.", "We believe that the apparent significance of layer 4 in Figure 2(c) is statistical noise.", "- I would like to see some more interpretation on why this method works.", "We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.", "As a first step, we provide a careful empirical evaluation of its benefits.", "While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.", "Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.", "- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?", "A 10% change in FID is visually noticeable.", "However, we note that FID rewards both improvements in sample quality (precision) and mode coverage (recall), as discussed in Sec 5 of [1].", "While we can easily assess the former by visual inspection, the latter is extremely challenging.", "Therefore, an improvement in FID may not always be easily visible, but may indicate a better generative model of the data.", "[1] https://arxiv.org/abs/1806.00035", "- Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).", "We view this contribution as a simple yet generic architecture modification which leads to performance improvements.", "Similarly to residual connections, we would like to see it used in GAN generator architectures, and more generally in decoder architectures in the long term."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 96, "sentences": ["We are afraid that there seems to be some confusion regarding our paper.", "We apologize if this is caused by the lack of clarity in the use of abbreviation \u201cES\u201d (see general response).", "In the latest revision, \u201cEvolutionary structure search\u201d is abbreviated as \u201cESS\u201d for clarity.", "We emphasize that in the paper, NO \u201cevolutionary strategy\u201d but rather PPO is used to train the policy (see Section 2.1 and 3.2).", "We hope the reviewer can take time to revisit the paper in the light of this inconsistency.", "Also, we now have 5 baselines from previous research and modern variants, which we believe further showcases our contributions.", "Q1: The experiments do not include any strong baseline", "We added more baselines to further strengthen the significance of our work with respect to the previous approaches.", "The baselines now include (a)\u201cESS-Sims\u201d (Sims, 1994), (Cheney, 2014), (Taylor, 2017), (b) ESS-Sims-AF, (c) ESS-GM-UC, (d) ESS-BodyShare and (5) Random graph search.", "We refer to the details of each baseline in the general response.", "|      NGE         | ESS-Sims  | ESS-Sims-AF  | ESS-GM-UC | ESS-BodyShare |  RGS", "fish         | **70.21**    |", "38.32     |", "51.24         |", "54.40", "|", "54.97         |", "20.96", "Walker   |", "**4157.9** |", "1804.4   |", "2486.9        |", "2458.19   |", "2185.1        |", "1777.3", "The results show that NGE is significantly better than previous approaches and baselines.", "We did an ablation study by sequentially adding each sub-module of NGE separately.", "The table shows that submodules are effective and increase the performance of graph search.", "Q2: a) Optimizing both the controller and the hardware has been previously studied in the literature.", "Is it worth using a neural graph?", "b) All algorithms should optimize both G and theta for a fair comparison.", "By \u201coptimizing both G and theta\u201d, we meant to indicate that the learned controllers can be transferred to the next generation even if the topologies are changed (instead of throwing away old controllers).", "We note that only NGE among all the baselines has the ability to do that.", "Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.", "To the best of our knowledge, the traditional methods require re-optimizing theta from scratch for each different topology, which is computationally demanding and breaks the joint-optimization.", "NGE approximately doubles the performance of previous approach (Sims, 1994) as shown in Q1.", "Please refer to Section 3.1 and Section 3.4 for more details.", "Q3: You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.", "Again, we apologize for the confusing use of \u201cES\u201d abbreviation.", "Evolutionary strategy is not used in the paper.", "We invite the reviewer to re-read our paper, since it seems to have led to a major misunderstanding.", "CMA-ES updates and utilize the covariance matrix of sampling distribution, which is not directly applicable to discrete structure optimization.", "We believe it will be a valuable future research direction.", "Q4: Providing the same computational budget seem rather arbitrary and depends on implementation.", "We are unsure what the reviewer is indicating, and would appreciate the additional clarification.", "In terms of the computational budget for each experiment, we compared different algorithms under different computational budget metrics, more specifically,  \u201cwall-clock time\u201d, \u201cnumber of updates\u201d, and the \u201cfinal converged performance\u201d.", "NGE performs best among all algorithms.", "We emphasize the fact that wall-clock time is a more common and realistic metric for comparing the structure search in practice.", "We agree that computational budget depends on implementation, and the curves in the paper are plotted based on the number of iterations/parameter update, which is independent of the implementation.", "Q5: The writing of the paper", "We sincerely thank the reviewer for the suggestions.", "We updated the changes in the latest version accordingly."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_refute-question", "rebuttal_social", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_followup", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 97, "sentences": ["We thank for the reviewer for their comments on our work, and we share our responses below.", "1) We agree that we did not provide a clear definition of \"task\".", "In the present paper there are two tasks: classification into primary labels, and classification into secondary labels.", "We did not mean to imply that the classification of a specific class is a task on its own.", "We agree however that a clearer introduction of the terminology would be clearly helpful and we plan to add this to the final submission.", "2) This comment is not entirely correct and we would like to apologies for any confusion in the paper.", "Actually, the update of the generator depends on the improvement of the classifier for the *principal* labels on the *meta-training* data, i.e. the improvement in generalisation to unseen data.", "Thus, the optimal auxiliary labels are not the ground-truth labels for the principal classes, since this would make both terms in the minimisation for $\\theta_1$ (the second equation in 3.2) identical and not allow any leveraging of the meta-training data.", "Also, we would argue that the KL-divergence, rather then introducing noise, allows us to avoid collapsing classes which we would claim are due to dying neurons (again, there is not loss/mechanism drawing the auxiliary labels to be the same as the primary ones).", "These claims are supported by showing that providing random labels does not lead to any improved performance and by our experience that using hard labels does indeed improve performance.", "3) Providing fair comparisons across a range of very different methods is not easy when other methods aim to solve a different problem.", "Concerning the comparison with prototypical networks, we do agree that this is not a fair comparison and we would like to change the phrasing in the paper.", "The original reason for associating this to the prototypical network was that we employ their zero-shot setup: i.e. we use a VGG network to obtain prototypes on the meta-data and then use these prototypes to define an auxiliary task on the training-data.", "4) We do agree that requiring the class hierarchy is a current limitation of the work.", "While it is still general enough for solving classification tasks (we merely have to choose a fixed number of sub-classes per task, e.g. 5 without having to provide anything else), we would want to look at more general auxiliary task in future.", "One option we are considering is employing an auxiliary regression task, where the generator network would provide vectors and the corresponding loss would be simple regression.", "However, since this is the first work to use a double gradient method for auxiliary task generation, we believe that presenting results with a comparison to human auxiliary labels, which itself also requires this hierarchy, is a good starting point.", "5) We would very much like to test our approach on more complex datasets with more varied classes, and this will be part of future work.", "However, we would like to repeat that our approach can work with an arbitrary hierarchy (e.g. assigning the same number of sub-classes to every class).", "The reason why we only used 100 classes in our experiments is for allowing the comparison with human-defined classes, but in principle we could use any number of sub-classes per primary class.", "In the CIFAR10 dataset in which a hierarchy is not defined, we show that using 6 different hierarchies all lead to a better generalisation."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 98, "sentences": ["Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments.", "Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.", "> \u201c...the additions proposed are small modifications to existing algorithms", "We concede that the modifications to the existing models is a minor contribution.", "We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details.", "We plan to make our code public to aid research in the area.", "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294).", "This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers.", "> \u201c...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017).\u201d", "Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesn\u2019t support relationship types.", "Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI.", "In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT.", "In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d.", "During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search).", "This leads us to conclude that vanilla GAT would not perform well on the RDF tasks.", "As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results.", "As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile.", "We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour.", "> \u201c...the results achieved in the experiments are very small improvements compared to the baseline of RGCN", "\u2026\u201d", "We agree that any improvements compared to RGCN are marginal.", "In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT.", "The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before.", "We also see value in reporting these negative results.", "It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].", "The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.", "On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.", "> \u201c...often these small variations in results can be compensated with better baselines training", "\u2026\u201d", "We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true.", "To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN.", "In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017).", "On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance.", "This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_by-cr", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 99, "sentences": ["Thank you for your time and effort of reviewing our paper. Please see our response below.", "\\kappa is an assistant notation to remove the ambiguity of the two \\gammas in G_{\\gamma}^{q_{\\gamma} (y)}. \\kappa stands for the parameter/variable of which the gradient information is needed.", "For example,", "(i) g_{\\kappa}^{q_{\\gamma}(y)} = frac{-1}{q_{\\gamma}(y)} \\nabla_{\\kappa} Q_{\\gamma}(y)}, where \\kappa is \\gamma, as in Theorem 1;", "(ii) g_{\\kappa}^{q_{\\gamma}(y|\\lambda)} = frac{-1}{q_{\\gamma}(y|\\lambda)} \\nabla_{\\kappa} Q_{\\gamma}(y |\\lambda), where \\kappa could be \\gamma or \\lambda.", "Eqs. (7) and (8) are the foundations GO is built on, but they are not our GO.", "GO is defined in Eq. (9) of Theorem 1.", "For Eq. (9), yes, y_{-v} is selected from one sample y in the experiments.", "But GO is not the local expectation gradient (Titsias & Lazaro-Gredilla, 2015), because GO uses different information (the derivative of the CDF and the difference of the expected function).", "As pointed out in the last paragraph of Sec. 3, when y_v has finite support and the computational cost is acceptable, one could use the local idea from Titsias & Lazaro-Gredilla(2015) for lower variance, namely analytically evaluate a part of expectations in Eq. (9).", "For a detailed example, please refer to Appendix I.", "The main difference between the local expectation gradient and the proposed GO is that the latter is applicable to where the former might not be applicable, such as where y_v has infinite support or the computational cost for the local expectation is prohibitive.", "Please note our GO is defined in Eq. (9).", "As pointed out in the last paragraph of Sec. 3, calculating Dy[f(y)] (requiring V+1 f evaluations) could be computationally expensive.", "We also stated there, \u201cfor f(y) often used in practice special properties hold that can be exploited for ef\ufb01cient parallel computing\u201d.", "We took the VAE experiment in Sec 7.2 as an example and gave in Appendix I its detailed analysis/implementation, in which you might be interested.", "More specifically, the two bullets after Table 4, should be able to address your question on fast speed.", "Also, as noted in the penultimate paragraph of Sec. 7.2, less parameters (without neural-network-parameterized control variant) could be another reason for GO\u2019s efficiency.", "As for computation complexity, since different random variables (RVs) have different variable-nabla (as shown in Table 3 in Appendix), GO has different computation complexity for different RVs.", "After choosing a specific RV, one should be able to obtain GO\u2019s computation complexity straightforwardly.", "For quantitative evaluation, the running time for each experiment has been given in the corresponding Appendix.", "Please check there if interested.", "Thank you for pointing out the concern on multi-sample-based REINFORCE.", "We have added another curve labeled REINFORCE2 to the one-dimensional NB experiments (see Fig. 8 for complete results), where the number 2 means using 2 samples to estimate the REINFORCE gradient.", "In this case, REINFORCE2 uses 2 samples and 2 f evaluations in each iteration, whereas GO uses 1 sample and 2 f evaluations.", "As expected, REINFORCE2 still exhibits higher variance than GO even in this simple one-dimensional setting.", "Multi-sample-based REINFORCE for other experiments is believed unnecessary, because (i) the variance of REINFORCE is well-known to increase with dimensionality; (ii) after all, if multi-sample-based REINFORCE works well in practice, why we need variance-reduction techniques?", "Please refer to Sec. 7.2 and Appendix I, the author released code from Grathwohl(2017) (github.com/duvenaud/relax) were run to obtain the results of REBAR and RELAX.", "We adopted the same hyperparameter settings therein for our GO.", "So, we do not think the hyperparameter settings favor our GO in the reported experiments.", "Please refer to the first paragraph of Sec. 7.2, \u201cSince the statistical back-propagation in Theorem 3 cannot handle discrete internal variables, we focus on the single-latent-layer settings (1 layer of 200 Bernoulli random variables).\u201d", "If you are interested, as stated in the last paragraph of Sec 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.", "We believe that procedure might be useful for the inference of models with discrete internal RVs (like the multi-layer discrete VAE).", "Please refer to the last paragraph of Appendix I, where we explained this misunderstanding in detail.", "In short, GO does not suffer more from overfitting; one reason is GO can provide higher validation ELBO.", "Actually, we believe it is GO\u2019s efficiency that causes this misunderstanding.", "We hope your concerns have been addressed. If not, further discussion would be welcomed."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_other", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_followup", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 100, "sentences": ["We thank the reviewer for their time and detailed reading of the paper.", "In the following, we address each of the reviewers comments:", "> Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features [\u2026] Scattering is a perfectly fine approach for initial features.", "Our aim is to investigate the \u201cpower\u201d (or lack thereof) of current self-supervision techniques when applied to standard deep network models.", "This is of interest because self-supervision is a hot topic of research.", "Finding whether e.g. the Scattering Transform can replace the first few layers of a network is interesting, for example to know if handcrafted features can also do as well as (self) supervision for the first few layers, but not susbtitutive of our core investigation (furthermore, we also look ad deeper layers, where these features are unlikely to be competitive).", "Still, such an experiment can help put our findings in context.", "This is why we do include them in Table 2 of the paper, where we show that scattering is not quite as good as even single-image self-supervision.", "We do think that the suggestion of finetuning/retraining the rest of the model is also interesting after replacing the first few layers is also interesting.", "Still, we think that this can complement but not replace linear probing as the latter is a more direct way of finding what the probed layers can do.", "For instance, it is likely possible to learn a good network even by replacing the first layer with the identity function \u2014 it is just a slightly less deep model.", "For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.", "> Can the authors clarify how the neural style transfer experiment is performed?", "Indeed, the method by Gatys et al. uses deeper layers as well, which we also use \u2014 straight from the self-supervised method, without fine-tuning or anything else.", "We will update the paper with these details.", "> While I appreciate the computational burden of testing more images, it does feel that Image A and B are quite cherry-picked in being very visually diverse. [...]  It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties.", "Thank you, finding the best single training image, or finding useful synthetic images, are both very interesting ideas. While we are happy to consider doing so as a next step, it is next to impossible to do so in time for the rebuttal (we do not have access to thousands of GPUs).", "Nevertheless, we would argue that the paper stands on by making some interesting observation on the ability of self-supervision to extract useful information from more than one (or few) images, and by investigating the role of data augmentation in this process.", "We hope that the reviewer will agree that the community will be interested in hearing about these findings.", "> I disagree with the claim of practicality in the introduction (page 2, top). While training on one image does reduce the burden of number of images, the computational burden remains the same.", "Our intention wasn\u2019t to say that we can save compute time, but data collection effort (which is also a practical issue in some applications).", "Nevertheless, we agree that our findings have mostly a theoretical value, so we have adjusted the wording to reflect that.", "> And as mentioned above, it doesn\u2019t seem likely that *any* image would work for this method.", "It is true that we did not quite prove that, so we have reworded the text to tone down this claim.", "To be a bit more specific, obviously a blank image would not work, and textureless images would probably not work well either.", "However, we did use in the paper the first two images we manually selected from Google Image Search (while we did select images with some texture, they have not been otherwise been optimized for good performance in our evaluation)", ".", "Thus, we think that it is extremely likely that many other images would work just as well.", "> Finally, more images are needed to learn the deeper layers for the downstream task anyway.", "True, but even for deeper layers a single image achieves two thirds of the performance that self-supervision can squeeze out of a million images, which we think is interesting."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 101, "sentences": ["We thank the reviewer for their time and welcome feedback, which we are incorporating into the revised version.", "R: - Theoretical contribution", "A:", "-- Just a clarification regarding the first contribution: WE from the current task is not used to generate a saliency map for the next task; it is instead used to instruct the learner of the next task about which input areas are more important than others via the attention mechanism.", "This becomes a part of the future learning procedure, not just a post hoc visualisation method as in the original WE .", "As such, the first contribution is not solely about generating new visualisations; it is more about using the learned saliency maps from the past to attend in future learners.", "We therefore believe that the potential of this first contribution as a conceptual framework via which a learner can understand, attend, and then enhance its attention for future tasks, is not small.", "-- Importantly, we are the first to combine interpretability with continual learning and show that interpretability can help continual learning. It is a significant step to bring these two communities together.", "-- It is worth noting that VCL has achieved very good results on most of these benchmarks, so it is very hard to outperform it with a large margin.", "Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).", "R: - Related work", "A:", "-- Thank you.", "In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.", "We have added more to the revised version.", "R: - https://arxiv.org/pdf/1805.09733.pdf", "A:", "-- Thank you. We have cited the paper in the revised version, and plan to take it into further consideration in future work.", "R: - Yellow color in plots", "A:", "-- We have changed the yellow colour in Figures 2, 3 and 4 to black. Yellow is now no longer used in any plots."], "labels": ["rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_summary", "rebuttal_summary", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 102, "sentences": ["We are thankful for the valuable feedback.", "The main concern of this review is the quality of the writing and experimental details.", "We updated the paper to clarify all the names and ensure that all terms are introduced before they are used.", "The two tower design was necessary since the decision whether theorem T can be rewritten using parameters P requires both pieces of information, so we need to feed them to the network.", "In fact $\\omega$ does not need to predict p, but it gives extra supervision signal and therefore regularizes the prediction.", "The random baseline is necessary because of the unbalanced nature of the rewrite success, this is hard to control, so we added an extra baseline that shows that our results are better than just ignoring any of the input expressions (theorem or parameter).", "In addition, we have simplified the architecture described in the paper by combining the networks $\\sigma$ and $\\omega$, and included the results from this architecture.", "We have significantly improved the experimental section by further clarifying the experiments and expanding them with more supporting measurements.", "We have also moved the two non-baselines out of the baselines section.", "Finally, thank you for the insightful questions!", "With our current setup, the goal is to simply perform reasoning steps in latent space without specifically proving any statements.", "There are several approaches to make the network predict a closed goal, for example by predicting a fixed embedding such as the zero vector.", "We expect that most semantic aspects of the formula could be recovered, but not superficial features as the naming of the variables should not affect the rewriteability of formulas.", "The question how much of the formula can be recovered is probably dependent on the theorem database, since only those aspects that manifest in different rewrite successes are expected to be recovered.", "We don't have much intuition on the decomposability of embeddings, but it seems like a fascinating research direction.", "We are grateful for the feedback which has helped to make the paper much clearer and more readable."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 103, "sentences": ["We thank Reviewer 3 for the constructive feedback.", "Here is our point-to-point response to the comments and questions raised in this review:", "1. \u201cThe novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily\u201d", "GAN inference and adversarial training seek different goals.", "Adversarial training addresses a supervised learning task while GAN inference focuses on an unsupervised learning problem.", "Due to the inherent difference between supervised and unsupervised learning problems, the notion of generalization is defined differently between them.", "Arora et al. (2017) provide the standard definition of generalization error for GANs which is very different from the standard generalization error considered in supervised learning.", "Furthermore, no work in the literature theoretically guarantees that spectral normalization closes the generalization gap for either adversarial supervised learning or GAN unsupervised learning.", "2. \u201cIt is not clear to me that these are some novel results that can better help adversarial training\u201d", "Our work\u2019s main contribution is the theoretical generalization guarantees for spectrally-normalized adversarially-trained DNNs.", "Introducing the adversary can significantly grow the capacity of a DNN.", "Therefore, existing DNN generalization bounds are not applicable to adversarial training settings.", "Our work, to our best knowledge, is the first to show that the adversarial learning capacity of a DNN for FGM, PGM, WRM training schemes can be effectively controlled by regularizing the spectral norm of the DNN\u2019s weight matrices.", "Our numerical results further support our theoretical contribution."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 104, "sentences": ["(1) Prior Network:", "During training phase, we sample from prior network to generate \"pseudo\" observations for unobserved modalities.", "The pseudo observations are then used to estimate the conditional likelihood for such modalities (E_x_j in the ELBO).", "Practically, we follow a two-stage method in our implementation.", "At each iteration, the first stage imputes unobserved modalities (with latent code sampled from approximate posterior for observed modalities, and prior for unobserved modalities), followed by the second stage to estimate ELBO based on the imputation and backpropagate corresponding gradients.", "(2) Conditioning on Ground-Truth Mask:", "We conduct experiments with decoder p(x|z, m) conditioned on the original mask in training set, and observe comparable performance and convergence time.", "The mask distribution might be easier to learn as compared to data distribution (since the mask is fully-observed)", ".", "However, we argue that jointly learning the mask distribution and data distribution provides us an opportunity to further analyze the missing mechanism and potentially can facilitate other down-stream tasks.", "(3) Image Inpainting:", "We appreciate the reviewer's suggestion on evaluate the effectiveness of our model on image inpainting task.", "However, with our current setup, an encoder is trained for each modality respectively, making it difficult to scale to inpainting task, if we treat each pixel as an individual modality.", "Nevertheless, we believe this is an interesting extension.", "The backbone models and mathematical formulations can be very similar, if not the same.", "A potential solution could be to employ patch level encoders to reduce the total number of encoders needed."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 105, "sentences": ["We appreciate your time and effort of reviewing our paper, and thank you for the insightful and constructive comments.", "For simplicity of the main paper, we moved all the detailed proofs to the Appendix.", "More specifically, the proofs for Theorem 1, Lemma 1, Theorem 2, Corollary 1, and Theorem 3 are given in Appendix A, C, D, E, and F, respectively.", "Thanks a lot for pointing out the smoothness conditions for reparameterization; we have carefully revised our paper to remove the misleading statements and to make it clearer when our method (and also the reparameterization trick, Rep) is applicable.", "For your comments wrt discrete random variables (RVs), unfortunately, we haven\u2019t found a principled way to back-propagate gradient through discrete internal RVs (like in multi-layer sigmoid belief networks).", "However, as stated in the last paragraph of Sec. 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.", "We believe that procedure could be useful for the inference of models like the multi-layer sigmoid belief networks.", "As for the conditional independency, it is actually removed after marginalizing out additional continuous RVs (which could be non-reparameterizable RVs like Gamma).", "Also note that one can strengthen the aforementioned procedure by inserting more additional continuous internal RVs into the inference model to enlarge its (marginal) description power.", "The notations are chosen for harmony and also to keep consistency with the main literature.", "For example, one can add another expectation wrt the true data distribution q(x) to the ELBO in Eq. (1), that is, E_{q(x)} [ELBO] = E_{q(x) q(z|x)} [log p(x,z) \u2013 log q(z|x)]  \\propto  - KL[q(x)q(z|x) || p(x,z)].", "For dropout, since the dropout rate is a tunable hyperparameter that need not be learned (thus no back-propagation is required)", ", one can use Rep to construct the q distribution you defined.", "If we understand correctly, in that case we cannot demonstrate our advantages.", "Currently, the proposed method cannot be directly applied to multi-layer sigmoid belief networks (without the procedure in Appendix B.4).", "We have made an explicit statement of this in the revised manuscript.", "Thank you for pointing this out.", "However, it\u2019s believed that Rep cannot be applied to Gamma distributions [1,2].", "We have revised our statement to \u201cThere are situations for which Rep is not readily applicable, e.g., where the components of y may be discrete or nonnegative Gamma distributed\u201d.", "[1] F. Ruiz, M. Titsias, and D. Blei. The generalized reparameterization gradient. In NIPS, pp. 460\u2013468, 2016.", "[2] C. Naesseth, F. Ruiz, S. Linderman, and D. Blei. Rejection sampling variational inference. arXiv:1610.05683, 2016.", "Yes, Lemma 1 shows that our deep GO will reduce to Rep when Rep is applicable.", "We are not sure whether you were asking about the difference in Fig. 1 or Fig. 2.", "So, two responses are given below.", "(A) In Fig. 1, the difference comes from the definition of node y^(i).", "For deterministic deep neural networks, node y^(i) is the activated value after an activation function, where deterministic chain rule can be readily applied; while for deep GO gradient, node y^(i) might be the sample of a non-reparameterizable RV, where deterministic chain rule is not applicable.", "Please also refer to the main contribution (ii) of our response to Reviewer 2.", "(B) If you were interested in the difference in Fig. 2 (a)(b), the reasons include (1) the standard Rep cannot be applied to Gamma RVs; (2) both GRep and RSVI are designed to approximately reparametrize Gamma RVs; (3) GO generalizes Rep to non-reparameterizable RVs; or in other words, GO is identical to the exact Rep for Gamma RVs.", "Yes, the sticking approach was implicitly adopted for all the compared methods when it is applicable.", "We have made a clear statement in the revised paper.", "Since stochastic computation graph (SCG) is based on REINFORCE and our method is based on GO, the comparison between SCG and our method is (roughly speaking) identical to that between REINFORCE and GO.", "That is, SCG is more generally applicable but with higher variance; the proposed method has less generalizability but with much lower variance.", "We have added the following discussion into Related Work.", "\u201c\u2026as the Rep gradient (Grathwohl et al., 2017).", "SCG (Schulman et al., 2015) utilizes the generalizability of REINFORCE to construct widely-applicable stochastic computation graphs.", "However, REINFORCE is known to have high variance, especially for high-dimensional problems, where the proposed methods are preferable when applicable (Schulman et al., 2015).", "Stochastic back-propagation\u2026\u201d", "Thank you for pointing out these fundamental conditions, which we have added into the revised manuscript.", "We hope your concerns have been addressed. If not, further discussion would be welcomed."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 106, "sentences": ["Thank you for your review!", "> You have the phrase \"allowing to imagine thousands of trajectories in parallel\". I would like some elaboration on this. I think you have ideas of what is happening in the latent space that I am not following.", "The latent states are defined as 330 dimensional activation vectors with 300 deterministic and 30 sampled components.", "We can predict imagined trajectories for thousands of initial states in parallel since they fit into the memory of the GPU at once.", "Specifically, Dreamer predicts imagined trajectory of length 20 from each of the 50x50=2500 latent states for the current training batch.", "Performing the same amount of imagination steps with a dynamics model that generates images during inference would be challenging.", "For example, we could only fit up to 500 trajectories of length 10 into GPU memory with the SV2P model (Babaeizadeh et al. 2017).", "Besides the memory constraints for predicting multiple trajectories in parallel, predictions in the latent space are often an order of magnitude faster than in pixel space.", "> I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. How does the effort compare to the 1E9 steps of the base line your refer to?", "We will include more details in the final version.", "Dreamer was run for 2e6 environment steps (20 hours) compared to D4PG that was run for 1e9 environment steps (24 hours).", "Both algorithms used a single GPU each.", "As outlined in our previous answer, Dreamer performs 10 billion imagination steps throughout training.", "Please note that imagination steps are often considered free for robotic learning, because the bottleneck is the time of physical interaction with the real world.", "> [...] understanding what structures get learned in latent space, are the in fact compact, diverse?", "The amount of information in the latent representation is upper bounded by the KL divergence loss.", "We observed a typical KL divergence of 15 bits per time step,", "compared to the 64 x 64 x 3 x 8 bits of the corresponding images.", "This bounds the compression ratio to at least 1 : 6500.", "We make no claims regarding diversity.", "However, since the behaviors are learned purely in latent space, there must be a sufficient amount of diversity to solve the presented tasks.", "We agree that exploring the semantics of the latent space is an interesting orthogonal direction for future work.", "> Perhaps there is room for memory/memories in the latent space?", "It would be interesting to combine Dreamer with external memory modules.", "Gregor et al. (2019) provide a comparison of such modules.", "However, this would better be addressed in a separate work to keep the paper focused on the main contribution of learning behaviors by latent imagination."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 107, "sentences": ["The reviewer feels that the proposed model is too simple, and suggests comparing against more complex models, suggesting a few in particular.", "Our response is twofold:", "- The accuracy of the proposed simple model exceeds the accuracy of far more complex models by a wide margin, and this consistently over a range of networks (all commonly used networks in this literature), against a range of baselines (all either commonly used baselines, or methods known as achieving state-of-the-art accuracies), and on two important tasks (link prediction and multi-label classification).", "We do not agree that its simplicity reduces its merit, we think it rather contributes to its merit.", "- We thank the reviewer for suggesting additional comparisons with specific more complex models, although we feel that calling these methods 'stronger' requires some clarification or support.", "Of the suggested methods, graph convolutional and message passing neural networks need attributed graphs as inputs, and are thus not applicable.", "We have in the meantime been able to include struc2vec in the evaluation, again showing superiority of CNE by a wide margin -- showing that it is maybe more complex but certainly not 'stronger' as in 'more accurate'.", "The paper will be updated very soon to include these results.", "Perhaps the reviewer is incredulous regarding this large increase in performance a method as 'simple' as CNE achieves w.r.t. the state-of-the-art.", "We believe that this is due to the conceptual advance made in CNE.", "In our opinion a conceptual advance that achieves a strong boost in accuracy without increasing complexity, is at least as valuable as a method that achieves the same boost in accuracy while also increasing complexity.", "Also note that all code is provided, and we invite the reviewer to replicate our experiments."], "labels": ["rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 108, "sentences": ["We thank the reviewer for the comments on our paper.", "Designing more efficient streaming algorithms with machine learning techniques is a relatively new research topic and we have included more related work in our updated version of the manuscript (highlighted in the blue color)."], "labels": ["rebuttal_social", "rebuttal_done"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 109, "sentences": ["Thanks a lot for your review.", "We address your remarks below:", "\"RNN with an accumulator / too minor a contribution \"", "We want to emphasis that the accumulator implemented in the newly proposed architectures has an inherently different nature than accumulators used so far: While accumulators such as LSTM cells accumulate knowledge about the state of a sequence, our architectures produce meaningful intermediate results, which can simply summed up to estimate the final set utility.", "Producing such intermediate results, which model the nature of the problem much better than previous approaches, is the key idea presented in this paper and a major benefit of the proposed architectures.", "This also follows the idea of the Choquet integral.", "\"Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.\"", "To improve reproducibility, we published the data and the code.", "We implemented in our work the most basic version of our idea as well as the most basic version of each reference model.", "Hence, the code of the implemented architectures only consists of few lines and can be checked easily.", "We think that it is not fair to simply mistrust our results since we made our work fully transparent."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 110, "sentences": ["We thank the reviewer for the constructive feedback.", "The use of a fixed embedding space $L$ and a separate space $L^\\prime$ was useful as it naturally prevents the collapse of embeddings.", "However this could be counteracted by stopping the gradient at the right place in the simplified architecture  which was suggested in the original paper and is now described in the updated paper.", "As suggested, we have added further analysis of failure cases, and describe strategies for negative mining from these examples.", "In addition, we have included a brief description of the graph neural network architecture used in Paliwal et al (2019).", "We also include further details on the construction of training set.", "Training a decoder to predict the results of rewrites from the latent space is an interesting idea, but is technically challenging and we felt it was out of scope for this paper.", "We managed to counteract the noisiness of predicted embedding by training on noisy embeddings which trains the network to be robust to random changes and improves the prediction of multi-step rewrites significantly.", "We are grateful for the suggestions that contributed significantly to improving the quality of the paper."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_reject-request", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 111, "sentences": ["We thank you for the constructive feedback and are glad that you enjoyed the paper.", "Here we discuss some of your comments.", "R1: \"missing in the paper is the comparison to two other class of RL methods: count-based exploration... In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.\"", "=> We chose to focus on dynamics-based approaches in this paper because we found them more straightforward to efficiently parallelize than the published pseudo-count methods.", "This allows us to be able to run more and larger experiments on many environments.", "Interestingly, increased parallelization also significantly helped the exploration strategies as shown in Figure 3(a).", "Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.", "In particular, we were not able to find any official public implementation of the pseudo-count methods.", "We experimented with a third party implementation trying to see if it could play Breakout without extrinsic rewards, but did not achieve sufficient success and found it to be too slow for scaling it up to a large-scale study.", "R1: \"the experiments around VAE... While the paper shows experimentally that they aren't as successful... there's no further discussion on the reasons for poor performance.\"", "=> We found that VAEs overall worked well and were sometimes better than other representation learning methods, but often were causing instability at training.", "We don't claim such instability is an inherent property of the VAE feature learning method, but probably stems from the continually changing data distribution as agent makes progress.", "Indeed modeling the density of a non-stationary distribution, with modes appearing and disappearing, is a challenging and an active research problem.", "We will clarify this in the final version.", "R1: \"An interesting area for future work could be on early stopping techniques for embedding training\u2026 maybe somewhere in between could be the sweet spot with training\"", "=> Thank you for the excellent suggestion.", "We agree that there may be some optimal tradeoff between features that are stable and features that adapt to the environment.", "Such tradeoffs would be interesting to investigate, and might be crucial to getting learned features to perform significantly better than fixed random features.", "We will add this in the discussion/future work section of paper.", "R1: \"What are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).\"", "=> Thank you. We will add more details on the architectures to the appendix."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 112, "sentences": ["Thank you very much for your strong recommendation!", "1) Intuition about the improvement", "Though not explained in Section 4.", "The intuition for NADPEx is given in Section 3.", "Interpretation for as efficient or even faster exploration in dense environment (4.1) is that NADPEx could encourage more diverse exploration, while absorb experience from it in a relatively efficient way.", "For sparse environments (4.2), where temporally consistent exploration is crucial for learning signal acquisition, NADPEx outperforms vanilla PPO.", "It could also beat parameter noise if difficulty is increased, because intuitively low variance in gradients is a boon for faster learning.", "Improvement in 4.3 and 4.4 are basically from the theoretical grounding of NADPEx, which we believe is one of our contributions.", "Specifically, improvement in 4.3 is from high level stochasticity's adaptation to the low level; while that in 4.4 could be interpreted with the idea of trust region, that policy should be updated to somewhere near the sampling policy in the policy space, such that collected experience are usable (on-policy).", "In NADPEx, trust region also contains the meaning that dropout policies are close to each other for more efficient exploration.", "2) Limitation of NADPEx", "One of the limitation we see from NADPEx is that dropout policies are not directly interpretable from their network structures, while interpretability and composibility are prerequisites for reusing them in more complicated tasks.", "Luckily, modeled as latent random variables, an information term could be added to the objective as in [1, 2].", "This is also a direction for future research work.", "[1] Florensa et al., \"Stochastic neural networks for hierarchical reinforcement learning\", ICLR 2017.", "[2] Hausman et al., \"Learning an Embedding Space for Transferable Robot Skills\", ICLR 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 113, "sentences": ["First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers, and apologize for the late response due to additional experiments and modifications of the paper.", "Remark 0. It needs other theoretical explanation (ex. co-training)", "A: We have modified our paper and added some theoretical explanation in the introduction on page 2.", "Remark 1. \" Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter. \"", "A: We do not agree with your opinion.", "The formulae of the softmax and sigmoid are as follows.", "The softmax function : exp(f_j(z)) / sigma(exp(f_k(z))", "= 1 / ( 1 + exp(-f_j(z)) \u00d7 (exp(f_1(z)) + ... + exp(f_j-1(z)) + exp(f_j+1(z)) + ... exp(f_n(z)))", "The sigmoid function : 1 / (1 + exp(-g(z)))", "where z, f(z), and g(z) represent the final layer of the backbone network, classification network, and selection network respectively.", "As you said, if f_j(z) is very high and the other f(z)s are moderate, it can work like sigmoid.", "However, even if f_j(z) is not much high, the softmax output can be close to 1 with extremely smaller values for other f(z)s", "because:", "The softmax output : 1 / ( 1 + exp(-f_j(z)) \u00d7 0) = 1", "We experimented with a high softmax output threshold (epsilon = 10^(\u22124)).", "Although epsilon was 10^(\u22124) (threshold = 0.9999), an average of about 800 unlabeled data was added for the case of 100% of the non-animal data.", "Even at 0% of the non-animal data, performance is lower than the fixed mode of the sigmoid.", "This shows the limitation of thresholding with softmax.", "The result of new SSL problems on the CIFAR-10 dataset with 5 runs are as follows:", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "activation function / softmax (error/added data) / sigmoid (error/added data)", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "supervised", "/", "( 22.27 / 0 )", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "0%", "/", "(18.27 / 4,306.8)", "/", "(17.84/2,338.8 )", "25%", "/", "(18.35 / 3,350.4 )", "/", "(18.38 / 1470.0 )", "50%", "/", "( 18.72 / 2580.0 )", "/", "(19.04 / 811.2 )", "75%", "/", "(20.33 / 1,711.2 )", "/", "( 20.07 / 315.6 )", "100%", "/", "(20.71 / 864.0 )", "/", "( 20.24 / 1.2 )", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "Remark 2. Expression, and details (ex. number of iterations, stopping criteria, typos and grammar errors)", "A : We apologize to the reviewer for the lack of clarity in the manuscript.", "We have modified our expression, typos and grammar errors.", "Regarding the details on hyper-parameters:", "- We set parameters as follows.", "The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.", "In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.", "While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.", "Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added.", "The growth rate of epsilon is determined according to when the validation accuracy saturates.", "The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.", "If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.", "If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.", "We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.", "(In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.", "However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.", "In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.", "In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.", "Other details are the same as those of the first experiment.", "(In previous versions, the training iterations of fixed mode had been fixed.", "Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)", "Remark 3. \"SST itself is only comparable with or even worse than the state-of-art methods.\"", "A : As mentioned in our paper, SST has comparable performance to other conventional SSL algorithms.", "In Table 2 of our paper, SST achieves 34.89% on CIFAR-100, which is higher than TempEns[1](38.65%), 11.82% on CIFAR-10, which is slightly worse than VAT+EntMin[2](10.55%), and perform worse 6.88% on SVHN.", "However, SST can solve the real problem of the existence of out-of-class unlabeled data.", "[1] Laine, Samuli, and Timo Aila. \"Temporal ensembling for semi-supervised learning.\" arXiv preprint arXiv:1610.02242 (2016).", "[2] Miyato, Takeru, et al. \"Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning.\" arXiv preprint arXiv:1704.03976 (2017).", "Remark 4. \"Combining SST with other existing techniques can help.", "However, the additional cost is expensive.", "Further demonstrations are necessary for the proposed SST method.\"", "A : It is true that combining and the additional cost is expensive.", "Therefore, we have modified our expressions in the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 114, "sentences": ["Thank you for your thorough assessment and helpful comments! To answer your two questions:", "1) Upper bound", "You are right that it would be ideal to optimize the target loss L_T(h, f_T) directly.", "However, this is not possible because we do not have labelled target data (i.e., f_T is unknown).", "Minimizing an upper bound is arguably the only viable option *with theoretical generalization guarantee*. It is a common practice in the domain adaptation community (Mansour et al., 2009a, 2009b; Ben-David et al. 2007, 2010; Cortes and Mohri, 2011), and it is essentially the key idea of PAC learning and generalization analysis (Gy\u00f6rfi et al., 2006; Sch\u00f6lkopf et al., 2002; Vapnik, 2013).", "Besides, our method *directly* optimizes the upper bound without resorting to heuristics, unlike prior methods.", "Ref:", "- Gy\u00f6rfi, L., Kohler, M., Krzyzak, A. and Walk, H., 2006. A distribution-free theory of nonparametric regression. Springer Science & Business Media.", "- Sch\u00f6lkopf, B., Smola, A.J. and Bach, F., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.", "- Vapnik, V., 2013. The nature of statistical learning theory. Springer science & business media.", "2) More experiment", "Thank you for pointing out these datasets.", "We have conducted further experiments on the Office-Home dataset,  using the ResNet50 as the backbone architecture and changing the classification head to 65 classes.", "As we can see in the new Section 5.3, our method achieve state-of-the-art performance and outperform all alternatives", "; these results are statistically significant.", "In addition, we have added one more competitive baseline (M3SDA) from the DomainNet paper you mentioned, using their public code with a few necessary adjustments for each dataset (e.g., network architecture, etc).", "We ensure that all methods use the same backbone architecture for a fair comparison.", "Again, our method outperform M3SDA in all datasets.", "If you have any other comments or concerns, we are happy to provide further feedback.", "Thank you!"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 115, "sentences": ["We thank our third reviewer for his comment.", "We do understand your concern about the significant increase in computational time.", "However, we believe that in the context of active learning, the main problem is not related to computational power, rather to the scarcity of data.", "Therefore, a better way of making the most out of little data is critical.", "For example, a 10 \\% increase for only 300 samples acquired, could make a huge difference in a critical field where active learning is most valuable.", "We believe that this is exactly what we manage to achieve with our method and this comes as a result of a better representation of uncertainty during AL.", "Furthermore,  Beluch et al. (2018) showed that going beyond 3 networks in their deterministic ensemble method does not add any significant improvements in terms of performance.", "Therefore we use 3 stochastic ensembles for our method.", "As for the novelty of this method, although it seems more like an engineering solution, we believe that it makes a significant contribution in the field of deep active learning."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 116, "sentences": ["We thank the reviewer for their time and welcome feedback, which we are incorporating into the revised version.", "R: - \"important features for the new task should be in similar locations ...\"", "- \"the locations for important features should be comparatively stable ...\"", "A:", "-- Continual learning typically assumes a degree of similarity among the tasks.", "If tasks are completely different from each other, then most continual learning frameworks will somehow struggle.", "For example, the standard Split MNIST benchmark is in line with this \u201clocations of important features\u201d assumption.", "Having said that, we acknowledge that more agility to, at least, discover that early on would be beneficial.", "More importantly, a normalization strategy on top of our attention map would help enhance its invariance properties, potentially leading to a more robust treatment of the locations of important features.", "In page 4 in the revised version (footnote 3), we have clarified this and notified its potential for future work.", "-- Thank you for the suggestion regarding the fixed attention map.", "We tried an experiment using the fixed attention map as a baseline, and as expected it performs significantly worse than ours.", "We have added that to the revised version (see p.6 and Appendix A).", "R: - FSM vs. Classification performance", "A:", "-- It is true that evaluating the FSM is not necessarily the same as the classification results, which is precisely the reason why we show both in our results.", "As specified in page 2, \u201cHere we propose a new measure ...\u201d - our point in this regard is to propose another (different) manner via which catastrophic forgetting can be estimated, which is not the same as the classification accuracy.", "The goal is that (as we know and agree they are two different measures that might agree or disagree in their judgments on catastrophic forgetting) both can be used to inspect the degree of catastrophic forgetting.", "We have further clarified that in Section 6.2 in the experiments by stressing that the obtained FSM results \u201calong with the classification results\u201d denote the significance of the whole framework in addressing catastrophic forgetting.", "It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.", "R: - Statistical significance", "A:", "-- Thank you.", "We have added the statistical significance results to the revised version.", "Since we were concerned that adding this information to the plots would make them harder to read", ",  statistical significance of the the average accuracy and FSM results obtained after completing the last two tasks from each dataset, i.e. the corresponding values of the last two tasks of all the plots in Figures 1, 2, 3 and 4, are now displayed in the tables in Appendix A.", "Checking cases where the learner incorrectly classifies the image in the second time step is sound and will be inspected in future work.", "R: - Clarity", "A:", "-- We have fixed the typos in the revised version, thank you: i) The first sentence of the third paragraph in Section 4 now reads: \u201cFor input images of ..., the averaged weight of evidence matrix  is referred to as $\\text{WE}_{\\bm{i}}(\\bm{x}) \\in \\RR^{\\bm{r} \\times \\bm{c}}$.\u201d  ii) In page 6: \u201cThe size of the surrounding square \u2026 is 16 $\\times$ 16 pixels."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 117, "sentences": ["Thank you for the comments!", "To review\u2019s questions:", "- As the experimental results shows, with position information alone, the agent is able to learn to push or pick up the object, therefore we consider position information alone (without velocity information) is sufficient in our case.", "- For MISC, the method needs to know what are the states of interests and what are the context state.", "While, the states of interest can be any states that users are interested in, such as a part of the robot states or the object states.", "The context states are some other states, which are different from the states of interest.", "In robotic tasks, the states of the robot and the object states are normally available [Andrychowicz et al 2018, Plappert et al 2018].", "To automatically detect the state of interests and the context states, we can train the agent with random state splits and then chose the combination, which is suitable for the tasks at hand.", "References:", "[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.", "[2] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow- ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce- ment learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.", "- Yes, MISC can deal with the case, when there are multiple objects of interest.", "We added new experiments showing the agent can learn to manipulate two balls.", "We define the mutual information intrinsic reward as I(S^i_{1}, S^c)+I(S^i_{2}, S^c).", "The experimental results are shown in the new video at https://youtu.be/l5KaYJWWu70?t=148, where we show that a robot car can learn to manipulate two balls in the same episode.", "- Equation (4) is not the mutual information between two trajectories of states.", "It is an estimation of mutual information between two sets of states. And the states are sampled from the same trajectory.", "Therefore, we do not need to decompose Equation (4) to evaluate Equation (3).", "- We add the experimental details in the Appendix.", "- The discriminator is trained along with the policy.", "For example, in the case that we update the agent 200 times in each epoch, then we also update the MISC 200 times per epoch.", "For more detailed information, please refer to our code at https://github.com/misc-project/misc", "- Compared to the dense reward, with the negative L2 distance between the robot and the object, the robot can only learn to reach the object but will not learn to push or pick up the object because when the robot reaches the object, the negative L2 distance is already zero.", "However, MISC has the advantage that it not only enables the agent to learn to reach but also learn to push and pick & place.", "- If we train the MISC and DIAYN at the same time, the DIAYN reward might be dominant. Subsequently, The agent might not learn to control the states of interests with MISC.", "- MISC-p works similarly to PER.", "The main difference is that MISC-p uses the estimated mutual information quantity as a priority, while PER uses the TD-error as a priority for replay.", "For more detail on PER, please refer to the original PER paper [Schaul et al 2016].", "Reference:", "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, 2016.", "- We first scale the intrinsic and the extrinsic reward between 0 and 1 and then use equal weights for these two rewards.", "- For the opposite situation, we can use negative mutual information rewards to encourage the agent to learn to \u201cavoid\u201d some objects.", "- The discriminator uses the same amount of (s,a,s') experience as VIME consumes because the discriminator is fixed after pre-training.", "VIME can only be trained along with the policy.", "VIME cannot be pre-trained, otherwise, it won\u2019t detect novel states.", "- Transfer the learned discriminator from Push to the Pick&Place should still help the agent to learn the pick & place task because the transferred discriminator will help the agent to learn to reach the object at least. As long as the state inputs for the discriminator are the same, then MI discriminator can be transferred among different tasks."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 118, "sentences": ["We thank the reviewer for his comments and observations.", "Following are the answers to each question you have raised.", "R1Q1(a): \u201cThe classification of base class into super classes seems questionable to me.", "In the meta-learning language, the author attempts to learn a good representation of graphs", "based on different graph classification tasks generated by a task distribution", ".", "In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).\u201d", "R1A1(a): Our model has two major components, $C^{sup}$ (for super-class prediction) and $C^{GAT}$ (for graph label prediction).", "During the training phase of our classification, $C^{sup}$, which is a MLP layer, learns the super-class labels of the samples based on GIN\u2019s extracted feature vectors (which represent base class labeled graphs).", "While, $C^{GAT}$ takes as input the \u201cgraph of graphs\u201d (supergraph) which models the latent inter-class as well as intra-class information and is constructed in every training batch, along with base-class labels, to learn the associated class distribution.", "Then, during the fine-tuning phase on graphs with novel class labels, the feature extractor\u2019s (GIN) parameters are fixed and $C^{sup}$ is used to infer the super-class label of the novel class labeled graphs.", "Then, the parameters learned by $C^{GAT}$ get updated and further \u201cfine-tuned\u201d for better performance on the novel samples.", "In addition to our brief overview, you could also find a very neatly detailed summarization of our method in reviewer 2\u2019s comments (paragraphs 1-4).", "The meta-learning framework, where batches are sampled as \u201cepisodes\u201d with N-way K-shot setting, does not perform as well in our few-shot setting on graphs for the following reasons:", "1) We have very limited total number of training classes (in order of 10s), when compared to the image domain (order of 100s and 1000s).", "This limitation hampers learning across tasks and generalization to new unseen tasks.", "2) In each of our batches, we randomly sample a fixed-size of training samples belonging to the set of N labels chosen.", "Therefore, when building our supergraph, we end up with k-NN graphs of \u201cvariable size\u201d per super-class, compared to fixed size (K nodes) k-NN graphs that we would have got using episodic learning.", "We suspect this further allows our GAT to learn and generalize better to unseen graphs.", "Furthermore, in [1], the authors use a similar strategy in their \u201cbaseline++\u201d method and produce good results.", "Their findings are also in sync with our empirical finding.", "[1] Chen et~al. \u201cA closer look at Few-Shot Classification\u201d, ICLR 2019", "R1Q1(b): Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.\u201d", "R1A1(b): In every batch of graphs during both training and fine-tuning phase, each graph is associated with its corresponding graph label.", "In case of training, its a base-class and in the case of fine-tuning its a novel class", ".", "In the case of $C^{GAT}$, the graph is accompanied by a regular class label and in case of $C^{sup}$, the graph is accompanied by a superclass label.", "R1Q2: \u201cThough seemingly very important to the architecture, the purpose of constructing the super-graph $g^{sup}$ in the training of $C^{CAT}$ seems to be unclear to me.\u201d", "R1A2: What makes few-shot learning particularly difficult compared to common machine learning settings is the dearth of training examples, which results in a bad empirical risk approximation for the expected risk and therefore gives rise to an empirical risk minimizer that is sub-optimal.", "Reducing the required sample complexity can result in a better empirical risk minimizer.", "Therefore, given a very large space of hypotheses H, our goal is to further restrict and constrain H using some prior knowledge because a reduced H has reduced sample complexity and thus requires fewer training samples to be trained.", "We provide this \u201cprior knowledge\u201d in the form of a \u201cgraph of graphs\u201d, namely our super-graph $g^{sup}$, which captures both the latent inter-class and intra-class relationships between classes.", "Observe that in $g^{sup}$, we build a k-NN graph PER super-class, restricting any flow of information between super-classes, thus further restricting H. We force our model to jointly learn both the superclass and graph class labels.", "This way similar classes (grouped under a superclass) together contribute to learning a general prior representing the superclasses and each superclass also provides \u201cguidance\u201d to better train with the few samples assigned to that superclass.", "The introduction of this prior knowledge in the form of a supergraph in $C^{GAT}$ during training also helps generalize better to the novel samples that are presented to our model in the fine-tuning stage.", "Additionally, we would also like to draw attention to the supergraph usage summary provided by reviewer 2 in paragraph 3 of their comments."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 119, "sentences": ["Thank you very much for your supportive remark! We are happy that the writing is clear to you. Below we provide additional comments regarding your questions.", "1). Low-rank assumption:", "This work was primarily motivated by the observation that many systems exhibit strong relationship among states and actions, governed by potentially simple dynamics.", "This might eventually lead to structures within the optimal solution.", "We hope that this empirical study would motivate further theoretical analysis on structures within the community.", "Below are some thoughts on the potential theoretical motivation for this assumption:", "a) It is possible that the states and actions in consideration have some latent variable representations.", "If the optimal Q function is a piecewise analytic function on the latent variables, then there are works arguing the approximately low-rank property of the resulting matrix [1].", "b) There are theoretical works in RL and Markov process that assume that the transition kernel can be decomposed to a low-dimensional feature representation [2,3].", "These assumptions on the transition kernel may lead to low-rank optimal Q matrices.", "c) For continuous problems, theoretical analysis often needs to assume some sort of smoothness in the Q function [4,5].", "It is possible that such smoothness in the Q function will result in a low-rank Q matrix when evaluated at finite but fine enough discretized grid.", "[1] Udell, Madeleine, and Alex Townsend. \"Why Are Big Data Matrices Approximately Low Rank?.\" SIAM Journal on Mathematics of Data Science 1.1 (2019): 144-160.", "[2] Yang, Lin, and Mengdi Wang. \"Sample-Optimal Parametric Q-Learning Using Linearly Additive Features.\" International Conference on Machine Learning. 2019.", "[3] Sun, Yifan, et al. \"Learning low-dimensional state embeddings and metastable clusters from time series data.\" Neural Information Processing Systems 2019.", "[4] Yang, Zhuora, Yuchen Xie, and Zhaoran Wang. \"A theoretical analysis of deep Q-learning.\" arXiv preprint arXiv:1901.00137(2019).", "[5] Shah, Devavrat, and Qiaomin Xie. \"Q-learning with nearest neighbors.\" Advances in Neural Information Processing Systems. 2018.", "2). Dynamical manner for the number of incomplete observations and whether the strategy can be adapted to the nature of the problem:", "This is a great point and definitely an interesting future direction.", "In the current work, it is not immediately that one could easily detect the rank and adapt the algorithm in a principled manner.", "As one practical solution, it may be possible to dynamically adjust the regularization in a manner similar to cross validation.", "At each step, for the submatrix, one could randomly sample a portion of the entries for ME, while keeping another fraction of the remaining entries as a validation set.", "If the recovered matrix via ME has a low reconstruction error on the validation set, it is likely that a suitable low-rank approximation is sufficient and has been found by the ME oracle.", "In contrast, if the reconstruction error is large, the algorithm might have been too aggressive on finding a low-rank solution while a higher rank solution is indeed necessary.", "As such, one could then adjust the algorithm to increase the number of observations for ME or try to reduce the level of low-rank regularization.", "The above cross validation scheme might be an interesting complement to our current approach.", "Overall, we believe that principally solving those questions you posted are meaningful and important directions that worth further investigations.", "3). Beyond the low-rank assumption and use a more elaborate type of prior:", "Thank you for your inspiring advice.", "Without any elaborate prior information, rank is a natural point to study the global property of a matrix.", "In principle, understanding structures in MDP could also be potentially explored, and we believe that it is possible to extend to other types of scenarios with prior information about the MDPs.", "However, at the current stage, we do not have a particularly systematic approach to explore more elaborate type of structures in MDPs.", "While this paper is focusing on low-rank structures, as the reviewer noted, there can be other structures to be explored.", "We hope that our paper could serve as an example, and further motivates future studies for exploiting structures in MDP.", "4). Notation in Section 4.2:", "Thank you for the suggestion.", "We will expand the definition of the notation to make them clearer."], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_future", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_social", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_by-cr_label"]}
{"abstract_id": 120, "sentences": ["We would like to thank Reviewer 3 for the review and constructive suggestions.", "Our responses inline:", ">it would be nice to also propose unconditioned experiments.", "-We agree; this was simply not within the scope of the work we conducted.", ">I understand that no data augmentation was used during training?", "-This is correct, and consistent with previous works (Spectral Normalization and WGAN-GP).", "We briefly experimented with data augmentation (random crops and horizontal flips) but did not notice any measurable performance difference.", ">clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?", "-Yes, this can effectively be seen as modifying the PDF of z to have no mass outside of the truncation threshold.", "TensorFlow offers a built-in implementation with tf.random.truncated_normal.", ">A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.", "-We have revised the abstract to explain the truncation trick as controlling the tradeoff between fidelity and diversity by reducing the variance of the Generator\u2019s input.", ">A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.", "-Thanks for the pointer! We have added this reference.", ">It would be nice to add a figure of random generations.", "-In the caption of Figure 5, we include a link to an anonymous drive folder with sample sheets at different resolutions and truncation values, with 12 random images per class.", ">make the bib uniform: remove unnecessary doi - url - cvpr page numbers", "-Thanks, we have fixed this."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 121, "sentences": ["We thank the reviewer for the thoughtful review.", "Responses:", "- We clarified the unclear parts, and will upload the revised version asap. (Also see below for specific responses.)", "- It is unclear to us if the reviewer thinks the computational complexity is high, or the mathematical complexity.", "With regards to mathematical complexity, we believe the model is actually rather simple (see also other reviews).", "Thus, we assume computational complexity is meant.", "Computational complexity is discussed in detail in the manuscript though, and is certainly not higher than competing methods (in part thanks to the low mathematical complexity of the model).", "See also next point.", "- The datasets we used are as large as the datasets used in other related work in the area.", "To demonstrate CNE's superior scalability, we included another network with around 200.000 nodes and around 1.000.000 edges (http://snap.stanford.edu/data/loc-Gowalla.html), run on a basic single CPU laptop.", "Again, CNE outperforms all other methods in accuracy by a wide margin, and is substantially faster as well.", "The results are included in the revised manuscript.", "Detailed comments:", "- \"In the introduction, \"it is in general impossible to find an embedding in R^d such that ...\", [...]?\"", "We apologize for having been a bit brief here, we will clarify this in the revision (uploaded asap).", "We meant to say that in network embedding methods that aim to model first-order proximity (where proximity in the embedding space implies a higher probability of being linked), this is a requirement (otherwise, proximity of v and v' would imply they are likely to be linked).", "Thus our argument only applies to such first-order proximity methods.", "Methods that aim to model second-order proximities (where proximity in the embedding space implies a greater overlap between the sets of adjacent nodes), however, are similarly vulnerable.", "For example, there can be a 50% overlap (which is highly significant in sparse networks) between the neighborhoods of nodes A and B, as well as between the neighborhoods of nodes B and C, but zero overlap between the neighborhoods of nodes A and C.", "This would mean that nodes A and B need to be embedded close to each other, nodes B and C as well, but nodes A and C distant from each other.", "The triangle inequality makes this hard.", "Finally, these are but examples of how a Euclidean embedding on its own lacks representational power.", "We believe that our empirical results also demonstrate this without having to refer to easy-to-identify problematic situations for pure embedding-based methods.", "- \"In Equation (2), How is P_ij defined exactly [...]?\"", "They are not parameters: they are numbers between 0 and 1 representing the prior probability of a link between nodes i and j (i.e. prior to seeing the embedding).", "These numbers are such that the prior knowledge of the types described are satisfied in expectation.", "In other words, they are implied and can be computed automatically and highly efficiently based on prior work, after one has decided on which prior knowledge to use.", "For details about how P_ij are fitted given such prior knowledge constraints, and how they can be represented efficiently, we have to refer to Adriaens et al. (2017) and van Leeuwen et al. (2016).", "We have however summarized the relevant aspects: the fact that all probabilities P_ij, although there are n^2 of them, can be represented using much fewer parameters, and the fact that they can be fitted highly efficiently (in our experiments, even on the largest networks, this always took only a tiny proportion of the total computation time).", "In the new version to be uploaded soon, this will be further clarified.", "- \"In Equation (6), the posterior distribution should be P(X|G) [...]?\"", "No, the equation is correct as stated.", "Footnote 2 warned the reader about this, as we know it is unusual.", "The posterior is a distribution for the network, such that finding the best embedding is indeed a maximum likelihood problem (not a maximum a posteriori problem), even though the likelihood function is computed as a posterior given a prior for the network and a conditional for the embedding given the network.", "We suspect that it is this unusual aspect of the CNE formulation that makes it original with respect to the state-of-the-art.", "- \"In Table 2 and 3, how are [...]?\"", "Equation (6) defines the posterior of the network given the embedding, which we maximize w.r.t. the embedding.", "It explicitly depends on the prior probabilities P_ij, which are computed based on the prior knowledge about the degrees or about the block density structure of the adjacency matrix.", "Thus, this information is brought into the model by considering the posterior distribution for the network, where the prior models the degrees of the nodes, or the block structure."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 122, "sentences": ["1. We aimed to provide a broad variety of example applications (playing tennis, walking, fencing, dancing), while mainly focusing on the most complicated (tennis) application, for a thorough analysis of our method.", "Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.", "2. The Pose2Pose and Pose2Frame networks are trained separately.", "Specifically, the P2F network is trained on the original data, and not on the output frames of the P2P network.", "You are correct that some artifacts are added to the final P2F output at test time, yet they are minor due to the structural stability of the poses generated by the P2P network.", "Furthermore, training the P2F network with the P2P outputs is problematic, since we do not have the ground-truth for the new pose generated by the P2P network.", "3. The mask loss proposed in the review is similar to our implementation, except that we make a distinction between an inner-mask control and an outer-mask control.", "Our mask regression losses consist of a first loss penalizing the mask from being active outside the densepose mask, and a second loss penalizing the mask from being inactive inside the densepose mask.", "Combining them both results in the suggested loss."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 123, "sentences": ["Thanks for your thoughtful review.", "We have given serious considerations of your concerns and revise our manuscript to accommodate your suggestions.", "Please see the details below.", "Q1: \u201cThere are a few grammatical/spelling errors that need ironing out.\u201d", "A1: We have fixed the typos and grammatical errors in the revision.", "Q2: \u201cPioneering work is not necessarily equivalent to \"using all the GPUs\"\u201d", "A2: This claim is indeed not accurate we have delete this claim in the revision.", "Q3: \u201cThere are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work!\u201d", "A3: We have changed the word to \u201cimpressive\u201d in the revision. However, DSO-NAS indeed outperforms DARTS on ImageNet dataset as illustrated in Table2.", "Q4: \u201cFrom figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?\u201d", "A4: In the search stage, the scaling factors are only used to indicate which operators should be pruned.", "The value of scaling factors do not represent the importances of kept operators since they can be merged into the weights of convolution.", "We also add experiments in CIFAR-10 to compare the performance between keeping the non-zero weightings and equal weightings.", "The result shows that both of them yield similar performances.", "---------------------------------------------------------------------------------------------------------------------------", "Architecture         \t                    params(M)", "test error", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-share+c/o", "3.0                        \t2.84", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-share+c/o+k/w", "3.0                              2.88", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-full+c/o", "3.0                        \t2.95", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-full+c/o+k/w", "3.0                        \t2.96", "---------------------------------------------------------------------------------------------------------------------------", "where \u201cc/o\u201d represents that training the searched architectures with cutout and \u201ck/w\u201d represents keeping the non-zero weightings in the architectures.", "Q5: \u201cWhy have you chosen the 4 operations at the bottom of page 4?\u201d", "A5: These four operations were used by ENAS and commonly included in the search space of most NAS papers.", "Q6: \u201cHow do you specifically encode the number of surviving connections?\u201d", "A6: We don\u2019t directly encode the number of surviving connections.", "Instead, the number of surviving connections is determined by the weight for L1 regularization, which can be incorporated with certain budget.", "Q7: \u201cMeasuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?\u201d", "A7: All of our experiments were conducted by NVIDIA GTX 1080Ti GPU, which was also used by ENAS and DARTS.", "We have added it in the paper."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 124, "sentences": ["We thank the reviewer for the insightful comments.", "We address the questions in the following:", "- How many images did you have in the experiment?", "We had 7500 images in total.", "We had 3 concept classes, and 2500 images for each concept.", "We will mention the total number in the main text.", "- The proposed network is not deep, but shallow", "We agree that a clear distinction line between shallow and deep networks does not exist.", "So we will make a note on that issue.", "- More experiments on the number of layers", "We had experimented with fewer layers.", "We realized that in this case the width of the network should be increased to compensate for the representation power of the network.", "As we already had an extensive set of experiments, we decided not to report that.", "As the proposed architecture already performs well to solve the ordinal embedding problem, we found it unnecessary to try deeper networks.", "- \"I don't see a clear conclusion of how to pick the width of hidden layers, maybe a better representation could be used.\"", "There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.", "We also generated line plots (multiple curves in one plot) and 3D mesh plots to show the dependency.", "In the end, we found the heat-map more informative.", "In the revision, we will add the other plots to support the claim.", "- \"I don't see a discussion about the downsides of the method\"", "One of the drawbacks is that our method needs GPUs, while the more traditional algorithms run on CPUs.", "This can be of disadvantage if non-machine learning experts want to use our method.", "However, this is the case for most recent ML methods based on neural nets.", "The number of required triplets is theoretically lower bounded by nd log n, and this is also being confirmed by our experiments (our algorithm, as well as our competitors, break down when they get fewer triplets).", "Therefore, in a setting with passive triplet answers, and without extra information, it is impossible to overcome this problem.", "- \"in section 4.4", "when comparing the proposed approach with another method why not use more complex datasets (like those used in section 4.3)\"", "Independent of the dataset complexity, provided with enough triplet answers, all methods can yield less than 5% triplet error.", "However, the computation time is significantly lower for our proposed method.", "Due to the iterative nature of all algorithms, the computation time does not depend on the data distribution, but on the number of input points.", "Thus, a simple uniform dataset could serve to show our intention in this section.", "- \"in section 4.3, there is no guarantee that the intersection between the training set and the test set is empty.\"", "Yes, in theory that is true, but in practice this is negligible: the total number of possible triplets is about 10^9. So the likelihood that two sets of size 1000 intersect is close to 0.", "- \"in section 4.3", "how is the reconstruction built (Figure 3b)?\"", "Figure 3b is the exact output of the ordinal embedding in two dimensions.", "The colors are the initial labels of the input items.", "There are two or three labels assigned to demonstrate the quality of reconstruction.", "Note that the ordinal embedding output is unique only up to isometric transforms.", "In other words, every valid output is still valid with rotation, scaling and translation."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 125, "sentences": ["Many thanks for the detailed review!", "1/ We agree that there are many elements of our architecture that are similar to that of a convolutional network, however the network does not perform convolutions.", "To reflect both points, we have revised the text to:", "``The network does not use convolutions.", "Instead, the network does have pixelwise linear combinations of channels, and just like in a convolutional neural network the weights are", "are shared among spatial positions.", "Nonetheless, they are not convolutions because they provide no spatial coupling between pixels, despite how pixelwise linear combinations are sometimes called `1x1 convolutions.' '',", "and we have also added a subsection comparing the compression performance of our architecture to that of a decoder with convolution layers.", "In a sense, what the deep decoder is doing is separating multiple roles that proper convolutional layers fill:  the DD breaks apart the spatial coupling inherent to convolutions from their channel dependence and equivariance.", "Further, it says that the spatial coupling need not be learned or fit to data, and can be directly imposed by upsampling.", "2/ Yes, the upsampling analysis in Figure 5 also extends to two-dimensional images.", "We agree that natural images are only approximately piece-wise smooth after all, and the deep decoder only provides an approximation of natural images (albeit a very good one).", "3/ We agree and have changed `batch normalization' to `channel normalization' throughout.", "4/ Great point; we have added the sentence ``The optimal $k$ trades off those two errors; larger noise levels require smaller values of $k$ (or some other form of regularization).", "If the noise is significantly larger, then the method requires either choosing $k$ smaller, or it requires another means of regularization, for example early stopping of the optimization.", "For example $k=64$ or $128$ performs best out of $\\{32,64,128\\}$, for a PSNR of around 20dB, while for a PSNR of about 14dB, $k=32$ performs best.''", "5/ We do not mention the standard deviation, but do specify the SNR throughout (e.g., in table 1 in column identity).", "We have clarified this in the caption of the table.", "6/ It essentially produces smooth noise then.", "The weights learned by the deep decoder pertain to the source noise tensor.", "We have added a corresponding figure to the jupyter notebook for reproducing Figure 6."], "labels": ["rebuttal_social", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 126, "sentences": ["Thank you for your thoughtful review.", "We appreciate that you think our problem and approach are interesting! We have made some substantial revisions to clarify the presentation, we hope that these will help address your concerns.", "We respond to some specific comments below:", "> No comparisons were provided to any baselines/alternative methods.", "We do compare to a number of lesions in the supplement, and to an alternative method (performing tasks from a description alone).", "We have moved this latter comparison to the main text at the suggestion of Reviewer 1.", "> in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.", "Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively.", "This makes a comparison with MAML even more desirable. Without any comparisons it\u2019s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.", "Our main contribution is to propose a meta-mapping framework for zero-shot task performance, and parsimonious method for performing these meta-mappings.", "MAML as such is not a method of zero-shot task performance, it requires examples to learn", "from", ".", "We could therefore compare to MAML for our basic-meta-learning results, but those are simply a sanity check.", "We also compare to a variety of baselines, including chance and optimal performance, untransformed representations, and the most correlated task experienced (in the cards domain).", "However, if our paper is accepted, and you feel that the comparison to MAML for basic meta-learning is useful, we will run MAML on our tasks before the camera-ready submission.", "One might take inspiration from our framework try to use MAML for zero-shot task performance by transforming task representations would require adopting our meta-mapping framework, as well as a number of ideas of our architecture (where do the task representations come from, and how are they used?), and so its not clear to us that this is an appropriate baseline, rather than simply another implementation of our technique.", "Instead, we feel the more appropriate baseline is performing tasks from a natural language description, as many prior zero-shot works have, and so we have moved this result to the main text, as noted above.", "> The experimental results presented were all done on small synthetic datasets and it\u2019s hard to evaluate whether the method is practically useful.", "We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.", "We remained with synthetic tasks for two reasons: 1) to illustrate the method in settings we thought would be clearer, 2) because as we highlight in the future directions, there is a lack of meta-learning datasets that contain as structured of relationships between tasks as we consider.", "(Taskonomy, for example, has at best a notion of \"similarity\" in terms of transfer.) We are working on creating several such datasets, but we think that this paper as it stands is a useful contribution that illustrates the concept and ideas -- the datasets themselves will also require further description, and including them in a paper of this length would likely result in even more material being cut, and so a less clear presentation.", "We do think this is an important direction, but we think this paper makes a useful contribution by highlighting this new perspective. We hope that you agree.", "> Also, the work by Platanios, et al. on contextual parameter generation is very relevant to this work as it tackles multi-task learning using HyperNetworks.", "It may be worth adding a short discussion/comparison to that work as it also considers zero-shot learning.", "Thanks, we weren't familiar with this very interesting work! We have added a reference and a brief discussion of the relationship.", "- Capitalize: \u201csection\u201d -> \u201cSection\u201d, \u201cappendix\u201d -> \u201cAppendix\u201d, \u201cfig.\u201d -> \u201cFigure\u201d.", "Sometimes these are capitalized, but the use is inconsistent throughout the paper.", "Thanks for pointing this out, fixed.", "- \u201cHold-out\u201d vs \u201cheld-out\u201d", ".", "Be consistent and use \u201cheld-out\u201d throughout.", "We are making a grammatical distinction here -- \"held-out\" is an adjective and \"hold-out\" is a noun. That is, one might refer to a \"held-out task\" or to a \"a hold-out.\" Hopefully this clarifies things.", "We hope that this clarifies things, and will help to address your concerns. Please let us know if further changes would be useful."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_social_label"]}
{"abstract_id": 127, "sentences": ["We thank the reviewer for their comments and for noting correctly that our modification is quite effective, particularly regarding the large improvements on human evaluations.", "Our method is simpler in both conception and implementation than coverage, while requiring less parameters and being twice as likely to be chosen as better by human judges.", "We agree with the reviewer on the simplicity of our method, which we believe to be an asset.", "In addition to that, we believe the Scratchpad Encoder is fundamentally interesting as a mirror to the \u2018attentive read\u2019 common in seq2seq models.", "We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_accept-praise", "rebuttal_answer", "rebuttal_concede-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 128, "sentences": ["We would like to thank the reviewer for the comments and for raising some subtle yet important questions.", "We address and clarify specific comments below.", "We have also made corresponding changes in the revised paper, and have added a proof map, in addition to the Table 3, for easier navigation of the results.", "We have also added comparisons with Mairal `09, and experimental evaluation of computational time.", "1. Noise Tolerance \u2014 NOODL also has similar tolerance to noise as Arora et. al. 2015 and can be used in noisy settings as well.", "We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.", "Nevertheless, the proposed algorithm can tolerate i.i.d. sub-Gaussian noise, including Gaussian noise and bounded noise, as long as the ``noise\u2019\u2019 is dominated by the ``signal\u2019\u2019.", "Under the noisy case, the recovered dictionary and coefficients will converge to a neighborhood of the true factors, where the neighborhood is defined by the properties of the additive noise.", "In other words, the noise terms will lead to additional terms which will need to be controlled for the convergence analysis.", "Specifically, the noise will add a term to the coefficient update in Lemma 2, and will effect the threshold, tau.", "For the dictionary, the noise will result in additional terms in Lemma 9 (which ensures that the updated dictionary maintains the closeness property).", "A precise characterization of the relationship between the level of noise the size of convergence neighborhood requires careful analysis, which we defer to future effort.", "2. On eps_t and A.4. \u2014  Indeed, we don\u2019t need to assume that eps_t is bounded.", "Specifically, using the result of Lemma 7, we have that eps_0 undergoes a contraction at every step, therefore, eps_t <= eps_0.", "For our analysis we fix eps_t = O^*(1/log(n)), which follows from the assumption on eps_0= O^*(1/log(n)) and Lemma 7.", "On reviewer\u2019s comments, we have updated A.4., and moved the note about eps_t = O^*(1/log(n)) to the Appendix A.", "3. Exact recovery of factors \u2014 Also, we would like to point that NOODL recovers both the dictionary and coefficients exactly at a geometric rate.", "This means that as t\u2014> infinity both the dictionary and coefficients estimates converge to the true factors without incurring any bias.", "We have added a clarification corresponding to this in the revised paper in Section 1.1 and after the statement of Theorem 1 in Section 3."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 129, "sentences": ["Thank you for the detailed review.", "We appreciate your extremely useful pointers to existing dependency testing techniques, many of which are new to us. Before delving into them, here's our initial thoughts.", "First and foremost, we would like to know more details on the reasoning behind the rejection rating.", "It seems that the criticisms are on citations to other dependency testing approaches and time-dependency of fMRI data.", "The suggestions are invaluable and we'll gladly include them (work in progress).", "But comparison with them is not apples to apples , so we are not sure to what extent that adds value to our work, where we already compared to the KSG estimator for MI estimation and Pearson's correlation for dependency testing.", "Comparing to other dependency testing approaches, our technique allows the use of arbitrary neural networks and directly tests MI>0, the necessary and sufficient condition of dependency, is more general and is complementary to other techniques that make stronger assumptions about the data.", "We want to point out that a simple and most widely used dependency testing technique is Pearson's correlation through test of linear correlation (sufficient condition of dependency), which we compared to and show that our technique provides complementary value on sine wave and the fMRI dataset.", "We are aware that low sample complexity dependency tests exist and can be achieved by making additional assumptions on data, and we discussed that in the conclusion section, but our technique makes less assumptions and is applicable to general datasets which may not satisfy stricter assumptions.", "We are very interested in the HSIC-based techniques which seem to be popular and we could show complementarity. But at the same time the conclusion will be the same, so we have question on what value does it add for the audience over Pearson's correlation.", "Regarding other methods for dependency testing through mutual information, after following the line of work by Barrett et al. 2017, we reached this concise summary http://ims-vilnius2018.com/content/pdf/ivc293.pdf, which explains the smoothness assumptions made to data, as well as the fact that they used an asymptotic variance which holds when a large number of samples is given (page 2 top).", "It implies that the resulting confidence intervals as well as the test results are asymptotic and not guaranteed, which puts the resulting statistical tests into question.", "We have to admit that we did not thoroughly understand this line of work because of our background, so please comment if we are wrong about that.", "Instead, our dependency test does not make assumptions about data.", "Our lower bound and its confidence interval are not asymptotic.", "Theorem 1 provides a guaranteed confidence interval for arbitrary number of samples (so do our baselines, MINE and Pearson's r).", "We compared with the KSG estimator, which also only has asymptotic confidence intervals in literature.", "In addition, the proof provided in our work is concise and is easy for readers to understand and verify.", "Regarding time dependence and test threshold, it's important and thanks for pointing it out! We think that there are two ways time affects dependency listed below.", "1) First, we assume non-overlapping windows of TRs as the basis for computing number of i.i.d fMRI samples, but did not mention that in our current draft. We'll update and make it clear.", "2) Second, we can see an argument on whether or not segments of fMRI signals qualify as i.i.d, although not sure if this is a problem for our MI estimation approach.", "If things change very little from one second to the next, the signals could be very similar and may not really be, intuitively, independent samples and may bias result of the study.", "However, which independence assumptions to use is not in scope for our paper, because our fMRI study is trying to show that dependency testing works and is complementary to Pearson's correlation, not so much on drawing neuroscience conclusions.", "On a side note, it may turn out that which independence assumptions to make is a deeper question that doesn't yet have a clear answer.", "Regarding the level of the test, our theorem 1 already provides a proof based on Hoeffding inequality.", "This proof could be experimentally verified through computing MI on 300 test set samples and see how the estimate changes if there were >1million test set samples, repeat say 1000 times using different random seeds."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_followup", "rebuttal_followup", "rebuttal_by-cr", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 130, "sentences": ["We would like to thank the reviewer for the time and useful feedback.", "Our response is given below.", "- The paper is mainly empirical, although the authors compute two diagnostic statistics to show the effect of the self-modulation method.", "It is still not clear why self-modulation stabilizes the generator towards small conditioning values.", "We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.", "As a first step, we provide a careful empirical evaluation of its benefits.", "While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.", "Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.", "- It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].", "It seems that the authors are not aware of this difference.", "We are aware of this key difference and we apply the sigmoid function to scale the output of the discriminator to the [0,1] range for the non-saturating loss.", "Thanks for carefully reading our manuscript and noticing this typo which we will correct.", "- In addition to report the median scores, standard deviations should be reported.", "We omitted standard errors simply to reduce clutter.", "The standard error of the median is within 3% in the majority of the settings and is presented in both Tables 5 and Table 6."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 131, "sentences": ["Thank you for your fruitful comments.", ">> 1.", "[...]", "One observation from the submission is that the token set may need to be very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive [...] I think some more motivation or exploration (what kind of information did BERT learn) is needed to understand why that is the case.", "Our BERT vocabulary sizes (13.5k for the gumbel version and 23k for the k-means version) compare favorably to the setups commonly used in NLP where vocabularies are double or triple of our sizes.", "We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.", "Here we focus on a new quantization method evaluated via downstream performance in phone and speech recognition settings by employing models that worked well (and were extensively tuned) in NLP contexts.", ">> 2.", "A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.", "Yes, this is an interesting avenue for future work!", "We did not follow this direction due to two motivations: first, our aim is to contribute a new quantization scheme for audio data that is trained to predict the context in a self-supervised way.", "Second, we wanted to show that good performance can be achieved with discretized audio on actual speech tasks.", ">> 3. One concern I have with discrete representation is how robust they are wrt different dataset.", "We agree that an ablation study on robustness of the embeddings across different datasets would be very interesting.", "Here we are mostly focusing on relatively clean data (WSJ, TIMIT, Librispeech) following the original wav2vec paper but we would be interested in exploring robustness in the future.", "However we note that representations transfer at least well across datasets from the \u201cclean speech\u201d domain: vq-wav2vec and BERT is only trained on Librispeech and never tuned on TIMIT/WSJ.", ">> 4.", "Another curious question is whether the features would still provide as much improvement when a stronger ASR system than AutoSeg (e.g., Lattice-free MMI) is used.", "The original wav2vec paper (Schneider et al., 2019) reports better results than LF-MMI on the WSJ benchmark, however, the two setups are not strictly comparable.", "In some sense, the LF-MMI result has an edge because it is based on a phoneme-based ASR system which is typically stronger than the character-based ASR system used with wav2vec.", "We agree that evaluation on stronger baselines is an important future direction though."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label"]}
{"abstract_id": 132, "sentences": ["Thank you for your comments and suggestions.", "We will address the issues you mentioned.", "1.\tThank you for the insightful suggestion.", "We now have added related work about video compositional methods in section 2.3 in the second version of the paper.", "2.", "In the original version of the paper, all experiments are conducted on trimmed video classification datasets.", "Although most papers in this field only report results on the trimmed video datasets, we do agree that more complicate cases should be tested.", "Additionally, we evaluated our V4D for untrimmed video classification on ActivityNet v1.3, which contains videos of 5 to 10 minutes and typically large time lapses of the videos are not related with any activity of interest.", "The very competitive result is reported in the appendix of the second version of paper, which demonstrated the generalization and robustness of our V4D.", "In fact, unlike previous video compositional methods, even when local events are not well aligned or misclassified, long-term modelling with 4D convolution and video-level aggregation with global average pooling are very likely to correct the partial error.", "3.About complexity, in the original version of the paper, we have reported parameters and FLOPs of V4D and compared it with other baseline methods in Table 2.", "4. We have already corrected the typo in title in the second version of the paper. Yet it seems that we are not able to modify the title on OpenReview. Thank you for pointing it out.", "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 133, "sentences": ["Thank you for your constructive comments.", "We are glad that you found our experiments extensive and that our approach provides significant improvements.", "In response to your comment that \"similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts\" we would like to take this opportunity to clarify the novelty of our approach.", "First, with regards to (Mescheder et al 2018), our method is not simply the application of existing gradient penalties (GPs) in the context of semi-supervised learning.", "Our approach is conceptually different since the regularizer proposed by (Mescheder et al 2018) is an (isotropic) ambient regularizer in the input space, whereas the regularizer we used performs (anisotropic) smoothing on the manifold parametrized by the latent generative model.", "We believe we are the first to show the benefits of anisotropic Jacobian regularizers in the context of semi-supervised learning.", "Moreover, an important contribution of our work is the efficient computation of such gradient penalties in the context of semi-supervised learning.", "Current application of such penalties uses the exact Jacobian which is especially computationally expensive in the case of semi-supervised learning as it is now a tensor (one matrix per class in the case of Improved GAN), which quickly becomes intractable with large numbers of classes.", "We proposed and demonstrated the effectiveness of an efficient (non-obvious) approximation of the Jacobian-based regularizer which significantly accelerates training.", "We provide responses to further questions/comments below:", "Q: \"A comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).\"", "A: Methods such as (Kipf and Welling 2017) are designed for semi-supervised learning on graphs; here a key challenge is in defining the structure (edges and edge weights) of the graph.", "Defining the graph structure is not trivial for the image datasets commonly used as benchmarks.", "In this light, one of the advantages of our approach is that the manifold (graph structure) is implicitly learned by the GAN, thus avoiding the need to explicitly define it.", "That said, it is an interesting direction for future work and we thank the reviewer for the suggestion.", "Q: \"How do the FID/Inception improvements compare to (Mescheder et al 2018)?\"", "A: We cannot directly compare our image generation scores with those reported in (Merscheder et al 2018) as we used different GAN architectures; for reference, they reported an Inception score of 6.2.", "We have updated the paper with Inception/FID scores from the ambient regularizer on CIFAR-10 (Table 4), which is an approximation of the proposed regularizer in (Merscheder et al. 2018) using stochastic finite differences.", "As mentioned earlier, it is not practical to compare the non-approximated regularizer due to the substantial increase in computational complexity in the semi-supervised GAN setting.", "We observe that ambient regularization gives better image generation scores; however it does not perform as well on semi-supervised learning.", "This tradeoff between image generation and semi-supervised learning performance was previously reported in (Salimans et al., 2016) \"Improved Methods for Training GANs\".", "Q: \"It would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.\"", "A: We re-checked our FID computation for this case and fixed a bug.", "We have updated the paper with updated FID scores; we note there is a high variance in the FID so while there is an improvement on average, it occasionally may not be better.", "Q: \"Although there is a clear improvement in FID scores for Cifar10. It would be informative to show the generated images w/ and w/o manifold regularization.\"", "A: We have included generated images with and without manifold regularization in the Appendix (Figure A5 for CIFAR-10, Figure A6 for SVHN) - these show clear improvements as well.", "Q: \"More analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.\"", "A: We note that although on average the method of (Kumar et al 2017) performs better on SVHN, the standard deviation is also much higher than many other methods (including ours) on both SVHN and CIFAR-10 indicating that it is not as robust.", "Q: \"It should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach.\"", "A: We have revised the tables such that bold values represent the best results for clarity."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_done", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 134, "sentences": ["Thank you for the insightful review.", "We updated the paper with better results and more tasks.", "We show that our method outperforms the human baseline in terms of training speed and either matches or outperforms the human in terms of final accuracy on all tasks.", "While it is true that the human baseline does not require any additional computational resources for training, it does require domain expertise acquired through years of learning, which is arguably even more costly.", "Notably, in all 4 problems where we compare to the human baseline, we believe that human researchers used a similar or higher number of runs as our tuner to design the baseline schedules that we compare against.", "We also updated the paper with more details regarding Transformer and Proximal Policy Optimization.", "Thank you for mentioning the existing learning curve modeling methods.", "We added an explanation of differences of our method with those works.", "[1] learn a probabilistic model of one training curve using a handcrafted basis of nonlinear functions of shapes similar to the training curves being modelled.", "Our method does not make any assumptions about the shape of the modelled curves and is able to jointly model many training curves - in our experiments, training and validation loss and accuracy.", "[2] learn a deterministic model of a learning curve, while our method also models stochasticity, hence providing diverse experience for training a reinforcement learning agent.", "Also in contrast to [1] and [2], our method allows the hyperparameters to change over the course of training and models the influence of those changes on the training metrics.", "[1] Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).", "[2] Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 135, "sentences": ["We thank the reviewer for the comments on our paper.", "A prior work of Hsu et al. (ICLR'18) showed that heavy hitter oracles exist and that they can be constructed using machine learning techniques.", "We are using the same type of oracles in our current submission.", "Similar oracles have been studied in previous works too, e.g., membership oracles for Bloom filters in Kraska et al. Both the previous works and the experiments in the current submission demonstrate that it is reasonable to make such an oracle assumption.", "We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.", "The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.", "The prior work by Hsu et al. showed that the oracle trained by deep learning has high accuracy (see Section 5.3 in their paper): for Internet traffic data, the AUC score is 0.9, and for search query data, the AUC score is 0.8.", "The performance of a simple online algorithm would likely depend on the type of classifier used and input feature representation.", "Linear classifiers with IP addresses represented as individual bits are unlikely to work well because their expressive power is limited.", "For instance, at the very least, we would like our classifier to express a DNF hypothesis of the form:", "(IP address = a1) or (IP address  = a2) or ...", "We have updated the introduction to rephrase and clarify the lower bound claims.", "The added/modified text are highlighted in the blue color."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 136, "sentences": ["We thank the reviewer for a detailed review.", "We have implemented the suggestions for improving clarity.", "Regarding our use of \u2018axioms\u2019: We follow the economics literature in using axioms as normative concepts, i.e., to denote desirable properties that a neuron importance methods.", "And not the use in the mathematical literature, which is to denote statements that are self-evidently true.", "We have clarified this in the submission."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 137, "sentences": ["We thank the reviewer for the thoughtful review.", "The reviewer points out that CNE works well for link prediction and visualization.", "We wish to point out that our experiments indicate CNE consistently outperforms the state-of-the-art not only for these tasks but also for multi-label classification.", "Our responses:", "a) Variational inference is useful in particular when the partition function is hard to compute, which is not the case here.", "So we believe it would be overkill in this case.", "In any case, it is not needed to achieve the performances CNE achieves at a very modest computational complexity and fast practical runtimes.", "b) No hand-engineering is needed to use CNE even when using more informative priors modeling node degrees and block structure.", "The degree of each node can simply be computed on the training set, so no hand-engineering is needed (just the choice to include it or not -- and it always better to include it).", "The block structure, on the other hand, will often be part of the data specification or meta-data of the nodes.", "For example, the network may be a multi-partite network representing a relational database, or it may be a company social network where the nodes are employees, and generic job titles are known for each of the employees (as attributes of the nodes).", "The entity types in the first example, and the job titles in the second example, would then define the blocks, and the density of the parts of the adjacency matrix between any two such blocks can again easily be computed on the network.", "Our method imposes no constraints on such blocks (e.g. they may even be partially overlapping).", "Again, all that is needed is choosing whether to use a block prior for any specified attribute (in the two examples: entity type, and node attribute).", "Again, empirically, including it always appears to be better, so one could even avoid having to make the choice.", "Inferring structural properties of the graph to be used in the prior (e.g. using GraphRNN), as we understand the reviewer suggests, certainly sounds potentially interesting.", "However, while it may improve accuracy, we do not believe that it adds value in reducing the amount of hand-engineering needed, as the amount of hand-engineering needed is very minimal already.", "The fact that CNE could be combined with such inferred structural properties increases its potential impact though, and this remark of the reviewer further underscores the need for methods such as CNE that can take such structural information into account.", "The boost in accuracy achieved by CNE, using a model that is arguably also a lot simpler than the state-of-the-art network embedding approaches, is thus achieved without any increased need for hand-engineering."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 138, "sentences": ["Thank you for your review and valuable comments!", "Summary: our response includes: (1) Clarification on language translation baselines; (2) Discussion on image translation evaluation; (3) Reference and clarification.", "** Language Translation Baselines **", "1. For the baseline models reported:", "1.1) We use the transformer model with \"transformer_big\" setting [1], which is a strong baseline that outperforms almost all previously popular NMT models based on CNN [2] and LSTM [3].", "Transformer is the state-of-the-art NMT architecture.", "Our numbers of the baseline transformer model match the results reported in [1].", "1.2) In addition to the standard baseline models, we also compare our method against all the relevant algorithms including knowledge distillation (KD) and back translation (BT).", "1.3) As can be seen in many well-known and recent NMT works ([4], [5])", ", it is a common practice to use transformer as the robust baseline model.", "Furthermore, it is also shown from these works that it is hard to improve over the transformer baseline, and 0.5-1 BLEU score improvement is already considered substantial.", "2. We further add newly obtained results on the WMT18 challenge.", "We compare our method with both the champion translation system MS-Marian (WMT18 En->De challenge champion).", "Our method achieves the state-of-the-art result on this task.", "---------------------------------------------------------------------------", "WMT En->De", "2016", "2017", "2018", "---------------------------------------------------------------------------", "MS-Marian (ensemble)", "39.6", "31.9          48.3", "Ours (single)", "40.68", "33.47       48.89", "Ours (ensemble)", "41.23", "34.01       49.61", "---------------------------------------------------------------------------", "Please refer to Section 3.4 \"Study on generality of the algorithm\" for more details and Table 4 for full results in our updated paper.", "** Image Translation Evaluation **", "For image-to-image translation tasks, we further add two quantitative measures: (1) We use the Fr\u00e9chet Inception Distance (FID) [6], which measures the distance between generated images and real images to evaluate the painting to photos translation.", "(2) We use \"FCN-score\" evaluation on the cityscape dataset following [7].", "The results are reported in Table 6 and Table 7 respectively.", "Multi-agent dual learning framework can achieve better quantitative results than the baselines.", "We are not sure what you meant by \u201cHow does their ensemble method compare to just their single-agent dual method?", "\u201d", ".", "The standard CycleGAN model (baseline) already leverages both primal and dual mappings, which is equivalent to our \u201cDual-1\u201d model in NMT experiments, i.e., the dual method with only one pair of agents f_0 and g_0.", "Our model involves two additional pairs of agents (f_1 and g_1, f_2 and g_2) during training.", "Unlike ensemble learning, only one agent (f_0 for forward direction, or g_0 for backward direction) is used during inference.", "** Reference **", "Thanks for pointing a reference paper \"Multi-Column Deep Neural Networks for Image Classification\" (briefly, MCDNN) and we have added reference to it (Section 4).", "Although MCDNN also uses multiple agents (i.e., several columns of deep neural networks), it differs from our model in two aspects: (1) Our work leverages the duality of a pair of dual tasks while this paper does not; (2) In an MCDNN framework, during the training phase, all the columns are updated by winner-take-all rule; and during inference, all columns work like an ensemble model through weighted average.", "In comparison, we only update one primal and one dual agent during training, and use one agent for inference.", "** Clarity **", "Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.", "Please kindly refer to first paragraph in Section 3.3.", "You may check our updated paper with clarification and new experimental results.", "Thanks for your time and feedbacks.", "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" In NIPS. 2017.", "[2] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional Sequence to Sequence Learning. In Proc. of ICML, 2017.", "[3] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).", "[4] Chen, Mia Xu, et al. \"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.\" In Proc. of the ACL, 2018.", "[5] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proc. of NAACL, 2018.", "[6] Heusel, Martin, et al. \"Gans trained by a two time-scale update rule converge to a local nash equilibrium.\" In NIPS, 2017.", "[7] Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" In CVPR, 2017", "Dear AnonReviewer1,", "Before the final decision concludes, do you have further questions regarding our rebuttal and updated paper?", "Our paper revision includes reorganization of the introduction to our framework (Section 3.1), the additional experiments on WMT18 English->German translation challenge (Section 3.4), the additional study on diversity of agents (Appendix A), and quantitative evaluation on image-to-image translations (Section 4.3 and 4.4) following your suggestions.", "In particular, we would like to highlight that:", "(1) The calibration of BLEU score: We would like to point out that our improvement over the previous state-of-the-art baselines is substantial.", "For example, on the WMT2014 En->De translation task, the performance of the transformer baseline is 28.4 BLEU score [1] (our baseline matches this performance).", "The improvement over this baseline is 0.61 in [2], 0.8 in [3] (1.3 BLEU improvement over the re-implemented 27.9 baseline in [3]) and 0.9 in [4], while ours is 1.65 BLEU score.", "(2) The baselines: As we explained in the previous response, we are using the state-of-the-art transformer as our backbone model, and comparing against all the relevant algorithms including KD, BT and the traditional 2-agent dual learning (Dual-1).", "Moreover, we also show on WMT18 En->De challenge that our method can further improve the state-of-the-art model trained with extensive resources (Section 3.4 of our updated paper).", "We hope our rebuttal and paper revision could address your concerns.", "We welcome further discussion and are willing to answer any further questions.", "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.", "[2] He, Tianyu, et al. \"Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation\". Advances in Neural Information Processing Systems. 2018.", "[3] Shaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. \"Self-Attention with Relative Position Representations.\" In Proc. of NAACL, 2018.", "[4] Anonymous. Universal transformers. In Submitted to International Conference on Learning Representations, 2019.", "URL https://openreview.net/forum?id=HyzdRiR9Y7.", "Under review as a conference paper at ICLR 2019"], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_followup", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label"]}
{"abstract_id": 139, "sentences": ["Thank your very much for your review.", "We have updated the manuscript with more details in the derivation of the first order approximation of KL divergence.", "1) Elaborated derivation of Eq. 10", "Q1: We have added one more line to explain the derivation.", "Basically a baseline is subtracted, and GAE is introduced.", "2) Gradient update on \\phi from KL divergence", "The gradients w.r.t. \\phi from the KL divergence is stopped for variance reduction with acceptable bias, which we prove with MuProp [1].", "Details could be found in Appendix C.", "Q3: Rather than [2], we employ MuProp to reduce variance in our development of NADPEx.", "Thank your for your suggestion.", "Q4: Yes \\theta and \\phi are jointly and simultaneously optimized at Eq. 12, though the gradients w.r.t. \\phi from the KL divergence is stopped.", "Q7: Due to the stop-gradient manipulation in the KL divergence, gradients w.r.t. \\phi remains the same as in stated in last subsection.", "3) Mean policy in the KL divergence", "What motivates the mean policy is not variance reduction, but the idea that dropout policy had better to be close to each other.", "As intuitively \\phi is controlling the distance between dropout policies, it would further remedy the little bias mentioned above.", "However, the computation complexity for \"close to each other\" would be O(N^2), with N being the number of dropout policies in this batch.", "We employ mean policy to make it linear. And it could be regarded as an integration on a Gaussian approximation of the Monte Carlo estimate according to [3].", "Details could be found in Appendix C.", "Q2: No the mean policy is not used due to the likelihood ratio trick.", "And the approximation of using mean policy is discussed in [3], with a sound deduction.", "Q3: Mean policy is not motivated by variance reduction, which is addressed as introduced above.", "Thank you for your suggestion.", "Q5: In the updated version, we have explicitly pointed out that the gradients w.r.t. \\phi from KL divergence is stopped. Thanks for this suggestion.", "Hope our response addresses your concerns!", "[1] Gu et al., \"MuProp: Unbiased Backpropagation for Stochastic Neural Networks\", ICLR 2016.", "[2] Titsias et al., \"Local Expectation Gradients for Black Box Variational Inference\", NIPS 2015.", "[3] Wang et al., \"Fast dropout training\", ICML 2013."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_social", "rebuttal_done", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 140, "sentences": ["Thank you for your insightful feedback and suggestions! To address your comments:", "- Following your suggestion, we have added one experiment on a synthetic regression task in Appendix C. Here, we show that our method can learn meaningful models for target domains, and also learn the source domain weights in a way that selects only relevant source domains for training.", "More importantly, we can learn the model and the domain weights simultaneously, unlike many existing works that use two-stage learning (learn the weights then the model).", "- We also followed your suggestion and show the domain weights of MDMN on the Amazon dataset for comparison (see the new Section 5.4).", "As you mentioned, our weights are theoretically justified while theirs are only heuristically computed.", "We see that the weights provided by MDMN are not very stable, changing from one source domain to another drastically during training.", "This instability makes their weights difficult to interpret.", "- W.r.t. the naive method of using (fixed) coefficients.", "Note that whichever domain discrepancy is used, it has to depend on the data representation.", "For complicated models like deep neural networks, the feature representations (hidden layers) are changing constantly during training and arguably there is no \"right\" way to compute *fixed* coefficients/weights", "based on ever-changing representations", ".", "Computing the coefficients directly from the images is extremely difficult, if not impossible, because calculating the domain discrepancy using such high-dimensional data is not feasible.", "Note that MDMN DOES use W1-distance to compute domain weights, and our comparison in Section 5.4 shows that it is not very stable, as mentioned above.", "To summarize, there is no easy way to compute meaningful fixed coefficients and our method is indeed suitable for dynamic representations during neural network training.", "We hope our explanation and additional results address and resolve your concerns. If you have any other comments, we are happy to have further discussions.", "Thank you!"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 141, "sentences": ["We thank the reviewer for their review.", "The reviewer notes the need to emphasize how and why to use this approach.", "In the new revision, we have added a discussion section to make a case for this.", "We will publish the code to compute conductance after the blind-review phase.", "The reviewer also mentioned that the paper doesn\u2019t compare directly against various attribution methods.", "For this, we refer the reviewer to our response for the comment by anonymous."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 142, "sentences": ["1.", "The purpose of the counterexample is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds.", "We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions.", "A good generator and discriminator would definitely be a solution as well.", "However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions.", "We found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging, however, it does sometimes results in reasonable generations (although worse than proposed PC-GAN).", "So, we do not consider the argument to be unrealistic as we often observe the degeneracy.", "So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training.", "Other architectures like RNN might work, but they are not permutation invariant, which is a desirable property for set data like point clouds.", "More comparisons between using RNN and DeepSets for other tasks on set data can refer to Zaheer et al., (2017).", "2.", "As we discussed in the end of Section 3, ALI and BiGan\u2019s goal is to match (z, G(z)) and (Q(X), X), which aims to infer the random noise z and enforce the latent code to follow noise  distribution (e.g. Gaussian).", "On the other hand, we do not enforce Q(X) to follow from Gaussian.", "Instead, we train the other G_theta(u) to match Q(X), which is more similar to AAE-like works (Engel et al., 2017; Kim et al., 2017; Achlioptas et al. 2017).", "The difference of the interpretation between PC-GAN and  those AAE-like work is also explained in the second paragraph of Sec 4.", "3. We followed the same protocol that we trained on ShapeNet55 and tested on ModelNet40 testing set.", "Please check Table 3 in the revision.", "PC-GAN achieves 86.9% accuracy which is better than AAE (84.5%),  3D-GAN (83.3%) and other unsupervised learning approach."], "labels": ["rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 143, "sentences": ["We thank the referee for their review.", "1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.", "2. The complete connectivity graph for our Boltzmann machine, as presented in Fig 2, can be interpreted as having two hidden layers.", "The graph has bipartite connectivity between the visible units and the first 128 hidden units and bipartite connectivity between the first 128 hidden units and the second 128 hidden units.", "We thank the referee for bringing the article [V] to our attention and we now have acknowledged the prior work properly in our introduction.", "We agree that it would be very instructive to evaluate the model in [V] for adversarial resistance, but we would argue that this evaluation is beyond the scope of this article.", "3. Due to the complexity of the network compared to e.g. LeNet and the higher adversarial resistance the optimisation procedure to find adversarial images takes a long time, making it hard to evaluate 10000 images for all training stages and different attacks.", "We have now evaluated the adversarial resistance throughout the article for 1000 images randomly selected from the 10000 MNIST test images.", "This should avoid placing too much emphasis on the cleaner images in the beginning of the MNIST test set.", "Fig.\u00a03 and other evaluations have been updated for the new test set.", "4. To our knowledge the boundary method is the strongest black box attack.", "The succesful transfer attack on CapsNet is based on transfer of adversarial images from a different model (LeNet).", "We have implemented this attack and added it to our evaluation.", "[V] Norouzi, Mohammad Ranjbar, Mani Mori, Greg: Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning. 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2735-2742 (2009)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_done", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label"]}
{"abstract_id": 144, "sentences": ["Thank you for your comments and suggestions!", "Summary: our response includes (1) Clarification on math equations; (2) Analysis on diversity of additional agents; (3) Quantitative analysis for image translation.", "** Clarification on Mathematics in Section 3.1 **", "We apologize for the confusions in Section 3.1.", "We have reorganized this section, as shown in our updated paper.", "For your questions:", "1. About equation 8, indeed there is a typo and should be a \"partial\" sign in front of the \"\\delta\" function in the numerator.", "Thanks for pointing this out.", "2. The details of derivative estimation can be found in Section 3.1 (especially equation 9 and 10 in our updated version.", "** Study on diversity of agents **", "1. You are right.", "We obtained distinct \"agents\" f_i and g_i through multiple independent runs with different random seeds and different input orders of the training samples.", "As far as we know, there's no common quantitative metric to measure the diversity among models in NMT.", "But we agree with you that intuitively, more diversity among agents leads to greater improvements.", "2. Following your suggestions, we add a study on the diversity of agents (presented in Appendix A of the updated paper).", "We design three group of agents with different levels of diversity: (E1) Agents with the same network structure trained by independent runs, i.e., what we use in Section 3.3; (E2) Agents with different architectures and independent runs; (E3) Homogeneous agents of different iteration, i.e., the checkpoints obtained at different (but close) iterations from the same run.", "We evaluate the above three settings on IWSLT2014 De<->En dataset.", "The diversity of the above three settings would intuitively be (E2)>(E1)>(E3)", ".", "We present full results in Figure 4 (Appendix A), where the BLEU scores with Dual-5 model are:", "--------------------------------------------------------", "E1", "E2             E3", "--------------------------------------------------------", "En -> De", "35.44", "35.56       34.97", "De -> En", "29.52", "29.58       29.28", "--------------------------------------------------------", "From the above results, we can see that diversity among agents indeed plays an important role in our method.", "There are, of course, many other ways to introduce more diversity, including using different optimization strategies, or training with different subsets as you suggested.", "All of these can potentially bring further improvements to our framework, yet are not the focus of this work.", "From the current studies, we show that our algorithm is able to achieve substantial improvement with a reasonable level of diversity.", "We leave more comprehensive studies on diversity to future work.", "Please kindly refer to Appendix A for more detailed results.", "** Quantitative analysis for image translation **", "Thanks for your suggestions.", "We add two quantitative measures on image translation tasks: (1) We use the Fr\u00e9chet Inception Distance (FID score) [1], which measures the distance between generated images and real images to evaluate the painting to photos translation.", "(2) We use \"FCN-score\" evaluation on the cityscape dataset following [2].", "The results are reported in Table 6 and Table 7 respectively.", "Multi-agent dual learning framework can achieve better quantitative results than the baselines.", "*", "* Term usage of \"multi-agents\" **", "Although with the same term, the \"multi-agent\" or \"agent\" in this paper has no relationship with multi-agent reinforcement learning.", "You are right in that the term \"agent\" in our context refers to \"mapping\" or \"network\".", "To avoid further confusion in the discussion period, currently we decide not to change the term usage throughout the paper during rebuttal; instead, we will change the term after the acceptance/rejection decision.", "You can check our updated paper with clarification and new experimental results.", "Thanks for your time and valuable feedbacks.", "[1] Heusel, Martin, et al. \"Gans trained by a two time-scale update rule converge to a local nash equilibrium.\" Advances in Neural Information Processing Systems.", "2017.", "[2] Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" In CVPR, 2017"], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_social", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_summary", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 145, "sentences": ["Thank you for your careful review and useful comments!", "Overall, in response to your review and that of referee 3 we will include a more intuitive discussion of our results in the next revision of our text.", "To reply to your other specific comments,", "1) The intuition for batchnorm can be put in a more general setting.", "If a function f: X -> Y tends to spread out small clusters in the input space almost evenly in the output space, then one can expect that its gradients will be large typically.", "In our case, a batchnorm network can be understood as a function that sends a batch of inputs to a batch of outputs.", "In the appendix, we showed that the correlation between two different batches tend to a constant value independent of the input batches.", "No matter how close two input batches are, the output batches will have the same \u201cdistance\u201d from each other -- small movements in the input space leads to large movements in the output space.", "Thus we can expect the gradients to be large as well.", "We have added a new figure to the Appendix to further support this intuition.", "In it, we pass through a linear batchnorm network 2 minibatches.", "Both minibatches contain points on the same circle and 1 point off the circle that is unique to each minibatch.", "While the circle in each minibatch will remain an ellipse as they are propagated through the network, the angle between the planes spanned by them increasingly becomes chaotic with depth.", "3) As observed in [1] and [2], depthwise convergence to covariance fixed points is bad for training, and the best networks are either moderately deep or initialized such that the depthwise convergence rate to the fixed point is as slow as possible.", "We observe that deep networks whose activation statistics resemble a non-BSB1 fixed point typically feature worse gradient explosion than BSB1 networks.", "This seems to be because the nonlinearities that induce these fixed points increase rapidly (for example, polynomials with high degrees), so that the corresponding derivatives are also large, causing gradient explosion.", "(The reason that rapidly increasing nonlinearities don\u2019t converge to BSB1 fixed points is that, after a spontaneous symmetry-breaking, begins a \u201cwinner-take-all\u201d covariance dynamics, in which the activations of a few examples in the batch suddenly dominates those of the others in the batch, and this dominance persists across each layer.)", "4) We were a bit confused by what was meant by \u201cpractice\u201d here.", "We have thoroughly verified that for realistic input distributions (MNIST and CIFAR10) and common initialization strategies (weights that are randomly distributed) our theory makes accurate prediction.", "Moreover, we have shown that these predictions can be connected to practice in the sense that they predict whether or not the network can be trained.", "Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.", "If this did not properly address your question, please feel free to let us to know and we will improve this response!", "[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein.", "Deep Information Propagation (https://arxiv.org/abs/1611.01232)", "[2] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)"], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_by-cr", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 146, "sentences": ["We would like to thank the reviewer for their feedback.", "We address each comment below individually with appropriate headings.", "- Summary", "We would like to point out that the reviewer in the summary incorrectly described that our approach uses the \"triplet loss as a convex relaxation of the ordinal embedding problem\".", "Using the triplet loss as a proxy does not make the problem convex.", "- The relation between data distribution and hardness of ordinal embedding", "Ordinal embedding is NP-hard independent of the data distribution.", "The paper \u201cLandscape of non-convex quadratic feasibility\u201d (Bower et al. 2018) can shed more light on this.", "The equation (1) in this paper rephrases the ordinal embedding problem as a homogeneous quadratic feasibility problem.", "The constraint matrices of the problem (P_i in the paper), which correspond to the triplet inequalities, are all indefinite which makes the whole optimization NP-hard.", "Moreover, many of our experiments in this paper feature the uniform distribution, which does not satisfy any nice structural assumptions.", "- Using a convex solver", "As we pointed out earlier, using the triplet loss does not make the optimization problem convex and hence using a convex solver would not be possible here.", "- \u201cEquations (3) and (4):  isn't this the same as using the hinge loss to bound the zero-one loss?\u201d", "Yes, that is true."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 147, "sentences": ["Thank you for the detailed comments.", "Our goal is to develop a quantitative understanding of AlphaGo Zero (AGZ), moving beyond the intuitive justification for the algorithms in the original work.", "We believe that a rigorous mathematical analysis is crucial to provide a solid foundation for understanding AGZ and similar algorithms.", "This requires developing (i) a precise mathematical model, (ii) a quantitative performance bound within the model.", "Our work takes an important step in this direction by modeling AGZ\u2019s self-play and its supervised learning algorithm accurately.", "In particular, we use the turn-based game model to capture the self-play aspect.", "We develop a quantitative bound in terms of cross-entropy loss in supervised learning, which is the \u201cmetric\u201d of choice in AGZ.", "While the cross-entropy loss seems intuitive, using it as a quantitative performance measure requires careful thought.", "For example, in Appendix F (page 19, 2nd paragraph), we discussed a scenario where this intuition is incorrect under a careless measure.", "That is, seemingly \u201cobvious\u201d algorithms can fail in the absence of a rigorous mathematical proof.", "We agree that there is a gap between AGZ and our model.", "As mentioned in our paper, MCTS converges to the optimal policy for both classical MDPs and stochastic games.", "Hence in this paper, we model the AGZ\u2019s MCTS policy by the optimal policy, and mainly focus on the other two key ingredients of AGZ, self-play and supervised learning.", "It will be interesting to study how the error between MCTS and the optimal policy affects the iterative algorithm.", "This is a research direction we think is worth pursuing in the future.", "We also agree with the reviewer that some of our statements might be too strong.", "We will revise accordingly. Instead of ``immediate justification``, we believe this work does provide a first-step, formal framework towards a better theoretical understanding.", "We will also revise the title, perhaps to ``applying AGZ`` so as to make the connection to MDP more clear in our paper."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_future", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_future_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_by-cr_label"]}
{"abstract_id": 148, "sentences": ["1. Comment:", "However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.", "1. Response:", "We study seq2seq classification tasks since they have been widely used in real world applications for RNNs.", "To name a few, in speech recognition, [1] hybridizes hidden Markov model with RNNs to label unsegmented sequence data; In computer vision, [2, 3] demonstrate scene labeling with LSTM and RNNs, achieving higher accuracy than baseline methods; In healthcare, [4] proposes a model, Doctor AI, to perform multiple label prediction (one for each disease or medication category).", "In addition, [5, 6] both apply RNNs to real-world healthcare datasets (MIMIC-III, PhysioNet, and ICU data) for mortality prediction and other multiple classifications tasks.", "We establish bounds for classification because it is typical in learning theory and is easy to compare among existing literature.", "On the other hand, our analysis applies in other tasks as long as a suitable Lipschitz loss function is chosen.", "Specifically, Lemma 4 establishes an upper bound for empirical Rademacher complexity of general Lipschitz loss functions (the last line in Appendix A.4).", "By replacing the loss function in Lemma 1, we can derive generalization bounds for various tasks other than classification.", "References", "[1] Graves, Alex, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. \"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.\" In Proceedings of the 23rd international conference on Machine learning, pp. 369-376. ACM, 2006.", "[2] Byeon, Wonmin, Thomas M. Breuel, Federico Raue, and Marcus Liwicki. \"Scene labeling with lstm recurrent neural networks.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3547-3555. 2015.", "[3] Socher, Richard, Cliff C. Lin, Chris Manning, and Andrew Y. Ng. \"Parsing natural scenes and natural language with recursive neural networks.\" In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 129-136. 2011.", "[4] Choi, Edward, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. \"Doctor ai: Predicting clinical events via recurrent neural networks.\" In Machine Learning for Healthcare Conference, pp. 301-318. 2016.", "[5] Che, Zhengping, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. \"Recurrent neural networks for multivariate time series with missing values.\" Scientific reports 8, no. 1 (2018): 6085.", "[6] Xu, Yanbo, Siddharth Biswal, Shriprasad R. Deshpande, Kevin O. Maher, and Jimeng Sun. \"RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient Monitoring Data.\" In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2565-2573. ACM, 2018."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 149, "sentences": ["Thank you for your encouraging comments especially with regards to novelty and thoroughness of our experiments.", "We have addressed the minor issues you highlighted; answers to your questions are also provided below:", "Q: \"It would be helpful to note in the description of Table 3 what is better (higher/lower).\"", "A: We have updated the table description (now Table 4) as suggested.", "Q: \"Also Table 3 seems to have standard deviations missing in Supervised DCGANs.\"", "A: The results we reported were taken from (Gulrajani et al. 2017) which did not include standard deviations for this setting.", "Q: \"And is there an explanation on why there isn\u2019t an improvement in the FID score of SVHN for 1000 labels?\"", "A: We re-checked our FID computation for this case and fixed a bug.", "We have updated the paper with updated FID scores; we note there is a high variance in the FID so while there is an improvement on average, it occasionally may not be better.", "Q: \"What is the first line of Table 4? Is it supposed to be combined with the second?\"", "A: It is the reference from which all 3 results (Supervised, Pi Model and Mean Teacher) were taken from; we have made this clearer in the revised paper.", "Q: \"And is the Pi model missing results or can it not be run on too few labels?\"", "A: We have updated the table with results on fewer labels reported in (Tarvainen & Valpola, 2017); results on fewer labels were not reported in (Laine & Aila 2017).", "Q: \"In Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there ?\"", "A: This is because the classifier is not smooth in this region.", "The classifier is probably not constrained enough.", "Q:\"What are the differences of the 6 pictures in Figure A7? Iterations?\"", "A: These are the results from 6 different runs."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 150, "sentences": ["Thank you for your review of our work.", "The following are your concerns of our work:", "a. Limited contribution to the qualitative understanding of the optimizers", "a.i. Informativeness of the proposed w-tunability metric", "b. Using wall-clock time instead of number of HPO oracle calls", "We address these concerns one-by-one.", "a. Limited contribution to the qualitative understanding of the optimizers:", "We consider the three main contributions of our work to be 1) a systematic evaluation protocol of optimizers, with off-the-shelf HPO to account for the cost of tuning of hyperparameters.", "This is missing in existing papers, which consider best attained performance alone.", "The importance of a proper hyperparameter search protocol is emphasized by Choi et al., 2019 (published after our submission and under review at ICLR).", "2) a \u201cw-tunability\u201d measure of the cost of hyperparameter optimization, and 3) under the experiments considered we find that Adam (with default beta and epsilon values) is the most tunable.", "a.i. Informativeness of the tunability metric:", "We propose w-tunability as a metric to incorporate the HPO tuning too in reporting the performance of an optimizer, and compute it as a linear combination of the incumbents of the HPO algorithm, though one can use an arbitrarily complex function trading off interpretability.", "It is true that Figures 4-7 essentially contain all the information needed to judge about the optimizer\u2019s tunability.", "However, a metric that is easy to compute, interpret and compare optimizers across tasks is crucial, for which we propose w-tunability.", "This is analogous to computing specific quantities like accuracy, FPR, TPR from the confusion matrix, even though a confusion matrix contains all the information (and is quite cumbersome to compare).", "The summary metric in Figure 2 provides a different interpretation: It reports a normalized performance i.e., the  normalized incumbent performance at iteration $k$. This doesn\u2019t explicitly include information about the previous $k-1$ iterations (which is our central argument).", "Thus our proposed tunability metric provides more information than the summary statistics plot (figure 2).", "Due to this novelty, we argue that our setup does contribute to the qualitative understanding of the optimizers.", "In fact, it yields to a drastically different valuation of adaptive gradient methods than popular previous work (Wilson et al, Shah et al).", "You mention that our work is incremental to the work on benchmarking of optimizers.", "Can you please provide respective references?", "We have modified parts of our paper to reflect these arguments better.", "b. Using wall-clock time instead of HPO oracle calls:", "Our reason for using a number of configuration trials instead of a time budget is that measuring number of hyperparameter configuration searches required is more relevant to understand the optimizers\u2019 dependence on the hyperparameters.", "However, we completely agree with you that computational budget is a relevant factor from the practitioner\u2019s point of view, and added a discussion of this in Appendix E of the paper.", "As you rightfully point out, the adaptive optimizers tend to converge in fewer number of epochs, amplifying the results that favor Adam over the variants of SGD.", "References:", "Wilson, Ashia C., et al. \"The marginal value of adaptive gradient methods in machine learning.\" Advances in Neural Information Processing Systems. 2017.", "Shah, Vatsal, Anastasios Kyrillidis, and Sujay Sanghavi. \"Minimum norm solutions do not always generalize well for over-parameterized problems.\" arXiv preprint arXiv:1811.07055 (2018).", "Choi, Dami, et al. \"On Empirical Comparisons of Optimizers for Deep Learning.\" arXiv preprint arXiv:1910.05446 (2019)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 151, "sentences": ["We thank the reviewer for the comments and suggestions.", "Our paper indeed focuses on theoretical results, but we believe the theory has some practical implications as well.", "In particular, while the exact form of the sufficiently bilinear condition may be somewhat unwieldy, the result gives concrete evidence that having higher bilinearity can aid convergence for certain algorithms, even for settings that are not purely bilinear.", "This indicates that one should pay attention to the magnitude and condition number of the off-diagonal of the Jacobian when constructing a min-max problem and choosing an algorithm to solve the problem."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 152, "sentences": ["We thank the reviewer for their encouraging words.", "\"There are better baselines for MNIST to MNIST-M\": yes, the presented method might not outperform all other methods on all other datasets. Still, we hope to convince the reviewer that the results on the other datasets are worth being considered.", "\"Office is not suitable unless one wants to be in a few-datasample regime or work with data with noisy labels\": we would like to point that this regime is quite realistic in Bioimage informatics (noisy, with few samples per class).", "\"The results on Cell are not convincing\": as our goal is multi-domain learning on this dataset, the relevant performance indicator is the average risk over all domains.", "Table 2 details what happens in various categories of cases (on classes with/without labelled samples).", "Despite the (well-known) degradation of the results on labeled classes when one also considers unlabelled classes, the bottom line is that -- regarding the average risk -- our method outperforms the baseline.", "Importantly, MuLANN results on Cell are significantly better than the baseline on all rows which involve unlabeled classes (rows  3, 6, 9, 13), while remaining not significantly different to the baseline on 6/9 of the other rows.", "\"Comparison with other methods did not take into account a variety of hyperparameters\".", "The reviewer is right.", "Complementary experiments have thus been performed, and tables 1, 2 updated.", "We investigated the impact of varying the learning rate, the weight lambda on the discriminator loss, the weight dzeta of the known-unknown discrimination loss, the learning rate schedule, lambda schedule as well as using different learning rates for pre-trained layers versus from scratch layers (see Table 5 for more detailed information).", "These results show a moderate sensitivity of MuLANN, MADA and DANN wrt hyper-parameters and confirm that MuLANN outperforms both MADA and DANN (detailed results available here https://drive.google.com/file/d/1NjtMKF53qmnx4_Jyvh-ofxb0WjzcDvow/view?usp=sharing)."], "labels": ["rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 153, "sentences": ["1. Comment:", "Missing comparison with parameter counting bounds [1, 2].", "1. Response:", "[1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results.", "Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions.", "Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation).", "[2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear.", "2. Comment:", "Vacuous bounds in the regime \\beta >1.", "2. Response:", "We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous.", "Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices.", "The exponential term stems from the layer wise covering argument rather than the range of the output.", "The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings.", "Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs.", "This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5].", "We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \\beta \\approx 1 helps balance the generalization and representation of RNNs.", "3. Comment:", "Technical contribution: marginal.", "3. Response:", "We provide new understandings of RNNs by connecting their generalization properties to their empirical success.", "We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs.", "In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2).", "This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t.", "We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs.", "The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument.", "Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \\beta > 1.", "To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3):", "1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds.", "2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5).", "Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions.", "4. Comment:", "Missing experiments to validate nature of bounds.", "4. Response:", "Please refer to the revised version for numerical evaluations in Section 6.", "In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \\beta > 1.", "References", "[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86, no. 1 (1998): 63-79.", "[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" In Advances in Neural Information Processing Systems, pp. 204-210. 1996.", "[3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. \"On orthogonality and learning recurrent networks with long term dependencies.\" arXiv preprint arXiv:1702.00071 (2017).", "[4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. \"Unitary evolution recurrent neural networks.\" In International Conference on Machine Learning, pp. 1120-1128. 2016.", "[5] Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).", "[6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 154, "sentences": ["1- We briefly mentioned the way problem embedding with similarity metric is used in the recommendation system in this work, but here is more explanation on that.", "The most similar problem is not necessarily recommended to a student.", "On a high level, if a student performs well on problems, we assume he/she performs well on similar problems as well, so we recommend a dissimilar problem and vice versa.", "More specifically, we project the performance of students on problems they solved onto the problems that they have not solved.", "This way, we have an evaluation of the performance of students on unseen problems.", "A problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time recommendation is done so that all the concepts necessary for students are practiced by them.", "An evaluation on real students is presented in part 2 of the comment titled \u201cResponse to questions about Prob2Vec\u201d on this page, and we observed that similar problems are more likely to be solved correctly at the same time or wrong at the same time.", "The math expressions are not ignored in our proposed Prob2Vec method.", "In the example given in the last paragraph on page 3 for example, math expressions are used to extract the concept n-choose-k.", "We both use math expressions and text to label problems with appropriate concepts.", "2- Prob2Vec only uses expert knowledge for rule-based concept extractor, but does not use selected informative words.", "The effort put for rule-based concept extractor is negligible compared to effort needed for annotation of all problems with their corresponding concepts.", "We both annotated all problems manually and used rule-based concept extractor for annotation.", "In the former method, we observed 100% accuracy in the similarity detection test and observed 96.88% accuracy in the latter method.", "However, the rule-based concept extractor needs much less manual effort than manual problem annotation and is capable to provide us with relatively high level of accuracy we need in our application.", "Note that our method is scalable as long as problems are in the same domain as the rule-based concept extractor is automated for a single domain, but for the case that problems span many different domains, it is the natural complexity of the data set that requires a more sophisticated rule-based concept extractor.", "Furthermore, in most realistic cases for education purposes, problems span a single domain not multiple ones.", "We also like to grab your attention to the negative pre-training method proposed for training on imbalanced data sets.", "You may want to refer to part 2 of comment titled \u201cResponse to Question on Negative Pre-Training\u201d and part 1 of our response to reviewer2."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 155, "sentences": ["Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?", "A: Yes we did and interestingly, it didn't show improvement.", "The results are reported in Appendix A, Table A1 in the row titled: \"Ours + classifier after\" and discussed in Section 4.2.4.", "Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together.", "Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?", "A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1.", "So latents are canonicalized in both possible orderings.", "We discuss this point at the bottom of page 4 after equation 6 and will further clarify.", "Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).", "There might be other constructions that are more efficient and less restrictive.", "A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work.", "While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement.", "First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar.", "However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further.", "Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?", "A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated).", "Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance.", "If the above mentioned assumption indeed holds, the font should be the only change in the set.", "We order the samples according to that axis and plot them in a row from left to right.", "We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 156, "sentences": ["The following link is a figure that explains the difference between zero-confidence attack and fix-perturbation attack.", "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure1.pdf", "As can be seen, the zero-confidence attack finds the closest point on the decision boundary; while fix perturbation-attack finds adversarial samples within a fix perturbation.", "Both attacks are equivalent if we only want to compute the attack success rate under a given perturbation level.", "However, we will be better off with zero-confidence attacks if we want to", "1) Compute the margin of each individual example; and", "2) Probe and study the decision boundary of a classifier", "Of course, we can also measure the margin of each example using a fix-perturbation attack, for example PGD, by binary searching over the perturbation levels.", "However, the computation cost will significantly increase.", "Consider, for example, the CIFAR-10 dataset.", "Since for our model, most margins fall within 10, so let\u2019s assume the binary search range is 10 (for adversarially trained models this number will be much higher).", "If we want to achieve a accuracy of 0.1, then we need at least 7 binary search steps.", "In other words, the computation complexity increases by 7 times.", "In fact, CW applies a similar binary search idea to achieve zero-confidence attack, and that is why its computation cost is high.", "The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently, but it at least provides a perspective on why zero-confidence attack is challenging, and why the complexity reduction as well as accuracy improvement of MarginAttack is valuable.", "Although this is not the major focus of your comment, we would like to revisit the theorem assumptions.", "While there are nine assumptions, these assumptions are in fact more realistic than expected.", "Take the convexity assumption, which you mentioned in your review, as an example.", "This assumption does not say that the constraint has to be convex.", "It only says that the constraint should not be \u2018too concave\u2019.", "In particular, the curvature of the of the decision should not exceed that of the L2 or L-infinity ball.", "For better illustration, we plotted some decision boundaries that are allowed by the assumption, and some that are not.", "Please check the following link:", "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf", "As can be seen, the convexity assumption permits a wide variety of decision boundaries.", "Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does.", "In this case, the critical point becomes a local maximum rather than a local minimum.", "The other assumptions are also more realistic than their names sound.", "The differentiability assumption does not stipulate that the constraint has to be differentiable.", "It actually permits countably infinite jump discontinuities.", "The Lipchitz continuous assumption does not assume Lipchitz continuity everywhere, but only at x*. We are not saying that the assumptions are very loose, but they are realistic enough to shed some light on the actual convergence property of MarginAttack.", "Nevertheless, we are considering adding a 2D toy example as you suggested.", "We will post further responses if there are further updates."], "labels": ["rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 157, "sentences": ["Q: \"...I find these assumptions too strong for the task of learning disentangled representation.\"", "A: We wish to emphasize that:", "(1) we only require access to meta-labels on the source set", "(2) our goal is not to find disentangled representations; Our goal is transferability so that we can learn on real data with minimal supervision.", "Q: \"you can simply train a network to predict the canonicalizations by simple supervised learning\"", "A: Finding the canonicaliers is not our goal.", "These are used as auxiliary constraints which guide representation learning and allow for latent data augmentation on our limited target data.", "Specifically, predicting the factor values will not help us in manipulating the target samples (note the significant improvement we get by using the majority vote).", "Q: method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].", "A: The suggested methods [1-3] are self-supervised methods using less information than the baselines we have included (for example, the AE+classifier baseline is trained on the same synthetic data with the same access to digit labels).", "We therefore expect that these methods will perform worse than our baselines.", "We have included  a comparison with method [3] (see general comment to all reviewers)."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 158, "sentences": ["Q: \"I had hard time to understand latent canonicalization...\"", "A: \"latent canonicalization\" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.", "Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?", "A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.", "In those cases full access to factors of variation is available as this is used to generate the data.", ", The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.", "We are not focusing on a setting where the source domain has no labels.", "That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).", "Q: How can the proposed method be generalized to non-image data?", "The experiments were only done on simple image datasets.", "I am wondering this method can be applied to other complex datasets whose latent factors are unknown.", "A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.", "In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.", "Critically, these two sources of data need not be identical!", "This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.", "Q: I do not understand this: \"to fit well the method overfitting rate\" in Section 3.3.", "Thank you for pointing this sentence out!", "It is indeed unclear.", "All we were trying to say is that each baseline\u2019s training duration was chosen independently to prevent overfitting.", "We have updated the draft to be more clear."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 159, "sentences": ["We thank the reviewer for the comments and suggestions.", "We address points individually below:", "1) The Higher-order Lipschitz condition is necessary for us to use the PL convergence guarantee.", "This condition is similar to assumptions made in convex optimization, especially where higher-order updates are involved (see eg. Agarwal et al. 2017 and Bubeck et al. 2019).", "If the iterates of the algorithm always have bounded norm (eg. due to constraints or regularization), then three-times differentiable functions will satisfy the Higher-order Lipschitz condition for our purposes.", "This is because it suffices for the condition to hold for only the iterates of the algorithm ($x^{(1)},x^{(2)},...$), rather than for all of $\\mathbb{R}^d$.", "2) We thank the reviewer for this remark.", "We wanted $L_H$ to be defined for our theorem statements, but we can see how it is confusing as is. We will make it clear that $L_H$ is the smoothness parameter of $H$.", "3) Theorem 3.4 holds in the broadest setting out of all of these results.", "Theorems 3.2 and 3.3 have slightly tighter bounds for their respective settings.", "We will clarify this in the surrounding text.", "4) It is true that these results rely on the PL condition, and this is unavoidable for our current results.", "The novel perspective in this paper is that we consider the PL condition on a different objective, namely the squared gradient norm, rather than on the game objective $g$. This perspective allows us to prove our new bounds, although we still require some nontrivial linear algebra.", "The PL condition also allows us to easily prove our stochastic HGD results.", "5) The 1/sqrt(2) should cancel out on both sides of the guarantee in Theorem 5.2 (and eg. in equation 68).", "Minor suggestions:", "-We appreciate the suggestion for page 5 and will make this change in the final version.", "We thank the reviewer for recognizing our theoretical contributions.", "We would be happy to include some further experiments in the final version comparing HGD with other algorithms such as extragradient.", "References:", "Agarwal, Naman, and Elad Hazan. \"Lower bounds for higher-order convex optimization.\" COLT 2018.", "Bubeck, S\u00e9bastien, et al. \"Near-optimal method for highly smooth convex optimization.\" COLT 2019."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_accept-praise", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 160, "sentences": ["Thank you for your thoughtful review. We will address your concerns in turn.", "Q1: The idea is very related to Yeh et al.\u2019s work which is not mentioned at all.", "A1: The entire first paragraph of our related work section is focused on Yeh et al.\u2019s work.", "As we explained in the paragraph, there is a major theoretical flaw in their method.", "Yeh et al. (2017) use the discriminator loss of a trained GAN as an indicator of how realistic their restoration is.", "However, Goodfellow et al. (2014) already prove that the discriminator is unable to identify how realistic an input is after several steps of training, if the GAN has enough capacity.", "Ideally the generator will have all the information of the data distribution while the discriminator will have none.", "That is why we use the generator of a trained GAN as an implicit probability density model in our method.", "Another difference between their work and ours is that they only focus on image inpainting problem, while our method applies to various image restoration problems.", "Q2: Total variation regularization can also handle different degradations.", "A2: We think you underestimate the difficulty of those restoration problems.", "Please check the degraded images in Table 3.", "These images are damaged so badly that TV cannot recover any meaningful thing.", "As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks.", "Q3: Does the proposed method learn the image inpainting mask as well?", "What are the parameters of the degradation in the applications?", "A3: The image inpainting mask is known and fixed.", "We use four different kinds of degradation to test the generality of our method.", "The first three kinds of degradation are 7\u00d7 downsampling, making a 14\u00d714 square hole in the center of the image, and adding Gaussian white noise with a standard deviation of 1.0, respectively.", "The last kind of degradation is a composition of a series of degradation in order, which are (a) adding linear motion blur by at most 14 pixels in any direction, (b) 4\u00d7 downsampling, (c) adding uniform noise between -0.05 and 0.05, (d) randomly removing 10% of the pixels."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 161, "sentences": ["We thank the reviewer for its valuable and insightful comments.", "We are reviewing our work from a theoretical point of view and will update the paper very soon to reflect this.", "Even though we have not yet proved the above, we have empirically showed that the benefit of DEBAL over plain ensemble methods consists of a better representation of uncertainty, that is paramount in active learning.", "By better we mean", "1) more meaningful and closer to what one would expect (Fig 4 & Fig 6 (right))", "2) better calibrated (Fig 6 (left)).", "Our initial aim was not to compare stochastic ensembles with deterministic or single MC-dropout but to correct for the mode collapse issue in estimating posteriors with MC-dropout.", "We have empirically shown that adding ensembles to this, greatly improves the MC-dropout technique and outperforms the deterministic ensembles as well.", "We had similar doubts about the benefit of adding MC-Dropout to an ensemble.", "Therefore, we contrasted the performance of DEBAL against the plain ensemble method and showed empirically that DEBAL gives rise to better measures of uncertainty.", "Finally, as we strive to make our assumptions hold theoretically, we agree that adding theoretical Bayesian support to our method is of great importance if we are to further improve the understanding of Bayesian deep learning.", "For your final point, although Beluch et al. (2018) showed better performance for ensembles, we have shown this in the context of a small dataset problem (i.e. the size of the final dataset acquired during AL is only a small fraction of the entire available unlabelled dataset), which we believe is more relevant to the real world cases if AL is to become a widely used method.", "As for the figures, we are aware of this and will try to make them more clear in a revised version."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 162, "sentences": ["We would like to thank reviewer #1 for the constructive critics.", "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup.", "We have updated the paper pdf and kindly ask the reviewer to take another look.", "THEORY:", "Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0.", "Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work.", "Theorem 1 formally states when k-addition is defined for spherical spaces.", "Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem.", "Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match.", "It also gives the derivative of the distance function w.r.t. curvature around 0.", "Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative.", "Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.", "This is a desirable property for Riemannian averaging.", "Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.", "LITERATURE:", "A more careful review of the literature has been incorporated into the introduction section.", "We have also incorporated the recent works [1,2] to appear soon at Neurips\u201919 (their text became available very recently and after the ICLR submission deadline).", "WRITING:", "We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings.", "We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.", "MOTIVATION:", "We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data.", "This is in accordance with the previous related work on non-Euclidean embeddings.", "Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.", "EXPERIMENTAL SETTINGS & HYPERPARAMETERS:", "Are added to section 4 and appendix E", "EXPERIMENTAL RESULTS:", "Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification.", "We believe further experimental investigations are needed to better train non-Euclidean graph neural network models.", "[1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips\u201919", "[2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips\u201919", "We would love to hear your feedback on our substantially improved paper (based on your suggestions).", "Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you!"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 163, "sentences": ["Thank you for your time and effort of reviewing our paper. Please see our response below.", "Our main contributions include:", "(i) For single-layer random variables (RVs), we propose a unified gradient named GO by exploiting the integration-by-parts idea, which is applicable to continuous/discrete RVs.", "In the special case of single-layer continuous RVs where GO recovers Implicit Rep or pathwise gradients, we consider it\u2019s our contribution to provide a principled explanation (via integration-by-parts) why Implicit Rep and pathwise gradients have low Monte Carlo variance; or in other words, we prove that their implicit differentiation originates from integration-by-parts.", "(ii) For multi-layer RVs, our main contribution is the discovery that with GO (or in other words, the introduced variable-nabla), one can back-propagate gradient information through a nested combination of nonlinear functions and general RVs (including non-reparameterizable continuous RVs, back-propagating through which is challenging).", "Another interpretation of this contribution is that GO enables generalizing the deterministic chain rule to a statistical version.", "Here, we refer to deterministic chain rule as back-propagating gradient through deterministic functions (like neural networks) or reparameterizable RVs (like Gaussian).", "By contrast, statistical chain rule is referred to as back-propagating gradient through more general RVs (including non-reparameterizable ones).", "Of course, statistical chain rule recovers deterministic chain rule for deterministic functions and reparameterizable RVs, because GO recovers the standard Rep.", "(iii) Another 2 minor contributions include Lemma 1 and Corollary 1.", "In Lemma 1, we explicitly prove that our deep GO gradient contains the standard Rep as a special case, in general beyond Gaussian.", "Note neither Implicit Rep nor pathwise gradients can recover Rep in general, because a neural-network-parameterized reparameterization usually leads to a nontrivial CDF.", "In Corollary 1, we reveal the fact that the proposed method degrades into the classical back-propagation algorithm under specific settings.", "Finally, we believe it is interesting to create a consistent architecture, which unifies (a) a GO gradient which contains many popular gradients as special cases, and (b) a more general statistical chain rule developed based on GO which recovers the well-known deterministic chain rule under specific cases.", "For your comments not addressed above, please see our additional response below.", "(1) We have made clearer the relationships among the standard Rep, Implicit Rep/pathwise, and our GO in the revised manuscript.", "In the revised paper we have explicitly pointed out that the experiments from (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) additionally support our GO in the special case of single-layer continuous RVs.", "(2) Please refer to our main contributions summarized above, where other contributions, beyond GO for discrete RVs, are clarified.", "(3) Please refer to our main contributions (ii)-(iii).", "As stated in our paper, many works tried to solve the problem of stochastic/statistical back-propagation.", "We consider our contributions in Secs. 4 and 5 as one step toward that final goal.", "Please note that what\u2019s done in Secs. 4 and 5 is not straight-forward and has not been reported before.", "Since stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) focuses mainly on reparameterizable RVs, deterministic chain rule as mentioned in main contribution (ii) can be readily applied.", "By contrast, we target towards more general situations in Secs. 4 and 5 where deterministic chain rule might not be applicable, such as for non-parameterizable (continuous) RVs.", "We prove that one can utilize our GO to sequentially back-propagate gradient though non-parameterizable continuous RVs, namely the statistical chain rule mentioned in main contribution (ii).", "We have revised the last paragraph of the Introduction to make a more explicit summation of our main contributions, as mentioned above.", "We hope your concerns have been addressed. If not, further discussion would be welcomed."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 164, "sentences": ["Your interpretation of section 3 is exactly right.", "Thank you for suggesting additional experiments to better understand the behavior of the scratchpad component.", "We would like to note that beyond the gains across all evaluated quantitative metrics (bleu, rouge, meteor), our method shows substantial gains on human evaluations.", "In future work we propose to use our method to generate a large dataset and evaluate its performance.", "We don\u2019t claim to be the first to generate questions from logical form, but the experiments within show that our approach is superior to standard approaches in the literature."], "labels": ["rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 165, "sentences": ["Many thanks for the review!", "Good point regarding the negative results; we have added a subsection in the revised paper entitled ``A non-convolutional network'', where we compare to a convolutional decoder and conclude that ``Our simulations indicate that, indeed, linear combinations, yield more concise representations, albeit not by a huge factor.''.", "Regarding the minor points, we have reworded the paragraph on regularizing, and changed `compression ratio' to `compression factor', and reworded such that `large compression factor' means large compression."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 166, "sentences": ["We thank the reviewer for the suggestions.", "Q1: Robot design were explored in (Sims, 1994) etc. The novelty of the paper is fairly incremental.", "We respectfully disagree and believe our contributions are significant.", "We note that only NGE among all the baselines has the ability to optimize both the graph G and the controller parameters.", "Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.", "To the best of our knowledge, the traditional methods (such as (Sims, 1994)) require re-optimizing parameters of the controllers from scratch for each different topologies, which is computationally demanding and breaks the joint-optimization.", "To further showcase our work with respect to prior art, we added (Sims, 1994) as an additional baseline in the latest revision.", "We refer the reviewer to the general response for details.", "NGE has about 2x performance of (Sims, 1994) in both fish and walker environments.", "Moreover, we argue the videos of (Sims, 1994) might be confusing as it mixes the results of policy evolution from human-designed robots and structure evolution.", "Q2: Can it be applied to more complex morphologies?", "Humanoid etc.", "maybe?", "NGE can be applied to evolve humanoids, however, there are two major difficulties in doing that in practice.", "1. Training humanoid controllers is of orders of magnitude more difficult than training cheetah (Schulman, 2017).", "2. To evolve realistic humanoid structure (e.g. hands, symmetrical limbs), one would need to have more realistic environments that better reflect tasks and complexity in the real world.", "However, we agree that this is a very interesting direction for the future.", "Q3: Comparison to more baseline, for example models with no message passing.", "We thank the reviewer for pointing out the baseline of no message passing in GNN, which we named as ESS-BodyShare.", "In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.", "In general, NGE has significant improvement both quantitatively and qualitatively.", "We refer the reviewer to the general response for further information.", "Specifically for ESS-BodyShare baseline:", "|", "NGE     | ESS-BodyShare", "fish         |  70.21   |  54.97", "(78.3% of NGE)", "Walker   |", "4157.9 |   2185.1 (52.5% of NGE)", "In environment where global information is needed (for example, walker with multiple rigid body contact), the performance is jeopardized. But in an easier environment, message passing is less needed.", "Q4: Clarification of Figure-4 (Section-4.2)", "Our aim was to show that in the case where the human-engineered topology needs to be preserved, it is better to co-evolve the attributes and controllers with NGE rather than only training the controllers (controllers are trained from scratch for both NGE and baselines).", "The x-axis was scaled according to the number of updates.", "We apologize for the lack of clarity.", "We revised the x-axis from \u201cgenerations\u201d to parameter \u201cupdates\u201d in the latest revision.", "In the latest revision, we also included the curve where the topologies are allowed to be changed, which leads to better performance, but does not necessarily preserve the initial structure.", "Schulman, 2017. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_other_label"]}
{"abstract_id": 167, "sentences": ["Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "Here we respond to your specific comments.", "\"Some technical details are missing.", "In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing.", "Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\"", ">>> Thanks for pointing out the details.", "We want to clarify the few-shot setting.", "We follow the widely-used episodic paradigm proposed by Matching Networks [1].", "In each episode (training batch), our algorithm solves a small classification problem which contains N classes each having K support and Q query examples (e.g., N=5, K=1, Q=15, totally 80 examples).", "The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset.", "This is very fast and efficient.", "In deep neural networks, there is a common trick in computing the gradient of operations non-differentiable at some points, but differentiable elsewhere, such as Max-Pooling (top-1) and top-k.", "In forward computation pass, the index position of the max (or top-k) values are stored.", "While in the back propagation pass, the gradient is computed only with respect to these saved positions.", "This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch.", "In our paper, we use the tensorflow function tf.nn.top_k() to compute k-nearest neighbor operation.", "\"Does episode training help label propagation? How about the results of label propagation without the episode training? \"", ">>> In our paper, the length scale parameter \\sigma is trained in an example-wise and episodic-wise way, as described in section 3.2.2 and Figure 4 of Appendix A. In order to investigate the benefit of episodic training, we combine the heuristic-based label propagation methods [2] with meta-learning to serve as a transductive baseline.", "Please refer to Table 1 and Table 2 line \"Label Propagation\".", "It can be seen that TPN outperforms naive label propagation with a large margin, thus verifying the effectiveness of episode training.", "[1] Vinyals, Oriol et al. \"Matching networks for one shot learning.\" NIPS. 2016.", "[2] Zhou, Denny et al. \"Learning with local and global consistency.\" NIPS. 2004."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_summary", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 168, "sentences": ["Thank you for your careful consideration and feedback.", "Following your request, we updated the paper to include mean learning curves for different models in Figure 6 in Appendix C. Our models converge faster than DNC.", "Some of them (especially DNC-MD) also have significantly lower variance than DNC."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 169, "sentences": ["We thank Reviewer 1 for the constructive feedback.", "Here is our point-to-point response to the comments and questions raised in the review:", "1. \u201cThe numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper\u2026 I wonder why the numbers are so different.\u201d", "Table 1 of \"Obfuscated Gradients Give a False Sense of Security\" reports an accuracy of 47% under 0.031 norm-inf perturbation for the CIFAR10 dataset (55% is reported for the MNIST dataset), approximately the same as the 44% accuracy in our Figure 5.", "The difference in performance stems from how we preprocessed the CIFAR10 images: exactly in the manner described by (Zhang et al., 2017)\u2019s ICLR paper \u201cUnderstanding deep learning requires rethinking generalization\u201d (we whiten and crop each image).", "2. \u201cWhat's the training time of the proposed method compared with vanilla adversarial training?\u201d", "We have added Table 2 to the Appendix which reports the increase in runtime for each of the 42 experiments discussed in Table 1 after introducing spectral normalization.", "For 39 of the cases, our TensorFlow implementation of the proposed method results in longer training times (from 1.02 to 1.84 times longer).", "In the 3 cases of iterative adversarial attacks with the Inception architecture, the proposed method actually results in faster training time.", "This is likely due to how TensorFlow handles training in the backend.", "We provide the code for full transparency.", "3. \u201cThe idea of using SN to improve robustness has been introduced in the following paper: \"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\" (but this paper did not combine it with adv training).\u201d", "Thank you for bringing this recent work to our attention.", "We cite and discuss this NIPS paper in our updated draft."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 170, "sentences": ["Pose2Pose -- An ablation study for the P2P network can be found in Table 2, with quantitative results for each contribution.", "We do not add a qualitative ablation study for the P2P network, since still-images (as opposed to videos) do not convey the temporal improvement in this case.", "Pose2Frame -- A qualitative ablation study can be found in Fig. 16.", "As can be seen, the results justify each component used.", "pix2pixHD -- the Pose2Frame network can be directly compared with the pix2pixHD network, since they both act as mapping functions between dense-pose representations to realistic images.", "A quantitative comparison can be found in Table 1, as well as a qualitative comparison in Fig. 14.", "As can be seen, the use of our different components described in the P2F ablation study (blending mask and regularization, object channel, two pose inputs, discriminator attention on character, etc.), results in much fewer artifacts, making the Pose2Frame network suitable for this application.", "Combining the Pose2Pose and pix2pixHD networks, would yield significant artifacts (as seen in Fig. 14), and is not suitable for this kind of application."], "labels": ["rebuttal_done", "rebuttal_reject-request", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 171, "sentences": ["We thank reviewer 3 for the detailed feedback.", "We are glad that the reviewer found the extensive evaluation appropriate, and that our model behaves well for the realistic and diversity measures.", "We now address all the individual questions.", "We added Section 3.5 to point out the differences between the VAE component of our model and the SV2P and SVG models from prior work.", "In Section 3.4, we clarified what frames the discriminator takes, and in Section 4.3 we added a description of the deterministic version of our model.", "In Section A.1.1, we provided a better description of how frames are predicted at each time step.", "In Section 3.5 and A.1.2, we clarified that the latent variables are sampled at every time step.", "We updated Section 4.4 to indicate that it is to be expected that although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG from Denton & Fergus (2018)).", "A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.", "In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.", "Note that proposing a generator architecture is not the goal of this paper.", "Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).", "We use a warping-based generator, from prior work (Ebert et al. 2017), and include a comparison to SVG for completeness.", "In the updated draft, we clarify in Section 3.4 that the warping component assumes that videos can be described as transformation of pixels, but that any generator (including the one from Denton & Fergus (2018)) could be used with our losses.", "Since evaluating generator architectures is not the emphasis of this paper, we did not test the importance of the warping component nor test on videos where this hypothesis is less suitable.", "We have included a revised plot in Figure 14 at the end of the Appendix (note that this temporary plot will be incorporated to Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the Learned Perceptual Image Patch Similarity (LPIPS) metric (Zhang et al., 2018).", "LPIPS linearly calibrates  AlexNet feature space to better match human perceptual similarity judgements.", "Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS performance.", "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077", "[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. 2018 PIRM Challenge on Perceptual Image Super-resolution. In Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018.", "https://arxiv.org/abs/1809.07517", "We have included a revised plot in Figure 15 at the end of the Appendix (which will be incorporated to Figure 7) that fixes the KTH dataset preprocessing.", "Our VAE-only model now achieves substantially higher accuracy and diversity than SVG (Denton & Fergus, 2018).", "As before, the GAN-only model mode-collapses and generates samples that lack diversity.", "Our SAVP method, which incorporates the variational loss, improves both sample diversity and similarities, compared to the GAN-only model.", "Our SAVP model also achieves higher accuracy than SVG.", "The experiments from our original submission (1) cropped the videos into a square before resizing, and thus discarded information from the sides of the video, and (2) did not filter out the empty frames, and thus our models were trained on uninformative frames.", "We fixed those issues to match the preprocessing used by Denton & Fergus (2018).", "In addition, we have also included experiments where we condition on only 2 frames instead of 10 frames, in order to test on a setting with more stochasticity."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 172, "sentences": ["Thanks for your valuable review! We've added an experiment section in the new revision, showing how BN helps convergence in the training process."], "labels": ["rebuttal_done"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_done_label"]}
{"abstract_id": 173, "sentences": ["We thank the reviewer for their review.", "We address the different remarks below.", "\u201cIt is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?\u201d", "We apply the data augmentation both at training and test time.", "\u201cIn section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?\u201d", "We agree that the full distribution of the softmax layer provides more information, but there is no straightforward way to extend the Kolmogorov-Smirnov distance to multi-dimensional distributions, beyond the two- and three-dimensional cases.", "We focus on confidence as a proxy to the loss, and we assume that the loss is the quantity that should be the most different between training and testing, as the optimization phase explicitly minimizes the loss on the training set.", "Moreover, early experiments showed that using the outputs of intermediate layers provide no improvement for membership inference (on preliminary CIFAR-10 experiments, we obtained respectively 67.7 accuracy with the output layer and 66.5 when using all layers).", "\u201cSection 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.\u201d", "We will update this section to make it clearer.", "\u201cThe experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.", "Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).", "This seems to be too low to be of practical use.", "This might be because the Bayes and MAT attacks are too simplistic.", "Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?\u201d", "We agree that better performance could be obtained by running the initial model for more epochs, but our goal is to stay close to the standard training of Imagenet models, i.e. 90 epochs with an initial learning rate of 0.1, divided by 10 every 30 epochs.", "We emphasize that the last line of Table 3 corresponds to the most difficult setup, where the network has been trained with a strong data-augmentation, and we only use the intermediate layers of the network (which amounts to less than 62% of the parameters for e.g. Resnet101), this is why the performance is significantly impacted.", "We experimented with more sophisticated models, and it did not bring any improvement (see shadow models in appendix E, e.g. the performance before the softmax layer is 58.2 for Resnet101 and 60.8 for our method)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 174, "sentences": ["We thank the reviewer for their evaluation.", "Please see our response at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X, where we also discuss our experimental framework.", "Even though we present results on two tasks, it appears the paper structure doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "We note in this context that even though we would also like to see Cakewalk evaluated on the domains mentioned by the reviewer, these are not part of our own research agenda, and accordingly our suggestions refer to other problems in combinatorial optimization.", "Next, we address other issues raised by the reviewer.", "First, we\u2019d like to emphasize that the clique problem studied in the paper is far from being a toy problem.", "All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization.", "Over the years, this dataset has become a standard benchmark for clique finding algorithms, and results on it are regularly published.", "In this respect, this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods.", "Notably, Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph, and which are tailored to this specific task.", "Note that none of the tested methods were given enough samples even to recover the graph itself, as most graphs have more than 100 nodes, and we\u2019ve allowed only for 100 |V| samples in each execution.", "To us this seems as a rather challenging setup, not just for the algorithms we\u2019ve tested in this paper, but for any clique finding algorithm.", "Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.", "Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.", "The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions.", "Since the objective is the only source of information for such algorithms, an all-or-none kind of objective would not be very useful.", "Instead, the objective is designed in a manner that provides information even for partial solutions, thus allowing the tested algorithms to gradually improve the objective.", "In terms of the sampling distribution, as our focus is on the update step, we decided to use the simplest possible sampling distribution we can think of.", "In such a regime, we can attribute any performance gains to the algorithms themselves, and not to any prior knowledge that is reflected by the structure of some complex sampling distribution.", "Next, we agree that local optimality is a mean rather than a goal (the objective itself).", "Nonetheless, as in the problems we seek to address the global optimum cannot be found in polynomial time", ", the second best approach is first to design a method that can recover locally optimal solutions.", "Once such a method is available, repeated applications of that method can allow one to select a good solution, very much like the standard practice of repeated applications of k-means which the reviewer mentions.", "This reasoning however is dependent on a method\u2019s capability of recovering locally optimal solutions, and therefore studying this ability makes for a worthwhile effort.", "Answers to the last comments:", "- Table 3 is indeed confusing, this is a good point. We will correct it.", "- Methods that apply a surrogate objective work best with AdaGrad.", "In this case, our data is a classical use which is explored in the AdaGrad paper uses as a motivating example.", "Not surprisingly, both Cakewalk and OCE work best with it.", "REINFORCE however is sensitive to the objective values, and it appears that Adam somewhat mitigates this problem.", "However, this is not as effective as applying a surrogate objective, and REINF_Z with Adam is outperformed by OCE_0.1 with AdaGrad in all measures.", "- Our frame of reference were algorithms that could be applied to any combinatorial problem, and which only rely on function evaluations.", "Control variates and reward shaping methods are mostly useful when tied to the particularities of a given objective, and thus do not fall into this category.", "In neural combinatorial optimization the study is focused on designing a sampling distribution that reflects some prior knowledge about a problem, and thus, we consider this line of work as orthogonal to ours.", "Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.", "- We selected the name \u2018Cakewalk\u2019 after consulting with a few colleagues. Following a joint discussion, we concluded that this name has the best chance for increasing our work\u2019s impact."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_by-cr", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label"]}
{"abstract_id": 175, "sentences": ["We really appreciate your constructive comments.", "We respond to each comment as follows.", "1. Meta dropout does not regularize the variational framework because there is no variational inference framework.", "- Thank you for your comment.", "We agree with you that the current lower bound is not a variational form due to the assumption of q=p. In Section 3.2, we toned down the original expression \u201cLearning to regularize variational inference\u201c into \u201cConnection to variational inference\u201d, and corrected the corresponding sentences.", "Still, there exists a clear connection between standard variational inference and our learning framework.", "Thus we believe that discussion in Section 3.2 will be helpful to readers who want to understand the meaning of learning objective Eq.(2) in depth.", "2. Improving adversarial robustness experiment.", "- Thank you for the helpful suggestion.", "During the rebuttal period, we conducted additional experiments on adversarial robustness as you suggested:", "a) We replaced the previous FGSM attack with stronger PGD attack (200 iter.), with $L_1$, $L_2$, and $L_\\infty$ norm constraints.", "b) We included more baselines (e.g. Mixup, VIB, and information dropout), and show that our meta-dropout largely and consistently outperforms all of them.", "c) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results.", "d) We further show that the learned perturbation from our Meta-dropout also generalize across different types of adversarial attacks with $L_1$, $L_2$, and $L_\\infty$ attacks.", "The generalization to different types of attacks is an important problem in adversarial learning, and most existing models fail to achieve this goal.", "Please see the corresponding section in the revision.", "We believe that the adversarial robustness part of our paper has become much stronger than before, thanks to your suggestion."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 176, "sentences": ["Thank you very much for the constructive comments.", "We tried to strengthen our claims by adding more experimental data which the Reviewer requested.", "1. The proposed \"Multi-bit-quantization + Viterbi-based binary code encoding\" requires slightly larger memory footprint than \"Multi-bit quantization only ([4])\" because some of the Viterbi encoded bits have different indices from their corresponding quantization bits.", "Hence, the \"Multi-bit quantization only\" requires 10 % to 20 % smaller memory footprint than \"Multi-bit-quantization + Viterbi-based binary code encoding\" case.", "However, the main reason why we apply the Viterbi weight encoding is that parallel sparse-to-dense matrix conversion can be done by applying same Viterbi encoding process to the non-zero values and indices of the non-zero values in parallel.", "This parallel sparse-to-dense conversion makes the speed of feeding parameters to PEs 10 % to 40 % faster compared to [1] (Figure 6c).", "2. Per Reviewer\u2019s suggestion, the experimental results for the effectiveness of \"Don\u2019t Care\" term have been moved to Section 4.1.", "3. Per Reviewer's suggestion, we measured accuracy differences before and after Viterbi encoding for several quantization methods such as linear quantization ([2]), logarithmic quantization ([3]), and alternating quantization ([4]) methods with the same quantization bits (3-bit).", "The result shows that combination with alternating quantization and Viterbi weight encoding had only 2 % validation accuracy degradation after the Viterbi encoding was applied first right after the quantization and the accuracy was easily recovered with retraining.", "On the other hand, the combination with the other quantization methods and Viterbi weight encoding showed accuracy degradation as much as 71 %, which was too large to recover the accuracy with retraining.", "The accuracy difference mainly results from the uneven weight distribution.", "Because weights of neural networks usually are normally distributed, the composition ratio of '0' and '1' is not equal when the linear or logarithmic quantization is applied to the weights of neural networks.", "As we stated in the manuscript, Viterbi encoder tends to produce similar number of '0' and '1'.", "Therefore, we can conclude that under the same bit condition, alternating quantization method shows best accuracy and compatibility with our bit-by-bit Viterbi encoding scheme regardless of the type of neural networks.", "4. We conducted additional simulations to compare sparse matrix reconstruction speed of [1] and the proposed method.", "We used a random 512-by-512 size matrix with various pruning rate ranging from 75 % to 95 %.", "We conducted the simulations under the assumptions described in Figure 6c.", "The simulation results are shown in Figure 6c in updated manuscript.", "We could observe that the proposed method could feed 10 % to 40 % more nonzero weights and input activations to PEs in same 10000 cycles compared to [1].", "Proposed method could also feed parameters to PEs 20 % to 106 % faster compared to baseline method, which reads dense weight and activation matrices directly from DRAM.", "The improvement in the proposed scheme mainly comes from the parallelized process of assigning non-zero values to their corresponding indices in the weight matrix.", "While preparing addition data for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.", "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.", "Therefore, we updated Figure 7 in original manuscript to Figure 6c in updated manuscript according to the new data.", "5. We added the change of the exact weight representation at each process in Figure 1 to clarify the flowchart.", "Reference", "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "[2] Darryl D. Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks.", "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pp. 2849\u20132858. 2016.", "[3] Daisuke Miyashita, Edward H. Lee, and Boris Murmann. Convolutional Neural Networks using Logarithmic Data Representation. CoRR, abs/1603.01025, 2016. URL https://arxiv.org/abs/1603.01025.", "[4] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 177, "sentences": ["We thank you for the constructive feedback and discuss some of your comments below.", "R3: \"it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards\"", "=> We would like to highlight that evaluating success of pure curiosity-driven exploration (no extrinsic rewards for training) by measuring the extrinsic score of game is just a proxy to evaluate exploration.", "Our results show that exploration via curiosity has striking correlation with game scores.", "But we expect that when environments have a well-defined (and well-shaped!) extrinsic reward, a policy trained using that extrinsic reward should outperform the policy trained with only curiosity especially when the performance is measured by the extrinsic return.", "There are, however, examples, such as the Bowling Atari game, where a policy trained with only curiosity does *better* than a policy trained with extrinsic rewards.", "The purely curious agent learns to play the game better than agents trained to maximize the (clipped) extrinsic reward directly.", "We think this is because the agent gets attracted to the difficult-to-predict flashing of the scoreboard occurring after the strikes.", "We expect such examples to come from environments with misleading or poorly-shaped extrinsic rewards.", "R3: \"...it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.\"", "=> In the paper, Section 2.1, we discuss that random features have advantages that they are they are stable, compact, and tend to include most relevant information about the observation.", "However, in our opinion, a more interesting question is not why random features perform so well, but rather why the feature learning methods perform so poorly (relative to this baseline).", "Learning the features introduces non-stationarity that confounds the effects of learning the dynamics.", "We believe that if methods are developed to address this non-stationarity, or environments that are more visually complex are used, then the benefits of the learning the features will become more noticeable."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 178, "sentences": ["Thanks for your comments.", "To our knowledge, close to 2% improvement of accuracy is not small in CIFAR100. Because we only change pooling layers while keeping others exactly the same.", "Now, we respond to your questions one by one:", "1.", "The results of AA-pooling and F-pooling are not the same.", "In Fig. 1, we show the results of average-pooling and F-pooling.", "If you carefully look at the corner of curves, you can find the differences.", "Without convolution, AA-pooling is similar to average-pooling (both of them are low-pass filters but with sightly different kernels.", "So AA-pooling gives different results for sine waves.", "2. We believe F-pooling plays a more important rule in applications where shift-equivalent is serious, such as object detection and object tracking.", "Because we need to predict the location or shifts of an image object.", "Moreover, F-pooling may be better for complex-valued CNNs, such as [1].", "3. The limitation of imaginary part is easy to overcome: set the resolution of F-pooling\u2019s output to an odd number or padding it to an odd number when the resolution is an even number.", "In this way, the imaginary part is zero.", "Moreover, the word shift in this paper means circular shift.", "So it is better to use circular padding in convolutional layers.", "However, we find circular padding slower the training speed in PyTorch.", "If we use zero paddings as in most situations, the beneficial of F-pooling is reduced.", "Our current experiments use zero paddings.", "See our general response for what happens when we replace zero paddings with circular padding.", "4. In all experiments of our current paper, the imaginary part is already ignored.", "We can\u2019t directly measure how the imaginary part affects the performance unless we use complex-valued CNNs.", "Ignoring this part will destroy the reconstruction optimality, but the effect is small.", "Suppose the output size of F-pooling is 2N+1.", "We first transform a signal into frequency domain and keep 2N+1 components with the lowest frequencies: f(-N), \u2026 , f(0), \u2026 ,f(N).", "Then we transform it back into time domain.", "In this case, the imaginary part in time domain is zero because of symmetry.", "Now, suppose the output size is 2N+2: f(-N), \u2026 , f(0),", "\u2026 f(N), f(N+1)", ".", "In this case, the imaginary part is not zero.", "However, if we set f(N+1) to 0, it imaginary part becomes zero again.", "Thus, the error of ignoring imaginary part is not larger than ||f(N+1)||.", "Fig.4 shows an example of odd and even output size of F-pooling.", "[1] Deep complex networks, ICLR2018"], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 179, "sentences": ["Dear Reviewer:", "Thank you for your valuable comments.", "We have addressed typos in the revision accordingly.", "And please find our response as follows.", "-  Can you be more specific about the gains in training versus inference time?", "We would like to emphasize that the our goal is to speed up the inference time for softmax, so we do not include any comparisons in terms of training time.", "According to our experiments, most speedup can be achieved in few epochs (given all other layers are pre-trained) so that the training time increase is not significant compared to the original one.", "- You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?", "Thanks for the suggestion.", "We demonstrate that ambiguous words are often overlapped between clusters as illustrated in Figure 3(b).", "We added one more Figure in Appendix B, Figure (b), to demonstrate the distribution of overlapping.", "- It wasn't clear how the sparsity percentage on page 3 was defined?", "Sorry for the possible confusion.", "The sparsity in page 3 means the percentage of pruned words.", "We have added more clarifications in the revised version.", "- Can you motivate why you are not using perplexity in section 3.2?", "We use top-k accuracy, instead of perplexity, because approximating top-k is required for most inference tasks in practice (see [1]).", "Perplexity captures the normalized log-likelihood of all possible words, while top-k accuracy is a better measure for inference speedup for top-k retrieval.", "For example, in some extreme cases, if a word only has a very small probability which makes it unpredictable at all (i.e. couldn\u2019t be retrieved by top-k for any reasonably small k)", ", it could still have a huge impact in terms of perplexity, but has a much smaller impact on top-k accuracy, which seems more reasonable given the goal of top-k retrieval.", "[1] Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS), NIPS 2014"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 180, "sentences": ["We appreciate your constructive feedback.", "Specifically, your comments about the motivation and problem definitions greatly help us to improve the quality of our paper.", "(Importance and motivation)", "To step forward to artificial general intelligence, we should further consider making an agent that can learn and remember many tasks incrementally [1].", "However, this is particularly challenging in real-world settings: the agent may observe different tasks sequentially, and an individual task may not recur for a long time.", "In this settings, a learned model might overfit to the most recently seen data, forgetting the rest, a phenomenon referred to as catastrophic forgetting, which is a core issue CL systems aim to address [2].", "Recently, GR-based methods, inspired by the generative nature of the hippocampus as a short-term memory system in the primate brain [3], have been widely studied to address the catastrophic forgetting problem.", "In terms of GR, we are trying to address the two open questions mentioned above.", "(Use of labels and novelty) In GR-based approaches, the quality of generated samples is crucial to keep the performance of previous tasks.", "If we use labels, we can construct a conditional generative model.", "Generally, conditioning on a generative model yields higher quality samples than unconditional one and makes it possible to generate class-balanced samples [4]; the importance of conditional generation is also described in section 6.1 in our paper.", "In this paper, we showed that discriminative regularization could make VAE possible to conduct both class conditional generation and classification with one integrated model.", "Thus, we do not need to train an additional classifier, e.g., deep CNN, which is necessary for other works, including Narayanaswamy et al. There is also classifier integrated VAE such as [6].", "The difference with [6] is the use of class-conditional priors; more details are explained at the response (Difference with CDVAE) for reviewer 3 and section 4.1 in our paper.", "(Domain translation) Even though the conditional generation improves the quality of the generated samples, there is still a big difference between real and generated images.", "Because a deep neural network is vulnerable to even single-pixel perturbation [5], the difference can seriously affect the classification performance of GR-based algorithms.", "Thus, we suggested applying the domain translation to address this issue.", "By narrowing distribution discrepancy between real and generated images using the domain translation technique, we were able to alleviate the catastrophic forgetting problem successfully (Table 2).", "(Modeling) Good point.", "Since we consider finite discrete conditions, we can directly optimize $\\mathrm{\\mu_c}$ and $\\mathrm{\\sigma_c}$ for each c as parameters without the prior network.", "However, introducing a prior network makes our model become a more general framework that can address continuous-valued conditions.", "Also, in our paper, we set the prior network as a single fully-connected layer for easy handling of conditions and simple implementation.", "Otherwise, we should keep an additional mapping table between class conditions and its $\\mathrm{\\mu_c}$ and $\\mathrm{\\sigma_c}$.", "(Experimental settings)", "Generally, CL systems assume that each task comes sequentially, and an agent can not directly access previous experience [2].", "We exactly follow the assumption.", "Also, we train DiVA sequentially for each task with one same model.", "To clarify our training process, we provide a brief summarization.", "Firstly, we train DiVA with task 1 that consists of real images and labels.", "Then, when new task 2 is coming, DiVA generates images and its labels of task 1 and learns both task 2 and the generated task 1 simultaneously.", "We added an additional figure in Figure 6 in Appendix E, for helping conceptual understanding.", "(Domains of CIFAR dataset) Since current generative models are not perfect for generating complex natural images, there is always a discrepancy between generated images and real images.", "Thus, we can define two domains: real image domain (realistic) and generated image domain (blurry).", "We used the domain translation for narrowing the gap.", "[1] Legg, Shane, and Marcus Hutter. \"Universal intelligence: A definition of machine intelligence.\" Minds and machines 17.4 (2007): 391-444.", "[2] https://sites.google.com/view/continual2018", "[3] Shin, Hanul, et al. \"Continual learning with deep generative replay.\" Advances in Neural Information Processing Systems. 2017.", "[4] Lesort, Timoth\u00e9e, et al. \"Marginal Replay vs Conditional Replay for Continual Learning.\" International Conference on Artificial Neural Networks. Springer, Cham, 2019.", "[5] Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. \"One pixel attack for fooling deep neural networks.\" IEEE Transactions on Evolutionary Computation (2019).", "[6] Mundt, Martin, et al. \"Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition.\" arXiv preprint arXiv:1905.12019 (2019)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 181, "sentences": ["Thank you for your review and very useful comments! We\u2019re happy you found our manuscript interesting.", "To address your comments:", "1) Thank you for pointing out that we had not defined the delta.", "Here delta is the Kronecker delta defined so that \\delta_{a,b} = 1 if a = b and 0 if a != b. In the context of the variance of the multivariate normal distribution, the delta function indicates that the different neurons in each layer have zero covariance.", "We\u2019ll add an explicit discussion of this fact to the manuscript.", "2) Thanks for pointing this out, we\u2019ll correct it in the next revision.", "3) It is true that the extent to which randomized weights describe trained networks is unclear.", "However, it is true that most commonly used weight initialization schemes are random.", "For example, He initialization [1] and Xavier initialization [2] strategies are both special cases of the setup considered here.", "We therefore view our theory as a theory of neural networks at initialization.", "(There are, however, initialization schemes that are not random and that are not described by our theory).", "[1] K. He, X. Zhang, S. Ren, J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. (http://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html)", "[2] X. Glorot, Y. Bengio, Y. W. Teh, M. Titterington. Understanding the difficulty of training deep feedforward neural networks. (http://proceedings.mlr.press/v9/glorot10a.html)"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 182, "sentences": ["Thank you for your valuable feedback!", "After carefully reading it, we plan to modify the manuscript as discussed below (the planned changes are shown by **", "\u2026 *", "*", ")", ".", "We would appreciate it if you could let us know whether the proposed changes address your concerns, or whether we have misinterpreted your comments.", "\u2022", "You have raised an interesting question about how the accuracy of the GAN impacts the accuracy of the proposed method.", "In order to address this, we have developed analytical estimates for the error in the point estimates computed using the proposed approach and show that these are intimately tied to error in computing the point estimates for the prior using the GAN.", "We have also demonstrated that as the generator and the discriminator of the GAN become more expressive this error tends to zero, and the exact point estimates, for both the prior and the posterior, are recovered. ** In the revised manuscript, we will include this mathematical analysis in the Appendix and refer to it in the main text. **", "\u2022", "We note that our method of inferring the desired image from the measured image is an unsupervised method; for training we only need a set of desired images  to construct the prior.", "We are not aware of any other unsupervised learning approach for solving these types of problems with quantified uncertainty.", "In that regard, the calculation of point-wise variance (our metric of uncertainty) is possible only using our approach, and therefore a direct comparison is not possible, since other supervised methods (explained below) cannot work in this setting where only set of desired images are available. ** We will clarify this unique aspect in the revised version of the manuscript. **", "\u2022\tThere has been some work on computing the uncertainty in an inferred image within a supervised learning framework where pairs of measured and desired images are used for training the network [1, 2].", "In these articles the authors have used methods like Bayesian dropout and variational autoencoder to compute uncertainty in the inferred images.", "*", "* We will refer to these works in the revised version to better orient reader.", "*", "*", "[1]. A. Kendall and Y. Gal, \u201cWhat Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?", "\u201d, NIPS (2017).", "[2]. Kohl, S.A., Romera-Paredes, B., Meyer, C., Fauw, J.D., Ledsam, J.R., Maier-Hein, K.H., Eslami, S.M., Rezende, D.J., & Ronneberger, O. \u201cA Probabilistic U-Net for Segmentation of Ambiguous Images\u201d, NeurIPS (2018).", "Thank you for your valuable feedback! After carefully reviewing it, we have modified manuscript as discussed below (description of changes is enclosed within ** ... **).", "\"Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.", "In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.", "Hence, the effectiveness and advantage of the proposed methods are not clear.\"", "We have addressed this by responding to the specific questions below.", "\"- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc. \"", "We are not aware of any other methods for computing uncertainty in recovered images that have been used to drive an active learning task in image inpainting.", "While methods based on dropout (\\cite{Kendall2017a, Kendall2019}) or variational inference (\\cite{Kohl2018a}) could be extended to accomplish this, this has not been done thus far.", "**We have added this comment in Section 3.1**", "Another big difference between the methods mentioned above and our approach is that while they require image pairs (true and corrupted images) for training, our approach only requires uncorrupted images.", "Thus while our algorithm relies on unsupervised learning, the other algorithms fall under the category of supervised learning.", "*", "*We have also clarified this within the \"Our Contributions\" Section**", "\"- How does the estimation accuracy of GAN relate to the estimation accuracy of the proposed method? Showing a quantitative description would be nice.\"", "We thank the reviewer for raising this important question.", "*", "*We have addressed it thoroughly in Appendix A. We have provided a proof that demonstrates the weak convergence of the posterior density calculated using our method to the true posterior density as the number of weights in the discriminator and generator components of the GAN is increased.**"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_none", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_none", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_none", "rebuttal_by-cr", "rebuttal_none", "rebuttal_none", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_none", "rebuttal_done", "rebuttal_structuring", "rebuttal_social", "rebuttal_none", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_summary_label", "rebuttal_done_label"]}
{"abstract_id": 183, "sentences": ["We think your suggestions are very meaningful. We respond to them one by one:", "1. We will explain anti-aliasing in our updated paper.", "Roughly, anti-aliasing is helpful for signal reconstruction.", "However, we can\u2019t provide a strict treatment of how anti-aliasing relates to classification.", "But we have intuitions: first, we believe reconstruction relates to classification (see our next response); second, frequency components are orthogonal.", "Aliasing means different components are mixed again.", "This may mislead the next layers for processing.", "2. To our knowledge, researchers haven\u2019t fully understood the whole process of image classification until now.", "Thus, we can\u2019t provide a strict treatment of how reconstruction optimality relates to classification optimality.", "But we have intuitions and empirical evidence of their relation: convolution layers are used to transform a signal which makes it easier to be classified.", "So if we accept that the feature extracted by previous convolution layers is useful, then it is best to keep it as much as possible for the current pooling layer.", "In this way, it is reasonable to assume that reconstruction optimality is consistent with classification optimality.", "On the other hand, it is difficult to directly define classification optimality for an intermediate layer.", "Moreover, several works, such as [1] have shown that using self reconstruction loss as an auxiliary is helpful for classification.", "3.", "Please refer to our general response.", "With suitable settings, the shift consistency of F-pooling is much better.", "[1] Semi-Supervised Learning with Ladder Networks, NIPS2015"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 184, "sentences": ["We sincerely appreciate your constructive comments.", "We respond to your main concerns below:", "1. The Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?", "- To demonstrate that strong generalization performance of Meta-Dropout is not the effect of using larger number of model parameters, we doubled the number of channels for the base model and report its performances (MAML(x2)).", "Models\t\t   #param.\tOmni-1shot\tOmni-5shot\tmimg-1shot\tmimg-5shot", "MAML\t\t   x1\t        \t95.23+-0.17\t98.38+-0.07\t49.58+-0.65\t64.55+-0.52", "MAML(x2)", "x4", "94.96+-0.16\t98.36+-0.08", "48.19+-0.64", "65.84+-0.52", "Meta-SGD         x2", "96.16+-0.14", "98.54+-0.07", "48.30+-0.64", "65.55+-0.56", "Meta-dropout  x2", "96.63+-0.13\t98.73+-0.06", "51.93+-0.67", "67.42+-0.52", "The number of parameters of MAML(chx2) is four times of that of MAML, while Meta-dropout is only doubled.", "Nonetheless, MAML(chx2) does not improve on MAML, demonstrating that the effectiveness of meta-dropout does not simply come from using larger number of parameters.", "Meta-SGD also doubles the number of parameters in the base MAML model, but is significantly outperformed by Meta-dropout.", "We want to emphasize that Deterministic meta-dropout is also one of our models, and that its good performance does not hurt our claim on the effectiveness of the multiplicative noise.", "This is because meta-dropout consists of two parts: meta-learned deterministic multiplicative perturbation and random noise.", "Thus the deterministic meta-dropout still \u201clearns to perturb\u201d, although not random, and is actually a core component of meta-dropout (See Table 3 in the revision).", "Please also see our response to the Reviewer #3, comment #4."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 185, "sentences": ["Thank you for your detailed review.", "On releasing a structured (parsed) form of the dataset: we agree that examining performance on structured input is a very useful exploration direction, that can give insight into what effect parsing has on ease of training.", "We feel, however, that there\u2019s no single canonical choice for the structure that may be suitable for all types of networks (e.g., tree networks, graph networks, etc), or different levels of structure that aid the network to different amounts, from completely unstructured to tree-like structures that essentially determine the required order of calculation.", "For example, in the question type of \u201cmultiple function composition\u201d, one could have a structure that lists the functions, and also the desired composition order; or one could actually have a tree structure with the functions already embedded in the correct composition order (which we suspect would be quite easy to learn models on).", "In lieu of this, we hope the released dataset source code will allow researchers to easily tailor the dataset to their specific problems and models.", "We have rewritten the section describing the neural models, with clearer terminology, and the differences between the different models made much more explicit.", "Thank you for pointing this out, and please let us know if any parts are still unclear.", "The \u201cattentional LSTM\u201d model is just the standard encoder/decoder+attention architecture prevalent in neural machine translation as introduced in \u201cNeural machine translation by jointly learning to align and translate\u201d (Bahdanau et al).", "However, we confusingly used the terms \u201cparser\u201d instead of \u201cencoder\u201d, and we have fixed the description.", "On running the decoding LSTM for a few steps before outputting the answer: we found that it was one of the few (relatively simple) architectural changes to the standard recurrent encoder/decoder setup that significantly helped performance (thus the performance on the standard architecture can be taken to be slightly worse than the numbers reported in the paper for the architecture with \u201cthinking steps\u201d), but we also realize that it is not a widespread architectural change. (Possibly the need for this is less in standard machine translation tasks.) Since your review, we have also ran experiments using the published architecture introduced in \u201cAdaptive Computation Time for Recurrent Neural Networks\u201d (Graves).", "This architecture has an adaptive number of \u201cthinking\u201d steps at every timestep dependent on the input, learnt via gradient descent.", "More specifically we investigated the use of this for both the recurrent encoder and decoder (replacing the single fixed number of \u201cthinking\u201d steps at the start of the decoder).", "After some tuning, its test performance was still around 3% worse than the same architecture without adaptive computation time.", "We\u2019ve updated the paper to mention this.", "Please refer to the updated PDF of the paper to see these changes.", "We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_social", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 186, "sentences": ["Thank you very much for your encouraging review.", "> I read the paper and understand it, for the most part.", "The idea is to interpret some regularization techniques as a form of noisy bottleneck, where the mutual information between learned parameters and the data is limited through the injection of noise.", "While the paper is a pleasant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation.", "Perhaps other referee will have a clearer opinion.", "The main contribution of our paper is indeed to establish a connection between variational inference and regularization by observing that Gaussian mean field introduces an upper bound on the mutual information between data and model parameters.", "Reinterpreting mean field as point estimation in a noisy model allows us to quantify observed regularizing effects.", "We show links to existing regularization strategies and validate the usefulness for regularization in targeted experiments.", "While the focus of our present work lies on establishing links between existing directions of research, we believe that our information-theoretic perspective on regularization opens up plenty of avenues for future work, both in supervised and unsupervised learning.", "For example, we are interested in improving extraction of unsupervised representations by controlling the amount of extracted information.", "In particular, we aim to mitigate latent collapse, a problem reported for example in language generation [1] and autoregressive image generation [2], which is currently mitigated with ad-hoc strategies such as KL annealing.", "Intuitively, if all information can be stored in the model itself, there is little incentive to use a per-sample latent.", "This is also known as the information preference problem, as briefly discussed at the end of section 2.1.", "Therefore, limiting mutual information of the data with the model might offer a robust mitigation strategy.", "Additionally, we believe that the approach can lead to improved representations through disentanglement, as done by beta-VAE [3].", "Our formal connection to beta-VAE derived in Appendix C offers a promising information-theoretic perspective on their empirical results.", "More generally, we want to explore non-MAP inference on noise-injected models as this would allow for using highly expressive variational distributions while enjoying the information-theoretic guarantees of simpler approximate distributions, as motivated in section 3.3.", "Since these directions are rather orthogonal, we think that sharing our theoretical framework with the community in an independent piece of work is the most effective way of communicating our ideas.", "> I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)", "Reference priors are opposite to our work in the sense that they maximize the amount of information data provides about the parameters, while we aim to find models to limit it.", "Also, see [4] for the relation of Fisher information to generalization.", "References", "[1] Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R. & Bengio, S. (2015). Generating sentences from a continuous space.", "arXiv preprint arXiv:1511.06349.", "[2] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D. & Courville, A. (2016). Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013.", "[3] Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G. & Lerchner, A. (2018). Understanding disentangling in beta-VAE.", "arXiv preprint arXiv:1804.03599.", "[4] Ly, A., Marsman, M., Verhagen, J., Grasman, R. P. & Wagenmakers, E. J. (2017). A tutorial on Fisher information.", "Journal of Mathematical Psychology, 80, 40-55, page 30"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_structuring_label"]}
{"abstract_id": 187, "sentences": ["Thank you for the review and careful reading of our paper! We\u2019re glad that you found it of interest. On revision we will fix the typos that you identified.", "Regarding the first point, your intuition is exactly correct and a slightly simpler discussion of this phenomenon can be found in [1].", "When the network is deep enough that the covariance matrix has reached its fixed point, the distribution of the outputs of the network will be independent of the inputs.", "At this point the network becomes untrainable.", "To reconcile this with the commonsense intuition that \u201cdeeper is better\u201d, our answer is twofold.", "1) As in [1] and [2] it is often possible to find configurations or architectural modifications where the covariance matrix doesn\u2019t approach its fixed point over depths often considered in machine learning.", "When this is the case one can safely increase the depth without sacrificing accuracy.", "2) It seems that the role of depth in performance is more subtle than standard intuition would dictate.", "For example, in [3] note that although the authors were able to train a 10k hidden layer network, they did not observe any improvement in accuracy.", "In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.", "[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein.", "Deep Information Propagation (https://arxiv.org/abs/1611.01232)", "[2] G. Yang and S. S. Schoenholz. Mean Field Residual Networks (https://arxiv.org/abs/1712.08969)", "[3] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)"], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 188, "sentences": ["Thank you very much for your review! Please see our responses below regarding your comments:", "\u201cEvaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (D\u00e9j\u00e0Vu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)\u201d", "Response: We investigated this question by running jscpd (a popular code duplication detection tool, https://github.com/kucherenko/jscpd ) on our entire data set and found that only 2.7% code is duplicated.", "Furthermore, most of these duplicates are intra-project.", "Thus, we believe that code duplication is not a severe problem in our dataset, and we will include more details about this result in any future revision.", "========", "\u201cThe hyperparameter selection regime (and the experiments used to find them) is not described\u201d", "Response: We selected hyperparameters in a standard way by tuning on a validation set as we were developing our model.", "We\u2019ll include more details about hyperparameters and hyperparameter selection in any future revision."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 189, "sentences": ["Thanks for your valuable comments and feedback.", "R: \"The authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks? \"", "A: Yes but not only.", "We propose to go beyond the classical markov hypothesis of cascade models that states that any infected node owns the same transmission probabilities whatever from whom comes the propagated content.", "We indeed do this by replacing the traditional parametric functions with  deep neural networks, which enables to consider recurrent latent states for infected nodes.", "This allows us to embed the past in node states and hence to output different future diffusion distributions regarding the past trajectory of the propagated content, which is our main contribution (a cascade model with neural network was already proposed for instance in (bourigault et al., 2016) but without past inclusion).", "While existing works on cascade models learn parameters by inferring the direct infector of every infected node (i.e., estimating $P(I_i|D_{\\leq i})$), we need to infer the whole past trajectory to compute node states (i.e., considering $P(I_i|D,I_{<i})$), which is greatly more difficult but the proposed learning approach allowed us to efficiently deal with it.", "R: \"The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works. This could have made the paper much stronger.\"", "A: To give more clues about the good behavior of the algorithm, we added results about the accuracy of the sampled trajectories on the artificial datasets (for which we have the ground truth on who infected whom).", "We report the rate of good infector choices (i.e., the rate of I_i that equal the ground truth) for our approach and the others.", "Results show that our approach actually performs better infector choices than CTIC which does not consider the history of the diffusion in its infection probabilities.", "The use of our recurrent architecture helps the process to distinguish some different diffusion contexts from the past.", "We also added a second artificial dataset to further analyze the behavior of the approaches.", "R: \"It was nice that the paper iterated and reviewed the possible inference and learning ways. There is one more way. Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.\"", "A: Thanks for the proposal and the reference that we added in the paper.", "The full computation of the posterior distributions could indeed be avoided by using an importance sampling MCMC procedure with auxiliary variables", "(such as done in [1] in the context of diffusion source detection), but in our context we think that the increased computation efficiency would be at the cost of a very higher variance in the learning process, due to the strong intrication of latent and observed variables.", "In [1], the problem is easier: they do not have to perform optimization on the diffusion parameters (since relying on a diffusion model learned a priori), the problem is to sample hidden infection times to estimate likelihoods and then identifying the most probable source of diffusion.", "R: \"The paper can benefit from a proofreading.\"", "A: Thanks, we indeed corrected serveral typos like this in the new version of the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 190, "sentences": ["We appreciate your constructive feedback.", "Specifically, your comments about our derivations greatly help us to improve the quality of our paper.", "We hope you to also consider our notable experimental results as well.", "(Bounds of KL divergence) Thank you for this good comment.", "We claimed that the Equation 1 can be maximized indirectly by maximizing Equation 2 which is a lower bound of Equation 1.", "If we understand your primary concern correctly, the concern comes from the bound of KL divergence in Equation 5.", "To prove correctness of our formulation, we can rewrite the pointed term in Equation 5 by using simple bayes rule as follows:", "$$\\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\ \\mathrm{log}\\ \\frac{\\hat{q}\\mathrm{(z|c)}}{\\hat{p}\\mathrm{(c|z)}} = \\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\bigg(\\mathrm{log}\\ \\frac{\\hat{q}\\mathrm{(z|c)}}{\\hat{p}\\mathrm{(z|c)}} + \\mathrm{log}\\ \\frac{\\hat{p}\\mathrm{(z)}}{\\hat{p}\\mathrm{(c)}} \\bigg)$$", "Because the $\\hat{p}\\mathrm{(c)}$ is constant, and $\\hat{p}\\mathrm{(z)}$ is not included in our optimization, we just optimize $\\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\mathrm{log}[\\hat{q}\\mathrm{(z|c)} / \\hat{p}\\mathrm{(z|c)}]$. Since the $\\hat{q}\\mathrm{(z|c)}$ and $\\hat{p}\\mathrm{(z|c)}$ are both normalized distributions, the $D_{KL}[\\hat{q}\\mathrm{(z|c)} || \\hat{p}\\mathrm{(z|c)}]$ is always positive.", "Then, we can conclude that Equation 2 becomes the lower bound for Equation 1.", "(lambda)", "Actually, Equation 7 consists of three terms.", "Since only the third term is proposed additional regularization, we applied weighting parameter lambda to the third term only.", "(Difference with CDVAE) To clarify the difference our DiVA with CDVAE, we write derivations for both models here.", "CDVAE: $\\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ p_{\\theta '}(\\mathrm{x|z)}] - D_{KL}[q_{\\theta}\\mathrm{(z|x)} || p\\mathrm{(z)]} + \\lambda \\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\hat{p}_{\\phi '}\\mathrm{(c|z)}]$", "DiVA: $\\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ p_{\\theta '}(\\mathrm{x|z)}] - D_{KL}[q_{\\theta}\\mathrm{(z|x)} || \\hat{q}_{\\phi}\\mathrm{(z|c)]} + \\lambda \\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ \\hat{p}_{\\phi '}\\mathrm{(c|z)}]$", "As we discussed in section 4.1, below the table for Algorithm 1, the key difference is that we consider class-conditional Gaussian distributions as priors for variational posteriors.", "Since CDVAE assumes the prior as unit Gaussian for all classes and optimizes classification loss simultaneously with the KL divergence, the latent space does not follow the prior exactly.", "As a result, CDVAE sometimes generates ambiguous samples (Figure 2 (c)).", "Interestingly, RtF [1] also does not consider the class-conditional priors even though they consider a classifier integrated VAE similar to CDVAE.", "In contrast, we assume class-wise specific Gaussian for each class.", "As a result, we can stably generate more realistic samples than CDVAE.", "[Additional feedback]", "(dt in Algorithm 1) dt means the domain translation explained at section 5.", "(Figure 1)", "- We corrected the typo.", "- The 3d plot conceptually represents class-specific one mode Gaussians.", "- The classification loss has implicit dependency with input conditions by minimizing the KL divergence in Equation 2.", "(heavy classifier) A classifier such as resnet.", "We used this term to distinguish the additional classifier from our integrated encoder that has discriminative power.", "(Redundant weights)", "If we extend to a more complex dataset such as ImageNet, it will become highly redundant.", "Furthermore, if we consider fully-convolutional architecture (without fully-connected layers), redundancy becomes a serious problem.", "For example, a feature map that has shape of [W x H x dim] becomes [W x H x (dim + the number of classes)].", "In contrast, using discriminative conditional distributions can keep the dimension of the feature map as [W x H x dim] regardless of the number of classes.", "(Notations) Thank you for commenting this.", "We corrected the notations of section 3 to match with later sections.", "(Complexity of encoder) We intended that the encoder network can have enough both discriminative and generative power with a powerful architecture such as a deep residual network.", "[References]", "[1] van de Ven, Gido M., and Andreas S. Tolias. \"Generative replay with feedback connections as a general strategy for continual learning.\" arXiv preprint arXiv:1809.10635 (2018)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 191, "sentences": ["Dear reviewer:", "We appreciate your comments but it appears that there is some misunderstanding regarding our contribution in this work.", "Our work is for softmax inference speedup", "while Sparse-Gated MoE (MoE) was not designed to do so.", "It was designed to increase the model expressiveness.", "It cannot achieve speedup because each expert still contains full softmax space as we mentioned in the background section (page 2 line 21st) and method section (page 2 last 4th line).", "And since it is slower than the standard softmax by definition, we chose not to compare with it in the paper.", "Our algorithm addresses speed up in softmax inference.", "This is fundamentally different from Sparse-gated MoE. We divide the output space into multiple overlapped subsets.", "To find top-k predictions, we only search a few subsets.", "While in full softmax or MoE, the complexity is linear with output dimension.", "Therefore, we did not include a comparison with Sparsely-Gated MoE in our article and only compare with full softmax.", "Just for additional reference, we tested Sparsely-Gated MoE with different experts in PTB dataset; we compared the results to DS-Softmax.", "As expected, the Sparsely-Gated MoE does not achieve speedup in terms of softmax inference.", "_____________________________________________", "_", "Method | Top 1 | Top 5 |Top 10| FLOPs|", "DS-8       | 0.257 | 0.448 | 0.530 | 2.84x |", "MoE-8    | 0.258 | 0.448 | 0.530 |  1x", "|", "DS-16     | 0.258 | 0.450 | 0.529 | 5.13x |", "MoE-16  | 0.258 | 0.449 | 0.530 | 1x", "|", "DS-32     | 0.259 | 0.449 | 0.529 | 9.43x |", "MoE-32  | 0.259 | 0.450 | 0.531 | 1x", "|", "DS-64     | 0.258 | 0.450 | 0.529 |15.99x|", "MoE-64  | 0.260 | 0.451 | 0.531 | 1x", "|", "_____________________________________________", "_", "* FLOPs means FLOPs reduction (i.e. baseline's FLOPs / target method's FLOPs)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_none", "rebuttal_reject-criticism", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_none_label", "rebuttal_none_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_none_label", "rebuttal_none_label", "rebuttal_none_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 192, "sentences": ["We thank the reviewer for the reading and suggestions of our paper.", "Q1: The exact difference between the proposed method and the ES baseline is not as clear as it could be.", "We agree and apologize for the lack of clarity in some parts of our paper.", "We renamed all the models based on the original papers and their properties.", "We refer the reviewer to general response for further details of each baseline algorithms.", "We also improved clarity in the revised version.", "Q2: The second point is that the proposed approach seems to modify a few things from the ES baseline.", "We thank the reviewer for the insightful suggestion.", "In the latest version, to test the efficacy of each submodule of NGE, the baselines now include the algorithm with the inclusion of the pruning step, and the algorithms with AF and without AF using MLP.", "More specifically, the baselines are named:", "1. ESS-Sims", "It is the baseline algorithm without the use of AF, as use by (Sims, 1994), (Cheney, 2014) and (Taylor, 2017).", "2. ESS-Sims-AF", "The modern variant of ESS-Sims with the inclusion of AF.", "3. ESS-GM-UC", "The modern variant of ESS-Sims with the inclusion of AF and graph mutation with uncertainty (pruning).", "For this baseline, we included the pruning module on top of ESS-Sims-AF.", "Similar to the original baselines available, we performed a grid search of hyperparameters and plot the average performance of the best set of hyperparameters.", "|        NGE         | ESS-Sims  |  ESS-Sims-AF  | ESS-GM-UC | ESS-BodyShare |  RGS", "fish        |", "**70.21**", "|", "38.32      |", "51.24         |", "54.40", "|", "54.97         |", "20.96", "Walker  |", "**4157.9**  |", "1804.4    |", "2486.9        |", "2458.19     |", "2185.1       |", "1777.3", ".", "On the other hand, the use of AF can greatly affect the performance.", "The previous approach ESS-Sims can only get 38.32 / 1804 average final reward for fish and walker, respectively.", "The performance of walker is even very close to random graph search with no evolution.", "With the help of AF, the performance increases from 38.32 to 51.24 and 1804.4 to 2486.9, respectively."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_other", "rebuttal_done", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_other", "rebuttal_mitigate-criticism", "rebuttal_other", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 193, "sentences": ["We thank the reviewer for mentioning the benefits of the proposed model.", "We now clarify some of the reviewer\u2019s doubts:", "**QUESTION**", "BERT-based models in the low-resource case is not very surprising", "**ANSWER**", "While this result may not look surprising, to the best of our knowledge it was not addressed before for the specific case of BERT.", "In [1], the authors claim that a large pre-trained model can be very helpful in transfer learning scenarios, and they also suggest how to best fine-tune BERT.", "We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).", "**QUESTION**", "Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW.", "**ANSWER**", "The reviewer is correct.", "Apart from the reasons mentioned in the paper, it is possible to observe (by manual inspection) that the tweets of the HATESPEECH dataset are very noisy, short and often similar in meaning.", "This clearly helps models based on n-gram features, as we have argued in our work.", "SPOUSE and MOVIEREVIEW are different in this sense.", "SPOUSE, which is where PARCUS performs very well, contains sentences of very different nature and context, which makes it very important to focus on specific concepts (hence the use of prototypes seems appropriate).", "MOVIEREVIEW, on the other hand, contains very long reviews that need \u201cfiltering\u201d to highlight the important concepts.", "This is another context in which PARCUS can be successfully applied, as training a complex model on few data points that contain \u201clengthy\u201d sentences can be a hard task to solve."], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 194, "sentences": ["Thank you for your review!", "> Pros:", "> 1.", "The method used a latent dynamics model, which avoids reconstruction of the future images during inference.", "> 2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.", "> 3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.", "This is an accurate summary.", "We would like to highlight two additional points.", "First, the improved performance is attributed to a novel actor-critic algorithm that uses analytic multi-step gradients of predicted state-values (not Q-values).", "Second, in addition to outperforming previous latent space planning methods, the proposed algorithm also outperforms the model-free D4PG algorithm, the previous state-of-the-art on this benchmark suite.", "> 1.", "The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.", "In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.", "However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).", "Dreamer is a novel algorithm that belongs to the family of actor critic methods.", "At a high level, previous approaches can be grouped into those using Reinforce gradients with V baselines (A3C, PPO, ACER) and those using deterministic or reparameterization gradients of learned Q functions (DDPG, SAC, MVE, STEVE).", "In comparison, Dreamer uses reparameterization gradients of V functions by backpropagating the value estimates through the latent dynamics.", "Specifically, while Reinforce estimators typically learn V functions, these are only used to reduce the variance of the gradient estimate rather than directly maximizing them with respect to the actor.", "Actor-critic algorithms that use analytic gradients of Q critics differ from Dreamer in two ways.", "First, they learn a Q function rather than just a V function.", "Second, the actor only maximizes the Q value predicted for the current time step rather than maximizing multi-step value estimates.", "While MVE and STEVE learn dynamics models (from proprioceptive inputs), the dynamics are not directly used to update the policy.", "Instead, they only serve for computing multi-step Q targets for learning the Q critic.", "Thus, no gradients are backpropagated through the dynamics model for learning the actor or critic.", "Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.", "> 2.", "Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.", "As summarized above, Dreamer differs from previous actor-critic algorithms not just by using latent dynamics but also by using analytic multi-step gradients of a V function rather than one-step gradients Q function.", "This renders Dreamer conceptually distinct from DDPG, SAC, MVE, and STEVE.", "We have run experiments with MVE in the latent space of the same dynamics model and tuned the learning rate for actor and Q function.", "We did not find an improvement over Dreamer (MVE worked worse across tasks) in these experiments, possibly because it only updates the Q function at the initial state of the imagination rollout.", "Note that with a model, Q values can be computed by combining the dynamics with a value function, so learning Q is not necessary anymore.", "Since using V in Dreamer outperforms the state-of-the-art D4PG agent and is simpler than the Q function in DDPG and MVE and substantially simpler than STEVE (ensemble of models) and SAC (two Q functions, one V function), we argue for this design choice.", "> 3. [...] It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.", "We have run these experiments and it prevented learning completely.", "Using gradients of the action or value models to shape the dynamics allows them to \"cheat\".", "Specifically, the actions maximize value estimates; using these to update the dynamics results in overly optimistic dynamics.", "The values maximize Bellman consistency; using these to update the dynamics can encourage collapse of the latent space.", "As a result, we suggest the perspective of viewing the dynamics as a fixed MDP during imagination training.", "We will add a discussion of this to the paper.", "If we addressed your concerns satisfactorily, we would be happy if you would consider updating your score."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_social_label"]}
{"abstract_id": 195, "sentences": ["Thank you for your encouraging comments.", "We agree with your suggestions and we will revise our paper accordingly.", "We will also comment on the gap between our analysis and AGZ in the introduction to make it clearer, and discuss potential future work (e.g., considering approximation errors due to MCTS and the value function) in the conclusion."], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 196, "sentences": ["Thank you for your comments and suggestions.", "We will address the issues you mentioned.", "1.\tWe have added more details about sampling strategy to section 3.1 in the new version, with mathematical definition and dimensionality explicitly described.", "2.", "We did not argue the computation cost of 3D kernels in section 3.2.", "Instead, we argued that 3D kernels usually are not large enough to cover the holistic video so that Max Pooling operations are applied in most 3D CNNs to enlarge the receptive field.", "Yet this causes the loss of detailed information.", "But indeed, in order to preserve the details and increase the receptive field, simply enlarge the 3D kernels to cover the holistic video will bring enormous computation cost.", "Considering a video of size UxTxHxW, where U is number of action units, and T,H,W means temporal length, height and width of each action unit.", "In order to model the interaction of 1st frame and the (kT+1)th frame, a 3D kernel of at least (kT+1) x k x k has to be applied, which brings linear increasing of computation cost.", "Yet with our 4D kernels, a simple k x k x k x k will cover the interaction from the 1st frame to the (kT+1)th frame, because 4D convolution can go beyond space and time, making the long range interaction possible.", "For parameters, 4D kernels are k times larger than 3D kernels.", "So in order to reduce the parameters, we apply k x k x 1 x 1 kernels in most experiments, as mentioned in section 3.2 and section 4.2.", "We also propose Residual 4D Blocks to ease the optimization and preserve short-term details.", "3.", "Yes, Mini-Kinetics and Kinetics contain videos about 10 seconds.", "For Something-Something-v1, they select one frame from every 12 frames so that the original video should be around 430 frames to 860 frames, which are of about half or one minute.", "We agree that these are still too short to be called videos.", "Yet here we call our method \u201cvideo-level\u201d mainly to stress that our V4D models the holistic duration instead of a certain part.", "Additionally, we evaluated our V4D for untrimmed video classification on ActivityNet v1.3, which contains videos of 5 to 10 minutes and typically large time lapses of the videos are not related with any activity of interest.", "The very competitive result is now reported in the appendix of the second version of paper.", "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 197, "sentences": ["We thank for the reviewer for their comments on our work, and we share our responses below.", "1. Novelty: To the best of our knowledge, this is the first paper presenting a simple solution to generating useful auxiliary tasks in a self-supervised manner.", "The idea indeed was inspired by other works in auxiliary learning, but only to the extent that we also use auxiliary tasks to improve performance of a principal task.", "The method is not a heuristic; it is theoretically motivated by use of the double gradient, and inspired by the success of this in meta learning (e.g. MAML [1]).", "If the reviewer thinks our method is an incremental contribution or similar to previous algorithms, please list the specific references.", "2.", "The theoretical insight in this paper comes from the recent advancements in using a double gradient, such as in MAML [1], or understanding what makes a good auxiliary data sampler [2].", "The inner gradient is based on the standard auxiliary learning loss as proposed in other works, whereas the outer gradient uses this inner gradient to actually learn the auxiliary tasks.", "The use of an outer gradient for auxiliary learning is our key novelty, and has not been used in any works before.", "3. Feature distributions of training and meta-training data (target and auxiliary data in your language) are actually not identical.", "The \"learning to generalise\" success from our method is due to closing the *existing* distribution shift in these two datasets. If the distributions are identical, then we wouldn't have any improved generalisation from our method.", "4. Both CIFAR10 and CIFAR100 are the subsets from 80 million tiny images dataset [3].", "As described in the website and paper, all images are collected from the internet and partially labelled by humans, and thus indeed present a real-world setup rather than a synthetic setup.", "Further, we show that if a harder test set with a more variety exists (CIFAR10.1v6), out method could provide even better generalisation (Figure 4).", "Thus, we hope the reviewer could better explain why you think our algorithm could fail in real-world scenarios.", "[1] Finn et al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ICML, 2017.", "[2] Zhang et al. Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data, ECCV 2018.", "[3] http://people.csail.mit.edu/torralba/tinyimages/"], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_followup", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 198, "sentences": ["We thank the reviewer for the detailed comments.", "\u201c1.[...] without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct.", "Also, the claim in Fig. 1 that the transition from \u2018\u2019high capacity\u2019\u2019 to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand (*)\u201d", "We agree raw parameter count is not a fine estimate of the capacity of the network.", "However, an information-theoretic argument shows that an upper-bound of the capacity is the raw parameter count times the size of the representation (i.e. 32 bits for float32, this argument is close to that of [A]).", "Experimentally, we show that networks with no data-augmentation (figure 1 - purple curve) stop fitting perfectly when the parameters get within 1/10 of the quantity of information in the learning set, thus we think that raw parameter count is a good first-order approximation up to that factor.", "\u201c2. Sec. 3.3, [...] capacity alone cannot explain why VGG converges faster than Resnet-18 [...]\u201d", "We observe that the rate of memorization depends on the architecture and the optimization algorithm, but predicting or explaining this rate is beyond the scope of this paper.", "\u201c(*) 3. Scenario discussed in Sec. 4 seems somewhat impractical. [...] one might also need to figure out if it is neither train nor val\u201d", "In section 4, we do not train a classifier to distinguish between a training and a validation set.", "Rather, we use a readily-available classifier (trained for e.g. image recognition) for a completely different purpose than what it was trained for, i.e. to distinguish datasets of images (section 4.1) or detect if a set of images comes from a given set (section 4.2).", "Section 4.2 shows how to use the K-S test to detect leakage, but the same test could tell if the m-set comes from neither the train nor the validation sets.", "\u201c4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image \u2018m\u2019 corresponds to is useful or practical, as this seems to be a property of the set \u2018m\u2019 rather than the property of the trained classification model (f_\\theta). Please clarify. [...]\u201d", "Being able to tell from the classifier output (using e.g. the confidence) if a set of images comes from the training or the validation set is a good indicator of how much the network has memorized these images.", "In our opinion, the most important outcome of the experiment of section 4.1 (figure 3, right) is to determine how many samples are needed to reliably discriminate the training set from the validation set (this corresponds to the solid curves), which is related to how much the model has memorized images from the training set.", "\u201c6. Does [section 5] use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?\u201d", "The baseline approaches make use of the loss of the model (which is not the same as the confidence).", "The proposed approach uses the lower layers of the original model, and upper layers learnt on a separate, public set (this is the \u201cpartial-layers\u201d setting).", "Table 3 reports the results of the Bayes method on top of this network with upper layers retrained, as the MAT usually gives similar results on this task (for instance, Table 4 reports 60.8% performance with Softmax + Flip, Crop on Resnet101 for the Bayes method, and the MAT gets 61.14%).", "\u201c7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images\u2019\u2019 -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen\u2019\u2019 images which it labels as the negative class, thus the negative class is also seen by the memorization model.", "(*)", "\u201d", "We feed our model an equal number of positives and negatives (chosen randomly) at each epoch.", "For n < 10K, after 300 epochs the model has seen at most 3M negatives out of 15M, and yet still generalizes to the unseen negatives.", "\u201c1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization\u2019\u2019.", "In addition, the paper would also need to show that such a model does not generalize to a validation set of images. [...]\u201d", "With the downstream application of sections 4 and 5, we are interested in \u201cmemorization\u201d in the sense of any classifier that can tell apart images marked as \u201cpositives\u201d from images marked as \u201cnegatives\u201d.", "This notion is somewhat different from", "memorization as defined in other papers, where it is related to having a good training accuracy and a a validation accuracy close to random guessing.", "With the setup used in section 3, there is no good notion of validation: our model is expected to predict \u201c0\u201d on held-out data.", "\u201c2. Figure 3: [the term CDF] is confusing\u201d", "We will update the caption to make it less ambiguous."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_future", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label"]}
{"abstract_id": 199, "sentences": ["Thank you for your comments.", "We have performed additional experiments in order to address them, particularly training and evaluating HOF as in [1].", "Q1: Evaluation as in [1]", "In accordance with these recommendations in [1], we have trained HOF on the dataset provided by the authors of [1] and evaluated it according to the F1 metric that is proposed.", "We find that HOF provides competitive performance to existing methods, giving the highest average F1 score of all methods in [1].", "We will report quantitative results on this benchmark in a revised PDF submitted before the end of the discussion period.", "Q2: Input shape", "We have performed new comparison experiments to address this question.", "Varying the input shape does affect performance; sampling the surface of the 3d sphere gives worse performance than sampling the interior of the sphere (1.369 average chamfer distance for surface of sphere versus 1.247 for interior).", "In addition, we find that sampling the interior of the 4D sphere, rather than the 3D sphere gives a fairly significant improvement in performance (1.195 average chamfer distance for 4D sphere vs 1.247 for 3D).", "Since we are learning an arbitrary mapping (rather than, for example, a projection to a manifold), the mapping domain does not explicitly induce any topological constraints such as genus.", "If our mapping didn't have to be continuous, it wouldn't matter what shape we sampled.", "However, because we use neural networks with continuous activation functions (relus) to represent the mapping, it must be continuous.", "And because the experiments above indicate that the mapping domain affects the quality of the reconstructions, it is possible that the sampling domain imposes topological constraints on the set of objects.", "It's difficult to say what the \"best\" shape to sample from is; however, in future work, we would like to investigate strategies for learning the best input shape to sample from.", "Q3: Sampling the input shape", "Our input 3D points are sampled from the interior of the unit sphere, rather than the surface (we have clarified this in the revised manuscript).", "Samples are drawn uniformly at random from within the sphere in order to avoid overfitting to a particular gridding structure.", "Q4: Value of $c$ for composition networks", "In order to apply composition, the reviewer is correct that c must equal 3 (this is the case in our composition experiments).", "However, if we are not composing the reconstruction function, c could be anything greater than zero.", "For example, c=4 in our ablation experiment in which we sample input points from the 4D sphere rather than the 3D sphere.", "Q5: Clarifying values of $k$", "$k=1$", "for both HOF models in Table 1", ".", "We have edited the manuscript to clarify this point.", "The small difference in results between HOF-1 in Table 1 and HOF-1 ($k=1$) in Table 4 as well as HOF-3 in Table 1 and HOF-3 ($k=1$) in Table 4 is a matter of different initialization on a later training run.", "We have updated these tables so that the numbers are computed from the same model, rather than separate training runs.", "If we keep the number of compositions fixed while training and test with a larger value of k, we observe that the performance degrades significantly.", "On the other hand, when we use a varying number of compositions (1,2,...,k) at training time, we find that the results do generalize to higher values of k. After several additional compositions (such as k+3, k+4) however, the results start worsening similar to the trend in the fixed k evaluation.", "Finally, we have updated our pdf to reflect the additional related work and stylistic suggestion that you have brought to our attention.", "Thank you again for your feedback, and please let us know if you have any additional questions or concerns.", "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_other_label"]}
{"abstract_id": 200, "sentences": ["Thank you for your comments. Please find below our response to your questions and concerns.", "1) Technical contributions", "We are glad that the reviewer agrees that we are tackling a long standing and important problem and acknowledge the fact that neither the definition of constrained MDPs nor the application of Lagrangian relaxation to solve these problems is novel by itself.", "We should have stated our exact technical contributions more clearly and have adapted the paper to do so.", "For completeness we will list these below:", "a) We introduce pointwise, per-state constraints to learn more consistent behavior compared a single global constraint, and regress the resulting state-dependent Lagrangian multipliers using a neural network to exploit generalization across similar states.", "b) Instead of recombining the reward and cost directly on the environment side and learning a single value estimate, we train a critic network to output both return and penalty value estimates as well as the Lagrangian multipliers themselves, effectively providing more structure to the critic.", "We only combine the different terms appropriately for the actor update.", "c) We show that we can train a single, bound-conditional policy that can optimize penalty across a range of bounds and can be used to dynamically trade off reward and penalty.", "2) Comparison with the original benchmark reward", "We have extended the results on Cartpole to include the original reward as defined in the DM Control Suite (incl. bonus for low control).", "We found that compared to the original setting, our method is able to reduce the average control norm by over 50% across the entire episode, and by over 80% after the swingup phase, without significant reduction in the average return as measured without control bonus.", "3) Claims about bang-bang control in continuous RL", "The reviewer is right in that the claim of RL often leading to bang-bang control is too strongly worded.", "This is only the case when the objective function is not well-designed and one is naively optimizing for success only.", "Designing a proper objective function is however often not trivial and more of an art, requiring several iterations to achieve the desired behavior.", "This work tries to remove some of the complexities in designing such a function.", "4) State-dependent lower bound", "Defining a state-dependent bound is indeed not trivial and requires knowledge of what is feasible in the system, and as such we leave this up to future work.", "In this paper we have made the approximation that the state distribution is stationary and the discount is large enough to assume that the value is more or less constant.", "While this holds for locomotion tasks, this does not apply in e.g. the swingup phase of the cartpole task and as a result the penalty is completely ignored during this phase."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_future", "rebuttal_future", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 201, "sentences": ["Thank you very much for the constructive review.", "Summary of our response", "-------------------------------------", "We are certain that the data processing inequality is used correctly.", "As you stated, the DPI implies for any Markov chain X -> Y -> Z that I(X,Y) >= I(X,Z).", "Unlike suggested in the review, our model is defined in the form \\theta -> \\tilde{\\theta} -> D, as shown in Figure 1a.", "Following your feedback, we updated section 2.1 and 2.3 for more clarity.", "Detailed response", "-------------------------------------", "We interleave parts of the review with our detailed response for ease of reading.", "> [...]", "the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.", "As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z).", "However, based on the definition of \\theta and \\tilde{\\theta} given in the first sentence of section 2.3, the relation between \\theta, \\tilde{\\theta} and D should be: D <- \\theta -> \\tilde{\\theta} (if it is a generative model) or D -> \\theta -> \\tilde{\\theta} (if a discriminative model).", "Response: We are interested in limiting the mutual information I(\\theta, D) between our learned parameters \\theta and the dataset D.", "However, this is hard to calculate for typical deep models.", "Therefore we introduce a model that forms a Markov chain \\theta -> \\tilde{\\theta} -> D, as shown in Figure 1a.", "Hereby, \\tilde{\\theta} is a noisy version of the parameters \\theta.", "Crucially, the data D is defined to be dependent only on the noise-corrupted version \\tilde{\\theta}. By choosing a convenient noise process and prior for \\theta we can easily control I(\\tilde{\\theta}, \\theta).", "This gives us an upper bound on the mutual information I(D, \\theta) between data and parameters, according to the DPI.", "We updated section 2.3 to reflect this more clearly.", "> I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me.", "For example", "- the sentence under eq. (2)", "- the sentence \"Because the identity of the datapoint can never be learned by ...\" What is the identity of a data point?", "It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods.", "Somehow, those connections are not clear to me.", "Response: The aim of section 2.1 is to motivate limiting mutual information for the purpose of generalization.", "We link generalization problems reported in the literature to the introduced information measure.", "The information necessary to identify or distinguish between training samples is quantified by the empirical entropy, and we called it the identity of the samples.", "We updated the section to address all of your feedback."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 202, "sentences": ["Thank you for your helpful comments.", "We have improved the writing to incorporate your feedback.", "We have also performed more experiments to compare APO to manual learning rate schedules.", "Q: Please explain more how gradients w.r.t hyper-parameters are computed.", "We implemented custom versions of the optimizers we consider (SGD, RMSprop, and K-FAC) that treat the optimization hyperparameters as variables in the computation graph for an optimization step.", "We then use automatic differentiation to compute the gradient of the meta-objective with respect to the hyperparameters (e.g., the learning rate).", "Q: Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?", "Each meta-optimization step requires approximately the same amount of computation as a parameter update for the model.", "By using the default meta learning rate suggested in our updated paper, we can amortize the meta-optimization by performing 1 meta-update for every K steps of the base optimization.", "We found that K=10 works well across our settings, while reducing the computational requirements of APO to just a small fraction more than the original training procedure.", "We have added a discussion of our meta-optimization setup and the efficiency of APO in Section 5 of the updated paper.", "Q: No need to write so much decorated bounds in section 3.", "The convergence analysis is on Z, not on parameters x and hyper-parameters theta.", "So, bounds here cannot be used to explain empirical observations in Section 5.", "The convergence of the network output Z directly indicates the rate of decrease of the loss function, which is exactly what we observe in practice.", "Although the assumption of a global optimization oracle is not realistic, we believe our theoretical justification provides insight into why the method works.", "One important takeaway from the theoretical analysis is that running gradient descent on output space can potentially accelerate the optimization (since the convergence bounds have better constants).", "This directly motivates the regularization term in our meta objective to be defined as the discrepancy of network outputs instead of the network parameters, which is essential to our technique.", "Q: Could the authors compare with changing step-size?", "Thank you for the suggestion.", "We have added comparisons with custom learning rate schedules for CIFAR-10 and CIFAR-100.", "We updated our results for CIFAR-10/100 using a larger network, ResNet34, instead of the VGG11 model used in the previous version, and we used a manual learning rate decay schedule where we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training.", "We found that APO is competitive with the custom schedule, achieving similar training loss and test accuracy.", "We provide results in our response to all reviewers at the top.", "Q: How to tune lambda?", "Tuning a good lambda v.s. tuning a good step-size, which one costs more?", "We tune lambda by performing a grid search over the range {1e-1, 1e-2, 1e-3, 1e-4, 1e-5}. Because each lambda value gives rise to a learning rate schedule, tuning lambda yields significantly more value than tuning a fixed learning rate.", "Instead of trying to come up with a custom learning rate schedule, which would require deciding how frequently to decay the learning rate, and by what factor it should be decayed, all one needs to do is perform a grid search over a fixed set of lambdas to find an automated schedule that is competitive with hand-designed schedules (which are the result of years of accumulated experience in the field)."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 203, "sentences": ["(1) Multimodal setting:", "We apologize for not describing experimental settings clearly.", "In general, we believe multi-modal data is more general than simply image-text or video-text pair.", "By unifying tabular data also as multi-modal data (with each attribute as one modality), we show that VASE provides us a principled way for imputation and is capable of generalizing to more data families.", "We update additional multimodal dataset experiments in the point (3) below.", "(2) Prediction and Representation learning:", "We consider conducting these experiments during the rebuttal but none of the paper's code has been released by the authors.", "We agree deep latent variable models explicitly model the data distribution and provide a natural way for representation learning, but in our paper we evaluate the model from the perspective of imputation and generation.", "(3) Additional experiments:", "We updated additional imputation experiments on multimodal datasets (see in Appendix C.5) : CMU-MOSI/ICT-MMMO (Tsai et al. 2019), FashionMNIST/MNIST (Wu et al. 2018).", "Each dataset contains two or three modalities.", "VSAE outperforms other baselines on multimodal datasets under partially-observed setting.", "(4) Require mask during training:", "In our experiments, the binary mask is always fully-observed as is the nature of partially-observed data.", "A mask simply indicates which  modalities are observed and which are not.", "We agree that it is very interesting to design a model with partially-observed or even unobserved mask.", "However, it is beyond the scope of this work and we will consider it in future work.", "[1] Wu et al. Multimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018.", "[2] Tsai et al. Learning Factorized Multimodal Representation, ICLR 2019."], "labels": ["rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 204, "sentences": ["Thank you for your thoughtful and helpful comments.", "Following the suggestions, we added additional results for the associative recall task for many network variants.", "We also report mean and variance of losses for different seeds.", "This shows that masking improves performance on this task especially when combined with improved de-allocation, while sharpness enhancements negatively affect performance in this case.", "From the variance plots it can be seen that some seeds of DNC-M and DNC-MD converge significantly faster than plain DNC.", "In our experimental section, we added requested references to methods performing better on bAbI, and point out that our goal is not to beat SOTA on bAbI, but to exhibit and overcome drawbacks of DNC.", "Comparison to Sparse DNC is an interesting idea, and we are currently running experiments in this direction. We intend to make the results available in the near future.", "We are unable to provide a fair comparison for the lowest bAbi scores, having reported 8 seeds compared to the 20 seeds reported by Graves et al. Indeed, the high variance of DNC (Table 1) suggests that it may benefit a lot from exploring additional seeds.", "We incorporated all of the smaller notes, including a comparison to the original DNC equations in Appendix A."], "labels": ["rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 205, "sentences": ["Thank you for your thoughtful review.", "We have been hard at work to perform additional experiments to compare with other state of the art methods on a broader dataset.", "We summarize the changes here and will upload a revised the manuscript with complete quantitative evaluations before the revision deadline.", "Q1: More extensive evaluations", "In response to your comments, we have trained and tested our method on the dataset provided in [1], using the F1 score rather than Chamfer Distance in accordance with the recommendations in that work.", "This dataset contains more than 4 times as many classes as our original dataset.", "We find that HOF is competitive with the performance of various state of the art methods reported in [1], showing the highest average F1 score out of all methods compared in [1].", "See Section 4.1.2 for added discussion, and the Appendix Sections A7 and A8 for complete class performance breakdowns for both our original experiments as well as the new comparison with [1].", "We hope this extended comparison provides a more convincing experimental evaluation of HOF.", "Q2: Technical description and justification", "In the paper, we make the observation that codeword based approaches are equivalent to learning the biases of a fixed network, whereas the fast-weights-based HOF approach learns all of the weights.", "Therefore, we conclude that HOF is mathematically at least as general as codeword based architectures.", "We further show, with experiments, that the coding provided by HOF is more efficient than codeword-based approaches in terms of number of parameters in the decoder.", "There is similar evidence in the literature which suggests that fast-weights based approaches can be more efficient than static networks.", "However at this point, similar to our paper, the evidence is empirical and a theoretical justification of this phenomenon is missing.", "In response to your comments, in our concluding remarks, we mention this lack of theoretical analysis and note it as an important direction for future research", "Q3: Architecture of encoder/mapping network", "In addition to experiments on a new dataset, we have performed new evaluations of variants of HOF on our original dataset to demonstrate that HOF performs competitively even when we change the encoder architecture, decoder depth, decoder activation function, or input sampling for the decoder network.", "For example, using Resnet18 as the encoder network gives almost identical performance in terms of average chamfer distance on our original test set.", "The complete quantitative results of these comparisons will be included in an updated PDF before the end of the discussion period.", "Q4: Number of mapping layers", "Our original results reported in Table 1 compare two different mapping function architectures.", "HOF-1 has one hidden layer with 1024 units, HOF-3 has 3 hidden layers with 128 units each.", "We have updated the text to clarify this distinction.", "In response to your comments, we have also conducted an additional experiment with a mapping network with 6 hidden layers with 128 units each; the test performance of this architecture is almost identical to that of HOF-3 (1.2485 average Chamfer distance with 6 layers compared with 1.247 average CD for 3 layers).", "Q5: Vanishing/exploding gradients", "In all of our experiments, we address the problem of vanishing/exploding gradients by dividing by the square root of the in-degree of each neuron (as in [2]).", "Using the same initialization in the encoder network, we find that training a mapping function with 6 hidden layers (\"HOF-6\") trained easily with no modifications to our training code.", "Another advantage of HOF over deeper, fixed decoder architectures is that it admits extremely shallow decoders, which require less careful tuning of hyperparameters such as initialization scaling and normalization compared with deeper networks.", "For this work, our goal was not necessarily to find the optimal architecture for the decoder, but rather to demonstrate that the usage of the higher-order function paradigm allows for a much smaller decoder architecture than LVC methods.", "Thank you for bringing the additional literature to our attention.", "We have included it in our discussion of related work in a revised manuscript.", "We have also updated the text to more clearly explain the path-based evaluation and its motivation.", "We hope that these additional experiments better demonstrate the effectiveness of HOF as a competitive, parameter-efficient 3d reconstruction paradigm.", "Thank you again for your feedback.", "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019.", "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_social", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 206, "sentences": ["Thank you for the useful feedback.", "We\u2019ve updated our paper to take it into account -- we\u2019ve updated the model description and the notation in Section 4 to clarify our method.", "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "We also made several updates that address your specific questions.", "1. Are e_{i,t} and lambda_{i,t} vectors?", "Scalars?", "Abstract node notations? It is not clear in the model section.", "Also, it took me a long time to figure out that \u2018i\u2019 is used to index each entity (it is mentioned later).", "The entity and location embeddings  e_{i,t} and lambda_{i,t} are d-dimensional vectors, although we also overload the symbols to refer to abstract nodes in the model\u2019s knowledge graphs.", "In the updated manuscript we state both these facts explicitly and state much earlier that \u2018i\u2019 is the index for entities.", "2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context.", "What happens if there are multiple mentions in the text? Which one does it look at?", "When there are multiple mentions of entity i, the initial representation v_i is formed by summing the representations of each mention.", "We have updated the paper to clarify this (Sec 4.1).", "3. For the LSTM in the graph update, why does it have only one input? Shouldn\u2019t it have two inputs, one for previous hidden state and the other for input?", "Good point! We\u2019ve improved the notation used to describe the model in Section 4.", "The update equation now shows clearly that the LSTM takes in the concatenation of two node inputs (entity and location embeddings) along with the previous hidden state.", "4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k.", "This is great, but could you also report the number when the full dataset is used?", "We\u2019ve completed an experiment on the full Recipes dataset and updated the paper to describe the result (this experiment did not finish in time for the initial submission).", "The model\u2019s F1 score improves from 51.64 on the partial data to 54.27 on the full data, surpassing the previous state of the art by a more significant margin.", "5. What does it mean that in training time the model \u201cupdates\u201d the location node representation with the encoding of the correct span. Do you mean you use the encoding instead?", "We meant that we perform teacher-forcing to train the model.", "During training, we extract the context encodings for the groundtruth span and use these in downstream operations  to obtain the node representations.", "At test time, we use the MRC module\u2019s predicted span rather than the groundtruth.", "6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score?", "Is it the threshold that maximizes F1?", "For ProPara task 2, our model was optimized for micro averaged F1 on the development set.", "Tandon et al. (2018) were kind enough to provide us with their evaluation script."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 207, "sentences": ["Thank you for the comments!", "To reviewer\u2019s concerns:", "- First of all, the state of interest does not have to be the object state.", "It can be the state of the robot, for example, the state of actuators.", "Maximizing the mutual information between two sets of actuator states can help the agent to learn to control itself.", "We did a new experiment in navigation environments, where train the agent to maximize the mutual information between its left wheel states and its right wheel states.", "The agent learns to run in a straight line instead of in random directions.", "The video showing experiment results is available at https://youtu.be/l5KaYJWWu70?t=134", "- Although we evaluated our method in robotic manipulation tasks, it does not mean it won\u2019t work for other tasks.", "We added additional experiments in a new navigation task, see the video at https://youtu.be/l5KaYJWWu70?t=104", "We consider our algorithm as a general-purpose skill learning algorithm in the sense that it guides the agent to learn any skills to control the states of interests.", "The states of interest could be any states, such as the robot states, the object states, or the states of the environment.", "- The state of interest is specified by the user with little domain knowledge.", "However, when there is no clear divide from the user, the agent can learn from different combinations of the states of interest and the context states.", "In the end, the user can choose skills from the learned skill sets that are useful for the task at hand.", "- The combination of our method and DIAYN enables DIAYN to learn manipulation skills efficiently, while DIAYN alone did not learn.", "Furthermore, compared to MISC, the combined method enjoys the benefits brought by DIAYN, such as learning combinable motion primitive with skill-conditioned policy for hierarchical reinforcement learning [1].", "Reference:", "[1] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 208, "sentences": ["We thank the reviewer for the comments and feedback.", "We will also include the suggested experiment that shows the plug-and-play nature of PMN.", "1. Residual modules", "- Residual modules are small neural networks (e.g., an MLP for Mvqa, Sec. 3.4, (4)) that a task module may use when other lower level modules are incapable of providing a solution to a given query.", "For example, consider the question \u201cis this person going to be happy?\u201d on an image of a person opening a present.", "Lower level modules of Mvqa may not be sufficient to solve the question.", "Therefore, Mvqa would make use of its residual module, which would essentially learn to \u201cpick up\u201d all queries that lower level modules cannot answer.", "2. Effect of fine-tuning", "- While it might be beneficial to fine-tune the modules for a specific parent task we want each module to be an expert for their own task as it facilitates a plug-and-play architecture.", "Fine-tuning may push the modules towards blindly improving parent module\u2019s performance but (i) badly affect interpretability of inputs and outputs; and (ii) may also reduce the lower module\u2019s performance on its own task.", "Most importantly, it would not scale with the number of tasks, as for each task the agent would need to keep several fine-tuned modules of the lower tasks in memory.", "3. Feeding in the ground-truth", "- Thanks for this great suggestion.", "We performed an experiment where we evaluate the benefits that the VQA model may achieve by using ground-truth captions instead of captions generated by the caption module.", "Our preliminary experiments show a gain of about 2.0% which is a relatively high gain for VQA.", "This points to important properties of the PMN allowing human-in-the-loop type of continual learning, where a human teacher can pinpoint flaws in the reasoning process and potentially help the model to fix them."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 209, "sentences": ["Dear reviewer,", "Thank you for your comment. Before we reply to the points you have made, we would kindly ask you to please add the references that are missing from your review.", "Thank you for your review of our work.", "The following are your concerns of our work:", "a. Prior distributions of hyperparameters", "b. Loss landscape plots and relation to tunability", "c. Importance of search order", "d. Details of the calibration procedure", "We address them as follows (in two parts):", "a. Prior distributions of hyperparameters:", "We envisage an optimizer not merely as update equations, but as the conjunction of the update equations, the hyperparameters, and distributions of those hyperparameters.", "Those distributions should be prescribed by the designers of the optimizer.", "This is crucial: For example, if we take Adam with LR between $10^1$ and $10^5$ and claim that Adam is less tunable than others, the evaluation is inherently faulty, as it doesn't capture where the mode of the distribution of LRs for which Adam is expected to work.", "These prescriptions are absent for the optimizers considered in the paper.", "Therefore, we define them from either mathematical reasoning (say learning rate is non-negative, $\\beta_1, \\beta_2$ in Adam are between (0, 1) and close to 1) or using the calibration step, where we determine those distributions by fitting on the configurations that yielded reasonably good results.", "We choose simple priors for their ease of estimation, though given enough computation, arbitrarily complex priors can be computed and used.", "We fail to see the explicit relationship between our work and the papers you have referenced.", "Specifically [1] only proposes that there is an optimal batch size that is dependent on the momentum parameter.", "We do not consider tuning the batch size, as we do not consider it a hyperparameter of the optimizer itself.", "[2] shows that instead of using LR decay schedule, increasing batch size has a similar effects on training, but results in faster training.", "[3] talks about the existence of an effective learning rate as a function of learning rate and the norm of the weights, and proposes that the optimal learning rate is inversely proportional to the weight decay parameter.", "This doesn\u2019t, however, trivially lend itself to modeling priors.", "In summary, these papers show a complex interplay between the parameters giving rise to other notions, but not provide any methods to jointly model these hyperparameters.", "In the absence of such knowledge, we use our calibration procedure.", "The distributions we use are justified in section 3.2 in the paper.", "However, we accept the fact that a more complex distribution that might model the interaction between these hyperparameters might exist, and using that to sample for an HPO would be better.", "b. Loss landscape plots and relation to tunability:", "We show in figures 1.a, 1.b, as you rightly pointed out, the landscapes of loss function of the HPO objective as a function of the hypothetical hyperparameter $\\theta$. There seems to be a misunderstanding of the purpose of figures 1.a, and 1.b.: These figures do not show what we try to measure, but they merely illustrate by example what properties we would like a tunability metric to have.", "We describe this in the beginning of Section 2.", "In Section 5, we explain why existing measures of tunability are unable to make the distinction between the cases in Figure 1a.", "We would like to emphasize that the point of Figures 1a, 1b is to illustrate the necessary properties that a proposed metric for tunability - it is not our intention to create such plots for our actual experiments.", "If you are interested in these nonetheless, a very recent publication by Asi & Duchi (2019) shows the plot of lr vs performance.", "In summary, they show that the sensitivity of SGD to stepsize choices, which converges only for a small range of stepsizes.", "AdamLR exhibits better robustness when tested on CIFAR10."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 210, "sentences": ["We thank the reviewer for insightful feedback and for noting that our\u200b experiments \u200bare\u200b \u200bsolid\u200b and our setup and analyses are sound.", "The reviewer asks great questions, and we provide the answers below.", "RE: Total running time", "The reviewer raises an interesting point about total training time, which includes the time to pre-train a GNN and the time to fine-tune it on a downstream task.", "To address this point, below, we give the results of the total training time as well as the amortized total time over different downstream tasks.", "We will include detailed results and a discussion in the final version of the paper.", "We note that although pre-training does take some time, it is a one-time-effort only.", "That is, we pre-train a GNN model only once and then reuse it many times by fine-tuning the model on any number of downstream prediction tasks.", "Overall, we find that GNNs, once pre-trained, tend to converge much faster on downstream tasks.", "Most importantly, we find (details below) that validation set performance converges 5-12 times more quickly when GNNs are pre-trained.", "We emphasize that this cannot be achieved by mere training of (non-pre-trained) GNNs longer.", "The following summarizes training time for chemistry and biology datasets.", "1) Chemistry dataset (single GPU implementation)", "**", "Pre-training**", "\u2014 Self-supervised pre-training: 24 hours", "\u2014 Supervised pre-training: 11 hours", "**Fine-tuning on MUV dataset** [Time to achieve the best validation set AUC]", "\u2014 From random initialization (i.e., no pre-training):", "1 hour; 74.9% AUC", "\u2014 From a pre-trained GNN:", "5 minutes; 85.3% AUC", "2) Biology dataset", "**", "Pre-training**", "\u2014 Self-supervised pre-training:  3.8 hours", "\u2014 Supervised pre-training: 2.5 hours", "**Fine-tuning** [Time to achieve the best validation set AUC]", "\u2014 From random initialization (i.e., no pre-training):", "50 minutes; 84.8% AUC", "\u2014 From a pre-trained GNN:", "10 minutes; 88.8% AUC", "On chemistry dataset, we see that fine-tuning a pre-trained GNN on the MUV required only 5 min.", "This is in sharp contrast with training a GNN from scratch, which required 12x more time, yet it gave a worse performance.", "We can reach similar conclusions on the biology dataset.", "We thus recommend using pre-trained models whenever possible as they can give better performance and can be reused for any number of downstream tasks.", "We shall add these results and explanations to the final version of the paper.", "RE: Analysis of different pre-training strategies", "Thank you for bringing up this valuable point.", "We agree that it is important to understand why some pre-training strategies work better over others.", "Our key insight backed up with extensive empirical evidence is that a combination of graph-level and node-level methods (Figure 1) is important because it allows the model to capture both local and global semantics of graphs.", "Further, we find that our structure-based node-level methods (Context Prediction and Attribute Masking) are preferred over position-based node-level methods (Edge Prediction, Deep Graph Infomax). As future work, we plan to further investigate what graph-level and node-level methods are most useful in different domains, and understand what domain-specific knowledge has been learned by the pre-trained models."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 211, "sentences": ["We thank AnonReviewer1 for their positive comments about the interesting-ness of our proposed abductive reasoning tasks (inference and generation) and the associated benchmark dataset.", "We address specific concerns individually below:", "Discussion about e-SNLI:", "A key distinction between e-SNLI and Abductive-NLI is that the explanations in e-SNLI serve the purpose of justifying model decisions.", "In contrast, the goal of Abductive-NLI and Abductive-NLG is to select or generate explanatory hypotheses for given observations.", "Indeed, analogous to e-SNLI for SNLI, Abductive-NLI can be extended to \u201ce-Abductive-NLI\u201d by providing explanations that justify the selected hypothesis.", "Consider the following example that BERT fails to predict correctly:", "O1: Chad loves Barry Bonds.", "H1: Chad got to meet Barry Bonds online, chatting.", "H2: Chad waited after a game and met Barry.", "O2: Chad ensured that he took a picture to remember the event.", "The e-Abductive-NLI task would require models to generate an explanation for selecting H2.", "For the above example, a possible explanation for selecting H2 could be: \u201cPeople need to be physically co-located to take a picture with someone. Meeting online does not mean two people are physically co-located\u201d.", "We think generating such justifications is a great next step and hope that our work will foster such interesting future research.", "Re. somewhat limited contribution:", "We appreciate the opportunity to briefly restate our contributions and to discuss its significance.", "Abductive Commonsense Reasoning, a critical capability in human reasoning, is relatively less studied in NLP research.", "To support this line of research, our work introduces a dataset that focuses explicitly on this important reasoning capability.", "Furthermore, several recent works [1,2,3,4] have shown the presence of annotation artifacts in crowdsourced datasets -- which poses a significant challenge for dataset curation.", "Our work makes the following contributions:", "i) proposes and formalizes two novel tasks of Abductive Inference and Abductive Generation,", "ii) presents a new dataset in support of these tasks collected through careful crowdsourcing design and an adversarial filtering algorithm,", "iii) establishes strong baselines on the task proving the difficulty of the tasks and", "iv) analyses the types of commonsense reasoning that current state of the art models fall short on.", "Re. limited form of Abductive Reasoning:", "The simplifying assumptions, mentioned in the paper, allow us to i) formulate the tasks concretely and ii) curate the dataset and evaluate models viably.", "We show that in spite of the assumptions, our dataset presents significant challenges for current models.", "We totally agree that in its most general form, there should be any number of observations and models should be required to generate explanatory hypotheses in natural language (as in the alpha-NLG task).", "We hope our work will lead to this future line of research.", "Re. the title:", "Thanks for the suggestion. We will update the title to reflect that this work is aimed at language-based abductive reasoning.", "Table 7 vs Table2:", "Thanks for catching that. We\u2019ve updated the paper with the fix.", "[1] Gururangan et al. Annotation artifacts in natural language inference data.", "[2] Poliak et al. Hypothesis only baselines in natural language inference.", "[3] Tsuchiya e. al. Performance impact caused by hidden bias of training data for recognizing textual entailment.", "[4] Sakaguchi et al. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_done", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 212, "sentences": ["Thank you for the detailed review.", "We appreciate your comments on the contributions of our work and invaluable suggestions to improving our paper.", "Before delving into the details and providing a more detailed rebuttal, here are some of our initial thoughts.", "Regarding comparison with MINE on fMRI dataset, MINE could be applied to fMRI data for MI analysis.", "After all, our DEMINE-vr is simply MINE under cross-validation.", "Our first guess is that MINE will not be able to identify significant dependency in Table.1 due to large confidence intervals, i.e, the rows and cols for MINE will be all 0s.", "For segment classification, MINE-f-ES uses the same model architecture and training process as DEMINE-vr and uses hyper parameters from DEMINE-vr hyperparameter search, and will provide identical segment classification accuracy as DEMINE-vr.", "Thanks again for pointing out the typos, we'll post an updated version shortly."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 213, "sentences": ["We thank for the reviewer for their positive comments on our work, and we share our responses below.", "The purpose of our work is not to achieve state-of-the-art performance simply by incorporating the latest network architectures and optimisers.", "Instead, we provide a novel general framework for automating generalisation, and show that when used with standard classification networks across all baselines, our method performs the best.", "Furthermore, as we also explained in Reviewer #3, the hyper-parameters for defining a hierarchy is not critical, and we can choose an arbitrary hierarchy whilst still achieving better performance than baselines.", "In the future work, we would like to explore how to find the optimal hierarchy in an automatic manner, or provide an alternative solution on building a general type of auxiliary tasks (such as regression).", "However, this is the first work to present a double-gradient method for auxiliary task generation, and we believe that it is important to present the success of this initial method now given how simple and general it is, and then fine-tune other aspects in future work."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_future_label", "rebuttal_answer_label"]}
{"abstract_id": 214, "sentences": ["Thanks for your positive feedback.", "(1).", "Lemma 2.4, Point 1: The gradient in your example is indeed perpendicular to w which can be seen as follows.", "w\u2019 * \\nabla L(w) = w\u2019 * (2/w\u2019*w)(Aw - L(w)*w) =  (2/w\u2019*w)(w\u2019Aw - L(w)*(w\u2019*w)) =  (2/w\u2019*w)(w\u2019Aw - w\u2019Aw) = 0.", "In case of one variable vector, our proof is to take the derivative of c on both sides of F(w) = F(cw), which is the definition of scale-invariance.", "Then the left-hand side becomes 0 and the right-hand side becomes w\u2019 * \\nabla F(cw)", "by chain rule.", "Taking c = 1, we can conclude", "that w\u2019 * \\nabla F(w)  = 0.", "(2)", ".", "Theorem 2.5: Sorry G_t should be G_t^{(i)}. We will correct this typo in the next revision of this paper.", "For t = 0, G_t^{(i)} are all initialized to some value.", "The recursion formula for G_t^{(i)} is shown in equation (9)."], "labels": ["rebuttal_social", "rebuttal_none", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_none", "rebuttal_none", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 215, "sentences": ["Dear Reviewer,", "Thank you for your valuable comments.", "We have revised our writing in the revision, and will further improve its clarity.", "Please find our response as follows.", "- Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.", "Mitosis training can be considered as executing Algorithm 1 for multiple times with an increasing number of experts and inherited initialization from last round by changing W^e and W^g.", "Also, training with mitosis achieves similar performance as training without it shown in Appendix B, Figure (a).", "- How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?", "The hyper-parameters related to DS-softmax (such as lambda) are tuned according to the performance on a validation dataset.", "Also, as we mentioned in the paper, only one hyper-parameter (group lasso lambda) needs to be tuned.", "The heuristic we use to tune group lasso lambda is to increase lambda, starting from a small value, until it hurts the performance.", "Also threshold and balancing lambda variables are kept fixed as (0.01 and 10).", "- Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?", "In terms of baselines, SVD-softmax (NIPS\u201917) was chosen since it is a recent method that provides a significant inference speedup for softmax.", "Other alternatives, such as D-softmax and adaptive-softmax, focus on training instead of inference speedup.", "Furthermore, as claimed in their papers, they achieve limited speedup (around 5x) in language modeling, which is much worse than ours.", "With regards to Sparsely Gated MoE, it cannot speed up inference, since they select expert with full softmax.", "We would like to emphasize that most existing methods for inference speedup focus on approximating trained softmax layer, which usually suffers a loss on performance.", "Our model allows the adaptive adjustment of the softmax layer, achieves speedup through capturing the two-level overlapped hierarchy during training, which is novel and does not suffer from the performance loss."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 216, "sentences": ["Thank you very much for the comments.", "We believe that this response can help the Reviewer to be more convinced about the validness of our experiments; in particular, the validness of our retraining methodology.", "Q1. The paper is not very self-contained, and I have to constantly go back to [1] and [2] in order to read through the paper.", "In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.", "Based on the Reviewer\u2019s comments, we added more description about the schemes we adopted from [1] and [2] in Appendix A.1 and A.2 of the revised manuscript.", "Q2. The input and output types of each block in Figure 1. should be clearly stated, and the figures are almost useless because the captions contain very little information.", "We tried to add more information to the figures in the revision.", "First, in Figure 1, we added the more mathematically precise description of input and output of each block to show how the exact weight representation is changed at each process.", "We also added additional explanation for 'D' of Figure 2 in its caption.", "For the Figure 4, we added the description of the underlined numbers.", "Q3.", "Optimizing compression rates should be done on the training set with a separate development set.", "The test set should not used before the best compression scheme is selected.", "Both the results on the development set and on the test set should be reported for the validity of the experiments.", "Thanks for pointing this out.", "We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.", "We agree that optimizing compression rates should not use the test set before the best compression scheme is selected.", "In fact, in case of PTB and Wikitext-2 corpus, we already used the provided validation set and measured the test PPW only once after training (Table 2) in the original manuscript.", "From the Table 2, we can see that our proposed scheme maintains the accuracy of the uncompressed baseline network.", "On the other hand, the CIFAR-10 dataset does not include a separate validation set, so we had to use the test set in the retraining process.", "To avoid using the test set in the retraining process as the Reviewer pointed out, we randomly selected 5K validation images among the original 50K training images in CIFAR-10 dataset, and applied our scheme.", "Then, we observed the training and validation accuracy at each training epoch, and measured the test accuracy once after training.", "The accuracy results are as shown in the following table.", "Note the compression rates are the same as the data in Table 3 in the original manuscript.", "----------------------------------------------------------------------------------", "Compression scheme   Validation Error (%)    Test Error (%)", "------------------------------  ---------------------------", "---------------------", "Baseline", "11.5                         12.2", "Pruning [1]                         11.4                         12.2", "VWM (Ours)", "11.4                         12.4", "----------------------------------------------------------------------------------", "The test accuracy in the above table is about 1 % less than the accuracy which we reported in the originally submitted manuscript because the number of training data was decreased as part of the data set is used as a validation set.", "However, the results show that our proposed method does not make the network be overfitted to test data as the Reviewer doubted because the difference between the accuracy for validation set and test set are consistent with the values from the previous works.", "Note that even the uncompressed baseline network exhibits similar accuracy difference between the validation error and the test error compared with the compressed networks.", "Therefore, we believe that our proposed compression method does not suffer from the concerned overfitting problem regardless of the types of neural networks or dataset.", "Reference", "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "[2] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 217, "sentences": ["We thank the reviewer for their encouraging words and will correct the errors of expression."], "labels": ["rebuttal_by-cr"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 218, "sentences": ["Thank you to Reviewer 1 for noting the clarity of our presentation and reproducibility.", "We also appreciate the constructive criticism and thought that went into your review.", "We spent a considerable amount of time trying to fulfill the reviewer\u2019s request to match state of the art (SOTA) on PTB.", "To get SOTA on PTB, we need massive architectures, which considerable computing power and experimentation at the extreme limit of what is achievable for our team.", "Still, we pursued two directions.", "First, we tried to reimplement an architecture similar to  Melis et al. 2017.", "However, they did not publish their code, hyperparameters, or weights, requiring re-implementing and re-training from scratch.", "We tried this path, but soon realized we would not be done in time (especially with a hyperparameter search).", "We then tried to weave neuromodulation and differentiable plasticity into the architecture and code base of Merity et al., ICLR 2018 (also tied for SOTA).", "However, while they could simply leverage existing PyTorch implementations of LSTMs (written in extremely fast C++), we had to re-implement LSTMs \u201cby hand\u201d (i.e. as a series of connected layers) in PyTorch to introduce plasticity and neuromodulation.", "As a result, our networks thus ran considerably slower, by more than 10x (not because our method is intrinsically slower, but just for lack of engineering optimizations on our bespoke Python implementations; we confirmed this by observing that a similar \u201chand-built\u201d reimplementation of simple, non-plastic LSTMs ran similarly slower, while producing results identical to Merity et al.).", "These experiments are thus unfortunately still running.", "For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.", "That said, we still believe the results in the current paper demonstrate the benefits of our techniques on a sizable model, and thus it would benefit the community to allow people to know about, and build upon, these new methods and results.", "The purpose of the present paper is to introduce a novel technique and show that it can produce an advantage in realistic settings, which we believe our PTB task confirms.", "Our claim is that, all other things being equal (especially the number of parameters), a neuromodulated plastic LSTM outperformed a standard LSTM on this particular benchmark task.", "We do **not** want to claim that our results are anywhere near SOTA.", "We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).", "Additionally, philosophically, If SOTA results are the bar for all papers to be accepted into conferences like ICLR, then those venues will be the exclusive domain of those with either the computation or time (i.e. large-scale resources) to dedicate to such results.", "In that case, many cutting edge ideas will by necessity be excluded from the discussion, as will many research groups.", "Moreover, insisting on papers to be SOTA to be accepted also likely encourages p-hacking and shoddy science to game the results (even if unintentionally), reducing the quality of science our community tries to build on.", "Re: \"Parameters of the model\": All trainable parameters of the Hebbian synapses (alpha and w in Equation 1, plus the neuromodulation parameters) are included in this parameter count.", "To equalize the number of parameters across architectures, we reduce the number of hidden units in the plastic models in comparison to the non-plastic baseline.", "We have clarified this in the text.", "Re: \"Attention\": Non-trainable, homogenous plasticity can indeed be compared to a form of attention, i.e. \u201cattending to the recent past\u201d in the words of Ba et al. 2016.", "However, differentiable plasticity allows for the plasticity of each connection to be trained; as a result, different connections play different roles and it is not at all clear that the analogy with attention remains relevant (see e.g. the clever mechanisms automatically implemented by the trained plasticity connections in the image completion experiment of the Differentiable Plasticity paper, Miconi et al. 2018, sections 4.3 and S.3, which can hardly be described as simply \u201cattention\u201d)", "Re: \"Style (font)\": We used the template and do not see the discrepancy. Can you clarify? We are happy to fix it."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 219, "sentences": ["Thank you very much for the highly constructive review.", "> I think this is a very interesting direction, but the present paper is somewhat unclear. In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have \"training algorithms that are exactly equivalent.\" I think this example needs to be clarified.", "We realized that the naming was very confusing and consequently, we renamed \\tilde\\theta to \\tilde\\mu in the noise-injected model.", "Now,", "- the original, noise-free model p has the structure \\theta -> D (no bottleneck) while", "- the adapted, noise-injected model p\u2019 has the structure \\mu -> \\tilde\\mu -> D (containing a bottleneck).", "Hereby, \\tilde\\mu is a noise-corrupted version of the new parameters \\mu, and we obtain a limit on the mutual information between \\mu and D. We simplified Figure 2 and 8 to make this more clear.", "To better characterize Gaussian mean field inference on the original model, we aim to find an inference procedure on p\u2019 so that both algorithms result in exactly the same outcome, e. g. the same calculations are executed when running the corresponding program.", "We show that there is such an inference procedure on the noisy model, and it has the character of MAP.", "Note that only if generative and inference model are adapted simultaneously we end up with equivalence.", "Hereby, \\mu (the mean of the Gaussian q) and \\theta (the original parameter in p) correspond to \\mu (the MAP point-mass of q\u2019) and \\tilde\\mu (the noise-injected version of \\mu in p\u2019).", "> Many of the parameters here are also unclear and not properly defined/introduced.", "What is the relationship between \\theta and \\tilde\\theta exactly?", "In this example, \\theta and \\tilde\\theta never appear in the same model (they are part of p and p\u2019, respectively).", "We realized that this is confusing and have therefore renamed \\tilde\\theta to \\tilde\\mu.", "> In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?", "This is an excellent question.", "In fact, we believe that trying to construct noise-free deep models with a specific mutual information of data and parameters for the purpose of generalization would be an interesting research direction.", "Due to nonlinearities in typical deep models, it is at least not obvious how to calculate the mutual information between data and parameters.", "The main challenge here would certainly be to come up with an effective estimator.", "Relatedly, one would have to design priors and architecture to achieve a specific mutual information.", "> The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper [...]] and further exploration is desirable.", "This paper is giving an information-theoretic perspective on existing variational inference methods.", "Such a perspective is interesting, but needs to be further developed and explained.", "Specifically, how can mutual information in this context be formally linked to generalization/overfitting?", "We updated section 2.2 to relate to the references you mentioned.", "They explore the link of limiting mutual information and generalization error mostly in theory (and in particular for adaptive analysis).", "In contrast, we deploy this principle in a practical model structure that is easily applicable to many existing deep and variational learning approaches and provide empirical evidence of the validity of our framework.", ">", "Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.", "As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.", "We want to emphasize that we do use the standard definition of mutual information.", "Therefore, the bottleneck implied by Eq. 5 is purely a property of the generative model and not influenced by the approximate inference distribution q.", "Eq. 2 is only introduced to provide additional motivation for our approach as it allows to characterize overfitting in variational inference.", "The guarantee derived in section 2.2 ties this quantity back to the mutual information from Eq. 5."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 220, "sentences": ["Q1: In Theorem 4.3, the result holds for any $k$ and $M$. The authors claim that if we take a limit of $M \\to \\infty$ with fixed $k$, the practical dynamics converges to the discrete-time mean-field limit, in Section 4.", "However, to state the result of Theorem 4.3, $k$ should be bigger than $M c_\\eta$ from the dentition of $\\tilde{\\rho}_k^M$, as shown under the equation (4).", "How do we take a limit of $M \\to\\infty$ ? Does k also go $\\infty$?", "A1: Thanks for pointing this out.", "The result of this theorem holds uniformly for any $k$ (not a fixed $k$).", "Besides, we do not require $k$ bigger than $M c_\\eta$ in the definition of $\\tilde{\\rho}_k^M$. When $k$ is no more than $M c_\\eta$, $\\tilde{\\rho}_k^M$ and $\\rho_k^M$ are stochastic processes with same distribution and thus the Wasserstein distance between them is 0.", "And for any $k$ is greater than $M c_\\eta$, we have the uniform bound (w.r.t. $k$) as stated in the theorem 4.3.", "We are sorry for not stating this clearly in the theorem and we have revisited the present of the theorem. We will fix this issue in the next revision.", "We also point out that, as our system is complicated, in taking the limit of $M\\to\\infty$, we need to ensure that the number of iteration we run is larger than $Mc_\\eta$. To be specific, the asymptotic convergence would be", "$$\\lim_{k,M \\to\\infty, \\eta \\to 0^+} \\mathbb{D}_{\\text{BL}} (\\rho_k, \\rho^*)=0$$", "where the joint limit of k and M requires that $k\\eta\\to\\infty$; $\\exp(C\\alpha^{2}k\\eta)\\eta^{2}=o(1)$; $(k\\eta)/(Mc)=q(1+o(1))$ with $q>1$. Here if $q \\leq 1$, we degenerate to Langevin. But when $q>1$ (intuitively that means, when $M$ is large, the number of iterations we run is larger), our dynamics is different from Langevin, which is what we do in the practice.", "Also, we would like to remark that this seemingly strange things is in fact the \u2018artifact\u2019 caused by the using of Langevin dynamics at beginning to obtain the $M$ initial samples when we designed the practical implementation of the proposed methods.", "However, it is not really necessary to use Langevin dynamics to get $M$ initial samples, as we can simply using some other initialization distribution and get the $M$ initial samples from that distribution (and by this setting, our dynamics is simply the second phases in Eq (3)).", "All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.", "Q2: Regarding other minor comments", "A2: Thanks for your notification! We will polish our paper and rewrite the corresponding part in the next revision."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 221, "sentences": ["Thank you for your comments. Please find below our response to your questions and concerns.", "1) Pseudocode", "We apologise that the optimization procedure was unclear.", "We have added pseudocode of the general optimization procedure in Appendix A.", "2) Hyperparameter selection", "The reviewer is completely right that we are removing one hyperparameter by introducing another.", "However, there are two reasons why this might still be beneficial: one is that the penalty coefficient is now effectively dynamic and can change during training, ensuring higher chances of finding a good solution.", "Second, by elevating the hyperparameter one level up, we hope that the learning is indeed less sensitive to its specific setting.", "Indeed, we found in practice that we get similar results for \\beta within some orders of magnitude, which requires significantly less tuning compared to a fixed \\alpha.", "3) Relation to safe reinforcement learning", "It is indeed the case that constrained MDPs are often considered in safe RL.", "In those cases there is generally an upper bound on a penalty function that should never be exceeded, including during training itself.", "These algorithms generally restrict policy updates to remain within the constraint-satisfying regime.", "While our approach can similarly be applied to upper bounds on penalties, there\u2019s unfortunately no guarantee that the constraints will be satisfied at every moment during training, but only at convergence.", "As such it is not clear how these methods would apply to our specific experimental setups."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 222, "sentences": ["1. I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful. If that is true, I would suggest the authors to make this hidden assumption clearer in the paper", "Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.", "2. The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment.", "The idea has a cross-disciplinary nature and is fairly interesting to me.", "I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works.", "Response: Thanks for recognizing the strengths of the paper. We will add the appropriate references regarding the \"differential testing\" concept in software engineering.", "3. One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them.", "However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.", "Response: Thanks for pointing it out.", "We agree with the reviewer that images falling into Case III can be used to distinguish the associated two classifiers using the proposed semantic tree distance.", "We will revise the writing to make it more rigorous.", "In our current subjective assessment environment, we choose to stop labeling images in Case III because it is difficult for humans to select one among 200 classes, especially when they are unfamiliar with the class ontology."], "labels": ["rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 223, "sentences": ["We would like to thank the reviewer for the time and useful feedback.", "Our response is given below.", "- Relationship to z-conditioning strategy in BigGAN.", "Thanks for pointing out the connection to this concurrent submission.", "We will discuss the connections in the related work section.", "The main differences are as follows:", "1. BigGAN performs conditional generation, whilst we primarily focus on unconditional generation.", "BigGAN splits the latent vector z and concatenates it with the label embedding, whereas we transform z using a small MLP per layer, which is arguably more powerful.", "In the conditional case, we apply both additive and multiplicative interaction between the label and z, instead of concatenation as in BigGAN.", "2. Overall BigGAN focusses on scalability to demonstrate that one can train an impressive model for conditional generation.", "Instead, we focus on a single idea, and show that it can be applied very broadly.", "We provide a thorough empirical evaluation across critical design decisions in GANs and demonstrate that it is a robust and practically useful contribution.", "- Propagation of signal and ResNets.", "Indeed, ResNets provide a skip connection which helps signal propagation.", "Arguably, self-modulation has a similar effect.", "However, there are critical differences in these mechanisms which may explain the benefits of self-modulation in a resnet architecture:", "1. Self-modulation applies a channel-wise additive and multiplicative operation to each layer.", "In contrast, residual connections perform only an element-wise addition in the same spatial locality.", "As a result, channel-wise modulation allows trainable re-weighting of all feature maps, which is not the case for classic residual connections.", "2. The ResNet skip-connection is either an identity function or a learnable 1x1 convolution, both of which are linear.", "In self-modulation, the connection from z to each layer is a learnable non-linear function (MLP).", "- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?", "Yes, we notice more improvements on the harder, more diverse datasets.", "These datasets also have more headroom for improvement."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 224, "sentences": ["Thank you for your helpful comments!", "1. You are right that Frank-Wolfe would be advantageous over PGD when the constraints are more complicated and adversarial attack may not be such a case.", "Yet it is also well-known that Frank-Wolfe has quite different optimization behavior compared with PGD even though they have the same order of convergence rate.", "Therefore, it is interesting and important to examine the performance of Frank-Wolfe algorithm for adversarial attack, given the fact that PGD has been shown to be a very effective for adversarial attack.", "In fact, from our work, we found that Frank-Wolfe based methods are generally more efficient than PGD method.", "From another perspective, Frank-Wolfe solves the problem by calling Linear Minimization Oracle (LMO) over the constraint set at each iteration.", "This LMO shares the same intuition as FGSM, which also tries to linearize the neural network loss function to find the adversarial examples.", "In this sense, it is a quite natural attempt to revisit FGSM under the Frank-Wolfe framework.", "2. We are sorry maybe we didn\u2019t explain it very well in the paper, but this is a misunderstanding.", "We indeed compared our method with generalized I-FGSM/BIM, which is exactly the same as PGD (In [Madry et al.] they also mentioned this in Section 2.1 and they refer it as FGSM^k).", "We decide to just call it PGD in the revision to avoid confusion.", "We hope this remove your concern.", "3", ".", "Indeed, theoretically we can only prove for $\\lambda$ = 1 case.", "Yet we found that larger \\lambda brings us more speedup.", "We have added further empirical evidence (performance comparison of our method with different \\lambda in Figure 1 in the revised paper) to justify it.", "Intuitively speaking, using lambda>1 is essentially a \u201crelax and tighten\u201d step by first relax the constraint to make the problem easier, and then tighten it back to the real constraint.", "The \u201crelax and tighten\u201d idea has been widely used in constrained optimization, and in this paper we adapted this idea into Frank-Wolfe algorithm to make it even faster.", "4. [Lacoste-Julien 2016] considered the general first-order Frank-Wolfe algorithm for nonconvex smooth optimization.", "The result of Theorem 4.3 in our paper is almost the same as the result in (Lacoste-Julien 2016), except that the choices the learning rate in these two papers are different though.", "We have made it clear in the revision.", "5. We have added detailed hyperparameter settings for CW and EAD in the revision in the supplemental materials.", "6. While Theorem 4.7 is new and may be of independent interest in the optimization community,  it is not the main contribution in this paper.", "We would like to emphasize that our major contribution in this paper is a Frank-Wolfe based algorithm for adversarial attack, which is more efficient than PGD based adversarial attack algorithm and other baselines.", "7. Sorry about the confusion.", "$y$ should be replace by $y_{tar}$. It is a simplified notation we mentioned in the proof in the appendix.", "Thank you for your suggestion and we have revised the notation $f(x,y_{tar})$ to $f(x)$.", "8. Thank you for pointing out several typos. We have fixed all of them in the revision."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_summary", "rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 225, "sentences": ["Thanks for your review.", "The ION theorem is an important part of explaining sparsity, dead units, and rank as well, but perhaps our writing was not clear enough.", "We will work on the writing in the future version of this work.", "As for the mutual information related comment (#2), the results that you have mentioned are well known from information bottleneck paper or from the following information invariance paper.", "Alessandro Achille and Stefano Soatto.", "Emergence of invariance and disentangling in deep representations.", "Journal of Machine Learning Research. 2018"], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 226, "sentences": ["We thank reviewer 2 for the detailed feedback.", "We are glad that the reviewer found the VAE-GAN model to be a natural extension for the problem and that our work provides a good baseline for future work.", "We address the individual questions below.", "We changed Section 3.1 to explain that the posterior dependence on pairs of adjacent frames is to have temporally local latent variables that capture the ambiguity for only that transition, a sensible choice when using i.i.d. Gaussian priors.", "Another choice is to use temporally correlated latent variables, which would require a stronger prior (e.g. as in Denton & Fergus (2018)).", "For simplicity, we opted for the former.", "The blurriness in a VAE can indeed be attributable to a weak inference model.", "Note that our VAE variant and both SVG variants are able to predict sharp robot arms in the BAIR dataset, but often blur out the small objects being pushed.", "We tried recurrent posteriors and learned priors with our models, and the results were similar.", "We are now running additional experiments with a deeper encoder and with more filters.", "Although in principle a strong inference model could produce sharper images, an alternative approach is to use better losses, which is the approach we chose in this work.", "It is an interesting suggestion to experiment with the effect of the hyperparameters on the trade-off between realism and diversity.", "We are currently running experiments for various weightings of the KL loss and the adversarial loss, and we plan to include results that illustrate the trade-offs based on these hyperparameters.", "We also plan to include results on the trade-offs between accuracy and realism.", "In fact, a recent result [1] proves that this is a fundamental trade-off for all problems with inherent ambiguity.", "The statement that \u201cGANs prioritize matching joint distributions of pixels over per-pixel reconstruction\" is a criticism of per-pixel losses, and not of VAEs in general. We clarified in the introduction that VAEs can indeed model joint distributions of pixels.", "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 227, "sentences": ["Thanks for pointing out the pros and cons of our method.", "We address your concerns as follows:", "Q1. \u201cThe search space of the proposed method, such as the number of operations in the convolution block, is limited.\u201d", "A1: First, the size of search space is not determined by the number of operations but the number of connections.", "The search space of our method is different from exiting NAS methods in that the number of input of certain operation is not limited.", "Second, the search space without block share is even much larger than existing NAS methods.", "Third, we can trivially extend our DSO-NAS to accommodate more operations such as dilated conv like our ongoing experiments on PASCAL VOC semantic segmentation task, we extend our search space to accommodate 3x3 and 5x5 separable convolution with dilated = 2.", "The following table shows the performance of our model on the PASCAL VOC 2012 semantic segmentation task, where DSO-NAS-cls represents the architecture searched on ImageNet with block structure sharing and DSO-NAS-seg represents the architecture searched on PASCAL VOC segmentation task.", "---------------------------------------------------------------------------------------------------------------------------", "Architecture", "mIOU                     Params(M)", "FLOPS(B)", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-cls", "72.1", "6.5                               13.0", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-seg(more operations)", "72.7                            6.7                               13.2", "---------------------------------------------------------------------------------------------------------------------------", "We combine DSO-NAS with Deeplab v3 and search for the architecture of feature extractor with block sharing.", "All above models have been pre-trained on ImageNet classification task first.", "It\u2019s notable that the architecture searched on semantic segmentation task with additional operations achieve better performance in our preliminary experiment, indicating that our DSO-NAS is capable to incorporate additional operations.", "We will present the full experiments of semantic segmentation in the future revision.", "Q2: \u201cThe technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.\u201d", "A2: Please refer to Q1.", "Moreover, we never claim the main contribution of our work lies in augmenting the search space.", "And in fact, most existing NAS papers share the same architecture search space, the main differences between them is the search strategy.", "We believe that judging the novelty of a NAS paper solely by its architecture space is unfair."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 228, "sentences": ["Thank you for your constructive comments!", "1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.", "2. We would like to argue that constrained optimization based formulation itself is not designed to achieve better distortion compared with regularized optimization based formulation.", "So there is no surprise that our algorithm\u2019s distortion is not the best.", "On the other hand, as mentioned by the other reviewer, distortion is usually not that essential in adversarial attacks as long as it is maintained in a reasonable range.", "We could actually remove the distortion column, instead, we chose to include it just to show that we did not trade a lot of distortions (to make problem much easier) and thus gains speedup.", "From our experimental results, you can see that our proposed method achieves significant speedup while keeping the distortion around the same level as the best baselines.", "3. Thank you for your suggestion.", "We have further added success rate vs queries plot (for black-box case) and loss vs iterations plot (for white-box case) in the revision.", "As you can see, in terms of number of iterations / queries, our method still outperforms the other baselines by a large margin.", "4. Thank you for your suggestion.", "We have further added experiments on ResNet V2 model and averaging over 500 correctly classified pictures to strengthen our result.", "Again, this additional experiments show that our method outperforms the other baselines for both white-box attack and black-box attack.", "5. Regarding poor time complexity in practice, first, as you mentioned, adversarial training currently is quite slow due to the slow adversarial attack steps.", "Better time complexity of adversarial attack could significantly speed up adversarial training algorithms.", "Second, it is worth noting that the running time complexity of adversarial attack also highly depends on the input size.", "For example, if you attack a CIFAR-10 classifier or an MNIST classifier, it could take only seconds per attack even for the slowest algorithm since the input size is only 32 by 32 (or 28 by 28).", "However, if you attack a ImageNet classifier or even higher dimensional data classifier, it could take significantly longer time (minutes).", "That is why reducing the runtime of adversarial attack is very important.", "6. We apologize for this confusion.", "Regarding \u201cgradient-based\u201d / \u201coptimization based\u201d methods and coordinate-wise black-box attacks, we have changed our description to avoid confusion.", "Thank you for pointing it out."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 229, "sentences": ["Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "Here we respond to your specific comments.", "\"What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \"", ">>> In few-shot learning, episodic paradigm proposed by Matching Networks [1] is widely adopted by current researchers (we follow the same setting to make a fair comparison).", "In each episode, a small subset of N-way K-shot Q-query examples is sampled from the training set.", "Typically, for 1-shot experiments, N=5, K=1, Q=15 and for 5-shot experiments, N=5, K=5, Q=15", ".", "Thus, the number of training examples are Nx(K+Q) (80 for 1-shot and 100 for 5-shot)", ".", "Constructing label propagation matrix W involves both support and query examples (80 or 100).", "So the dimension of W is either 80x80 or 100x100.", "Running label propagation on such small matrix is quite efficient.", "\"It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\"", ">>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.", "After we get the per-example feature representation f_{\\varphi}(x_i) for x_i, we feed it into the graph construction module g_{\\phi}. The output of this module is a one-dimensional scalar.", "f and g are learned in an end-to-end way in our approach.", "\"solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \"", ">>> We want to answer this question from two aspects.", "On one hand, few-shot learning assumes that training examples in each class are quite small (only 1 or 5).", "In this situation, Eq (3) and the closed-form version can be efficiently solved, since the dimension of S is only 80x80 or 100x100.", "On the other hand, there is plenty of prior work on the scalability and efficiency of label propagation, such as [2], [3], [4], which can extend our work to large-scale data.", "On miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps.", "This is slightly worse than closed-form version (53.75/69.43), because of the inaccurate computation and unstable gradients caused by multiple step iterations.", "[1] Vinyals, Oriol, et al. \"Matching networks for one shot learning.\" NIPS. 2016.", "[2] Liang, De-Ming, and Yu-Feng Li. \"Lightweight Label Propagation for Large-Scale Network Data.\" IJCAI. 2018.", "[3] Fujiwara, Yasuhiro, and Go Irie. \"Efficient label propagation.\" ICML. 2014.", "[4] Weston, Jason. \"Large-Scale Semi-Supervised Learning.\""], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 230, "sentences": ["We thank the reviewer for their positive evaluation of our work.", "In response to their suggestion under Point 4, we have added the suggested references to sections 2.3 (Related Work) and 4 (Discussion and Conclusion)", "New text:", "\u201cSimilarly, Wah et al. (2014) show a series of adaptive displays for an anchor c_i, where the subject must partition the queries c_j, c_l, \u2026 into a set of similar and a set of dissimilar queries.", "In contrast to our work, the aforementioned studies did not use sparsity or positivity constraints, nor did they intend to evaluate the interpretability of the embedding.\u201d", "New text:", "\u201cYet another possible extension is to consider different types of similarity judgments (Veit et al. 2017), e.g. resulting from asking subjects to group objects based on a specific attribute (size, color, etc.).\u201d"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 231, "sentences": ["Thank you for the detailed review.", "We appreciate your comments on the contributions of our work and the nature of our approach, as well as suggestion of experiments and paper writing.", "Before delving in and providing more details, we have some initial thoughts about the theoretical issues you brought up.", "Regarding theorem 1 and the sample complexity of MINE, we also had discussions on why we think they are comparable or not and discussed that on page 5 in our submission.", "The tldr is that the MINE sample complexity can not only be seen as 1) bounding best achievable MI estimation but also as 2) bounding distance from estimation to a proven MI lower bound.", "The former is a quite vacuous bound on generalization and would require advances in learning theory to improve, not MI estimation.", "Our theorem 1 is trying to improve the latter to enable practical applications.", "Improving the former to the level of practical use is a noble goal, let us know when you have an answer.", "Regarding \"false detection\" experiments.", "We really appreciate that you brought up this point.", "Our synthetic experiments on Gaussians rho=0.0 in Figure 1 do exactly this.", "Results show that MINE-f and MINE-f-ES estimates very much non-zero MI when there should have been 0 MI.", "MINE-f bar is not visible due to overshooting out of the chart.", "DEMINE approaches give estimations closer to 0.", "We often get questions about why our estimators give MI numbers lower than MINE and why are we claiming that our estimator is better.", "But in fact that's exactly because MINE gives false detection but our estimators provably don't.", "Hyper-parameter search example.", "Say we are given 3000 paired (x,y) observations.", "First divide into 1500 train, 1500 test.", "Take 1500 train and run Algorithm 1 using 3-fold crossval: use 1000 for (x,y)train and 500 for (x,y)val in each run (3 runs total).", "Get MI estimation m1,m2,m3 over 3 folds.", "Compute confidence interval v using Eq.8 using the hyperparameters and 1500 as test set size.", "Hyperparameter search DEMINE-vr maximizes mean([m1,m2,m3])-2std([m1,m2,m3]).", "DEMINE-sig maximizes mean([m1,m2,m3])-v.", "Will try to make it more clear.", "Regarding fMRI experiments, our focus is on demonstrating neural MI estimation and dependency test on fMRI data.", "We compare with pearson's correlation because it's another widely used technique that can perform both correlation analysis as well as significance test.", "We used a simple 1D CNN where convolution happens over the time dimension, not the spatial dimensions.", "Better architectures, e.g. transformers over time + graph networks over space could improve performance, but not our focus and we leave that to future work.", "Higher-order and nonlinear covariance tests may make very appealing comparisons and we are looking into it.", "A first impression is that our technique is more general and will probably give looser bounds, but may be applicable to a wider range of problems not only ones that have specifically that type of covariance, just like DEMINE vs Pearson's correlation.", "But at the same time we also have questions on how much additional insight it brings, as it's not an apples to apples comparison, so neither tight or loose estimations diminish the value of both types of approaches."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 232, "sentences": ["Thank you for your helpful comments.", "We have addressed your concern about the baseline models and learning rate schedules in our updated paper.", "Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.", "We have updated the baselines in our paper for CIFAR-10 and CIFAR-100, using a larger, modern network, ResNet34, in place of the VGG11 model used previously.", "We also compared APO to manual learning rate decay schedules.", "For CIFAR-10/100, we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training.", "The ResNet34 with a custom learning rate decay schedule achieves 93-94% test accuracy on CIFAR-10 and ~74% test accuracy on CIFAR-100.", "We believe that this is a strong baseline, and shows the applicability of APO in practical settings.", "The final test accuracies of the updated model using SGD/SGDm with and without APO are:", "| CIFAR-10 | CIFAR-100 |", "--------------------------+--------------+---------------+", "SGD (fixed lr)", "92.97            72.69", "SGDm (fixed lr)", "92.77            72.53", "SGD (decayed lr)", "93.29            73.45", "SGDm (decayed lr)", "93.53            73.80", "SGD-APO", "93.82            74.65", "SGDm-APO", "94.59            73.89", "The test accuracies using RMSprop and K-FAC with APO are shown in our response to all reviewers at the top.", "The results in these tables show that APO is competitive with manual schedules in terms of test accuracy.", "The updated figures in our paper show that APO is competitive with manual schedules both in terms of test accuracy and training loss.", "Q: Does the hyperparameter lambda itself benefit from some scheduling?", "In our updated paper we show that APO with a fixed lambda achieves comparable performance to manual learning rate decay schedules.", "While using a schedule for lambda can potentially further improve performance, a simple grid search over fixed lambda values already leads to strong performance, and has the advantage that it is easy to use in practice.", "Q: You mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it.", "I guess you mean", "w.r.t. the original RMSprop.", "Yes, we intended to say that on Rosenbrock, RMSprop-APO converges quickly compared to baseline RMSprop; we have updated the paper to clarify this.", "Thank you for your helpful feedback. We have incorporated your suggestions into the updated paper.", "Specifically, we have:", "* Updated the baseline model for CIFAR-10/100 from VGG11 to ResNet34.", "* Used manual learning rate decay schedules for the CIFAR-10/100 baselines.", "We obtained 93-94% test accuracy on CIFAR-10 (SGD/SGDm/RMSprop/K-FAC) and 73-74% test accuracy on CIFAR-100 (SGD/SGDm).", "All are compared to their APO variants, which performed as well or better.", "The final results are shown in the table in the response to all reviewers at the top.", "* Shown that APO is competitive with manual schedules both in terms of test accuracy and training loss with ResNet34.", "This demonstrates the practical applicability of APO for contemporary networks.", "* Updated Figure 2 on CIFAR-10 with SGD/SGDm/RMSprop, Figure 4 on CIFAR-100 with SGD/SGDm, and Figure 6 on CIFAR-10 with SGD.", "We also added Figure 3 on CIFAR-10 with K-FAC.", "Each figure compares the baseline optimizers with their APO variants.", "Thank you for having helped us improve the paper."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_social_label"]}
{"abstract_id": 233, "sentences": ["Thanks for the review.", "@Wall-clock: We don\u2019t quite understand the question.", "As mentioned in the response to Reviewer 3, our NLP example does answer the natural question about end-to-end gains. Is the reviewer only concerned with the location of the plots?", "- Another note: to perform a full wall-clock comparison with algorithms that have different per-iteration costs, one must disentangle and retune various hyperparameter choices, most notably the learning rate schedule.", "Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one.", "@L-BFGS: On a high level, we agree that GGT develops a similar window-based approximation to the gradient Gram matrix as L-BFGS does to the approximated Hessian.", "While adaptive methods have proven effective in practice, quasi-Newton algorithms are not in general regarded as competitive for deep learning (despite recent efforts [1,2]), and that\u2019s why it is not compared to in the vast majority of deep learning papers.", "- Quasi-Newton methods are suited for deterministic problems, while stochasticity is crucial in deep learning.", "This is because they try to approximate the Hessian by finite differences, which seems unstable with stochastic gradients in practice.", "- Direct second-order methods require significant modifications to converge in the non-convex setting (see [3,4]).", "Even these have not been observed to work well in deep learning.", "- One reason for the practical success of AdaGrad-like algorithms we believe is the difference of  -1/2 vs. -1 power on the Gram matrix, which seems to change the training dynamics dramatically.", "With the gradient Gram matrix and a -1 power, meaningful end-to-end advances have only been claimed for niche tasks other than classification.", "[1] Stochastic L-BFGS: Improved Convergence Rates and Practical Acceleration Strategies.", "R. Zhao and W. Haskell and V. Tan. arXiv, 2017.", "[2] A Stochastic Quasi-Newton Method for Large-Scale Optimization. R. Byrd, S. Hansen, and J. Nocedal, and Y. Singer SIAM Journal on Optimization, 2016.", "[3] Accelerated methods for nonconvex optimization.", "Y. Carmon, J. Duchi, O. Hinder, A. Sidford. SIAM Journal on Optimization, 2018.", "[4] Finding approximate local minima faster than gradient descent.", "N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and T. Ma. STOC 2017."], "labels": ["rebuttal_social", "rebuttal_refute-question", "rebuttal_followup", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 234, "sentences": ["We appreciate AnonReviewer3 for encouraging comments about the importance of the proposed abductive inference and generation tasks and about the value of our proposed dataset.", "We address the main concerns individually below:", "Adversarially filtering using BERT and GPT gives deep learning models a disadvantage:", "While BERT originally achieved high performance on the originally collected dataset, several recent studies [1][2][3][4] have found the presence of annotation artifacts in crowdsourced data that inadvertently leak information about the target label.", "This subsequently leads to overestimation of the performance of AI systems on end tasks.", "Our adversarial filtering (AF) algorithm aims to address the problem of overestimation of performance.", "In spite of targeting GPT/BERT during AF, human performance on the AF resulting dataset is still high.", "The significant gap between human and BERT performance leaves scope for inventing new methods for abductive reasoning.", "Ensemble of BERT models:", "An ensemble of three BERT models achieves an accuracy of 68.9%, very close to a single model 68.6%.", "Average score of human:", "The average score of human annotations is 89.4%.", "This is directly comparable with BERT-Ft [Fully Connected] model\u2019s performance of 68.6% in Table 1.", "Re. Ground Truth:", "The ground truth is assigned based on whether a hypothesis was collected during the plausible (Appendix A1 Task1) or implausible (Appendix A1 Task2) phase of the data collection procedure.", "To measure human performance, we had three annotators select the correct hypothesis and measured human performance as the accuracy of their majority-vote.", "Please let us know if this answers your question. If not, could you please clarify your question?", "Generative task vs classification:", "We completely agree.", "While the generative task is more general and much more interesting, the challenge of evaluating generations is significant, particularly for this task.", "This is due to the fact that there could be multiple distinct plausible explanations for a given pair of hypothesis.", "Consider the following example:", "O1: Kelly and her friend wanted to take a train to the city.", "O2: They had to wait for another one.", "Plausible explanations:", "1. They read the timetable incorrectly and arrived at the station just after a train had left.", "2. The train was full.", "Both explanations are plausible, and explain the observations, but automated evaluation metrics are not reliable enough to capture this phenomenon based on their reliance on surface level similarities.", "To simultaneously make progress on the novel abductive reasoning task and due to the ease of evaluation, we additionally introduce a discriminative version of the task.", "Nonetheless, we agree that in its most general form, there could be any number of observations and models should be required to generate explanatory hypotheses in natural language (alpha-NLG task).", "[1] Gururangan et al. Annotation artifacts in natural language inference data.", "[2] Poliak et al. Hypothesis only baselines in natural language inference.", "[3] Tsuchiya et al. Performance impact caused by hidden bias of training data for recognizing textual entailment.", "[4] Sakaguchi et al. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_followup", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 235, "sentences": ["(1) Reconstruction from prior during training:", "The crux of the proposed model is the selective proposal distribution.", "\"Pseudo\" sampling for unobserved modalities during training provides a way to facilitate model training process.", "We evaluated the model under two training settings: (I) optimize the final ELBO without conditional log-likelihood for unobserved modalities x_u; and (II) optimize the final ELBO with  conditional log-likelihood of unobserved modalities.", "This is realized by utilizing the \"pseudo\" sampling described before (and in the paper).", "The results are comparable but the added term in setting II shows benefits on some datasets.", "While setting I is solely based on the observed modalities, the setting II incorporates the unobserved modalities along with the observed ones.", "By using the complete data, the setting II describes the complete ELBO corresponding to the partially observed multimodal data (in consideration).", "(2) Comparison with VAEAC:", "In order to establish fair comparison, we used the same backbone network structures and training criteria for all baseline models and our proposed VSAE.", "Therefore, the implementation details differ from the original VAEAC paper.", "We did our best to maintain the optimization details described in all baseline papers.", "Experiments on VAEAC with partially-observed data are also conducted.", "Results show that VAEAC under this setting can achieve comparable performance on categorical datasets: 0.245(0.002) on Phishing, 0.399(0.011) on Mushroom while the errors of VSAE are 0.237(0.001) on Phishing,  0.396(0.008) on Mushroom.", "However, on numerical and bimodal datasets, partially trained VAEAC performs worse than VSAE :", "*VSAE:", "0.455(0.003) on Yeast; 1.312(0.021) on Glass;0.1376(0.0002) on MNIST+MNIST; 0.1198(0.0001) on MNIST+SVHN;", "*VAEAC trained partially:", "0.878(0.006) on Yeast; 1.846(0.037) on Glass;0.1402(0.0001) on MNIST+MNIST; 0.2126(0.0031) on MNIST+SVHN.", "(3) Experiments under synthetic non-MCAR masking:", "As mentioned by the reviewer, we conduct experiments on non-MCAR masking following state-of-the-art non-MCAR model MIWAE [2].", "Same as MIWAE, we synthesize masks by defining some rules to specify the probability of a Bernoulli distribution.", "Please refer to Table 3 and Appendix C.4 for updated comparison results.", "VSAE outperforms MIWAE under all MCAR, MAR and NMAR masking mechanisms.", "(4) Baselines:", "All baselines considered in the paper are designed to have comparable number of parameters (same or larger than our model) to make the comparison fair.", "We have updated the baseline details in the Appendix B.3.", "Although GAN-based models show promising imputation results, they usually fail to model data distribution properly.", "Therefore, we do not consider them as our baseline models.", "It is also important to note that VSAE is not a model designed only for imputation, but a generic framework to learn from partially-observed data for both imputation and generation.", "(5) Conditional imputation:", "When performing imputation, we assume that the generation is not conditioned on the observed image, but only conditioned on the factorized latent variables.", "Input an observed image to the model, we observe a \"conditional\" distribution if we independently sample from the latent variables.", "See Figure.7 in updated Appendix C.2.", "(6) Answers to the questions:", "1. Please refer to point (2) for detailed explanation on comparison with VAEAC.", "In summary, there are multiple reasons why the performance is not identical with the original VAEAC: (I) the back-bone structures are not the same; (II) training criteria (including batch size, learning rate, etc.) are not the same; and (III)  training/validation/test split is different.", "We would like to emphasize that the aforementioned changes are necessary to establish fair comparison.", "2.", "We adopt the calculation from [1] where NRMSE is RMSE normalized by the standard deviation of each feature followed by an average over all imputed features.", "The standard deviation of ground truth features does not guarantee NRMSE < 1.", "[1] Ivanov et al.Variational Autoencoder with Arbitrary Conditioning, ICLR 2019", "[2] Mattei et al. MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets, ICML 2019"], "labels": ["rebuttal_structuring", "rebuttal_refute-question", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 236, "sentences": ["Thank you for the review, and we really appreciate your suggestions!", "In the revision, we have added analysis in section 4.2 and section 5 on how the learned causal matrix can be used downstream, for example in RL/IL and interpretability of neural nets.", "In the discussion in section 5, we also analyze how the error may affect the tasks downstream.", "We are excited that various tasks may utilize or incorporate our algorithm, and benefit from the causal inference ability it enables.", "We have also added comparison with sparse learning/feature selection methods in the \u201crelated works\u201d section.", "In particular, we note that L1 and group L1 regularization is dependent on the model structure change and rescaling of input variables, while our learnable noise risk is invariant to both, making it suitable for causal discovery where the scale of data may span orders of magnitude and the model structure may vary."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 237, "sentences": ["1. Thank you for your suggestion, we have addressed this in the revision as you suggested. This is indeed a good motivation.", "2. Thank you for your suggestion.", "We have further added experiments using even stronger query limit (previously 500000, now 50000) for the additional experiments on ResNet V2 model in the supplemental material. (We did not choose to use smaller epsilon because first, we already used a quite standard choice of epsilon, second, as you said, going for extremely small distortion does not really mean anything in adversarial context.) As you can see, in this even harder setting our proposed algorithm still maintain a performance lead over other baselines.", "Also, we have revised the statement in the abstract as you suggested.", "3. You are right, it is a quite weak attack and we have removed it from the table (just mention it in the text).", "4. Yes, we could just remove the distortion column in our result.", "We choose to include it because we do not want others to think that we actually trade a lot of distortions (to make problem easy) for speedup in runtime.", "5. We have added further empirical evidence to show that in the revision.", "From an intuitive perspective, using lambda>1 is essentially a \u201crelax and tighten\u201d step by first relax the constraint to make the problem easier, and then tighten it back to the real constraint.", "The \u201crelax and tighten\u201d idea has been widely used in constrained optimization, and we adapted this idea to Frank-Wolfe algorithm to make it even faster.", "6. As mentioned in an anonymous comment, there is one paper which proposed a similar but different zeroth-order non-convex FW algorithm as well as convergence rate analysis ahead of us.", "We were not aware of this paper when we prepared our ICLR submission, since it was posted only ten days before the ICLR deadline.", "We have cited this paper and modify our claim correspondingly in the revision.", "Nevertheless, it does not affect the main contribution of our paper: a novel Frank-Wolfe based adversarial attack framework for both white-box and black-box attacks, which is much more efficient than existing white-box/black-box adversarial attacks in both query complexity and runtime.", "7. Thank you for your suggestion and we have explicitly written down the update for a better comparison in the supplemental materials (Section A) in the revision.", "8. It means it is invariant to an affine transformation of the constraint set, i.e., if we choose to re-parameterize of the constraint with some linear or affine transformation M, the original and the new optimization problem will looks the same to the Frank-Wolfe algorithm.", "Please refer to [Jaggi (2013)], [Lacoste-Julien (2016)] for more details.", "9. In white-box setting, we perform grid search / binary search for parameter epsilon (or c for CW) for all algorithms.", "This will lead to better/ closer distortions for all methods.", "In black-box setting, we care more about query complexity and thus did not perform the grid search/binary search steps to avoid extra queries in finding the best epsilon/lambda.", "10. Thank you for pointing these typos out, we have addressed it in the revision."], "labels": ["rebuttal_done", "rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 238, "sentences": ["Thank you for your review.", "We agree that further investigation is needed for mutual information, and we are currently working on it.", "As for the layer to investigate, we have presented the higher layer results because the representation regularizers showed the most improvements when applied to the higher (or even output) layer.", "We believe the representations in the lower layers are inherently less structured and therefore representation shaping can be harmful.", "The layer dependency is further explained in the following article.", "Daeyoung Choi and Wonjong Rhee, Utilizing class information for deep network representation shaping, AAAI 2019   (https://arxiv.org/abs/1809.09307)", ">> The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?", "Response: In general, representation regularizers showed better performance than the others.", "Among the representation regularizers, cw-VR and L1R frequently achieved the best performance.", "Nonetheless, we were not able to identify any specific task condition that makes a specific regularizer consistently best performing regularizer."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 239, "sentences": ["We thank you for the  detailed and thoughtful review.", "We are glad that you found the paper well-contextualized and the presentation high-quality. Here we discuss some of your comments.", "R2: \"this 'finishes' [Pathak et al., ICML17] to its logical conclusion for game-based environments and should spur interesting conversations and further research. In terms of actual technical contributions, I believe much less significant.\"", "=> In the light of the comments on originality and significance, we would like to highlight our finding that random features perform quite well and at times as well as learned features across many environments.", "This is a novel contribution since prior works have relied on learned features as a crucial requirement for good performance [Pathak et. al. ICML17].", "We believe this investigation would allow random features to be seen as an easily reproducible and strong baseline for future investigations of feature learning in exploration.", "Indeed, since the release of our paper, there has been some follow-ups on using random features for exploration in achieving state of the art results on hard exploration games when combined with extrinsic reward (in the interest of preserving anonymity, we don't include the references here).", "R2: \"However, it isn't entirely clear if the primary contribution is showing that 'curiosity reward' is a potentially promising approach or if game environments aren't particularly good testbeds for practical RL algorithms\"", "=> We believe that both are valuable insofar as generating discussion within the community and leading to follow-up experimentation.", "In particular, we hope our paper stimulates both, an interest in trying out more realistic/stochastic environments, *and* further research on curiosity as a potential useful reward.", "In addition to that, we have shown that curiosity could be a very strong baseline to compare against in future papers.", "All these, we argue, are valuable to the progress and health of the field.", "R2: \"Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method. However, as I said previously, this is probably a discussion worth having given the popularity and visibility of game-based testbeds\"", "=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.", "Our vivid demonstration of this issue in the maze environment has already inspired some recent papers to look into, in particular, by incentivizing episodic reachability (in the interest of preserving anonymity, we don't include references to these, but we will include them in the final version of the paper).", "R2: \"I think that more casual ML/RL researchers will find these results controversial and surprising while more experienced researchers will see curiosity-driven learning to be explainable primarily by the intuition...\"", "R2: \"Even the 'focused experiments' can be explained with the intuitive narrative that in the state/action space\"", "=> Indeed in our experience, although a few people were not surprised, most of them were very surprised at the agents being able to make progress without any any extrinsic rewards.", "This suggests that the game designers (similar to architects, urban planners, gardeners, etc.) are purposefully setting up curricula to guide agents through the task by curiosity alone [Lazzaro, 2004].", "R2: \"consider [Mirowski et al., ICLR17] with respect to auxiliary loss + RL extrinsic rewards to improve performance\"", "R2: \"RL + auxiliary loss isn't evaluated in detail\"", "=> We will add a discussion of recent works that deal with navigation tasks in maze environments [Mirowski et. al. ICLR 2017, Jaderberg et. al. ICLR 2017] in the related works section.", "In contrast to these works, we don't assume privileged access to the maze environment in the form of depth estimation or loop closure supervision.", "Auxiliary tasks are an important component of RL and exploration methods, however, in this work we chose to focus on the most generic setting with minimal assumptions about the environment: providing raw observations in response to actions.", "In environments with privileged access we expect auxiliary tasks to benefit both curiosity-driven and extrinsic-reward-driven RL methods."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 240, "sentences": ["We thank Reviewer 3 (R3) for their review and for clearly articulating their concerns regarding the paper.", "In our response below, we will clarify the design and results of our experiments as well as argue why we believe that these results should be of interest and are not, indeed, that predictable.", "R3 asked why training performance of many models is 100% when they do not generalize and suggested us to perform a large number of training runs to see if occasionally the right solution is found.", "First, we agree that from the point of view of training there are many equally good solutions, and in fact, this is the main and the only challenge of SQOOP.", "We designed the task with the goal of testing which models are more likely to converge to the right solution, with which they can handle all possible combinations of objects, despite being trained only on a small subset of objects.", "We argued extensively in the introduction that such an ability to find the systematic solution despite other alternatives being available is highly desirable for language understanding approaches.", "We fully agree with R3 that in investigations of whether or not a particular model converges to the right solution repeating every experiment several times is absolutely necessary, and we would like to emphasize that we did repeat each experiment 3, 5, or 10 times (see \u201cdetails\u201d in Table 1 and the paragraph \u201cParametrization Induction\u201d on page 8).", "In most cases we saw a consistent success or consistent failure, one exception being the parametrization induction results, where 4 out of 10 runs were successful (see Table 4, row 1 for the mean and the confidence interval).", "We hope that 3 takes this fact into account, and we will furthermore improve on the current level of rigor in the upcoming revision by repeating each experiment at least 5 times.", "We are not sure if we fully understand the question \u201cCould you somehow test for if a given trained model will show systematic generalization?\u201d that R3 asked.", "We test the systematic generalization of a model by evaluating it on all SQOOP questions that were not present in the training set.", "We hope that this answers the question of R3 and we would be happy to engage in a further discussion regarding this and make edits to the paper if necessary.", "We thank R3 for the suggestion to investigate the influence of model size and regularization on systematic generalization.", "It is indeed a very appropriate question in the  context of our study, however, we note that there exists a wide variety of regularization methods and trying them all (and all their combinations) would be infeasible.", "In the upcoming update of the paper we will report results of an on-going ablation study for the MAC model, in which we vary the module size, the number of modules and experiment with weight decay.", "We would welcome any other specific experiment requests R3 may have.", "Finally, we would like to discuss the significance of our investigation and its results.", "While we agree that the results that we report may not shock the reader (although perhaps hindsight bias plays a role in what people find surprising or not after reading an article) we find them highly interesting and not at all easily predictable.", "Reading prior work on visual reasoning may lead a researcher to conclude, roughly speaking, that NMNs are a lost cause, since a variety of generic models perform comparably or better.", "In contrast, our rigorous investigation highlights their strong generalization capabilities and relates them to the specific design of NMNs.", "Notably, chain-structured NMNs were used in the literature prior to this work (e.g. in the model of Jonshon et al multiple filter_...[...] modules are often chained), so the fact that tree-structured NMNs show much stronger generalization was not obvious prior to this investigation and should be of a high interest to the research community.", "Last but not least, an important part of our investigation (which the review does not discuss) is the systematic generalization analysis of popular end-to-end NMN versions, that shows how making NMNs more end-to-end makes them more susceptible to finding spurious solutions.", "As we argued in our conclusion, these findings should be of a highest importance to researchers working on end-to-end NMNs, which is a very popular research direction nowadays.", "We conclude our response by announcing that an updated version of the paper, that among others incorporates valuable suggestions by R3, will soon be uploaded to OpenReview.", "We are currently performing a lot of additional experiments, the results of which will make our investigation even more rigorous and complete.", "We sincerely hope that R3 takes into account the arguments we have made here and the new results that we will publish soon and reevaluates our paper more positively.", "Dear Reviewer 3,", "We thank you again for your informative review that you wrote before the revision period.", "In our response and the revised version of the paper we tried our best to address your concerns.", "We would highly appreciate to get some feedback from you regarding the changes that we have made and the arguments that we have presented.", "In particular, we report that NMN-Chains (with a lot of inductive bias built-in and also used in prior work such as Johnson et al. 2017) generalize poorly compared to even generic modules, and that layout/parameterization induction often fails to converge to the correct solution.", "We believe both these findings are quite surprising.", "We also report new experiments with the MAC model, including a hyperparameter search, a comparison against end-to-end NMNs, and a qualitative exploration of the failure modes of this model.", "All these experiments are repeated at least 5 times each, like you suggested in your review, although it\u2019s worth noting that results the original version of the paper also reported results after  multiple runs.", "We would highly appreciate a response on our newest revision and suggestions on how it could be improved. If you still think that paper is uninteresting or not well executed, could you then suggest what specifically it is lacking?", "We are sincerely hoping to hear from you."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_social", "rebuttal_social", "rebuttal_reject-request", "rebuttal_by-cr", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_followup", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 241, "sentences": ["We thank the reviewer for acknowledging the novelty of our work and for noting that our experiments are thorough.", "Thank you for pointing out a related preprint by Z. Hu et al. [arXiv:1905.13728].", "We note the work by Z. Hu et al. was developed independently and concurrently to our work here, and we were not aware of it at the time of writing our paper.", "We shall cite the preprint and include a discussion in our paper.", "Briefly, the key difference between our work and that of Hu et al. is that Hu et al. consider a more restrictive setting where graphs are completely unlabeled (i.e., graphs have no node features).", "Hu et al. then focus on extracting generic graph properties of unlabeled graphs by pre-training on randomly-generated graphs.", "While the approach is interesting, the limitation of such an approach is that it improves performance only marginally over ordinary supervised classification of the original attributed graphs.", "This is because it is hard for random unlabeled graphs to capture domain-specific knowledge that is useful for a specific application.", "Moreover, in practice, graphs tend to have labels together with rich node and edge attributes, but Hu et al.\u2019s approach cannot naturally leverage such attribute information, which then results in limited gains.", "In principle, we could compare our approach against Hu et al., however, right now, this would be extremely challenging because of the following reasons.", "(1) We cannot find a public implementation of Hu et al.\u2019s approach for reliable comparison.", "(2) Reimplementing their method requires knowledge of many specific implementational details and design choices (feature extraction, graph generation, etc.), which are not discussed in their preprint.", "(3) Finally, their pre-trained GNN operates on unlabeled graphs, and so it cannot be directly applied to our datasets of labeled graphs.", "Lastly, in contrast to Hu et al., our work focuses on important real-world domains, where one wants to pre-train GNNs by utilizing the abundant graph, node, and edge attributes.", "Importantly, our approach is able to learn a domain-specific data distribution that is useful for downstream prediction.", "We demonstrate on two application domains that such practical settings (i.e., labeled graphs with naturally-given node and edge attributes) are very important to consider and that our pre-training can substantially improve model performance."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 242, "sentences": ["Thanks for your valuable comments.", "It helps us to prepare the revision.", "We address all your concerns in the revision as below.", "Q1: Was the auxiliary tower used during the training of the shared weights W?", "A1: Auxiliary tower is used only in the retraining stage.", "Q2: \u201cDid the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule?\u201d", "A2:", "CIFAR: In the pretrain stage and search stage, the learning rate is fixed to 0.1 with batch size 128; In the retraining stage, we use cosine learning rate schedule.", "ImageNet: In the pretrain stage and search stage, the learning rate is fixed to 0.1 with batch 224; In the retraining stage, we use linear decay learning rate schedule.", "Q3: \u201cFigure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?\u201d", "A3: In the revision, we replace the Figure 4 with a new version which has more details.", "As show in Figure 4, all the operators in level 4 are pruned.", "Q4: \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d", "A4: The non-smooth regularization introduced by l1 regularization makes traditional stochastic SGD failed to yield sparse results.", "If we need exact zero, we have to use heuristic thresholding on the \\lambda learned, which has already been demonstrated in SSS [1] that is inferior.", "Besides, traditional APG method is not friendly for deep learning as extra forward-backward computation is required, also as shown by SSS.", "Q5: \u201cMissed citation: MnasNet also incorporates the cost of architectures in their search process. On ImageNet, your performance is similar to theirs. I think this will be a good comparison.\u201d", "A5: We have added the result of MnasNet [2] in Table 2.", "Indeed, MnasNet achieves similar results with us with less FLOPs.", "However, it is also need to note that MnasNet evaluates more than 8K models, which introduces much higher search cost than our method.", "Moreover, the design space of MnasNet is significant different from other existing NAS methods including ours.", "It is interesting to explore the combination of MnasNet with ours in the future work.", "Q6: \u201cThe paper has some grammatical errors.\u201d", "A6: We have fixed the typos and grammatical errors in the revision.", "Q7: About \u201cfirst NAS algorithm to perform direct search on ImageNet\u201d", "A7: We check this claim again and find methods like MnasNet [2] and one-shot architecture search [3] also have the ability to perform direct search on ImageNet, we have delete this claim in the paper.", "However, to the best of our knowledge, our method is the first method to perform directly search without block structure sharing.", "We also report preliminary results that directly search on task beyond classification (semantic segmentation).", "Please refer to Q1 of Reviewer3 for details.", "[1] Data-Driven Sparse Structure Selection for Deep Neural Networks. ECCV 2018.", "[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf", "[3] Understanding and simplifying one-shot architecture search. ICML 2018."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 243, "sentences": ["Thank you for pointing out the other datasets in algebraic word reasoning.", "We\u2019ve included these in an expanded discussion of related work with discussion on how they relate to the current dataset.", "Please let us know if we have missed other papers.", "Your proposal of combining multiple extant problem sets is a good idea.", "We\u2019d want to ensure the combined datasets have a common format (e.g., the same unambiguous freeform text format for reasons of transferability, etc as argued in the paper), and there are interesting problem types occurring in other datasets (such as logical entailment or boolean satisfiability) that we haven\u2019t yet included.", "We may in the future extend the dataset to include these other problem types if the current ones become solved, and of course we solicit contributions (in the form of generation code) to the dataset.", "We likely could not use workbooks etc as a source for problems without significant investment, since obtaining legal permission to redistribute copyrighted problems found in these books would probably be hard and/or expensive.", "Having said that, it is definitely important to ensure the problems remain grounded in real-life problems (thus our small list of real-life exam questions).", "This was the motivation for testing trained models against \u201creal life\u201d questions occurring in school-level examinations; these questions are not intended to be a primary benchmark (with more questions and detailed grades), but rather simply a rough indication of whether training models to answer school-level questions could be achievable.", "On the distribution of the sampled answer (and the related question of how difficulty levels are determined), these are great questions.", "For some modules with two output choices (e.g., True, False), we can simply split the answers 50-50.", "But in general, the answer distribution depends on the module, with hand-tuning to ensure the (question, answer) pair is of a reasonable difficulty level as judged by humans.", "In more detail: as mentioned in the paper, we want to achieve upper bounds on the maximum probability that any single (question, answer) is sampled; thus if we sample the answer from a set of N possible answers, then to achieve a maximum probability p of a given question, the remaining choices made in generating the question must", "be from a set of size p/N. We roughly aim to pick N (depending on p) so that conditioned on this, the question is as easy as possible; there is typically a hand-tuned sweet spot.", "On evaluating general-purpose models only, we may have phrased this badly in the paper, and have updated it.", "We are definitely interested in any models that learns to do mathematics and symbolic reasoning, which would include more sophisticated models tailored towards doing mathematics (one could imagine models with working memory, etc).", "However, we discount models that already have their mathematics knowledge inbuilt rather than learnt (for example, this includes many of the models that occur in algebraic reasoning tasks, where the model learns to map the input text to an existing equation template, that is then solved by a fixed calculator).", "We test DNC (differentiable neural computers) and RMC (relational memory core) models, which arguably are more specialized for doing mathematics, since they have a slot-based memory that may be appropriate for storing intermediate results.", "However these models obtained worse performance than the more general architectures, and we are not yet aware of models that are more tailored for doing mathematics that do not simply have their mathematics knowledge built-in and unlearnable; we hope the dataset will spur the development of new models along these lines.", "On the number of thinking steps, in our earlier analysis we trained up to 150k steps (compared with 500k for final performance reported in paper), and observed the following interpolation test performances by number of steps: 39% (0 steps), 46% (1 step), 48% (2), 49% (4), 50% (8), 51% (16).", "We are re-running experiments now to confirm the final performances, which we can include in the final paper."], "labels": ["rebuttal_structuring", "rebuttal_done", "rebuttal_social", "rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 244, "sentences": ["Thank you very much for your encouraging review and helpful comments.", "We will make revisions to address the several points you have raised in your review.", "Below we first address the main concerns.", "Q1: \u201cAlt-az\u201d rotation is not a group.", "A1: Thank you for pointing this out.", "You are correct.", "The Alt-az rotation, according to our definition, is not a group.", "SO(3)  is a group which can be parametrized by a 3-sphere .", "But when we reduce one parameter from it, it is not a group anymore mathematically; the composition of two alt-az rotations becomes a general rotation in SO(3).", "In the new revision, we will use the term alt-az rotation in \u201cquotient SO(3)/SO(2)\u201d  instead of alt-az rotation group.", "Moreover, the quotient SO(3)/SO(2) is isomorphic to $S^2$ and to avoid the ill-definition on the two poles (the two degenerate points), we will add a constraint to the alt-az rotation, i.e. $\\phi=0, if \\theta=0 or \\theta=\\pi$. This is because, when the altitude rotation is zero or PI, the azimuth rotation is meaningless in a alt-az rotation and is therefore fixed as zero.", "If $\\theta=0 or \\theta=pi, and \u201c\\phi \\neq 0$, this rotation belongs to the azimuthal rotation in SO(2) group.", "(Q2) Equivariance property of the Alt-az convolution", "We think we can still have the equivariance property but only for single alt-az rotation.", "Notice the definition of alt-az convolution do not use any composite rotation.", "Here is our tentative proof:", "Under the definition of alt-azimuth anisotropic convolution and using the unitary property (5) of rotation operators, we have (assume the number of channels K=1 for simplicity, assume Q and R be both alt-az rotations):", "************************************************", "\\begin{equation}", "\\begin{aligned}", "& (h \\star D_{Q} f) (R)", "\\\\", "&", "= \\int_{S^2}(D_Rh)(\\hat{u})f(Q^{-1}\\hat{u})ds(\\hat{u}) \\\\", "&", "=\\int_{S^2}h(R^{-1}\\hat{u})f(Q^{-1}\\hat{u})ds(\\hat{u}) \\\\", "&", "=\\int_{S^2}h(R^{-1}Q\\hat{u})f(\\hat{u})ds(\\hat{u}) \\\\", "&", "=\\int_{S^2}h((Q^{-1}R)^{-1}\\hat{u})f(\\hat{u})ds(\\hat{u}) \\\\", "&", "=(h \\star f)(Q^{-1}R) = D_{Q}( h \\star f)(R)", "\\end{aligned}", "\\end{equation}", "**************************************************", "This means that for a single alt-az rotation of input spherical image; the output of a convolution layer will rotate in the same way.", "Although the property doesn\u2019t hold if one performs multiple alt-az rotations to the input spherical image, it is still valuable because we assume the different SO(3) orientation of an input 3D shape is from a composite of an azimuthal rotation and an alt-az rotation, the azimuthal rotation is treated by data augmentation and the single alt-az rotation is treated by the network equivariance and invariance.", "Q3: alt-az convolution is not well defined on the south pole", "A3: Yes, we agree that our original definition of alt-az convolution is not well defined on both north and south poles.", "Therefore, in the new revision, we will add the constraints to the definition of alt-az rotation and make it one-to-one corresponds to the set points on $S^2$. See A1.", "Q4: The paragraph motivating the alt-az convolution on page 4 is not very clear.", "A4: Thanks for the comments, as you suggested, we will rewrite this paragraph in the new version, and acknowledge the importance and effectiveness of the recent work on the group equivariance and rotation invariant networks.", "Q5: It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2)", "A5: The two related papers (Cohen et al 2018 for general SO3 and Esteves, and Esteves et al 2018 for isotropic S2) both use lat-lon grid and Fourier domain convolution, while ours uses a icosahedron-sphere grid and direct spherical domain convolution.", "The use of different sampling in the input spherical image, and the use of filters are totally different.", "We think a direct comparison should be done in one of the following ways : (a) perform the three types of spherical convolution all using icosahedron-sphere grid and then convolve in the spherical domain.", "(b) perform the three types of spherical convolution all using lat-lon grid and convolve in the Fourier domain.", "For the first type of direct comparison, to implement isotropic spherical convolution (Type II), we should make the geodesic disc filter share an identical weight along the angular direction.", "To implement a general SO(3) spherical convolution, we should add a rotation degree of freedom into our disc filter.", "We are conducting this experiment and if the time and paper page limit are allowed, we will report the comparison result in the revised version.", "Otherwise, we will put it into our future work.", "For the second type of direct comparison, we need to conduct alt-az spherical convolution in the Fourier domain, this is possible by determining the spherical harmonic coefficient, $<g_0, Y_l^m> $ for the alt-az convolution in terms of the spherical harmonic coefficient of input spherical signal $f$ and the filter $h$. This comparison needs re-designing of our network and we can not finish it within the rebuttal period, we\u2019ll leave it for future work.", "Q6: Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1.", "I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned.", "Some more explanation / discussion would be good.", "A6: Theoretically, our method will be rotation invariant with only AZ rotation, it will be full rotation invariant with SO(2) rotation augmentation about an arbitrary axis .", "In table I, we believe the reason alt-az augmentation performs better because it contains more training data.", "SO(3) augmentation underperforms the AZ augmentation because several random SO(3) rotation augmentation might not be able to cover all the relative rotation wrt the filter's orientation (see appendix).", "Q7: It would be nice to explain the spherical parameterization in more detail.", "Is this operation itself rotation equivariant?", "A7: Due to the page limit of the conference paper, we could not explain the spherical parameterization method in detail.", "This operation is theoretically rotation equivariant.", "Spherical parameterization establishes a map that transforms the points of a closed surface into the points on the unit sphere.", "A good spherical mapping for a closed surface should satisfy the following properties:bijective mapping and least distortion.", "Bijective mapping is the most important but most difficult in this process which implies that the resulting map is one-to-one, fold-free, and therefore feature preserving (information lossless).", "Least distortion seeks a good sampling rate such that interesting features of the model receive enough real estate on the sphere in order to be accurately sampled.", "We achieved the bijective mapping by adapting a coarse-to-fine strategy with minor modifications (See http://hhoppe.com/proj/sphereparam/).", "The minimizing of the map distortion is obtained using the authalic parameterization proposed in Sinha et al 2016.", "This process is rotation equivariant because the initial bijective mapping is depends on the object orientation and the authalic remeshing does not change the orientation of the spherical embeddings.", "Spherical parameterization is a good way to retain geometric and topological information of original shapes (compared to the spherical projection method), but currently it works only for genus-0 closed object, extending it to 3d shapes with arbitrary topology is still an unsolved problem, that is why we could not adapt this method for dataset such as ModelNET and Shrec\u201917, they contain 3D objects with arbitrary topology.", "(Q8) Other typos and minor issues", "We will correct all the typos and other minor issues in the revised paper, thank you again for the detailed review. WE really appreciate your help."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_other", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_other", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 245, "sentences": ["We appreciate the reviewer 1 for his/her feedback on our paper.", "The reviewer mentioned: \u201cIn general it is very unlikely that you will be able to choose every variation of out-distribution cases\u201d:", "Actually, for training A-CNN (Augmented CNN), we did not train it on every variation of out-distribution cases, rather, we recognize a single representative out-distribution set among the available ones according to our measurement.", "Then using it for training A-CNN with the aim of effectively controlling over-generalization.", "The reviewer mentioned: \u201c Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax\u201d:", "We would be appreciated if the reviewer could provide us with the references that showing using only sigmoid could control such a challenging problem of adversaries.", "Please note we did not aim to devise a method that is able to reject all adversaries.", "Rather, we attempted to show that a CNN with less over-generalization is able to reject some of the adversaries while correctly classifies many of the remainder, particularly non-transferable attacks."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 246, "sentences": ["Thanks for the review.", "@Update overhead: We argue that per-iteration performance is a worthwhile objective in itself, which is less significant in some scenarios (e.g. costly function evaluation, like in RL, or expensive backprops, like in RNNs).", "That said, we were indeed not able to demonstrate end-to-end gains in vision.", "Please note that in the NLP benchmark our algorithm finds a better solution and wins in wall-clock time.", "@Switching: This is a good suggestion, and we indeed do cite one of the papers attempting to approach optimizer-switching in a principled way.", "We found that we could squeeze out some wall-clock gains by applying the expensive update more sparingly, but the value of including this in the paper was unclear (effectively adding a host of hyperparameters orthogonal to the central idea)."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_other", "rebuttal_done", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 247, "sentences": ["Overall:", "We thank you for your valuable suggestions in helping us avoid potential inefficiencies in our work, and suggesting ways to avoid misunderstandings.", "We have incorporated your comments to significantly improve our work, and hope our revised draft is able to convince you towards a favorable outcome.", "Concern 1: Concerns with title \u201cMeta Domain Adaptation\u201d", "\u201c\u2026unlike as advertised, the paper does not address", "\u2026 \u201c", "It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.", "We acknowledge this problem and agree with you about a possible misinterpretation.", "However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.", "We also do think that the problem setting we have proposed is an important problem that deserves attention, and has not been studied in the meta-learning paradigm.", "We are glad that you also agree that setting makes sense (\u201c... the combination \u2026 is fair\u201d).", "Overall, we think that we have made an important contribution to Meta-Learning literature, by identifying its limitation for few-shot learning under domain shift, and proposed a solution to tackle this problem.", "We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.", "In fact, we have mostly changed the name from \u201cMeta Domain Adaptation\u201d to \u201cMeta Learning with Domain Adaptation\u201d, and the rest of the paper is almost identical, which we believe addresses the concerns of false advertising.", "Concern 2: Experiments", "Domain Adaptation Baselines + Other datasets", "Being a new problem setting, designing appropriate baselines can be challenging.", "We considered the traditional meta-learning for few-shot learning approaches, and combined meta-learning with a popular domain adaptation baseline.", "We are grateful for your suggestions on the domain adaptation baselines, and fully agree that it is reasonable.", "It is something we should have done on our own.", "Accordingly, based on your suggestions, and suggestions from other reviewers, we have tried to expand the baselines substantially (specifically, we include three state of the art Domain Adaptation methods as baselines \u2013 RevGrad [1], ADDA [2] and CyCADA [3]), and our proposed methods outperform them.", "For the other dataset suggested (VisDA), for synthetic-real adaptation, it is difficult to match the training paradigm of meta-learning.", "Typically, we desire several classes for meta-train, and several classes for meta-test, so that a variety of (e.g.) 5-way tasks can be crawn.", "With just 12 classes, the dataset is not very suitable for such settings.", "[1] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\" The Journal of Machine Learning Research 17.1 (2016): 2096-2030", "[2] Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\" Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017", "[3] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K. & Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. ICML 2018", "We thank you for considering our rebuttal and updating the score.", "We are grateful for your time and advice, and would appreciate if we could further extend the discussion.", "We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.", "We have identified a novel problem setting, which is closer to the real world setting, than what has been studied so far under the meta-learning paradigm.", "Existing solutions are not effective in this setting, restricting their use in the real world.", "Addressing this setting in our framework gives us a direction to improve the practical utility of meta-learning solutions for few-shot learning.", "Specifically, we identify that the principle of image-to-image translation is very suitable for this setting, and apply those concepts to boost the performance of few-shot learning under domain shift.", "As a combination of problem setting and proposed solution, we do believe we have addressed an important problem, and made a novel contribution.", "As regards the experiments: \u201cfairly small datasets", "\u2026 feature extractor backbone\u201d", "Most domain adaptation experiments use MNIST, USPS, SVHN, which are comparable in size to our Omniglot experiments.", "The other popular benchmark is using the Office-dataset, which also we have used (although a more recent version of a similar dataset, i.e., office-home \u2013 more suitable for meta-learning evaluation, as it has larger number of classes).", "See for example some of the recent domain adaptation papers [1, 2, 3].", "While a feature extractor backbone network may have some influence, we would like to highlight three points.", "First, when networks are trained in one domain, and evaluated in another, regardless of the backbone network, it is the domain-shift that dominates the performance.", "For example, no matter how large the network is, if it is trained to recognize black and white digits, it will still struggle to recognize colored digits.", "Second, any benefit of a larger backbone network will likely also enhance the performance of our model.", "Third, we just wanted to clarify (if there was a misunderstanding), unlike domain adaptation papers, we do not use a pretrained network \u2013 we train the full network from scratch (following traditional meta-training settings).", "[1] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\" The Journal of Machine Learning Research 17.1 (2016): 2096-2030", "[2] Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\" Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017", "[3] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K. & Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. ICML 2018"], "labels": ["rebuttal_structuring", "rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 248, "sentences": ["We thank the reviewer for the valuable feedback!", "The suggestion comments were very helpful and led to a clear improvement of our manuscript.", "We reply to the answers and comments in the order they were raised:", "(1) If one uses the same matrix-variate normal distribution that we use for p(\\theta | x) as approximate posterior p(\\theta) of a BNN in conjunction with the ELBO objective, one arrives at a BNN proposed by Louizos and Welling (2016) [1], i.e. the Variational Matrix Gaussian (VMG).", "We found that VMG\u2019s results (obtained from their original code https://github.com/AMLab-Amsterdam/SEVDL_MGP) are not as good as that for the CDN, as shown in Figure 8 in the appendix.", "This is further discussed in the newly added section 6.4.", "(2) Thank you for this valuable suggestion!", "We have added a new section (Sec. 4) to discuss the differences between the objective used for CDN, when performing variational inference for BNNs, and in the variational information bottleneck (VIB) framework.", "Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).", "We found that the CDN objective leads to superior results, especially in the adversarial examples experiment.", "(3) We observed that as \\lambda increases, in the validation set, the uncertainty is increasing, while the accuracy is decreasing.", "So", ", a simple heuristic that we use is to choose the highest \\lambda that allow high validation accuracy (e.g. > 0.97 on MNIST).", "We found that this heuristic works very well in our experiments (the results have updated to reflect on this heuristic).", "We have made this procedure clear in the revised manuscript.", "Detailed comments about experiments:", "(a) Thanks for catching this. Indeed this was due to a bug in the toy regression experiment which we have fixed now.", "(b) We have revised the baselines so that they either use \\lambda = 1 or the settings that the original authors recommended.", "We detail this in Appendix F.", "References:", "[1] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 249, "sentences": ["Thank you for your review and comments.", "We\u2019ve made a number of additions and improvements to address them in the updated version of the paper, which we will submit before the end of the discussion period.", "First, we have performed a new set of experiments on the larger dataset in [1].", "HOF shows greater average reconstruction accuracy than the methods compared in [1].", "Second, we also perform ablation experiments to demonstrate that HOF performs competitively even when we vary the encoder architecture, decoder depth, decoder activation function, or input sampling for the decoder network.", "For example, using Resnet18 as the encoder architecture or using a decoder network with twice as many hidden layers showed nearly identical performance in terms of average Chamfer distance on the test set.", "The complete quantitative results will be included in an updated PDF before the end of the discussion period.", "\"The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples.", "The function composition doesn't capture that.", "I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).", "But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc. I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands  reasonably on its own without that.\"", "We agree that the current formulation of composition is not equivalent to a generative model.", "In our work, function composition primarily serves the purpose of demonstrating that the model learns a meaningful subspace of objects (rather than memorizing the training set, as you mentioned).", "We have revised the abstract to clarify this point.", "\"In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?\"", "We have clarified in the manuscript that our comparisons are between architectures, rather than training objectives/output representations.", "Thus our DeepSDF, FoldingNet, and HOF architectures all output point clouds, which can be compared directly.", "\"For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)? I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable).\"", "Additional techniques for promoting learning of composable representations such as skip connections and layer dropout are an exciting direction for future research.", "One way function composition might allow for R^3 -> R mappings by composing a mapping from R^3 -> R^3 and taking the only first dimension of each element in the final output.", "Thank you again for your feedback.", "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_social", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_other_label"]}
{"abstract_id": 250, "sentences": ["We thank the reviewer for acknowledging the technical aspects of the paper and for noting that our\u200b \u200bresults\u200b \u200bare\u200b \u200bsolid\u200b \u200band\u200b \u200bour\u200b \u200banalysis\u200b \u200bis\u200b \u200bthorough.", "RE: Source code", "The reviewer makes an important point about the availability of the source code.", "To address this point, in the link privately shared with the reviewers, we have provided all of our code, datasets together with their train/test splits, as well as our pre-trained models, to help with the reproducibility of our results.", "We note that we will share PyTorch implementations of all pre-training methods and datasets with the community upon publication.", "Please feel free to ask any further questions regarding our code and implementation.", "RE: Linear time complexity in Appendix F", "We acknowledge that the time complexity of our pre-training methods was not well explained in Appendix F. In Figure 2 (a) we show that we only sample one node per graph.", "We then use breadth-first search to extract a K-hop neighborhood of the node, which takes at most linear time with respect to the number of edges in the graph.", "As a result, pre-training via context prediction has linear time complexity.", "We will edit Appendix F to include more detailed information and cover this important point.", "Please let us know if you have any further questions or comments!"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_by-cr", "rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 251, "sentences": ["Question 3:", "We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background.", "The referee indicates (in sharp contrast to referee 3) that the paper is not nicely written, nor easy to follow.", "We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.", "Question 4:", "The optimization algorithm used is the ADAM optimizer (Kingma & Ba, 2014).", "We refer to section 4 of Kingma & Ba (2014) for a proof of convergence for convex functions.", "It is known that that loss surfaces of deep neural networks are typically non-convex, however the gap between global and local minima is believed to be small for (see our answer to the referee\u2019s last question for more detail on this statement).", "Question 5:", "We kindly ask the reviewer to elaborate on the given statement.", "Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?", "=====================================", "Second part of the review", "=====================================", "Question 1:", "We specify the Gumbel-max trick in the paragraph below equation 4.", "To make the paper more self-contained, we will extend this paragraph to further clarify the Gumbel-max trick.", "We also refer to our answer to the first question of this referee, in which we elaborated more on the Gumbel-max trick as well.", "Question 2:", "All training parameters were tuned empirically.", "However, we agree it is worth elaborating on our insights regarding the influence on performance of some of them.", "We experienced that performance was most sensitive to the learning rates for the sampling and task models, and the temperature parameter tau of the softmax relaxation.", "We augmented the discussion of our revised manuscript to share these insights.", ".", "Question 3:", "We know experience replay as a reinforcement learning technique for storing previous state/action pairs.", "However, our method does not make use of reinforcement learning, so could the reviewer please elaborate how experience replay would relate to our method?", "Question 4:", "In Section 4.1 (MNIST classification) we already compared our proposed sampling method to used Gumbel top-K sampling for data subsampling.", "We are currently also running experiments that allow for extensive comparison with the recently proposed LOUPE method by Bahadir et al. (2019).", "Question 5:", "A large part of the experiments in this work are focusing on compressive/partial Fourier measurements.", "This adequately reflects the measurement setup in many real-world problems, such as k-space measurement in magnetic resonance imaging (Lustig et al.), Xampling for ultrasound imaging (Eldar et al.), and non-uniform step frequency radar (Huang, 2014).", "In addition, we cover direct pixel sampling, related to real-world applications such as compressive cameras.", "We would like to emphasize that the proposed approach is measurement-domain agnostic, and therefore can be applied across a vast amount of real-world problem.", "In addition, our ongoing research already shows promising results for real-world applications such as magnetic resonance imaging and ultrasound imaging.", "This is part of future work.", "Question 6:", "The trends towards using deep learning for data-driven compressed sensing indeed has the downside of not having guarantees on finding a global minimum, as the loss surface of a NN is highly non-linear and non-convex.", "Still, these data-driven results have shown to be very promising (Gregor et al., 2010; Jin,2019; Bahadir et al., 2019; Mousavi, 2019)", "However due to the weight space symmetry problem (Goodfellow et al., 2016) the loss surface contains a vast amount of local minima with the same error value.", "The size of the gap between local and the global minima remains an open field of research.", "However, citing from Goodfellow et al. (2016):", "\u201cThe problem remains an active area of research, but experts now suspect that,", "for su\ufb03ciently large neural networks, most local minima have a low cost function", "value, and that it is not important to \ufb01nd a true global minimum rather than to", "\ufb01nd a point in parameter space that has low but not minimal cost (Saxe et al.,", "2013; Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2014)\u201d", "As such, we leverage the empirically-shown ability of stochastic gradient descent to optimize this non-convex function in our NN for finding local minima.", "Indeed there is no guarantee on finding a global optimum.", "We thank the reviewer for the feedback.", "The reviewer states in his/her summary: The parameterization is used to simplify the subsampling distribution.", "We would like to comment on this, by stating that the reparametrization is not used for simplifying the subsampling distribution; on the contrary it actually enables sampling from this trained distribution.", "In fact, without this reparametrization, our generative sampling model (DPS) would not be differentiable.", "Below we elaborate upon the questions and raised concerns by the reviewer:", "Question 1:", "We respectfully disagree with the referee\u2019s conclusions, and will elaborate on the above statements in the following.", "While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.", "Regarding the theoretical correctness of deep probabilistic subsampling, in section 3.2 we explain how we incorporate a well-known reparametrization trick, termed the Gumbel-max trick (Gumbel,1954), to sample from a categorical probability distribution.", "Note that this shares similarities with the reparameterization trick used for sampling from trained gaussian distributions in a vanilla variational autoencoder.", "The Gumbel-max reparametrization perturbs the logits of the categorical distribution with Gumbel noise after which, by means of the argmax, the highest value is selected.", "Gumbel (1954) showed that this reparametrization allows sampling from the original categorical distribution.", "Recent state-of-the art work on a relaxation of this trick, termed Gumbel-softmax sampling (Jang et al., 2017) or the concrete distribution (Maddison et al., 2016), allows us to apply this relaxed reparametrization inside a neural network as it enables gradient calculation, which is needed for error backpropagation in the training procedure of the network.", "We would like to ask the reviewer what is believed to be missing from this explanation on the subsampling part of our proposed method.", "Regarding the theoretical basis used for the design of the task network; we took a theoretically principled approach by exploiting a model-driven network architecture for the CIFAR10 reconstruction problem.", "To that end, we unfold the iterations of a proximal gradient scheme (Mardani et al., NeurIPS, 2018), allowing for explicit embedding of the acquisition model (and therewith the learned sampling) in the reconstruction network.", "Regarding the referee\u2019s conclusion that the manuscript lacks comparison to the approaches of (Xie & Ermon (2019); Kool et al. (2019); Pl\u00f6tz & Roth (2018): We would like to point out that these three references all together put forward the Gumbel top-k method.", "Note that the use of the Gumbel top-k method for compressive sampling is also new, and in fact constitutes a specific case (constrained version with shared weights across distributions) of the proposed deep probabilistic subsampling (DPS) framework.", "In the MNIST experiments we already included Gumbel top-k sampling, but we will also add this for the other experiments in the revised manuscript.", "In addition, we added a thorough comparison of the DPS to LOUPE (Bahadir et al, 2019), a recently proposed data-driven method for subsampling.", "Question 2:", "We would first like to refer the referee to third paragraph of the introduction, where we explicitly formulate the main shortcoming of compressed sensing:", "\u201cThese [compressed sensing] methods, however, are lacking in the sense that they do not fully exploit both the underlying data distribution and information to solve the downstream task of interest.\u201d", "Then, in the list of main contributions, we write:", "\u201cDPS: A new regime for task-adaptive subsampling using a novel probabilistic deep learning framework for jointly learning a sub-Nyquist sampling scheme with a predictive model for downstream tasks\u201d", "Subquestion 2:", "We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.", "We respectfully disagree with the referee\u2019s conclusion that the method does not support a significant contribution.", "We propose a fully-probabilistic generative model for trainable sampling, that exploits both the underlying data distribution and information to solve the downstream task of interest.", "Our generative model builds upon recent advances on Gumbel max and top-k reparameterizations and their relaxations, showing for the first time how discrete sample selection can be done in a data-driven and task-adaptive fashion.", "This opens up a vast array of new opportunities in compressed sensing."], "labels": ["rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_followup", "rebuttal_followup", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 252, "sentences": ["We want to thank reviewer#4 for your review. Your summary correctly reflects the content of our paper.", "We want to comment on the suggestions for new experiments:", "Comparing with exhaustive search: This is a good idea.", "However, one concern is that since the search space is combinatorial, even a shallow network (e.g., 5) with a smaller number of precisions (e.g., 32, 8, 1) can have a large search space of (e.g., 3^5 = 243 architectures) for which exhaustive search is intractable.", "Comparing with DARTS and ENAS: ENAS is not open-sourced, so a direct comparison is difficult.", "A more detailed analysis comparing DNAS with DARTS is discussed in the reply to reviewer#1, minor concern #2: https://openreview.net/forum?id=BJGVX3CqYm&noteId=S1lyG-h7A7", "We plan to perform the suggested experiments of comparing with exhaustive search and DARTS.", "The results will be hopefully updated before the revision deadline and the camera-ready if the paper is accepted."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 253, "sentences": ["Thank you for the review.", "While formulating a proximal function for model compression might be an interesting idea (if search space is highly limited) as the reviewer suggested, we believe that our proposed method is fundamentally different from proximal gradient descent approaches due to the following reasons:", "1) Proximal gradient descent is meant to solve a convex optimization problem while our aim is to solve a non-convex problem in which each local minimum exhibits vastly different test accuracy after compression.", "Jumping to another local minimum from a certain minimum would not be easily achieved by convex optimization methods.", "2) Finding a particular flat minimum is the key to obtaining good model compression (and good generalization as well).", "Such an exploration, however, cannot be obtained by a proximal function since we need to investigate lots of different local minima with different amount of flatness in loss surface.", "3) While proximal gradient descent can be useful to find a certain local minimum close to the starting point given a convex constraint, wide exploration (associated with possibly transient accuracy loss in the initial training as shown in Figure 2.(b)) is necessary to escape from a point with sharp loss surface.", "Investigating many different local minima would be only available with large learning rate (as we have chosen for our experiments) and/or large amount of weight distortion.", "4) Our effort to introduce optimal distortion step size and learning rate for a given compression problems is connected to exploration, not exploitation (which potentially supported by proximal functions where convergence matters).", "Even though proximal gradient descent selects step size only considering convergence, Figure 1 can lead to the results such as Figure 2(b) which cannot be obtained if only local exploitation is employed.", "Finding a flat minimum has been known to be a difficult work as shown in the paper \u201cOn large-batch training for deep learning: generalization gap and sharp minima\u201d, ICLR 2016.", "We firmly believe that our search space exploration method based on optimal distortion step size and amount of weight distortion enable us to produce better local minima well-suited to various model compression techniques.", "In short, unfortunately, we have failed to understand how you could connect our technique to proximal functions and proximal gradient descent.", "We strongly hope that you reconsider your decision."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 254, "sentences": ["We would like to thank you for the review and comments.", "We revised the manuscript to address your concerns.", "Below we summarized your concerns/questions with our answers.", "Q1: Some arguments that are presented could deserve a bit more precision.", "A1: We acknowledge that quantization is a very active research area in model compression and there are numerous quantization techniques with unique and distinct characteristics.", "We could not introduce and discuss lots of exciting quantization techniques such as vector quantization due to the limited space.", "We feel that introducing other quantization techniques in details would make the paper distracted since those techniques cannot be compared with compression ratio only (i.e., inference architecture, computation methods, and storage design would be different).", "Instead, we added more thorough introduction to binary codes in Section 1 to explain unique computational advantages of using binary codes.", "We introduced \"Hardware-aware Automated Quantization with Mixed Precision\" in Section 1 since fractional quantization on average is available as you pointed out, while FleXOR can also employ different quantization bits for each layer (i.e., we believe HAQ method can be applied on top of FleXOR).", "Q2: More extensive and thorough experiments could improve the impact of the paper.", "A2: We agree that including extensive quantization methods and model architectures would greatly improve the impact of the paper.", "Unfortunately, as we discussed above, our goal in this paper is to improve quantization schemes based on binary codes.", "Including quantization methods of different assumptions may require much lengthy discussions that make comparisons a lot complicated.", "For example, \"Hardware-aware Automated Quantization\" could be additionally applied to binary codes, and FleXOR is not conflicted with such an architectural techniques to improve compression ratio.", "Deep compression, TTQ, and TWN involve weight pruning that deserves large space for discussions (nonetheless, we compared TWN, TTQ, and BinaryRelax using ternary quantization scheme in Table 5 of Appendix).", "Deep compression also includes CSR format and Huffman coding which would make comparisons more complicated.", "We chose a few representative quantization methods mainly based on binary codes to facilitate fair and focused comparisons, and correspondingly, ResNet models on CIFAR-10 and ImageNet are selected for our experiments since most previous works (of binary codes) commonly include those models.", "For example, we could not include HAQ in the paper for experimental results, because HAQ chooses MobileNet and ResNet-50 only as model architectures while comparisons are made with only PACT and Deep Compression methods.", "Q3: Providing some code and numbers for inference time would be great.", "A3: Due to the internal policy of our organization, we cannot open our codes publicly at this moment. Hence, we provide a link to anonymous code to the reviewers only until we get an approval for public release.", "Please refer to our message available to the reviewers only.", "Overhead of weight decryption on-the-fly is extremely small even with CPUs or GPUs, since decryption involves only a binary matrix multiplication over GF(2), which can be easily supported by existing SIMD or vector operations.", "Since a binary matrix is too small (e.g., 10x8), computational overhead is just ignorable compared with other computations."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 255, "sentences": ["[Q] The paper presents a novel hierarchical clustering method over an embedding space.", "In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learned.", "The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods.", "[A1] Dear Reviewer 1, thank you for the thoughtful review.", "The reviewer mentioned our key point correctly.", "Many works on flat-clustered representation learning except for VAE-nCRP, has been limited to capture flat-level data structure.", "[Q] The paper address a relevant problem, which is of great interest for extracting knowledge from data.", "[A1] There are a lot of high-dimensional data around us, and it obviously contains complex structures inside. What we would like to argue through this study is that we can analyze the complex structure of data in the embedding space learned by a deep neural network.", "[Q] In general, the quality of the paper is high.", "The presented approach is based on a sound formalization of hierarchical clustering and deep generative models.", "The paper is easy to follow in spite of the technical difficulty.", "The experimental evaluation is really extensive.", "It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view.", "[A1] Thank you for the comment.", "As we assume a rather complex prior to embedding for flexibility, the technical depth of formalization has deepened.", "We concerned that it would be confused for the readers including the reviewers, to understand.", "Therefore, we carefully presented the figures, especially in Figure 3(a) showing an example of the variable values.", "In the case of experiments, we have devised various quantitative and qualitative experiments to assert why we need this hierarchically clustered representation learning.", "We empirically observed the performance improvement of both density estimation and hierarchical clustering, which motivates the joint optimization.", "Additionally, we qualitatively showed embedding plot, image generation, and result hierarchy with various datasets.", "[Q] The only issue with this paper is its degree of novelty, which is narrow.", "The proposed method adapt a previously presented hierarchical clustering method in the \"standard space\" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.", "The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.", "[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.", "The theoretical contribution of our study can be considered in conjunction with the unified model based on the fully Bayesian approach of the probabilistic graphical model and the neural network.", "Additionally, we tuned the several detailed heuristic algorithms for operations such as GROW, PRUNE, and MERGE.", "Also, if we take a naive pipelined approach of iterative training between the hierarchical Gaussian mixture model and representation learning, then this work would be an obviously incremental work.", "[A2] VAE imposes a single Gaussian prior on embeddings, which leads to 1) the over-regularization, and 2) poor representations [1,2,5].", "[1] Chen, Xi, et al. \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\" NIPS. 2016.", "[2] Hoffman, Matthew D., and Matthew J. Johnson. \"Elbo surgery: yet another way to carve up the variational evidence lower bound.\" Workshop in Advances in Approximate Bayesian Inference, NIPS. 2016.", "Therefore, the recently published researches can be divided into two branches: 1) designing of an objective function by introducing the additional regularized terms, or 2) constructing of a more flexible prior.", "Our work attempts to the latter approach, which proposes a new prior called a hierarchical-versioned Gaussian mixture distribution prior to the first trial of hierarchical density estimation in the embedding space.", "Another work of the latter approach is:", "- Variational Deep Embedding (VaDE) [3]: VAE+GMM", "- VAE-nCRP [4]: VAE+(nCRP+GMM)", "- VAE with a VampPrior [5]: VAE+ a variational mixture of posteriors prior.", "The contribution of these studies lies on 1) the formalization as a unified model based on the newly proposed prior, though not the original technique proposed by the authors, and 2) demonstrating the superiority of the prior.", "[3] Jiang, Zhuxi, et al. \"Variational deep embedding: an unsupervised and generative approach to clustering.\" IJCAI, 2017.", "[4] Goyal, Prasoon, et al. \"Nonparametric Variational Auto-Encoders for Hierarchical Representation Learning.\" ICCV. 2017.", "[5] Tomczak, Jakub, and Max Welling. \"VAE with a VampPrior.\" AISTATS. 2018.", "Best regards,"], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label"]}
{"abstract_id": 256, "sentences": ["> This suggests that PointGoal navigation with dense GPS signal is might be a poor choice to benchmark RL algorithms and we should proceed to harder tasks.", "Agreed.", "We hope that our algorithm, DD-PPO, and our pretrained models will help accelerate progress on harder tasks like PointGoal Navigation without GPS+Compass, ObjectGoal/RoomGoal Navigation, Instruction Following, etc."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label"]}
{"abstract_id": 257, "sentences": ["We want to thank the reviewer#1 for your feedback.", "Your summary correctly reflects the content of our paper.", "We hope this rebuttal can address your concerns.", "Major concern: Trained sampling vs random sampling", "We sample architectures every a few epochs, mainly because in our experiments, we want to analyze the behavior of the architecture distribution at different super net training epochs.", "This analysis is illustrated in figure 3 of our paper.", "We can see that at epoch-0, where the architecture distribution is trained for only one epoch (close to random sampling), the sampled architectures have much lower compression rate.", "Similarly, for epoch-9, architectures also have relatively low compression rate.", "In comparison, at epoch-79 and epoch-89, architectures have higher compression rates and accuracy.", "The difference between epoch-79 vs. epoch-89 is small since the distribution has converged.", "As the reviewer#2 suggests, we can train the super net until the last epoch, then sample and train architectures from this distribution.", "Figure 3 shows that the five architectures sampled at epoch-89 are much better than the five architectures at epoch-0, which are essentially drawn from random sampling.", "Also, note that for CIFAR10-ResNet-110 experiments, the search space contains 7^54 = 4x10^45 possible architectures, 45 sampled architectures are tiny compared with the search space.", "Reviewer #2 suggests comparing with a \u201ccost-aware\u201d random sampling policy.", "We tried a simple baseline that at each layer, we sample a conv operator with b-bit precision with probability", "prob(precision=b) ~", "1/(1 + b)", "The performance of this policy is much worse since for a conv operator with precision-0 (in our notation, bit-0 denotes we skip the layer), the sampling probability is 33x higher than full-precision convolution, 2x higher than 1-bit, 3x higher than 2-bit, and so on.", "Architectures sampled from this distribution are extremely small but with much worse accuracy.", "We understand this might not be the best \u201ccost-aware\u201d sampling policy.", "If reviewer#1 has better suggestions, we are happy to try.", "Minor concern #1: Value of the Gumbel Softmax function", "Yes. We agree with the comments that the advantages of the Gumbel Softmax technique are two-fold:", "1. It makes the loss function differentiable with respect", "to the architecture parameter \\theta", ".", "2. Compared with other gradient estimation techniques such as Reinforce, Gumbel Softmax balances the variance/bias of the gradient estimation with respects to weights.", "Minor concern #2: Comparison with non-stochastic method such as DARTS", "DARTS [1] does not really sample candidate operators during the forward pass.", "Outputs of candidate operators are multiplied with some coefficients and summed together.", "For the problem of mixed precision quantization, this can be problematic.", "Let's consider a simplified scenario", "y = alpha_1 * y_1 + alpha_2 * y_2", "Let's assume both y_1 and y_2 are in binary and are in {0, 1}. Assuming alpha_1=0.5 and alpha_2=0.25, then the possible values of y are {0, 0.25, 0.5, 0.75}, which essentially extend the effective bit-width to 2 bit.", "This is good for the super net's accuracy, but the performance of the super net cannot transfer to the searched architectures in which we have to pick only one operator per layer.", "Using our method, however, the sampling ensures that the super net only picks one operator at a time and the behavior can transfer to the searched architectures.", "Minor concern #3: Warmup training", "We use warmup training since in our ImageNet experiments.", "We observe that at the beginning of the super net training, the operators are not sufficiently trained, and their contributions to the overall accuracy are not clear, but their cost differences are always significant.", "As a result, the search always picks low-cost operators.", "To prevent this, we use warmup training to ensure all the candidate operators are sufficiently trained before we optimize architecture parameters.", "In our ImageNet experiments, we found that ten warmup epochs are good enough.", "In CIFAR-10 experiments, warmup training is not needed."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 258, "sentences": ["We thank for your valuable comments and suggestions.", "=== Regarding to the assumptions, specifically, twice differentiable/smooth Hessian =", "=", "=", "Twice differentiable/smooth Hessian are only used for analyzing the process of escaping saddle points.", "So we conjecture that one can relax the assumptions and introduce the notions like ``locally twice differentiable'' and ``locally smooth Hessian'', meaning that the assumptions only need to hold in the region of the saddle points.", "Since the gradient norm in the region of the saddle points is small, it implies that the Hessian should not change too much and ``locally smooth Hessian'' should make sense.", "However, we are not aware of any related works of escaping saddle points introducing any measures of ``locally smooth Hessian''.", "You might actually point out a good research direction.", "=== Regarding to the empirical results/experiments ===", "We appreciate your acknowledgment of our contributions and pointing out that the properties may only need to be satisfied at some critical points during training deep neural nets.", "We will keep updating the paper and conducting more thorough experiments.", "=== Regarding to the small step size ===", "We think that it is a gap, for which people in the community haven't have any good remedies yet.", "Almost all of the theoretical works in nonconvex optimization and deep learning require a small step size (e.g. works of natural tangent kernel, works of showing the global convergence for a two layer neural net).", "Nevertheless, we want to note that the step size $\\eta = O(\\epsilon^5)$ in our paper is of the same order as the closely related work (Daneshmand et al. 2018) of escaping saddle points."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 259, "sentences": ["Thank you for your constructive comments! We would like to address your concerns as follows:", "1. Clarification on Fig. 4.", "We rewrote the caption for Fig. 4.", "Specifically, for Wave-U-Net, the green curve indicates the fitting result compared against the noisy target, and the red curve is the result evaluated against the clean signal.", "Therefore, Wave-U-Net fits the noisy target fast but does not produce the clean version of the signal during fitting.", "For Convolution and Dilated Convolution networks, they do fit faster but saturates with low-quality output.", "Harmonic Convolution produces much better results, which is ~3.5 dB higher.", "We highly recommend listening to examples at https://anyms-sbms.github.io to feel the difference.", "2. Dilated convolution in paper\u2019s notation.", "We have added a section in the appendix to include dilated convolution in the paper's formulation.", "3. Clarification on Fig. 2.", "Since the plots in Fig. 2 are log-scale, one would expect nearly linear fall-off of energy from low-frequency components to high-frequency components, which is the case of (a).", "But (c)(e) exhibit drastically different fall-offs of energies compared with (a).", "We have modified the caption of Fig. 2 to be more specific.", "We compared our model with unsupervised/supervised NMF for sound source separation, a common unsupervised baseline for this task.", "The evaluations are reported as follows:", "----unsupervised----", "guitar:          SDR: 2.17   SIR: 2.78   SAR: 14.19", "congas:        SDR: -0.20  SIR: 0.23   SAR: 14.76", "xylophone:  SDR: 2.04   SIR: 3.61   SAR: 12.13", "----supervised----", "guitar:          SDR: 5.97   SIR: 7.56   SAR: 12.81", "congas:        SDR: 1.77  SIR: 2.76   SAR: 11.97", "xylophone:  SDR: 8.08   SIR: 12.33   SAR: 11.72", "Please let us know for any questions.", "Thanks again for your suggestions, which have made this submission stronger.", "Thanks,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 260, "sentences": ["We thank AnonReviewer1 for the careful reading and encouraging comments.", "Indeed, the idea of a non-linear or adaptive gain normalization is novel to our knowledge, and is the main reason for deciding to submit this work to ICLR.", "We based our theoretical insight on an extensive reading and experience on neurophysiological data which we tried as much as possible to reconcile with the latest literature in ML/DL.", "In particular, we think that this problem is resolved in most DL approaches using heuristics such as dropout ( http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf ) or batch normalization ( https://arxiv.org/abs/1502.03167 ).", "We acknowledge that the objective we use (equiprobability) may seem arbitrary, but we think that 1) it best fits constraints in biological populations of neurons 2) it can be adapted to other priors on the desired probability of nodes in the network.", "In our current revision", ",  while keeping the same theoretical framework and simulation results, we have highlighted our main contributions: 1/ to show that $\\ell_2$ normalization leads to non-homogeneous data 2/ provide with an exact rule 3/ propose a simplfied heurstics and show its effectiveness.", "Also, we have fixed the typos and minor issues  (in \"Misc\") in the revision that is being uploaded to the openreview preprint server.", "Thanks again for your careful reading."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 261, "sentences": ["We thank you for acknowledging the novelty our findings and your appreciation for the elementary nature of our theory.", "----------------", "- Q: How do Section 2 & 3 fit together?", "Although it is true that our paper can roughly be divided into two section, we want to stress that these sections are inextricably linked due to the nature of their topics, since we see invariance as a limit case of inverse stability.", "We therefore think it is natural to study both of them.", "However, the analysis of the limit case, invariance, admits more powerful tools (see e.g. Theorem 4), since one is only interested in whether a singular value is zero or not.", "Hence, the invariance is qualitative, whereas for stability we need to quantify singular values.", "-----------------", "-Q: Combinatorial problem to check Theorem 4:", "While there are indeed a combinatorial number of possible tuples that the Theorem 4 describes, we can use the following trick in the design of the Algorithm 1 (Appendix A3) to circumvent these computations: The set of tuples (A, b) that form omnidirectional tuples is a null-set in all tuples of same form, we therefore ignore this case in our numerical analysis.", "Hence, we only have to check whether we have a compact or unbounded preimage.", "This can be done by simply checking whether A is omnidirectional or not.", "----------------", "-Q: Upper bounds and inverse stability:", "The smallest singular values are directly linked to inverse stability for points from the same input polytope (where the linearization is exact).", "The upper bounds (Lemma 9) and the correlation effect are interesting, as they show how a well-conditioned matrix (subset of rows almost orthogonal) may become instable due to the removal of rows via ReLU.", "If the correlation of some rows is arbitrarily small (but non-zero) between remaining and removed rows, the upper bounds can be arbitrarily small.", "Thus, this Lemma provides an intuition how hard it is to globally control inverse stability with a vanilla architecture (linear mapping followed by ReLU).", "However, when considering an epsilon ball around activations, two main questions arise: 1) Are all points in the ball reachable from the considered input polytope?", "2) Do points from other input polytopes map to the epsilon ball?", "If the second case holds, one would need to consider different linearizations of the network and thus extend the analysis to movements between the polytopes.", "-> Added a comment in the newly written \u201cScope\u201d section in the revision", "-------------------", "-Q: Actionable consequences from paper:", "One consequence of our paper is that it is close to impossible (each layer need at least to double the number of neuron) to enforce invertibility and it is similarly hard to enforce compactness in ReLU layers.", "This leads to the conclusion that if one wants invertibility or even just compactness reliably over the whole space, vanilla architectures using ReLU are not a good tool for the task.", "Hence, our analysis can be seen as an argument for additional structure like dimension splitting in reversible networks (see e.g. Jacobsen et al. (2018)).", "These structures allow for guarantees as they are by design bijective, while vanilla architectures show a breadth of possible effects as shown in our analysis.", "-> Added a comment to \u201cPractical Implications\u201d in the revision", "- Q: Illustrative experiments:", "We currently thinking about an experiment to better illustrate the intuition of our theory and would appreciate any suggestions.", "We thank the reviewer for the helpful comments and we would appreciate further suggestions.", "We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.", "We would appreciate further suggestions."], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_followup", "rebuttal_social", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 262, "sentences": ["We thank Reviewer 1 for their detailed comments and feedback.", "Reviewer 1\u2019s main concerns are 1) that the related works section does not sufficiently frame our work with previous literature, 2) that the proofs of theoretical guarantees are not sufficiently rigorous, and 3) that the experiments section is not comprehensive enough.", "We have posted a significantly updated new draft to address these concerns.", "-------------------------------------", "Experiments", "Reviewer 1 claims that we do not sufficiently compare with enough other methods, and specifically asks for comparisons with Feudal Networks (FuN) and Roderick et al., 2017.", "We already comprehensively compare with the prior non-demonstration state-of-the-art, which use a comparable amount of prior knowledge, in each game.", "Since we already compare with the prior state-of-the-art approaches, and other approaches perform significantly worse than the prior state-of-the-art approaches, we do not compare with the many other deep RL approaches.", "In particular, FuN and Roderick et al., 2017 both report results on Montezuma\u2019s Revenge.", "The prior state-of-the-art approach we compare against, SmartHash, outperforms these approaches by 1.75x and 4x respectively, at the number of frames they report (200M and 50M respectively).", "Our approach further outperforms SmartHash by over 2x.", "Reviewer 1 further asks for evaluation on more games.", "We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.", "In particular, we follow Aytar et al., 2018, and evaluate on 3 of the hardest exploration games from the Arcade Learning Environment.", "We do not evaluate on many of the simpler other games (e.g., Breakout), because they do not require sophisticated exploration and can already be solved with current state-of-the-art methods.", "We use the same set of minimally tuned hyperparameters (tuned only on Montezuma\u2019s Revenge) and obtain new state-of-the-art results by over 2x, suggesting that our approach can generalize to new tasks.", "Our results are not cherry-picked as R1 suggests: following many recent deep RL works, e.g., Ostrovski et al., 2017, Tang et al., 2017, we run 4 seeds on each task, and obtain statistically significant results.", "Even our *worst seed* outperforms or is competitive with the prior state-of-the-art *best seed*.", "We note that running 10 seeds would approximately cost $30,000 per additional game in compute.", "Renting the appropriate equipment (e.g., via Google Cloud) to run a single seed to completion costs ~$1,500.", "To run 20 seeds (10 for our approach, 10 for the prior state-of-the-art) would cost 20 x $1,500 = $30,000 or roughly the median US annual salary.", "---------------------------------------", "Related Works", "We\u2019ve updated the related works section in our recently posted draft to more carefully compare  Please see Sections 1 and 7 for updated related work.", "The main critical difference between our work and other HRL works is that we build an abstract MDP, which enables us to plan for targeted exploration; other works also learn skills and operate in latent abstract state spaces, but not necessarily in a way that satisfies the property of an MDP, which can make effectively using the learned skills difficult.", "--------------------------------------", "Theory", "In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.", "To summarize: we\u2019re interested in the sample complexity of RL algorithms, i.e., the number of samples required for the learned policy to become near-optimal (achieve reward at most epsilon less than the optimal policy).", "Standard results (e.g., MBIE-EB, R-MAX) can guarantee a near-optimal policy, but they require so many samples (polynomial in the size of the state space) in deep RL settings, that the guarantees are effectively vacuous.", "In contrast, for a subclass of MDPs, our approach provably learns a near-optimal policy in a number of samples polynomial in the size of the *abstract* MDP.", "Responding to R1's additional feedback:", "R1 asks if our method applies to continuous spaces.", "Our method applies to continuous spaces with no changes, we can just discretize the abstract state (not the concrete state).", "In particular, our method may be well-suited for many robotics tasks, which often have the full state (e.g., joint angles and object positions) available.", "For example, in a task like stacking blocks with a robotic arm, a good state abstraction function would be the position of the end effector and blocks, which are directly available in the state (e.g., in Stacker from DM Control Suite).", "R1 says that the randomized exploration used by the discoverer is underwhelming.", "We view the simplicity of the discoverer as advantageous.", "Fundamentally, exploration requires some degree of randomness, and we were already able to achieve state-of-the-art results without overcomplicating the discoverer.", "We note that this random exploration is only for locally discovering nearby abstract states.", "Globally, we drive exploration by incrementally growing the safe set (renamed known set in the updated draft).", "R1 asks for experiments that do not use RAM state information.", "We clarify that we use the RAM state information for the state abstraction function, which is a fundamental component of our work, so it is not possible to run experiments without this RAM information.", "However, we explore the robustness of our method to the exact chosen abstraction in section 7.4 and find that our method achieves state-of-the-art results over a wide range of state abstraction functions, suggesting that alternate state abstraction functions could be used.", "We also note that our experiments compare with state-of-the-art approaches, which also use prior knowledge comparable to our usage of RAM state information."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 263, "sentences": ["Thanks for your comments.", "We missed this previous work.", "Jenatton et al. proposed an approach to predict tree structures by using Bayesian optimization to combine independent Gaussian Processes with a linear model that encodes a tree-based structure.", "We have cited and discussed the work in the revision.", "The focus of our work is to propose a new tree prediction problem (layout completion) and introduce Transformer-based approaches for addressing the problem.", "It would be future work to investigate other tree-based models for this problem."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 264, "sentences": ["Thank you for your thoughtful review.", "We have updated the paper based on your comments to improve clarity and reproducibility.", "We list a summary of our main changes below:", "- In order to make it easier for readers to understand the differences between different models and how they are related to InfoNCE, we have added a summary in Table 1.", "- We have improved notations by adding explicit definitions before they are used in Section 2 and Section 4, and added a short description of Deep InfoMax in Section 4.", "- We have included model and training hyperparameter details in Section 5.1 and Appendix B.", "- We added a motivation for mixing two different terms in the objective function.", "Our DIM is primarily designed to improve sentence and span representations.", "We combine it with MLM which is designed for learning (contextual) word representations, since our overall goal is to create better representations for both the sentence and each word in the sentence.", "We also note that Deep InfoMax for learning image representations mixes multiple terms in their objective function.", "We only take one of the terms from the full objective function and mix it with MLM.", "Regarding equation I_{DIM}, it is supposed to contain two g_{\\omega} and no g_{\\psi} as we use one network for encoding both the sentence and n-grams.", "This is not a typo."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_contradict-assertion"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 265, "sentences": ["Thank you for your comments and questions.", "We have updated the manuscript to clarify these questions.", "1) The VAE is hypothesized to produce blurry images when the inference/generative models are not sufficiently expressive for the data modeling task, and in particular due to the typical choice of MSE loss (i.e. Gaussian error model), thus blurring sharp edges in complex natural image data [1,2,3].", "In the case of cryo-EM, the high noise in the images is typically assumed to be Gaussian and therefore using the MSE loss has a denoising effect.", "In our experiments, we were able to achieve resolutions up to the ground truth resolution or matching published structures with our architecture and training settings, though we agree with the reviewer that exploring alternative generative models is a promising future direction.", "[1] https://arxiv.org/abs/1611.02731", "[2]  https://openreview.net/pdf?id=B1ElR4cgg", "[3] https://arxiv.org/pdf/1702.08658.pdf", "2) We observed accurate reconstructions as long as the dimension exceeded the dimension of the underlying data manifold and faster training with higher dimensional latent variables.", "We have added these results to the appendix in the revised manuscript.", "3) We varied the number of classes for comparison against SOTA discrete multiclass reconstruction and selected 3 classes which had the lowest error for our comparison in Table 2.", "We have added these results to the appendix in the revised manuscript.", "4) Our coordinate-based neural network model for volumes provides a general framework for modeling extrinsic orientational changes in a differentiable manner.", "This work could be applied in other domains of scientific imaging such as reconstruction of tomograms or CT scans."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 266, "sentences": ["Dear AnonReviewer2,", "thank you for your constructive feedback. Below we address your concerns and questions.", "\u201cJudging from Table 1, the proposed method does not seem to provide a large contribution.", "For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.\u201c", "\u2192 The authors of NASNet only provide results for two regimes of parameters (3.3M and  27M) as they do not perform multi-objective optimization but rather just vary two parameters for building NASNet models (number of cells stacked, number of filters).", "Their method might be optimized to yield good results in these regimes and, admittedly, LEMONADE does not outperform NASNet for models with ~4M parameters.", "However, from Figure 3 and Table 2 one can see that only varying these two parameters for NASNet models is not necessarily sufficient to generate good models across all parameter regimes.", "E.g., LEMONADE clearly outperforms NASNet for very small models (50k params, 200k params - Table 2).", "We also refer to Appendix 3 (\u201cLEMONADE with 5 objectives\u201d), Figure 6, in the updated version of our paper, where one can see that while NASNet has quite strong performance in terms of error, number of parameters and number of multiply-add operations, it performs poorly in terms of inference time.", "Hence, there is a benefit in doing multi-objective optimization if one is actually interested in multiple objectives and diverse models rather than a single model.", "This is the main contribution of our paper and different to, e.g., the NASNet paper.", "The same likely also applies for ENAS (as they use the same search space and conduct very similar experiments).", "We also would like to highlight two things: 1) NASNet requires 40x computational resources than LEMONADE, so even if NASNet performs better for ~4M parameter models, LEMONADE achieves competitive performance in significantly less time.", "2) Table 1 shows results for models trained with different training pipelines and hyperparameters, and hence it is hard to say architecture X performs better than architecture Y since differences could simply be due to e.g. different learning rates, batch sizes, etc.", "In contrast, all other results in the paper (e.g., Figure 3 and Table 2) provide comparisons with exactly the same training pipeline and hyperparameters.", ".", "\u201cIt would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix. \u201c", "-> Thanks, we agree; we re-organized our paper accordingly.", "\u201c- In the case of the search space II, how many GPU days does the proposed method require?", "-> We also ran this experiments for 7*8 GPU days, however the method converged after roughly 3*8 GPU days (meaning that there were no significant differences afterwards).", "\u201cAbout line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.\u201d", "-> The population is updated to be all non-dominated points from the current population and the generated children, i.e. the Pareto frontier based on all current models.", "We clarified this in Algorithm 1.", "Thanks for pointing us towards this.", "We hope this clarifies your questions. Thanks again for the review!"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 267, "sentences": ["Thank you for your review.", "We are a bit surprised since the paper provides answers to the exact questions you raised as missing. We are sorry you missed it, and we have cleaned up the presentation so it is hopefully now clear that we do answer these questions and more.", "The answers, as you pointed out, were much desired and not known before.", "Below are answers resultant from eq. 5 to the specific questions the referee raised, with some added definitions to make them concrete.", "1. \u201chow deep should a model be for a classification or regression task? \u201c", "We show in section 6.1 that the dependency of the classification error on the number of layers is also well approximated by eq. 5 (recall $m$ scales linearly with depth).", "So, if we consider some target error $\\epsilon_{target}$, we can solve eq. 5 for m or n given the other or for both, attaining the m,n contour for $\\hat{\\epsilon}(m,n) = \\epsilon_{target}$.", "2. \u201cWhat is the minimum/maximum layers of a deep model? \u201c", "For a fixed dataset size, model scaling eventually contributes marginally to error reduction and becomes negligible when $bm^{-\\beta} \\ll n_{lim}^{-\\alpha}$ (Eq. 5).", "Define the relative contribution threshold $T$ as satisfying $ T = \\frac{n^{-\\alpha} }{ bm^{-\\beta}}$. (For example, $T=10$.) Then the maximal useful model size meeting threshold $T$ is:", "$$     m_{max}(T) = \\left(bT\\right)^{1/\\beta} n_{lim}^{\\alpha/\\beta}  $$", "As for minimal depth, here too let\u2019s consider a definition as a working example: what is the minimum depth that could meet a certain error level $\\epsilon_{target}$ (if data is not a limit).", "For example, when the target error is small relative to the \u201crandom guess error\u201d $\\epsilon_0$ (equivalently when $ n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$), by solving eq. 5 for $m$ we have:", "$$ m_{min} = \\left(\\frac{b}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\beta} $$", "3. \u201cHow much data is sufficient for a model to learn? What is the minimum/maximum size of the data set?\u201d", "Similarly to the above:", "Minimum data needed for target error (if model size is not a limit):", "$$ n_{min} = \\left(\\frac{1}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\alpha} $$", "4. Maximum useful data (in the marginal sense $T$ for a limited size model, as above):", "$$n_{max}(T) = \\left(1/bT\\right)^{1/\\alpha} m_{lim}^{\\beta/\\alpha} $$", "In particular, note that there is also a minimal amount of data and model size needed for better-than-random-guess error level, characterized by the location of the pole $\\eta$: $n^{-\\alpha}+bm^{-\\beta}< \\eta$", "5. \u201cDo we really need a large data set or just a subset that covers the data distribution?\u201d", "Via careful dataset sub-sampling (as noted by reviewer 3) we show that indeed more data *is* needed to improve performance (reduce error) while holding the class distribution fixed (in expectation), for a given architecture and scaling policy.", "For directly viewing the error manifolds decoupling the dependency on model and data size, see figure 1 and in appendix C.", "6. \u201cWhat's the relation between the size of a model and that of a data set? \u201c", "The joint form in Eq. 5 captures the relation between data-size and model-size (and error) completely.", "7. \u201cBy increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?\u201d", "For example, from Eq. 5, it is clear that a sweet-spot in terms of balancing the effect of the data/model sizes on limiting the error is $n^{-\\alpha} \\approx bm^{-\\beta}$ .", "When considering this sweet spot for example, increasing depth/width/both such that the model size $m$ is increased by a factor $f$ to a new size is $m\u2019 = mf$, the corresponding increase in data maintaining the sweet-spot is $n\u2019 = nf^{\\beta/\\alpha}$", "8. How about the gain of the task performance?\u201d", "The effect on the performance is given by evaluating Eq.5 for the initial and scaled $m,n$.", "For example, in the powerlaw region ($c_\\infty \\ll n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$):", "The effect on the performance is $\\epsilon\u2019 = \\epsilon f^{-\\beta}$"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 268, "sentences": ["Thank you for your review.", "We agree that assumption (H2) is very restrictive and have added some results relaxing it in Section 3.4 in the latest version of the paper.", "Please see the comment above entitled: \u201cRelaxing Assumption (H2)\u201d for more details.", "However, it it worth pointing that even under Assumption (H2), learning does not necessarily converge.", "As shown in Fig 2. Left and Section 3.3, any initialization in the top left red region will fail to solve the problem.", "In that case, the confidence on the corresponding class will be 0.5 after a finite number of updates.", "As far as assumption (H1) is concerned, it is very classic in deep learning theory (see for instance [1,2,3]) and we have not been able to relax it.", "[1] T. Laurent and J. von Brecht. Deep linear networks with arbitrary loss: All local minima are global.", "ICML 2018", "[2] Z. Liao and R. Couillet. The dynamics of learning: A random matrix approach. ICML 2018.", "[3] S. Arora et al. On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization. ICML 2018."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_summary_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 269, "sentences": ["Thank you for your review.", "Below we attempt to answer your concerns.", "We also want to point out that we have added some insights/results relaxing one of our main assumptions in Section 3.4 of the latest version of the paper.", "For more details, please see the comment above entitled: \u201cRelaxing Assumption (H2)\u201d.", "The path we attempted to draw through the paper aims at the evolution of a nonlinear neural network\u2019s classification performance throughout its training and at the factors that influence it: from the norm of the input to the type of loss used for learning or the frequency of features present in the training data.", "Our framework is able to establish properties on the behavior/convergence of certain classifiers during their training on separable data.", "Those insights match some observations made by machine learning practitioners, in particular about the sigmoidal shape of learning metrics or the efficiency of the hinge loss on certain tasks.", "We have added an explanation of what we mean by \u201clearning dynamics of deep learning\u201d in the last paragraph of the first page.", "It usually refers to the evolution of weights and outputs of neural networks throughout training.", "For instance, the work by Saxe et al in 2013 is entitled \u201cExact solutions to the nonlinear dynamics of learning in deep linear neural networks\u201d.", "We based our title on that paper since it extends some of its results to nonlinear neural networks.", "We understand your concern and have made the title more specific.", "Tentatively, we chose: \u201cConvergence Properties of Deep Neural Networks on Separable Data\u201d.", "Let us assume for simplicity that in Corollary 3.3, p = 0.5 (ie that the classes are balanced) and that ||x_1|| = 1, ||x_2|| = 0.5.", "Then the confidence of the network on those classes corresponds to the red and dashed purple curves of Fig. 2.", "Right.", "In particular, we see that reaching any level of confidence takes approximately twice as much time on class 2 (red curve) than on class 1 (dashed purple curve).", "That is effectively what the corollary is expressing.", "We have edited the corresponding sentence to make it less assertive.", "We have added the missing labels in the latest version of the paper. Thank you for pointing it out."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 270, "sentences": ["Thank you for your comments, we will be responding with specific comments to AnonReviewer3 here, and more general comments to the reviewer above.", "R3: \u201cIt would be good to understand what benefit does the stochasticity of RBMs provide.\u201d", "The stochasticity of the RBM provides a number of benefits over more deterministic methods.", "Firstly, the stochasticity allows for full sampling from the RBMs distribution, and has the ability to identify all possible modes in a multimodal distribution if sampled for long enough.", "As there are many possible solutions to a Boolean logic query (and integers can have many different factors), we note that the statistics that this method provides can give us a variety of answers to the queries, allowing the user to evaluate each individual solution based on its individual merit.", "R3: How do deterministic neural networks perform on the addition and factoring tasks? The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across as arbitrary.", "To the best of our knowledge, deterministic neural networks have not been well studied for the integer factorization problem.", "In [4] deterministic neural networks are used, but are able to factor smaller integers, on a more restricted problem, and are fully trained on the subset of all integers.", "We have cited this work in our related works section, and mentioned its impact.", "R3: That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.", "We agree that this method of composing simple functions to compute more complex ones is intuitive, and may not be very surprising, but we think that this helps data and model efficiency in a different manner than presented in previous papers.", "As far as scaling up the tasks and problem sizes, we are showing a method of combination here, and are scaling up the problem sizes continuously.", "We believe this combination method could be used for other things, and have presented it here as a proof of concept rather than a definitive survey with all possible uses."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 271, "sentences": ["Thank you for your review and comments."], "labels": ["rebuttal_social"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 272, "sentences": ["We thank Reviewer 3 for their comments.", "Reviewer 3 points out the strong state-of-the-art performance of our approach as a strength and mentions prior knowledge (our use of RAM state information) as a minor weakness.", "To clarify, in our experiments, we outperform previous non-demonstration state-of-the-art approaches that use a comparable amount of prior knowledge.", "We discuss our usage of prior knowledge in greater detail in the section titled \u201cPrior Knowledge\u201d in our response to Reviewer 2."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 273, "sentences": ["Thank you for your thorough and helpful review.", "We also believe that the criteria we identified will be useful for others in narrowing the search for functions that approximate the generalization error of NNs in realistic settings with no access to the true data distribution.", "Concerns regarding overfitting and uncertainty estimation: Given your suggestion, we performed 10-fold cross validation in all tasks and found high quality results and cross-fold consistency.", "We now report updated cross-val for all results in section 6 including figures 3,4 and in the newly-added figure 5.", "We believe that this addresses both the overfitting concern and the uncertainty estimation concern.", "Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.", "Regarding the envelope function (equation 5): This form of function is a simple case of the (complex) rational function family (simple pole at $\\eta$, simple zero at the origin in this case).", "This family arises naturally in transitory systems in control theory and electrical engineering, e.g., when considering the frequency response of systems.", "It captures naturally powerlaw transitions.", "With that said, as we stress in the end of section 5, the particular choice of envelope is merely a convenience one and there may be other such functions / refinements.", "We leave further exploration of this aspect to future work.", "We have fixed the misspelling in \u201cdifferentiable\u201d.", "Thanks for pointing this out."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_done", "rebuttal_concede-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 274, "sentences": ["Thanks for your comments! First, we have to clarify some misunderstandings.", ">>> it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1)", "BadGAN has already theoretically proved that complement data are helpful for semi-supervised learning.", "In this paper, we demonstrate", "that,  using our unseen data, the proofs in badGAN still can be satisfied but in a more concise way.", "Therefore, compared to badGAN that requires extra PixelCNN, DSGAN saves more computational memory and is time-efficienct.", ">>> It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.", "In Novelty detection, we use the reconstruction error as a criterion to determine whether an image comes from seen class or unseen class.", "It is expected that images from the seen classes should be reconstructed better than those reconstructed from unseen classes.", "However, VAE cannot force the unseen classes with high reconstructed error.", "So, we combine DSGAN with VAE to deal with this issue.", "Due to the above reason, it is expected that \"our sampled reconstruction results are not good as VAE\".", "Note that the seen class, car, still can be reconstructed well by our method in Fig 8 (at the last row).", "The quantitative results in Table 3 further validate our approach.", ">>> I would also expect more ablation studies about how to pick p_{\\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.", "In fact, how to design $p_{\\hat{d}}$ depends on applications  instead of datasets, as described in Sec. 4.1 and Sec. 4.2.", "Please note that, in Section 5.2.1, we used the same $p_{\\bar{d}}$ for ALL datasets.", "We also want to clarify the datasets used in our experiments.", "In semi-supervised learning, we follow our competitors to conduct experiments on MNIST, SVHN and CIFAR10.", "In novelty detection, our method is evaluated on CIFAR10, which is also common in this application.", "Furthermore, we also add additional experiments about generating complement data in CelebA, which is a more complex dataset.", "We can see from Fig. 10 (Appendix G) that DSGAN can create complement data for complicate images well."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 275, "sentences": ["We thank the reviewer for the comments and appreciation, and would like to answer the reviewer\u2019s questions as follows:", "Q1:The authors claim that the LM optimization in BA-Net is memory inefficient and may lead to non-optimal solutions.", "It\u2019s not clear to me that the proposed method can guarantee optimality any better.", "It\u2019s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.", "A1: Thanks for pointing this out and sorry for the confusion!", "Here we don\u2019t mean that our method can fix the optimality problem in any way.", "We wish to provide some of our analysis of the limitation of BA-Net, and hope our method could provide complementary perspectives to rethink the problem and mitigate the non-optimal issue in terms of performance with more ML component.", "In terms of number of iterations, our method does not have a restriction, since our iteration happens outside the neural network and acts as an incremental improvement.", "In contrast, BA_Net\u2019s iteration is part of the LM optimization and it is inside the network.", "Thus if it unrolls more iteration steps, the memory cost will increase linearly.", "We have updated the paper for this.", "Q2: Show the test time behavior of the network when it is run with more iterations than it is trained with (say 10 or 20)", "A2: Thanks for the suggestion! We added Table 4 in Appendix C that shows performance of the network with more iterations(from 2 to 20).", "Q3:It\u2019s not made entirely clear whether the training back propagates through the update/construction of the pose and depth cost volumes.", "A3: Gradients can back-propagate through cost volumes, and cost-volume construction does not affect any trainable parameters.", "We updated this point in the revised version.", "Q4: In equation 5, \u201cx\u201d should be \u201ci\u201d.", "A4: Thanks for pointing out that! We have fixed the typo."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 276, "sentences": ["We thank the reviewer for valuable feedback.", "\u201cthe method they propose offers very little that is new when compared to e.g. Vaswani\u201d", "While the final representation of Time2Vec resembles that of positional encoding, the motivation behind Time2Vec is completely different than that of positional encoding.", "The new things offered by Time2Vec compared to positional encoding and other previous work include:", "- Instead of using time as a scalar feature similar to other features (which as the reviewer also pointed out is a naive way of handling time), we identify the characteristics that differentiate \u201ctime\u201d from other features and propose a representation that enables exploiting those characteristics.", "Note that using time as a scalar feature similar to other features is currently the prevalent choice (see the references in the last paragraph of section 2).", "- Obviating the need for hand-crafting functions of time by instead enabling these functions to be learned from data, and backing up the representation theoretically as, according to Fourier sine series, it can approximate any function in a given interval (see the last paragraph of our response to reviewer 5).", "- Providing a comprehensive set of experiments showing the merit of Time2Vec for time-series prediction problems where time is an important feature.", "This includes, among other things, results for modeling periodic behaviors of signals which is not a goal in positional encoding.", "Although our representation resembles positional encoding on the surface, it has not been clear in the time-series community if/how/why positional encoding can be used to replace hand-crafted functions of time, and there has been no empirical evidence to show its merit.", "\u201cWeak baseline\u201d", "Our goal is to propose a representation of time that can be used instead of merely a float notion of time (as opposed to beating the state-of-the-art on a particular dataset)", ".", "Therefore, all our comparisons are head-to-head comparisons between a model with and without Time2Vec.", "This includes LSTM+T vs LSTM+Time2Vec, TimeLSTM1 vs TimeLSTM1 + Time2Vec, and TimeLSTM3 vs TimeLSTM3 + Time2Vec.", "It would not be sensible to, e.g., compare LSTM+Time2Vec to TimeLSTM3 (or some other model) because the results of such an experiment do not provide evidence towards Time2Vec being useful or useless.", "Specific comments:", "We will clarify the sentence in section 2.", "Except for the hand-crafted experiment, we did not use Time2Vec in non-recurrent architectures.", "In Section 5.2, a \\tau is indeed missing; we\u2019ll fix this.", "If the paper gets accepted, we will expand the experiments with fixed frequencies in the final version.", "In both LSTM+T and LSTM+Time2Vec, for Event-MNIST \\tau is a feature between [0, 783], for NT-DIGITS and SOF \\tau corresponds to the Unix timestamps when events occurred, and in LastFM and CiteULike \\tau corresponds to the delta between the current and previous event."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_answer_label"]}
{"abstract_id": 277, "sentences": ["Thank you for the insightful comments.", "1. It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done.", "Is there any hope this could be applied to problems like 3D imaging data or videos?", "---", "Regarding the concerns on method scalability.", "The current bottleneck of our approach is processing of all feature maps pixel-wise.", "We see potential of scaling our approach by operating on superpixels, or NxN patches, instead of processing all pixels individually."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 278, "sentences": ["1. We first want to point out the main contributions of the paper.", "First, we address the catastrophic forgetting problem in continual learning.", "Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations.", "Hereby we extend the idea of [2] to generative networks.", "We highlight the differences to DGR [3] in the Sec. 2 of our work.", "2.", "Equation (5) and (6) are taken from [2] one to one.", "Equations (3) and (4) are adopted from [2]: equation (3) describes the annealing of the parameter s, we anneal it globally over the course of epochs, whereas [2] anneal it for each epoch over the number of batches; equation (4) is a simplified version of the one used by [2].", "3. To avoid confusion of the proposed method to utilize techniques of DGR[3] in order to prevent forgetting in the G, we kindly ask the reviewer to refer to our response (2) to the Reviewer 1.", "In the proposed work we adopt the generative replay not in order to avoid storing previous samples, but in order to prevent forgetting in the discriminator (which is used as a final classification model).", "Data synthesized by the generator is replayed for to the discriminator during the training of the subsequent tasks.", "There is no replay applied to the generator network.", "In order to avoid storing previous data, we utilize parameter level attention mechanism similar to HAT [2].", "Concerning the time comparison, there is no reason why our approach should be less time efficient then DGR based approaches [1, 3] as our method does not require retraining the generator from scratch at each time step.", "4. Why our method does not outperform joint training on SVHN?", "Using generated samples accommodates for better performance then joint training is the case of tasks of relatively low complexity such as MNIST.", "Indeed, such a result has been shown in other works, e.g. [1].", "As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with a steady quality of the generated samples.", "Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples.", "Thus, this effect can not be observed neither in the SVHN not the CIFAR10 benchmarks.", "5. Grammar mistakes and typos.", "This will be fixed in the updated version of the paper.", "6. No guarantee to work for any task or scenario.", "As pointed out by the reviewer and is true for many machine learning method, there is no guarantee that the proposed method will work for any task or scenario.", "[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.", "[2] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.", "URL http://arxiv.org/abs/1801.01423.", "[3] H. Shin, J. K. Lee, J. Kim, and J. Kim.", "Continual learning with deep generative replay.", "In", "Advances in Neural Information Processing Systems, pages 2990\u20132999, 2017."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 279, "sentences": ["Thank you for your comments and questions; we have incorporated these in the revision and respond to your questions below.", "Q1: Why is the color distribution generated using a subset of pixels?", "A1: We use a random subset of pixels simply for computational feasibility.", "Each distribution is compiled over 1000  images, and drawing a distribution over all pixels per image increases that number by 256^2, causing the computation to be very slow.", "In contrast for the remaining operations, we measure one statistic per image based on the bounding box, which is fast.", "Q2: What are the classes in the bottom right of the transformation limitation / data variability plots?", "A2: The classes in the bottom right corner of the plots are wooden spoon (shift x), cleaver (shift y), and computer keyboard (zoom).", "These classes are more difficult for BigGAN to model accurately, and they deform easily or become unrecognizable under alpha transformations, which may prevent the object detector from reliably detecting them.", "Q3: What are the results of manipulations in Stylegan z latent space?", "A3: We experimented with manipulations in the Stylegan z space \u2014 in general the effects of these transformations are weaker, and may entangle other transformations along with the target transformation.", "For example, when recoloring a car using a walk in z, it may inadvertently also rotate or zoom the car.", "On the other hand, in the w latent space we are better able to change the desired attribute without other side effects.", "We have added a new qualitative figure (Fig. 28) in the appendix illustrating these differences.", "Minor Flaws: Thank you for your careful review in catching these mistakes.", "We have updated the typos in the revision.", "In Fig 2 we optimize for a z which approximates a shifted version of the original image x.", "Hence, the G(z+\\alpha w) image does not exactly match the original image G(z) or the shifted edit(G(z), \\alpha), but is intended to approximate the shifted image."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 280, "sentences": ["Thank you for your thoughtful review.", "We have updated notations in Equations 1 and 2.", "The expectations are now taken over random variables (A and B) and the function takes particular values (a and b) of these random variables.", "Regarding your comment about increasing bias and reducing variance, we did observe that the quality of the InfoWord representations is relatively stable across different runs in our experiments (as evaluated by performance on downstream tasks). Could you please clarify a bit more whether this is what you are asking?"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 281, "sentences": ["We thank you for acknowledging our findings to be useful to shed more light on the inner workings of ReLU-networks.", "We respond to your raised points below:", "---------", "- Q: Algorithm applied layer-by-layer:", "As correctly observed, the application of our algorithm to classify the preimage of one data point of one ReLU layer does not easily translate to more than one layer.", "On the one hand, as pointed out, as soon as the preimage is no longer only a point itself it is no longer applicable.", "On the other hand it is a first step towards a multilayer analysis and allows a localized layer-by-layer analysis for the first time.", "-> For more on this we refer to the newly added Section \u201cScope\u201d in the revision.", "--------", "- Q: Applicable to CNNs:", "It is true that our analysis is quite general considering MLPs and not specifically CNNs and indeed we find it very likely that there are stronger results possible for CNNs than the ones we presented.", "-> Added a discussion on CNNs in the new \u201cScope\u201d Section in the revision", "------------", "- Q: Relation to Carlsson et al. (2017):", "While the work of Carlsson et al. (2017) rather focus on a general analysis on the shape of preimages of activities at arbitrary levels and gives a first geometrical view as a piecewise linear manifold, we present in our work an in-depth understanding for preimages and the inverse mapping of ReLU networks:", "1) We perform a qualitative analysis for the preimages and give computable conditions when the inverse image of an output is finite, infinite or a single point by performing an intuitive mathematical derivation.", "2) We analyze the stability of the inverse mapping by investigating the singular values of the linearization of the network and confirm our theoretical results by numerical experiments.", "---------", "We therefore think that our work can be seen as a significantly different approach to the one presented by Carlsson et al. (2017).", "We thank the reviewer for the helpful comments and would appreciate further discussions."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 282, "sentences": ["1. We believe that the transferability of SECRET is due to two major aspects: 1) that we keep representations for the credit assignment separate from those for the RL task and 2) that we use a self-attentional architecture, which was shown to transfer in settings other than RL.", "Better credit assignment is desirable and should arguably lead to better transfer results in the case of SECRET.", "Nevertheless, it is not necessarily true for other credit assignment methods available because they are designed for the online setting and intricately coupled with an RL agent.", "The focus of the paper being on transfer, we proposed a transfer method relying on credit assignment.", "In our opinion, comparing its credit assignment capabilities to other existing methods is outside of the scope of the paper.", "2. We included the results of varying the window size in the new Appendix B.1.", "Briefly, with bigger windows, there is less partial observability, and the attention no longer matches the trigger.", "Please see the new appendix for more details.", "3. Relational Deep RL ([1]) uses spatial self-attention to infer and leverage relations between \"objects\" (pixel representations).", "Crucially, it does not make use of the sequential aspect of the RL task.", "Instead, SECRET relies on temporal credit assignment, which could be presented as a form of temporal relations (as dictated by the reward function).", "Those are very different approaches to handling relations (if SECRET can be deemed as relational).", "We think it would indeed be an interesting research direction to combine both spatial and temporal aspects for credit assignment or relational reasoning.", "4. There are two different aspects here: 1) the reward model could be trained on very few trajectories in the source domain, or 2) it could be applied on very few trajectories to build the potential function in the target domain.", "For 1), in practice, we only redistribute the nonzero rewards that were successfully predicted by the reward model, so insufficient prediction capabilities are not a problem.", "We added a sentence in the main text to mention the fact that we consider correctly predicted nonzero rewards.", "If the model does not manage to predict nonzero rewards, then SECRET falls back to the Vanilla RL case.", "In the worst case scenario, SECRET could predict a small proportion of the nonzero rewards and assign wrong credit, which could lead to a slowed down procedure.", "For 2), the potential function used in SECRET relies on trajectories with nonzero rewards.", "In the worst case scenario, the potential function could not reflect accurately the structure of the MDP and lead to a slowed down learning procedure.", "We now include two additional experiments in Appendix B.3 that explore both scenarios.", "We show that with a small number of trajectories, either in the source or the target domain, the performance of the agent does not drop too much.", "5. The samples generated in the target domain are not included in the number of episodes reported in the paper.", "While debatable, our motivation to do so is that we use the same fixed policy we used in the source domain to generate those trajectories.", "Note that there is no learning procedure involved during the collection of the target samples.", "6. Maybe a follow-up to consider for the coffee test is to adapt from using a coffee-brewing machine to making it from scratch :)", "[1] Zambaldi V., Raposo D., Santoro A., Bapst V., Li Y., Babuschkin I., Tuyls K., Reichert D., Lillicrap T., Lockhart E., Shanahan M., Langston V., Pascanu R., Botvinick M., Vinyals O., Battaglia P. - Deep Reinforcement Learning with Relational Inductive Biases. ICLR 2019."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 283, "sentences": ["We thank you for your thorough review, which has undoubtedly helped improve the paper.", "First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.", "For more details, please see the comment above entitled: \u201cRelaxing Assumption (H2)\u201d.", "Nevertheless, we wish to emphasize that even under Assumption (H2), learning can still fail.", "Fig 2. Left and Section 3.3 show that any initialization in the top left red region will lead (after a finite number of updates) to a confidence of 0.5 on the corresponding class.", "The network does not provide correct classification at the end of training even though it does at the beginning.", "Here are responses to your other concerns:", "- Indeed, our intent in the statement of Theorem 3.2 was to describe the scaling of the solution with respect to those two quantities, but it can be misinterpreted. We have clarified it in the new version of the paper.", "- We have run that experiment and included it in Fig 3. Right among our other recent findings.", "- Corrected in the new version.", "- We have added a line in the last paragraph of Section 4 stating that for the Hinge loss, u(t) grows exponentially in t.", "- We agree that the observed phenomenon can appear in other machine learning methods and is not specific to gradient descent.", "However, in the case of deep neural networks, it is the prevalence of certain gradient directions that determine the final classifier.", "Our results suggests that models converge to solutions that privilege the \u201csimplest\u201d explanation, in an Occam\u2019s razor fashion, which provides an explanation to the \u201cimplicit generalization\u201d of deep nets characterized by Zhang et al.", "Our Kaggle experiment\u2019s aim is to emphasize potential failure modes of current architectures/algorithms (one can think of a self-driving car trained on a road with clear lane markings and operating on a road without such markings).", "The ability to transfer knowledge to test sets coming from a different distribution is key to building more intelligent and robust systems."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 284, "sentences": ["Thank you for your comments, we will be responding with specific comments to AnonReviewer4 here, and more general comments to the reviewer above.", "R4: \u201cWhat\u2019s less clear to me is what kinds of combinatorial optimization problems can be mapped onto the RBM *training* problem\u201d", "The combination method we propose here can be applied to RBMs that are calculated by directly setting weights and by training individual sub units.", "We acknowledge there are pros and cons to both approaches; directly calculating the weights gives guarantees on probabilities and mixing rates, while training can produce a more compact, data, and computationally efficient model.", "Some algorithms will be more amenable to training, while others more amenable to directly calculating and setting weights, so we believe that this should be addressed on an algorithm by algorithm basis.", "We try to present one possible algorithm and a possible combination mechanism that we believe could work for others.", "R4: \u201cThe paper states that the problem of training \"large modules\" is \"equivalent to solving the optimization problem\", but does not explain how.\u201d", "Training a full module to solve an optimization problem in the context presented here involves supplying samples from a large portion of the subspace that we are trying to model.", "Based on the results we have seen, we only achieve good performance once we have samples from >30% of the subspace (depending on the problem).", "In addition, the RBMs perform significantly better when trained on the full space we are trying to model.", "We view this as the RBM creating an associative memory where it \u201cmemorizes\u201d examples and recalls them afterward, and do not view this as a data and computationally efficient method of solving these problems.", "R4: An example is presented in Figure 3 but is not expanded upon in the main text.", "I\u2019d like the authors to validate my understanding:", "An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder\u2019s inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin].", "After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling.\u201d", "Yes, your understanding is correct.", "We train on the joint density over inputs and outputs, and solving a problem amounts to clamping (conditioning) a subset of the units and sampling the remaining units via Gibbs Sampling.", "We have made an effort in the revision to make sure that this is more clear.", "In the case of solving factorization problem, we clamp some of the visible units to the integer we are trying to factor, and use gibbs sampling to get statistics for the remaining units conditioned on the output number.", "R4: I\u2019m also confused by the presentation of the results.", "For instance, I don\u2019t know what \"log\", \"FA1\", \"FA2\", etc. refer to in Figure 6.", "Also, Figure 6 is referenced in the text in the context of binary multiplication (\"[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6\"), but presents results for addition and factorization only.", "We have presented results for addition and factorization in the main body of the paper, but refer to readers of the paper to the appendix where we have included a larger set of results.", "The results were omitted from the main body of the paper for the sake of brevity.", "The naming of units as \u201clog\u201d \u201cFA1\u201d, \u201cFA2\u201d, etc. are meant to represent the size of the base unit that was merged to create this larger unit, \u201clog\u201d referring to logical units (AND, XOR, etc.), \u201cFA1\u201d being 1 bit full adder, \u201cFA2\u201d being a 2 bit full adder, etc. we have made this clear in the figure caption.", "R4: The way I see it, implementing reversible boolean logic circuits using RBMs is an artificial problem, and the key idea of the paper -- which I find interesting -- is that in some cases it appears to be possible to combine RBMs trained for sub-problems into larger RBMs without needing to fine-tune the model.", "We also agree that there may be other applications to this type of merging of RBMs without further training, and we are working to look at those in greater detail.", "Invertible Boolean Logic provides a good test bed for this idea, and as explained above, we do believe it has a very intimate relationship with Boolean Satisfiability problems and other combinatorial optimization problems."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label"]}
{"abstract_id": 285, "sentences": ["It's true that there are many activation functions that the result doesn't apply to, and in fact isn't true for.", "The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general."], "labels": ["rebuttal_concede-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 286, "sentences": ["We thank the reviewer and its careful reading of our paper.", "Concerning point 6: Indeed, we acknowledge that this type of paper may be unconventional for the audience at ICLR.", "But we strongly", "believe that scientific knowledge on biological vision is essential to work out the models that will shape DL in the future.", "Thus, we fully understand the rating given by the reviewer and would like to suggest that our revision addresses the main comment and show that it is relevant for a presentation at ICLR.", "First, we have extended the results by using the useful suggestions of AnonReviewer3 (point 3):", "As suggested by the reviewer we have tested how the convergence was modified by changing the number of neurons.", "By comparing different numbers of neurons we could re-draw the same figures for the convergence of the algorithm as in our original figures.", "In addition, we have also checked that this result will hold on a range of sparsity levels.", "In particular, we found that in general, increasing the l0_sparseness parameter, the convergence took progressively longer.", "Importantly, we could see that in both cases, this did not depend on the kind of homeostasis heuristic chosen, proving the generality of our results.", "This is shown in the supplementary material that we have added to our revision (section \"Testing different number of neurons and sparsity\") .", "This useful extension proves the originality of our work as highlighted in point 4, and the generality of these results compared to the parameters of the network.", "Second, the comment made in point 5 is essential: figures 1 and 3  in our first revision where not showing appropriately the qualitative improvement which is achieved in the resulting filters.", "Indeed, we were showing 18 atoms chosen at random from the 441 filters from the dictionary.", "We initially thought that this \"blind\" shuffling would be a fair representation of the data, but as revealed by point 5, this was not true.", "We have now changed the strategy by now showing  \"the upper and lower row respectively show the least and most probably selected atoms.\"  (see captions of figures 1 and 3).", "This now shows clearly the qualitative improvement in using a proper homeostasis and in particular that using the $\\ell_2$ normalization leads to the emergence of filters which are aberrant (too or not enough selective).", "In particular, we now show quantitatively the probability of choice of each atom - showing that most active filters are used twice more as least active ones.", "Finally, we have made an extensive pass on the manuscript to take into account the different points  and make sure that this approach derived from biological vision is relevant for the audience at ICLR.", "As such, we believe this major change in the way we present the work, both in the quality of the resulting filters and in the generality of the results, have significantly changed the scope of our work to justify its acceptance to ICLR.", "We thank again the reviewer for these very useful contributions to our work."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 287, "sentences": ["Thank you for your constructive feedback!", "Main comment 1:", "Absolutely, this is a difficult issue: there is no perfect middle ground where it is possible to study the contributions in their simplest instantiations while at the same time verifying their practical effectiveness.", "We have opted to place the bulk of our emphasis on a realistic scenario (Atari with a Rainbow-like learning agent) that practitioners of Deep RL would find relevant.", "To isolate effects, our experimental section includes many variants and ablations, allowing us to state with confidence that modulating behaviour using the bandit improves performance compared to uniform (no bandit) or untuned (fixed modulation) baselines.", "And this is separately validated across multiple classes of modulations.", "But indeed, as you point out, we cannot guarantee that the improvements we see are purely due to exploration.", "At the same time, it\u2019s worth recognising that, by design, the method proposed will try to cater to the underlying learning algorithm and would ideally generate samples that would benefit the underlying learning procedure.", "We will highlight this ambiguity in the revised paper.", "Main comment 2:", "Sorry, this was not very clear: The performance outcome for each variant is measured on multiple independent runs (seeds).", "All outcomes are then jointly ranked, and the corresponding ranks are averaged across seeds.", "Finally, these averaged ranks are normalized to fall between 0 and 1.", "A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.", "Figure 4 then further aggregates these normalized ranks across 15 Atari games.", "Note that these joining rankings are done separately per subplot (ie modulation class).", "Thus the reason that no fixed arm is always good does not depend on the inter-seed variability as much as on the fact that the best arm differs in different games.", "We will clarify this in the caption too.", "The bandit does not generally do better than the best fixed arm in hindsight -- in general, this would still need to be identified --", "but it is not far off, and it handily outperforms untuned arms, allowing us to remove some of the hyper-parameter tuning burden.", "Additional question 1:", "We acknowledge that our presentation focused maybe more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).", "The updated paper will change the emphasis, and clarify that a closer, more faithful, learning progress proxy remains future work.", "We will also clarify that the little phrase \u201cAfter initial experimentation, we opted for the simple proxy\u2026\u201d implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.", "Additional question 2:", "Of course, even an ideal metric LP(z) would remain a local quantity, and pursuing it would not guarantee the maximal final performance -- but it is valuable if local optima are not the prime concern.", "Performance plateaus are a nuisance in general, and within the simple space of modulations we consider, there is no magic bullet to escape them.", "However, our approach does the next best thing: when performance becomes an uninformative (ie on a plateau), it encourages maximal diversity of behaviour (tending toward uniform probabilities over z), with the hope that some modulation gets lucky -- and then as soon as that happens, very quickly focusing on that modulation to repeat the lucky episode until learning is progressing again.", "Additional question 3:", "Indeed, thank you. We have updated the text to place more emphasis on this contribution.", "Additional question 4:", "The way we would summarize these results is that the bandit is more or less on par with the *best* fixed exploration policy, and so the added complexity is justified by reducing the need to tune exploration. Is this what you meant?", "We think we could address all your concerns, but please let us know if you have further questions, the discussion period lasts until the end of the week!"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 288, "sentences": ["[Q] The paper proposes using the nested CRP as a clustering model rather than a topic model.", "The clustering is on the latent vector input into a neural network for generating the observation.", "A variational approach is derived.", "The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.", "[A1] Dear Reviewer 2, thank you for the thoughtful review.", "As the reviewer mentioned, we exploited the nested CRP prior to the path selection process.", "For performing a hierarchical density estimation task in embedding space, we additionally designed a hierarchical-versioned Gaussian mixture model prior with the nested CRP prior.", "[Q] A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.", "From the generative model, it seems every data point has its own Dirichlet vector on levels.", "For topic models, this makes sense since that vector is then drawn from multiple times (once per word) from a Discrete, so there's a distribution to actually learn.", "My understanding is that this isn't being done here.", "[A1] Thank you for the very constructive comments.", "In fact, we intended to model the level proportion as shown in the third part of our generative process on page 4.", "Often, for grouped-data, the level proportion (or topic proportion) is modeled as a group-specific variable.", "Under our non-grouped data setting, for example, two following approaches are possible: 1) as the reviewer mentioned, globally define a level proportion once, take multiple level samplings for each data, and 2) as our modeling, locally define the data-specific level proportion, followed by sampling the level (this is actually auxiliary variable for specifying the Gaussian distribution).", "The reason we chose the latter approach is for modeling more flexible prior.", "The Gaussian mixture distributions exist separately for each level, and we assume the generative process that the mixing coefficient for the level would be different for each data.", "Please consider that the data-instance we handled is a high-dimensional data of a document/an image rather than a word/a pixel.", "The hierarchically Gaussian mixture distributions are learned for different levels, and here assuming a common level proportion for all data forcefully limits the expressive power of the model.", "Also, for preventing the overfitting, we placed the common prior, Dirichlet(\\alpha), on the data-specific level proportion, which can be considered as one of the regularization terms.", "[A2] Also, I would like to explain the reviewer\u2019s comment as the formulae.", "The prior we suggested is this: \\sum_{\\zeta, l} nCRP(\\zeta_n) * \\eta_{nl} * Normal(-) (\uf0e0 please refer to the Figure 3(a).).", "Moreover, the point that the reviewer pointed out is on \\eta_{nl}, i.e., \u2018the reason for designing \\eta as \\eta_{nl}, why \\eta is data-specific variable?", "\u2019", ".", "There are similar works, which previously published [1-3].", "They designed data-specific mixing coefficients of Gaussian mixture models, for improving more flexibility like ours.", "[1] Ban, Zhihua, Jianguo Liu, and Li Cao. \"Superpixel Segmentation Using Gaussian Mixture Model.\" IEEE Transactions on Image Processing 27.8 (2018): 4105-4117.", "[2] Zhang, Hui, et al. \"Automatic Visual Detection System of Railway Surface Defects With Curvature Filter and Improved Gaussian Mixture Model.\" IEEE Transactions on Instrumentation and Measurement 67.7 (2018): 1593-1608.", "[3] Ji, Zexuan, et al. \"A spatially constrained generative asymmetric Gaussian mixture model for image segmentation.\" Journal of Visual Communication and Image Representation 40 (2016): 611-626.", "Under the newly proposed Gaussian mixture models from the above papers, the cluster assignment of data is sampled once from the data-specific mixing coefficient, where there is no theoretical problem as a fully Bayesian formalization.", "[A3] We were very impressed with the mathematical detail of the reviewer\u2019s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.", "Best regards,"], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_answer", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label"]}
{"abstract_id": 289, "sentences": ["Thanks for your comments!", ">>> Experimental settings are clear, however, what makes me confused is that the construction for $p_{\\bar{d}}$ is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images.", "In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.", "In this experiment, we generate the color images of size 64 $\\times$ 64.", "Similar to 1/7 experiments on the MNIST dataset, we let $p_{\\bar{d}}$ be the distribution of face images with glasses and without glasses, and let $p_{d}$ be images without glasses.", "We sample 10000 images with glasses and 10000 images without glasses from CelebA, and we set $\\alpha$ to 0.5.", "In order to verify the generated image quality of DSGAN, we also train a GAN for comparison.", "GAN is trained with the same amount of training images (but only using face images with glasses since GAN is to learn the distribution of training data).", "In other words, we assume GAN can use complement data as training data directly.", "On the contrary, DSGAN only uses complement data indirectly (the difference between $p_{\\bar{d}}$ and $p_d$).", "Figure 10 in Appendix G shows generated images and FID for both methods.", "We can see that our DSGAN can generate images with glasses from the given $p_d$ and $p_{\\bar{d}}$, and the FID of DSGAN are comparable to that of GAN.", "The experiment validates that DSGAN still works well to create complement data for complicate images.", ">>> The model seems to be sensitive to the hyper-parameter $\\alpha$, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?", "Since the optimal $\\alpha$ of generating \"unseen\" data in DSGAN depends on the degree of overlap between $p_{\\bar{d}}$ and $p_d$, it might need to be fine-tuned for different datasets.", "However, in our experiments, we set $\\alpha$ to $0.8$ in most cases.", "Theorem 1 illustrates $\\alpha$ should be expected to be as large as possible if both network G and D have infinite capacity.", "Though the networks never have the infinite capacity in real applications, a general rule is to pick a large $\\alpha$ and force the complement data to be far from p_d, which is similar to the ablation studies in Sec. 5.1.", "According to our empirical observations, $\\alpha = 0.8$ is the good choice for all datasets.", "Table 11 in Sec. F of Appendix shows the experimental results of how $\\alpha$ affects the performances.", "We use different $\\alpha$ values in the MNIST, SVHN and CIFAR10 dataset, respectively.", "One can see that we achieve the best performances at $\\alpha = 0.8$."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 290, "sentences": ["Thank you for the valuable remarks.", "We have tested most of the concerns in points 1. - 4. during our experiments.", "We however could not provide full-extent analysis due to the limited length of the paper.", "Let us respond to each of the points separately below.", "1. How does varying the number of nearest neighbors change the network behavior?", "---", "We observe that using small k (~5) doesn't always provide enough information to perform the denoising and the network is therefore less robust against adversarial examples.", "On the other hand, having k too high (~20) yields too much regularization and the network original performance decreases more significantly.", "In our experiments, we have found k=10 to be a reasonable compromise.", "2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?", "---", "We refer the reviewer to the Section 3 and Section 4.1. of our paper where this is addressed in detail.", "3. Does just simple filtering of the feature map, say, by local averaging, perform equally well?", "---", "It does not.", "We have tried simple smoothing of the feature maps and it not only does not make the network robust against adversarial attacks, but also regularizes the original network too much which results in significant loss in classification accuracy.", "Moreover, local averaging uses the information from the corrupted image itself to filter the feature map, which could even further amplify the noise.", "4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?", "---", "This is of-course true.", "Obviously, selecting very poor nearest neighbors will definitely break the method as the newly created feature map will not express the original information anymore.", "In our paper, we even reason that the adversary often tries to fool the KNN algorithm directly, as we mention at the end of Section 4.3.2.", "Moreover, we believe that our results show when do things start to \"break down\".", "We explicitly mention that an unbounded attack will always fool the network.", "Also, our figures in the main text and tables in the supplementary material show that with increasing magnitude of the perturbation, things start to \"break down\".", "5. Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training.", "It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.", "---", "We provide an evaluation below as well as add an additional section with the results in the supplementary material.", "We have compared our approach to adversarial training method using the code provided by Madry etal.", "https://github.com/MadryLab/cifar10_challenge", ".", "The ResNet-32 baseline model provided in Tensorflow repository (the same we use as CNN baseline in our paper) was trained using the script provided in the cifar10_challenge repository above.", "We have used two training configurations producing two baseline models1 - the default one provided by the repository (ResNet-32 CNN A) and then the same one as in our paper (ResNet-32 CNN B).", "PeerNet was trained traditionally without adversarial training.", "The attack was left as defined by the repository by Madry etal.", "ResNet-32 CNN A: original_acc = 78.86% | adversarial_acc = 45.47%", "ResNet-32 CNN B: original_acc = 75.59% | adversarial_acc = 42.53%", "PeerNet:         original_acc = 77.44% | adversarial_acc = 64.76%", "Results show superiority of PeerNet on this benchmark.", "PeerNet was trained without considering any specific attacks and still outperforms ResNet-32 CNN, which was adversarially trained using this specific attack, by margin of 20%."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_none", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 291, "sentences": ["Thank you for your comments, we will be responding with specific comments to AnonReviewer3 here, and more general comments to the reviewer above.", "R2: For instance, in the introduced approach, only an example of combination is provided in Figure 1.", "It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.", "As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).", "We used Figure 1 and the combination matrices to show what exactly is happening when we combine the models, and how the models mathematically combine.", "In our revision we have made an effort to outline this in greater detail.", "R2: Overall, the paper seems to be a report consisting of a few interesting observations rather than introducing a solid and novel contribution with theoretical guarantees.", "In regards to the lack of theoretical guarantees, we have shown that the equilibrium distribution is what we expect it to be, and mathematically have shown that the final distribution of interest has the mode we expect it to.", "It has been shown in many texts that Gibbs Sampling converges to this equilibrium distribution at a geometric rate in Markov Random Fields.", "Finding the exact convergence rate involves calculation of the eigenstructure of the markov chain transition matrix, which is in general computationally intractable for RBMs of moderate size [1].", "Given this, we have added an extra theorem to show how the upper bounds on convergence rate changes as we merge RBMs, this can be seen in Section 3.1 on \u201cConvergence Rate and MCMC\u201d.", "We show that the rate of convergence of the RBM is geometric in the number of sampling steps, and that the combined RBM will have a convergence rate bounded by the sum of the convergence rates of the individual RBMs.", "If we want to have further theoretical guarantees, we have the ability to exactly set model parameters, as mentioned in section 3.2 to get the exact distribution of interest, and to combine those RBMs with directly calculated parameters.", "As mentioned in that section, this is not a data efficient, or computationally efficient method which is why we chose to not pursue it.", "[1] Pierre. Bremaud. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues, volume 1.Springer New York, 1999. ISBN 9781441931313."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 292, "sentences": ["Thank you for the review.", "First, we want to mention that DeepTwist is proposed not only for weight pruning, but also for other compression techniques, such as quantization and low-rank approximation, as we discussed in Section 4.2 and 4.3", "After weight pruning is performed and zero weights are removed, we usually obtain a sparse matrix to represent non-zero weights.", "There are lots of existing sparse matrix computation libraries to support SpMV (sparse matrix-vector multiplication) and so on.", "If a matrix is highly sparse, then we would reduce memory footprint and amount of computations (for example, we can skip zero weights during computation) significantly.", "There have been extensive studies of efficient hardware implementation after weight pruning, and we want you to refer to the paper \u201cEIE: efficient inference engine on compressed deep neural network\u201d or \u201cDeep compression: compressing deep neural networks with pruning, trained quatization and Huffman coding.\u201d", "In this paper, we have not discussed particular sparse matrix implementation methods which are not our focus in this paper.", "We would greatly appreciate if you can reconsider your decision based on our comments and other methods we also discussed (i.e., quantization and low-rank approximation)."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 293, "sentences": ["Dear AnonReviewer3,", "thank you for your positive review and constructive feedback!", "We agree that the structure of the paper was not optimal and reorganized it along the lines you suggested (thanks for the suggestion!).", "Below we address specific questions.", "\u201cI am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?\u201d", "-> The latter: we compared with the models with the closest match in # of parameters.", "\u201cWhy is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?\u201d", "-> We stated that defining a trade-off between objectives is not necessary (in case you are referring to this statement), which would, e.g., be necessary when one would scalarize objectives by using a weighted sum.", "Rescaling an objective, however, is different as it is independent from other objectives: it only depends on that specific objective and which scale is important to the user and the application.", "For the number of parameters, the log scale is natural to cover a large range of sizes: think of a plot of size vs. performance; in order to see anything for small sizes one would typically put the size on a log scale (and we indeed did, see, e.g., Figures 3 and 4).", "Therefore, it is most natural to also put the number of parameters on a log scale for LEMONADE.", "\u201cIt seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectives-1 dimensional surface with the population of parents. How could scaling be handled?\u201d", "-> We think having 4-6 objectives is a realistic dimensionality for NAS applications, and scaling to significantly more objectives (which would indeed be problematic for our method, but also for multi-objective optimization in general) is typically not necessary.", "In response to this question, to demonstrate this, wee conducted a new experiment with 5 objectives (performance on Cifar 10, performance on Cifar 100, number of parameters, number of multiply-add operations, inference time) to show that LEMONADE can handle these realistic scenarios natively.", "We refer to the updated version of our paper for the results (Appendix 3,\u201cLEMONADE with 5 objectives\u201d), but in a nutshell the results are very positive and qualitatively resemble those for two objectives.", "While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.", "We hope this clarifies your questions. Thanks again for the review!"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 294, "sentences": ["Thanks for your comments!", ">>> Only the 1/7 examples of MNIST dataset are provided in case studies.", "I am wondering for more complicated images, how is the performance?", "In responding to this comment and the first comment of Reviewer #2, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.", "In this experiment, we generate the color images of size 64 $\\times$ 64.", "Similar to 1/7 experiments on the MNIST dataset, we let $p_{\\bar{d}}$ be the distribution of face images with glasses and without glasses, and let $p_{d}$ be images without glasses.", "We sample 10000 images with glasses and 10000 images without glasses from CelebA, and we set $\\alpha$ to 0.5.", "In order to verify the generated image quality of DSGAN, we also train a GAN for comparison.", "GAN is trained with the same amount of training images (but only using face images with glasses since GAN is to learn the distribution of training data).", "In other words, we assume GAN can use complement data as training data directly.", "On the contrary, DSGAN only uses complement data indirectly (the difference between $p_{\\bar{d}}$ and $p_d$).", "Figure 10 in Appendix G shows generated images and FID for both methods.", "We can see that our DSGAN can generate images with glasses from the given $p_d$ and $p_{\\bar{d}}$, and the FID of DSGAN are comparable to that of GAN.", "The experiment validates that DSGAN still works well to create complement data for complicate images."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 295, "sentences": ["Thank you for reviewing our paper.", "We would like to make a quick clarification right away, which we hope will change your assessment.", "All works you cite use non-linear BoF encodings on top of pretrained VGG (or AlexNet) features; the effective patch size of individual features is thus large and will generally encompass the whole object of interest.", "In contrast, our BagNets are constrained to very small image patches (much smaller than the typical object size in ImageNet), use no region proposals (all patches are treated equally) and employ a very simple and transparent average pooling of local features (no non-linear dependence between features and regions).", "That\u2019s why BagNets (1) substantially increase interpretability of the decision making process (see e.g. heatmaps), (2) highlight what features and length-scales are necessary for object recognition and (3) shed light on the classification strategy followed by modern high performance CNNs.", "None of the cited papers addresses any of these contributions.", "PS: We do cite similar approaches in our paper, see first paragraph of related literature. We will add your references there.", "Maybe the following perspective also helps: the works you cite use BoF over larger image regions, but the embeddings for each region are still based on conventional, non-interpretable DNNs (like VGG).", "Our work \"opens this blackbox\" (to use a very stressed term) and provides a way to compute similar region embeddings in a much more interpretable way as a linear BoF over small patches.", "In other words, if the works you cite would use BagNets instead of VGG, they would basically use a \"stacked BoF\" approach: first, small and local patches are combined to yield region embeddings (BagNet), and these region embeddings are used by a second BoF to infer image-level object labels and bounding boxes."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_by-cr", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 296, "sentences": ["> All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.", "Yes, we experiment only with myopic variants of exploration, but (A) our approach is not limited to this initial set of behaviour modulations, and could be extended to trade off between intrinsic and extrinsic motivation, or between model-free and model-based mechanisms; and (B) the variations we consider may not be ideal, but they are the ones most commonly used in domains like Atari.", "> The proposed proxy is simply the empirical episodic return.", "It is not well explained in the paper how this proxy correlates with the Learning progress criteria.", "The proxy seems to encourage selecting modulations that lead to generate most rewarding trajectories.", "How this proxy incentives the agent to explore poorly-understood regions?", "In other terms, how this proxy help to tradeoff between exploration and exploitation ?", "Thank you for this suggestion, we have now clarified this connection in Section 3.", "We acknowledge that f departs from LP in a number of ways.", "First, it does not contain learner-subjective information, but this is partly mitigated through the joint use of with prioritised replay that over-samples high error experience.", "Another potential mechanism by which the episodic return can be indicative of future learning is because an improved policy tends to be preceded by some higher-return episodes -- in general, there is a lag between best-seen performance and reliably reproducing it.", "Second, the fitness is based on absolute returns not differences in returns as suggested by Equation 1; this makes no difference to the relative orderings of z (and the resulting probabilities induced by the bandit), but it has the benefit that the non-stationarity takes a different form: a difference-based metric will appear stationary if the policy performance keeps increasing at a steady rate, but such a policy must be changing significantly to achieve that progress, and therefore the selection mechanism should keep revisiting other modulations.", "In contrast, our absolute fitness naturally has this effect when paired with a non-stationary bandit.", "We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.", ">", "The modulation adaptation problem is framed into non-stationary multi-armed bandit problem but the authors present a heuristic to solve it instead of using provably efficient bandit algorithm such as [...]", "Thank you for the suggestion!", "We had experimented with a few of these variants before designing the proposed adaptation method.", "We have now included such a plot in the paper, comparing our method to UBC and Thompson sampling (Appendix E.3 and Figure 16).", "As you can see from this comparison, the performance of these well-known bandits depends on the game, and it is subject to tuning, which is what we wanted to avoid in the first place.", "In most games our bandit performs significantly better.", "> The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me", "[...]", "The distribution of f(z) does change as a function of the parameter change and thus as a function of time.", "This is precisely the kind of non-stationarity that our adaptive mechanism has to deal", "with", ".", "This is also the reason behind the adaptive window used in this work.", "In a sense, one can see the size of the window as a proxy for the effective time horizon at which things can be seen as stationary in the learning.", "The window over which we integrate evidence is chosen to make the best recommendation; thus every time we deviate too much from the sample distribution captured within it, we consider this as a sign of non-stationarity and shrink the window.", "This is by no means optimal, nor do we claim it is, but it seems to be a reliable enough proxy to outperform candidates that do assume stationarity (as portrayed by the comparison in Figure 16).", "> I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.", "Is it a baseline with the best hyperprameters in hindsight?", "The \u201cfixed reference\u201d is described in Appendix C, and corresponds to the most commonly used settings in the literature.", "We made this clear in the main body of the text.", ">", "From the plots of learning curves in appendix, the proposed methods doesn\u2019t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?", "Yes, we show this in aggregate in Figure 6 (old Figure 5-right): it shows how the bandit is roughly on par with uniform when the modulation set is curated, but the bandit significantly outperforms uniform in the untuned (\u201cextended\u201d) setting.", "We clarified the caption for this too."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 297, "sentences": ["We are grateful for your feedback.", "We hope that the above discussion assuaged the reviewer\u2019s concerns regarding novelty and some unclear details.", "We briefly address the two questions regarding the setup:", "During testing, in the setting with known GT boxes (Sec 4.2), we assume that the 2D instance boxes are given.", "In the detection setting, the 2D instance boxes are the result of the learned detector.", "Given the (detected or known) instance boxes, the union boxes and binary masks can be easily computed - the union box is just the larger box containing both instance boxes, and the mask highlights these instance boxes in the union box.", "Training and Testing  Inference Time on a single GPU (Maxwell Titan X)", "1. Train time: 65 hrs", "2. Test time: 0.55s per image"], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 298, "sentences": ["-----------------------------------------------------------------------", "Q\uff1aA lot of clarity is required on the choice of evaluation metric; for example, choice of distance measure?", "What is the L1 norm applied on?", "A: Answer about Evaluation metrics:", "(1) We want to evaluate if the generated graphs are scale-free graphs in the direct evaluation for dataset scale-free graphs.", "If the degree distribution of generated graphs is the same to the degree distribution of real target graphs, the generated graphs are good.", "(2) There are many classical evaluation metrics focusing on measuring the similarities or distance of two distributions.", "The four metrics in this paper are among the most authoritative and commonly used ones in existing works, e.g., [2][3][4][5].", "Answer about L1 norm:", "(1) L1 norm is applied to the weight adjacent matrix of the graph.", "Our methodology is achieved by a trade-off between L1 loss and adversarial loss (GAN-D).", "Specifically, L1 makes generated graphs share the same rough outline of sparsity pattern like generated graphs, while under this outline, adversarial loss allows them to vary to some degree.", "(2) L1 norm is commonly used in GAN in relevant domains, e.g., in image-translation domain, for example, reference [1] (with 600+ citations) and reference [6] (with 1300+ citations).", "They have done extensive experiments to show the advantage of such a strategy.", "(3) The experiment demonstrates its effectiveness.", "Specifically, the proposed GT-GAN that uses L1 norm outperformed all the other comparison methods shown in Table 2,3 and 4.", "-------[2", "]", "Schieber, T. A., Carpi, L., D\u00edaz-Guilera, A., Pardalos, P. M., Masoller, C., & Ravetti, M. G. (2017).", "Quantification of network structural dissimilarities.", "Nature Communications, 8, 13928.", "-------[3] Bauckhage, C., Kersting, K., & Hadiji, F. (2015, July). Parameterizing the Distance Distribution of Undirected Networks. In UAI (pp. 121-130).", "-------[4] Chiang, S., Cassese, A., Guindani, M., Vannucci, M., Yeh, H. J., Haneef, Z., & Stern, J. M. (2016).", "Time-dependence of graph theory metrics in functional connectivity analysis. NeuroImage, 125, 601-615.", "-------[5] You, J., Ying, R., Ren, X., Hamilton, W. L., & Leskovec, J. (2018). GraphRNN: A Deep Generative Model for Graphs. arXiv preprint arXiv:1802.08773.", "-------[6", "] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.", "-----------------------------------------------------------------------", "Q\uff1aI did not completely follow the arguments towards directed graph deconvolution operators.", "There is lack of clarity and the explanation seems lacking in parts in this particular section; especially since this is the key contribution of this work.", "A: (1) Our decoder is symmetric to the encoder in their architectures.", "The encoder does n-hop edge information aggregation from the input graphs and learns the latent representation of nodes.", "Then, we first decode the node embedding to get the n-hop aggregated information on edges by node-to-edge layer and then we further decode the n-hop aggregated information layer by layer by n-layers back to get the output adjacency matrix.", "(2) Different from image deconvolution, for each hidden channel, we have two filters vertical to each other, i.e., one is a column vector while the other is a row vector.", "To get the nth hop information of edge <i,j>, row filter decodes all the (n+1)-th hop information of outgoing edges of node i and column filter decodes all the (n+1)-th hop information of incoming edges of node j.", "(3) To make our description clearer, we have updated our paper in Section 3.2.2, e.g, by adding \u201cTo get the nth hop information Aij, row filter decodes all the (n+1)-th hop information of outgoing edges of Vi and column filter decodes all the (n+1)-th hop information of incoming edges of Vj.\u201d", "-----------------------------------------------------------------------", "Q: Typo:. The \u201cInf\u201d in Tabel 1", "A: As explained in Section 4.2.4 \u201cResults on Scale-Free Graphs\u201d, the \u201cInf\u201d in Tabel 1 represents the distance more than 1000.", "We really hope that we have explained every confused point clearly and please let us know if there are any other points.", "Thank you once again for your reviews.", "Dear Reviewer:", "Thank you very much for your comments and suggestions.", "We would like to answer your questions in detail as follows:", "-----------------------------------------------------------------------", "Q: The authors claim that their method is applicable for large graphs.", "However, it seems the experiments do not seem to support this.", "A: (1) We did not mention that we handle \u201clarge graph\u201d, but instead we only mention that we handle \u201clarger\u201d graph.", "In the domain of graph generation, currently, the proposed graph generative models can typically only deal with graphs with dozens of nodes or less (except GraphRNN which can scale to 300).", "Compared to them, our model handles relatively \u201clarger graph\u201d (6-10 times larger than most existing methods).", "(2) Translation in graphs is a new topic and we have not found many datasets in very large scale, so we do not test on much larger nodes. But the scalability experiments can still show the superiority of our model compared to others.", "(3) We typically test small-size graphs because most of the comparison methods can only handle small-size graphs.", "-----------------------------------------------------------------------", "Q: It is not clear how the noise is introduced in the graphs. I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.", "A: Thanks for the review comment.", "(1) The noise is introduced by the dropout function in each convolution layer.", "Dropout functions by randomly ignore 50% of neuron\u2019s output of a network in our mode by a uniform distribution.", "(2) The way we add noise", "is well-recognized and commonly-used in generative deep learning models[1].", "The noises added in GANs aim to enable the diversities in the generated graphs to avoid the problem that GANs tend to favor producing same output rather than spreading it evenly over the domain.", "(3) We have shown the analysis of the translation quality against noise in Figures 4 and 5.", "In Figure 5 (see in the supplementary material), each logarithm plot in each column show the power-law trend of each randomly generated graph, which will look linear in such a logarithm plot.", "It can be seen that the generated graphs show the similar randomness pattern as the real graphs.", "Moreover, the larger the graph is (see the graph size of 150), the smaller the randomness is, and the clearer the power-law trend is, which verifies that the translation quality of our method.", "------[1]", "Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.", "-----------------------------------------------------------------------", "Q: It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.", "Do we know how does the connectedness of the input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?", "Towards this, how does the computational complexity scale wrt to the connectedness?", "A: (1) Similar to all the existing graph deep generative learning methods for generic graphs, we do not have additional assumptions on the graphs.", "The domain of graph deep generative learning methods typically do not require to distinguish or preprocess specific topological types of graphs before applying it, no matter it is strongly- or weakly- connected graph, complete graph, planar graph, scale-free graph, or graphs that have other specific patterns.", "This is actually one of the core advantages of deep learning based models where the graph patterns are not extracted or pre-identified manually by the human but automatically discovered by the end-to-end deep models.", "(2) This paper has given the time complexity in the worst case: O(n^2) as shown in 3.4.", "The worst case happens when the graph is a complete graph.", "The time complexity of a strongly-connected graph will not be worse than that.", "Dear Reviewer,", "Thank you very much for your new and previous comments.", "We have revised our paper again in order to address all of them in the paper.", "The modifications are listed as followings:", "1. For graph deconvolution, we have modified and reorganized the content.", "The Section 3.2.2 on \u201cGraph Deconvolution\u201d has been reorganized to two subsections \u201cnode-to-edge deconvolution\u201d and \u201cedge-to-edge deconvolution\u201d.", "We also extended them to make the description on deconvolution operations clearer and more comprehensive.", "2.", "For graph deconvolution, we have also added a new figure and refined the equations\u2019 descriptions.", "Figure 3 is added to describe the mechanism of our proposed deconvolution operators as well as their correlation to the convolution operations.", "Equation 6, Equation 7, and their descriptions have also been revised to make them clearer and concrete.", "Specifically, Figure 3 describes how the node representation and edge representation are respectively decoded by our deconvolution layers, while Equation 6 and Equation 7 describe how to aggregate the decoded information into the final weighted adjacent matrix.", "3. We have referred to all the figures in the body of text.", "4. We have added statements to describe how to introduce random noises in the whole architecture, see in the 2nd paragraph of Section 3.1 in Page 4.", "5. We have added statements of describing the reason to use L1 loss and how L1 loss is applied, please see in the paragraph before Equation 2 in Page 4.", "Additionally, we also added the statements of how L1 norm and GAN loss function jointly, see in the paragraph after Equation 2.", "6. We have added the statements why the metrics are chosen to evaluate the scale-free dataset, please see in the 2nd Paragraph of Section 4.2.2.", "Additionally, to improve the reproducibility of the proposed methodologies and experiments, we have already released our code in https://github.com/anonymous1025/Deep-Graph-Translation-.", "More architecture parameters are also provided in Appendix E.", "Thank you very much again for the comments and please let us know if there are any other issues."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 299, "sentences": ["We thank the reviewer for the useful comments, below our replies.", "1. \"The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.", "It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way.\"", "We fully agree with this criticism. Following your suggestion (and also that of reviewer # 2) we have moved some material from the appendix to the main text.", "2. \"I also struggled a little to understand what is the difference between forward interpolate and filtering\"", "In this work we refer by filtering to the process of inferring the optimal latent state z_t  at time t, using observations x_{1:t} from the trial up to time t, not including observations to the future of t. By forward interpolation we refer to the process of smoothing, (inferring optimal z_t from observations of the complete trial x_{1:T}, including points to the future of t), and then evolving the inferred z_t with the learned VIND dynamics.", "After evolving for k steps, the Generative Model is used to generate data which is subsequently compared with the observations at time t+k.", "We do not refer to this procedure as \u201cprediction\u201d since the initial state z_t for the forward interpolation was obtained by making use of the full data.", "We have added clarifying comments at the beginning of section 4.", "3. \"Given the existing body of literature, I found the technical novelty of this paper rather weak\"", "We would like to reiterate that the novelty of the paper is <i>twofold.</i>", "First and foremost, we propose the use of a novel variational approximate posterior that shares the nonlinear dynamics with the generative model.", "This feature is powerful because it uses known information about the true posterior in the design of the approximate one.", "Naively, the feature also seems to be a curse because the variational approximation is rendered intractable for the case of nonlinear dynamics.", "This is the reason why such approximate posteriors have not been proposed before.", "We have added a sentence in the introduction emphasizing this crucial point.", "The second novelty is a method to deal with this intractability, via the Laplace approximation and the fixed-point iteration method.", "We showed that the resulting algorithm, which intercalates a gradient step and a FPI step yields very good results in well-known, difficult tasks such as dimensionality expansion in the single cell data or the WFOM task.", "4.- \"abstract: uncover nonlinear observation? -> maybe change \"observation\" to \"latent dynamics\"?\"", "The term \u2018nonlinear observation\u2019 in the line \u201c\u2026Variational Inference for Nonlinear Dynamics (VIND), that is able to uncover nonlinear observation and transition functions from sequential data \u2026\u201c, found in the abstract refers to the observation map in the Generative Model.", "That is, VIND uncovers both a nonlinear \u201cobservation\u201d model, that maps nonlinearly a latent state to the data, and nonlinear latent dynamics mapping the latent state at time t to the state at time t+1, which we refer to as the \u201cnonlinear transition functions\u201d.", "On the other hand, we agree that \u201cnonlinear latent dynamics\u201d is a better fit than \u201ctransition functions\u201d for the abstract and we have performed this replacement."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label"]}
{"abstract_id": 300, "sentences": ["Thank you for your comments.", "- LSTM", "LSTMs are indeed a strong model for tree prediction on previous tasks.", "To allow the model to access ancestry nodes during decoding, one way is to concatenate the parent node latent representation with the input of each step for decoding children, and then feed the concatenated vector to LSTM (e.g., Dong & Lapata ACL 2016).", "However, since the ancestry has a variable-number of nodes (as decoding proceeds)", ", to directly access these nodes during decoding, attentional mechanisms would be an efficient way, which is one of our motivations to use Transformer models that are attention-based.", "Of course, LSTM equipped with Attention would achieve the same benefit.", "In addition, positional encoding in Transformer also allows us to easily model spatial locations of UI elements.", "Our early experiments with LSTM did not yield good results on this spatial layout problem.", "That said, we agree it is worth investigating the performance of LSTM on this problem further.", "Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.", "- Eval metrics", "We agree the IR-based metrics have limitations.", "This is why we provided multiple eval metrics including edit distances and next-N accuracy.", "The Edit Distance metric was designed by taking into account human factors in interaction tasks based on the key-stroke level GOMS models.", "We can clarify this further in the revision."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_future", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 301, "sentences": ["Thank you for the review.", "While the weight formats after model compression follow well known ones, our model compression method is significantly different from the existing ones.", "Let us discuss some parts of reasons.", "- Training models after compression in order to recover accuracy is as important (if not more) as compressing weights.", "We have found that occasional distortions (not compressing weights for every mini-batch like previous techniques), relatively large learning rate, and training batches in full-precision (unlike previous ones which store compressed weights during entire training) would be the key to recovering or even increasing the accuracy.", "- Exploring large search space in much wider area is suggested in this paper through large distortion step and large learning rate (note that many compression-aware techniques perform compression at every batch has distortion step of \u201c1\u201d while much smaller learning rate for retraining that normal training is chosen).", "As we discussed in the paper, investigating various local minima is crucial for good model compression.", "- Our pruning method is fundamentally different from the previous ones because we do not incorporate a masking layer.", "While previous pruning ideas keep zero weights during training, we do not have any zero weights at any moment except at the weight distortion step.", "- Our low-rank approximation is also unique one since 1) we do not alter the structure for training even after performing SVD, 2) very high learning rate associated with transient accuracy loss is allowed for DeepTwist, and 3) we change SV spectrum continuously while the previous ones perform SVD only once (in practice, retraining low-rank approximated model has been considered to be very difficult, if not impossible).", "- Even though our pruning method is even simpler compared to the previous ones, compression rate is significantly better or very close to the one based on sophisticated Bayesian inference model.", "- Low-rank approximation results on PTB (Figure 2) shows even higher compression rate compared with weight pruning (Table 3), which is surprising to us because pruning has been known to show much higher compression ratio compared with SVD (fine-grain vs. coarse-grain or structured).", "- Quantization is performed also in a very different way.", "Unlike previous ones, we do not consider quatization during", "training. \u201cDo not perform quantization at every batch, but instead recover accuracy through full-precision training, high learning rate, and occasional quantization\u201d is the key message.", "- Overall, our occasional compression is a significant one since we can greatly reduce amount of computation overhead from compression.", "If our technique is a simple extension from the previous ones, we could not obtain such impressive results with high compression rate and improved accuracy.", "We believe that our paper suggests a wide view on how model compression should be performed."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 302, "sentences": ["Thank you for your comments. We would like to address your concerns as follow.", "1.", "We do not use the gradient penalty in WGAN-GP (1-GP) to improve the original GAN.", "Our 0-GP, although has a similar form as 1-GP,  is motivated from a very different perspective and produces very different effects.", "We assume that you find our 0-GP similar to 1-GP because of the use of the straight line from a fake to a real sample.", "In the response to reviewer 1, we propose a more sophisticated way to find a path from a fake to a real datapoint.", "The new method highlights the difference between our method and 1-GP.", "2. The 0-GP is not the only contribution of our paper.", "We start by analyzing the generalization of GANs, showing the problem of the original GAN loss.", "Although generalizability is one of the most desirable properties of generative models, it has not been studied carefully in GAN literature.", "Based on our analysis, we propose 0-GP to improve the generalization of GANs.", "On the 8 Gaussian dataset, GAN-0-GP can generate plausible unseen datapoints on the circle, implying better generalization.", "We show that the original GAN loss makes GAN focuses on generating datapoints in the training dataset.", "0-GP-sample proposed in [4] encourages the generator to remember the training samples.", "That result in the mode jumping behavior: when we perform interpolation between $z_1$ and $z_2$, the output does not smoothly transform from $x_1 = G(z_1)$ to $x_2 = G(z_2)$ but suddenly jump from $x_1$ to $x_2$. The behavior can be seen in figure 8 of BigGAN paper (https://arxiv.org/abs/1809.11096).", "3. We will include WGAN-GP to the baselines for the sake of completeness.", "However, as discussed in the previous paragraphs and in our paper, WGAN-GP and their 1-GP does not address the same problem as our 0-GP.", "As discussed in our paper, 1-GP does not help improving generalization in GANs.", "[4] even showed that 1-GP does not help WGAN (and the original GAN as well) to converge to an equilibrium.", "The phenomenon can be seen in our MNIST experiment where GAN-1-GP fails to produce any realistic samples after 10,000 iterations.", "It has been observed that WGAN-1-GP does not converge to an equilibrium, the generator continues to map the same noise to different modes as the training continues.", "In our synthetic experiment, WGAN-1-GP is less robust to change in hyper-parameters than GAN-0-GP.", "Detailed results will be included in our revision.", "Please refer to [4] for more in-depth discussion about the non-convergence of WGAN-GP.", "When $p_g$ is the same as $p_r$, the gradient of the optimal discriminator in GAN and the optimal critic in WGAN must be 0.", "Any non-zero centered GP will not help GANs to converge to the optimal equilibrium.", "Our 0-GP helps to improve both generalization and convergence of GANs.", "Our 0-GP can be applied to WGAN as well.", "Similar to the original GAN, WGAN and WGAN-GP can overfit to the dataset: the distance output by the critic can be larger than the Wasserstein distance between the two distributions.", "However, overfitting in WGAN and WGAN-GP is not as severe as in GAN.", "This is partly because the gradient in WGAN and WGAN-GP does not explode so mode collapse is much harder to observe.", "4. We will include more related works to our paper.", "The vast body of work on GANs makes it difficult to find all related works.", "We only focus on some key papers on the topic.", "Discussion about VEEGAN and Lucas et al. will be added to our next revision.", "However, we want to emphasize that our work is about improving the generalization of GANs.", "Reducing mode collapse is related to but is not exactly the same as generalization.", "As in the 8 Gaussian dataset, a GAN without mode collapse is the one that can generate all 8 modes.", "A GAN with good generalization should be able to generate unseen datapoints on the circle and to perform smooth interpolation between modes.", "5. We will add more details about the experiments to the appendix.", "The code for all experiments will be released after the review process.", "For the imagenet experiment, we used the code from [4] which is available on github.", "We note that [4] is a state-of-the-art method which is able to help GAN to scale to massive datasets and it is used in BigGAN paper.", "6.Thank you for your suggestion about the paper layout.", "Adding a table that summarizes referred gradient penalties is a good idea.", "Thank you again for your suggestions.", "We have revised our paper to address your concerns as follows:", "1. A background section is added with basic information about GANs and a definition of generalization.", "A table summarizing the referred gradient penalties is also added.", "2. We extended the Related works section to include papers which address the mode collapse problem.", "The writing of this part and the whole paper was revised.", "3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting.", "Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update.", "4. WGAN-GP is included to our ImageNet experiment.", "Our GAN-0-GP outperforms WGAN-GP by a large margin.", "5. Implementation details are added to the appendix.", "The code for all experiments will be released after the review process.", "6. We added the analysis for the 'mode jumping' problem to Section 6.2.", "We showed that GAN-0-GP-sample suffers from the problem.", "On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.", "7. A new algorithm for finding a better path between a pair of samples is added to our paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 303, "sentences": ["Thank you for your comments and questions; we have incorporated these in the revision and respond to your questions below.", "Main Argument:", "Q1: It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.", "A1: We performed some additional experiments using the progressive gan generator [1] on CelebA-HQ dataset.", "One interesting property of the progressive gan interpolations is that they take much longer to train to have a visual effect -- for example for color, we could obtain drastic color changes in Stylegan W latent space using as few as 2k samples, but with progressive gan, we used 60k samples and still did not obtain as strong of an effect.", "This points to the Stylegan w latent space being more \u201cflexible\u201d and generalizable for transformation, compared to the latent space of progressive GAN.", "Moreover, we qualitatively observe some entanglement in the progressive gan transformations -- for example, changing the level of zoom also changes the lighting.", "We did not observe large effects for the shift transformations, although perhaps more hyperparameter tuning may improve these results.", "We have added a section B.6 in the appendix and figures illustrating these results.", "Q2: Does training the generator and interpolation jointly improve the quality of the generator in general?", "A2: This is an interesting question.", "We are in the process of investigating this hypothesis to see if sampling from both the latent space and transformation alpha can help improve sample diversity and potentially FID.", "We did not yet observe an improvement in preliminary experiments on Cifar10, but the experiments are ongoing and we will add complete results on this question to the final version of the paper.", "Minor Comments:", "Q1: In appendix A.2 the authors explain how the range of is set for the different experiments. However it's not clear how is this range used in practice ? Do you sample uniformly in this range to train the linear interpolation ?", "Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?", "A1: We pick the ranges using two criteria: qualitatively acceptable and quantitatively under a fixed threshold for FID score.", "We pick alpha steps uniformly within the ranges (shifts and rotations are integer steps).", "For training, we have between 20k and 40k samples for all models, and beyond these numbers we don\u2019t see much improvement.", "Q2: There is a typo in equation 6", "A2: Thank you for your careful review in catching these mistakes. We have updated the typo in the revision.", "Q3: In figure 6: What does the right figure represent ? especially what are the different colours ?", "A3: The right side of the figure has three rows: the top row shows the plot of per-class zoom variability; there are two black datapoints we chose as examples to show instances of low and high variability classes.", "The middle row shows the distribution of the low variability datapoint (\u201crobin\u201d class), and the bottom row shows the distribution of the high variability datapoint (\u201claptop\u201d class).", "On the left of these plots we show qualitative results.", "In each of these two plots, we show dataset, -\\alpha^*, and +\\alpha^* distributions with black, green, red, respectively.", "We have updated the revision to clarify these in the figure caption.", "[1] Karras, Tero, et al. \"Progressive growing of gans for improved quality, stability, and variation.\" ICLR (2018)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label"]}
{"abstract_id": 304, "sentences": ["We thank the reviewer for the comments and appreciation.", "We have revised the paper according to the suggestions and would like to clarify as follows:", "Q1: In Sec. 3 the Authors write \"We then sample the solution space for depth and pose respectively around their initialization\".", "However in Sec 3.2 they write \"we uniformly sample a set of L virtual planes {dl} Ll=1 in the inverse-depth space\".", "In what way are the planes \"around their initialization\"? If the initial depth map spans over multiple orders of magnitude, will the planes be uniformly sampled between the minimum and maximum disparity of the initial map?", "If yes, it seems that the initial depth map is not really needed, just its minimum and maximum value is needed, but then how come the method can be applied iteratively with respect to depth?", "A1. Thank you for pointing this out. \"We then sample the solution space for depth and pose respectively around their initialization\" is a writing mistake and we have corrected it in our new version.", "Only the solution space for pose is sampled around initialization.", "We uniformly sample planes in the inverse-depth(disparity) space between a fixed minimum and maximum range.", "The initial depth is used for maintaining geometric consistency.", "The depth, under such a situation, could still be improved through iterations.", "Since the pose is improved over the iteration, the depth cost-volume would be updated accordingly, and better depth can be inferred from the more accurate cost-volume.", "Q2: The Authors mention that depth maps are warped onto the virtual planes using differentiable bilinear interpolation.", "Is there a mechanism to protect from interpolating across discontinuities? If no, were bleeding edge artifacts observed?", "A2.", "We thank the reviewer for pointing out the potential problem of our warping method on the depth maps.", "Since depth maps often have discontinuities, we agree with Review #3 that differentiable bilinear interpolation may do damage to the geometry consistency and smooth the edges.", "We also updated our experiment results with nearest neighbor instead of bilinear interpolation for depth warping, and revised the corresponding results (Tab. 1-3) and figures in the paper.", "Notably, our results can get slightly improved by the updated nearest neighbour method inspired by the question asked by Reviewer#3.", "To verify this, we added an experiment in Appendix C, which runs nearest neighbor sampling instead of bilinear interpolation.", "With nearest neighbor warping method, the performance of our model on DeMoN MVS dataset gains a slight boost with retraining.", "Here are the comparisons:", "MVS dataset", "L1-inv     sc-inv     L1-rel     Rot     Trans", "Ours (bilinear)", "0.023", "0.134", "0.079", "2.867    9.910", "Nearest neighbor(retained)", "0.021", "0.129", "0.076", "2.824    9.881", "This shows that nearest neighbor sampling is indeed more geometrically meaningful for depth.", "We updated the method to use nearest sampling and update the result accordingly.", "We also discussed the strengths and weaknesses briefly of each interpolation method in Appendix C.", "Q3: In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.", "A3.", "Empirically, learning based method may outperforms traditional feature matching methods on these situations since it relies on image priors.", "In addition, our method has geometry consistency between multiview depth maps as the input, which encourages local smoothness and consistency to some extent.", "In some textureless, reflective or transparent cases that feature matching methods does not work, our method gains extra information from the initial depth maps of other views by the depth consistency part of the cost volume.", "In Appendix D, Figure 8, some qualitative comparisons with COLMAP[1] are provided as an argument.", "We have updated our paper and show more visual examples in Appendix D, Figure 9.", "Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.", "A4. Thanks for your suggestions, we will release code upon the acceptance.", "Furthermore, we have put more details about model architecture as in Appendix A Figure 4 and Figure 6.", "[1] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4104\u20134113, 2016."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_other_label"]}
{"abstract_id": 305, "sentences": ["We thank the reviewer for having taken the time to judge our paper and to have detailed his judgement on their two points.", "We would like to point out that AnonReviewer4's final quantitative score as well as the confidence given will be crucial for the fact that this paper will or will not be presented at ICLR.", "We would like to respectfully detail how we completely disagree with the comments given in the two points, but acknowledge that this was mainly due to the way we presented the motivation for the paper.", "We hope the revised version of the paper now meets the standards for ICLR and justifies to update the \"red flag\" (clear rejection) to a green light.", "First, the goal is not faster computation on a CPU.", "Our (github-shared) code runs in a few dozens of seconds per learning on a standard laptop - but the goal is mainly to be able to test all parameters.", "We have not used SPAMS in this work as we could use the similar methods which are used in the sklearn library.", "However, SPAMS is a great inspiration for our framework.", "(For information, the complete simulations for this paper take approximately 12 hours --which are easily distributed on a cluster as we multiplied the number of independent learning runs using different classes of parameters, cross-validations and types of sparse coding algorithms - in total approx 500 experiments.", "It takes a dozens of minutes on a 100 nodes cluster.", ").", "Our motivation is mainly to understand biological vision and hope this would percolate to ML.", "Yes, we obtain faster convergence, but as an epiphenomenon of the better efficiency of our adaptive homeostatic algorithm.", "However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.", "We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.", "This shows a clear distinction between different methods and an important result: when $\\ell_2$ normalizing atoms, dictionary learning may converge to a result for which the ratio of activity between the most activated and the least activated is of the order 2.", "This result is often overlooked in dictionary learning and is a first novel result of the paper.", "This being said, Figures 1 and 3 now show the clear qualitative advantage of using homeostasis in unsupervised learning.", "This now certainly allow to understand *why* convergence speed is a good indicator ---not for an advantage on the running speed on a classical CPU--- but rather in showing that this allows a more efficient dictionary learning overall.", "Concerning the point \" It is not even clear that the final compression of the baselines would not be better.", "Even if they did show these convincingly, it is not obvious to me that it is valuable.\", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate.", "Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.", "Second, we had already done the comparison \"against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties\" but we had initially omitted to include this supplementary data (that takes the form of a single jupyter notebook which allows to reproduce all results).", "We have now included it in an anonymized format.", "This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.", "In short, we verified that the results we present are valid over a various number of parameters of the network, like the learning rates (figure 2) but also sparsity and the size of the dictionary (see Response To AnonReviewer3 @ https://openreview.net/forum?id=SyMras0cFQ&noteId=BylQtQPHRX ).", "As in Sandin, 2017 paper we have shown similar results in OMP.", "We are in the process of extending this framework to other sparse coding algorithms (LARS and lasso_lars) as plugged in from sklearn without any modification (in theory) to these algorithms.", "Indeed, we should remind that our adaptive homeostasis allows to be implemented by modifying the norm of each atom of the dictionary (as was done in the original work by Olshausen).", "We also show in the paper the application to a one-layer convolution network and our preliminary results show that we can extend this to a hierarchical network.", "I hope that with these clarifications on the form we gave to the paper (without changing the theory behind it), the statement that \" I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.\"  could be re-assessed to allow us to share this work inspired by biology to the ICLR community."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 306, "sentences": ["Thank you for your thoughtful review.", "We have updated Equation 1 and the paragraph above so that I(...) is consistently a function of two variables."], "labels": ["rebuttal_social", "rebuttal_done"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 307, "sentences": ["We thank you for interest in our work and your thorough review.", "We found your review particularly helpful in our efforts to create a more structured and formally sound version of the paper.", "------------", "- On Notation:", "+ We now have an introductory sentence to our notation section to improve the flow of reading and in order to not open the section with bullet points right away.", "+ As you suggested we changed our inequality notation to curly brackets to make it visually clearer that we are dealing with vectors.", "+ In fact we did introduce our subscript notation of the kind \"b |_{y>0}\".", "It is defined in our section on notation.", "Nevertheless your troubles compelled us to restate the meaning of this notation at the time of its first usage.", "------------", "- On Omnidirectionality:", "We are glad that you seem to find the concept of omnidirectionality intriguing. In the new version of our paper we therefore tried hard to make the definition as intelligible and intuitive as possible.", "We now use the following, equivalent, formulation as our definition: The matrix A of the form m x n is omnidirectional if for every given x in R^n \\ {0} there exists a row A_i of A such that <A_i, x> > 0.", "Or in less formal terms: There is no open linear half-space in R^n that does not contain an A_i.", "This geometric formulation of the definition is not only the origin of the naming, but it is also a mathematically sound formulation similar to the one you suggested \u201cA is full rank and there does not exist any X such that Ax < 0\u201d.", "The problem with your formulation (from the point of view of our notation) lies in the usage of the inequality sign since we defined it in the notation section to be element-wise.", "Your formulation would therefore require every entry of Ax to be negative, while for omnidirectionality one entry would be sufficient as long as the others are non-positive.", "This miscommunication also encouraged us to change our signs to the curly version as suggested by you.", "The original reason we used the previous definition was that we thought it would show more clearly what the core property is, namely that omnidirectionality can be used to nail down one precise solution. But we are now convinced that the best introductory formulation is the geometric one, as it offers an intuition of omnidirectionality.", "------------", "- On Orderliness in General:", "+ As you suggested we carefully restructured our paragraphs and removed the appearances of \u201c\\\\\u201d.", "+ We corrected as many typos as we could find, we would be very thankful for pointing out any further typos!", "+ We tried to improve the readability by increasing structure of longer segments of text e.g. by introducing informal titles.", "+ We added a clear and formal definition of the \u201cbinary\u201d diagonal matrices representing the application of ReLU.", "(Section 3.1)", "+ We rewrote potentially ambiguous statements in order to remove any inaccuracies.", "We hope we addressed your main concerns and our changes based on your review led to a paper that conforms with your standards of exposition.", "We want to thank you again for your thoughtful review and would welcome further advice."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 308, "sentences": ["-- We will add further clarification regarding what C, Z represent.", "-- As rightly mentioned by the reviewer, our method can handle very high dimensional control variates.", "-- Lemma 1: Yes, your assumption is correct in general for variational posterior.", "-- Improving disentangled representation learning over beta-VAE: Beta-VAE obtains disentangled representations by explicitly posing a trade-off between the \u2018quality of disentanglement\u2019 (factorisation of the posterior) vs. the image reconstruction quality.", "Our method removes this trade-off\u2014-we decouple \u2018disentanglement of the latents\u2019 from \u2018generation quality\u2019, specifically by having a two-stage training process.", "This allows us to potentially have much higher disentanglement, while still maintaining image quality, unlike beta-VAE where the quality of generation would necessarily be compromised.", "We would like to emphasize that this is possible only because of the two-stage training process (please see comments to Reviewer 2 regarding d-separation)."], "labels": ["rebuttal_by-cr", "rebuttal_accept-praise", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 309, "sentences": ["Thanks a lot for appreciating our contribution!", "> Comparison with attention models is necessary to compare the important patches obtained from conventional networks.", "In the paper  (section 4.3) we quantitatively show that the patches important to BagNets are also important for standard CNNs.", "Is that the direction you were thinking about? If you have a different experiment in mind we would like to kindly ask you for more details."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 310, "sentences": ["1. Thank you for your comments and thank you in particular for pointing us to a reference we missed, which we have added to the manuscript.", "Ullrich et al. introduce some of the same foundational building blocks for applying differentiable models to the cryoEM reconstruction task.", "In particular, they propose a differentiable voxel-based representation for the volume and introduce a variational inference algorithm for learning the volume through gradient-based optimization.", "Due to their voxel-based representation, they introduce a method to differentiate through the 2D projection operator.", "In contrast, we parametrically learn a continuous function for volume via a coordinate-based MLP, which seamlessly allows differentiation through the slicing and rotation operators without having to deal with discretization.", "Their method is able to learn a homogeneous volume with given poses, whereas we perform fully unsupervised reconstruction of heterogeneous volumes.", "They show empirical experiments that highlight many of the challenges for variational inference of these models.", "In particular, inference of the unknown pose is challenging with gradient-based optimization and contains many local minima (their Fig 6), which we address with a branch and bound algorithm.", "We report a Fourier Shell Correlation (FSC) metric, which is a commonly used resolution metric in the cryoEM field.", "Voxel-wise MSE is not typically used in the cryoEM literature as it is sensitive to background subtraction and data normalization.", "We have added training times for these methods to the SI.", "2. The normalization constant in Eq. 3 is the partition function over all possible values of the latent pose and volume.", "Instead of computing this (intractable) constant, coordinate ascent on the dataset log likelihood is used to refine estimates of pose and volume in traditional algorithms.", "3.", "The extent of the 3D space is determined by the dataset\u2019s image size and resolution.", "We define a lengthscale such that image coordinates are modeled on a fixed lattice spanning [-0.5, 0.5]^2 with grid resolution determined by the image size.", "The absolute spatial extent is thus determined by the Angstrom/pixel ratio for each dataset.", "Similarly, final volumes for a given value of the latent are generated by evaluating a 3D lattice with extent [-0.5,0.5]^3 with grid resolution determined by the dataset image size.", "We have added the absolute spatial extent to the description of each dataset in the revised manuscript.", "4. We have included additional architectural details in the revised manuscript, and we will be releasing the source code which will hopefully further clarify the architecture."], "labels": ["rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 311, "sentences": ["4.", "Our approach has 2 important hyperparameters: scaling parameter s used for calculating binary mask from the embedding matrix as well as  \u03bb_RU, that controls the size accuracy trade-off (see Sec. 4.1 \u201cjoint training\u201d).", "We add a table analyzing the sensitivity of the parameter \u03bb_RU observing the expected behavior: higher values of \u03bb_RU lead to a smaller model size, however, reduced G size is positively correlated with the final classification performance of D (smaller G -> lower accuracy of D).", "+---------+---------+-------+", "| \u03bb_RU  | Acc.5 | Size |", "+---------+---------+-------+", "| 2E-06 | 98.16 | 660", "|", "+---------+--------+--------+", "| 0.002 | 98.22 | 638", "|", "+---------+--------+--------+", "| 0.2     | 98.02 | 598", "|", "+---------+--------+--------+", "| 0.75   | 97.36 | 577", "|", "+---------+--------+--------+", "| 2        | 86.82 | 522", "|", "+---------+--------+--------+", "5. We use the baseline presented by [1], that tackles identical scenario.", "To our knowledge [1] provides the state of the art performance in \"strict\" class incremental setup without using real samples.", "We consider a joint training (JT, classical training) of the discriminator as the upper performance bound.", "Joint training features a setup in which the discriminator is trained on ALL real samples of the previous tasks.", "The reviewer proposes to simulate information loss and use a random subset of real samples to train the upper bound model.", "However, this would certainly give a worse performance than when using all real samples.", "We, therefore, think that used JT upper bound is appropriate.", "Furthermore, using generated samples accommodates for better performance than simply storing instances only in case of tasks of relatively low complexity such as MNIST.", "Indeed, such a result has been shown in other works, e.g. [1].", "As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with steady quality of the generated samples.", "Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples.", "Thus, this effect can be observed neither in the SVHN nor the CIFAR10 benchmarks.", "6. The CIFAR results will be provided in the Tab. 1 alongside with other datasets in the next version.", "To ensure a fair comparison with the benchmark methods that do not use any network expansion strategy for the generator (e.g. [1,6]), we initialize our G to be approximately 50% of the size of the G used in the benchmarks.", "Also a study on network growth dynamics is provided in Fig. 5 (Sec. 5.3), showcasing a lower network capacity than the worst case scenario.", "Growing the generator is an essential part of our method that addresses the scalability problem in continual learning, e.g. with always growing amount of data model\u2019s capacity will be exhausted at a certain point.", "Noteworthy, the discriminator is not affected by the proposed dynamic network expansion mechanism and features the same architecture as in the benchmark methods.", "We believe the comparison to the joint training is fair because DGM only grows the capacity of the generator.", "In the discriminator, only the last classification layer is expanded with the growing model\u2019s output space as new classes are added.", "Thus, for k-th task we compare the accuracy of a discriminator with identical architecture trained on real samples of all k tasks (JT) with one trained on DGM-synthesized samples of k-1 tasks+reals of k-th tasks.", "Thus DGM\u2019s discriminator has no advantages over the joint training generator.", "8. Finally, we will address typos, writing and presentation issues in the updated version of the paper.", "[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.", "[2] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.", "URL http://arxiv.org/abs/1801.01423.", "[3] H. Shin, J. K. Lee, J. Kim, and J. Kim.", "Continual learning with deep generative replay.", "In", "Advances in Neural Information Processing Systems, pages 2990\u20132999, 2017.", "[4] S. Rebuffi, A. Kolesnikov, and C. H. Lampert. icarl: Incremental classifier and representation", "learning.CoRR, abs/1611.07725, 2016. URL http://arxiv.org/abs/1611.07725.", "[5] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. CoRR, abs/1801.10112, 2018. URL http://arxiv.org/abs/1801.10112.", "[6] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990\u20132999, 2017.", "We thank the reviewer for their constructive comments.", "We address them as follows.", "1. We first would like to point out the contributions of our work.", "First, we address the catastrophic forgetting problem in continual learning.", "Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations.", "Hereby we extend the idea of HAT[2] to generative networks.", "Secondly, we address the scalability problem in continual learning.", "To ensure sufficient model capacity to accommodate for new tasks, we propose an adaptive network expansion mechanism in which newly added capacity is derived from the learnable neuron masks.", "2. We further we would like to clarify a possible confusion of the proposed method to be a combination of Deep Generative Replay (DGR)[6] and HAT[2].", "As pointed out in the Sec. 2 of our work, Deep Generative Replay (DGR) tries to prevent forgetting in the generator by retraining it from scratch every time a new data chunk becomes available.", "Thus, in DGR the generator would lose information at each replay step since the quality of generated samples highly depends on the quality of samples generated by the prior generator causing \"semantic drift\".", "This contrasts our method, which effectively retains the knowledge in the generator using HAT like neuron masking and only loses information through \u201cnatural\u201d forgetting.", "This allows us to use \u201ccomplete\u201d learned representation during learning and inference of the subsequent tasks as well as speed up the training (no replay of G is involved).", "3. We are not simply shifting the forgetting problem into G.", "Our work tackles the problem of class incremental learning.", "As opposed to task-incremental setup and shown in previous work, e.g. [3,4,5], models in class incremental setup (with single-head architecture) require a replay of previously seen categories when learning new ones.", "The reason for using G", "is not having access to samples of previous classes in the \u201cstrict\u201d incremental setup and using generated samples instead.", "As pointed out in our work, restricting storage of real samples represents a more realistic setup, since in real-world applications such an \u201cepisodic memory\u201d with real samples is often impossible due to memory and privacy restrictions."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_none_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 312, "sentences": ["Thank you for your review.", "We will revise our paper according to your suggestion.", "We would like to quickly address your question about the experiment here.", "For MNIST and ImageNet experiment, the whole dataset was used.", "For the ImageNet experiment, we used the code from [4].", "Details about all experiments will be added to the appendix.", "We thank you for pointing the typo in Figure 3.", "We will also add an in-depth discussion about our method and other related works to our next revision as suggested by other reviewers.", "Thank you for your constructive review.", "We have updated our paper to address your concerns.", "The changes are summarized as follow:", "1. A background section is added with basic information about GANs and a definition of generalization.", "A table summarizing the referred gradient penalties is also added.", "2. We extended the Related works section to include papers which address the mode collapse problem.", "The writing of this part and the whole paper was revised.", "3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting.", "Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update.", "4. WGAN-GP is included to our ImageNet experiment.", "Our GAN-0-GP outperforms WGAN-GP by a large margin.", "5. Implementation details are added to the appendix.", "The code for all experiments will be released after the review process.", "6. We added the analysis for the 'mode jumping' problem to Section 6.2.", "We showed that GAN-0-GP-sample suffers from the problem.", "On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.", "7. A new algorithm for finding a better path between a pair of samples is added to our paper."], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_social", "rebuttal_by-cr", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 313, "sentences": ["Thank you for your comments and questions.", "Classical cryo-EM reconstruction algorithms (e.g. cryoSPARC) are described in Section 2.2 at a high level and we refer the reader to its reference (Punjani et al. 2017) for more details on their implementation.", "To clarify the relationship between the cryoSPARC and cryoDRGN heterogeneous reconstruction in Figure 4, CryoSPARC imposes a discrete model for heterogeneity, specifically a mixture model of K volumes.", "The cryoSPARC results in Figure 4 are the volumes and the distribution of images over the 3 clusters from their unsupervised reconstruction.", "In contrast, the continuous latent variable from cryoDRGN unsupervised reconstruction is able to reconstruct the continuous motion of the ground truth volume.", "We have clarified the text to reduce any confusion and added training times for these methods to the appendix.", "Thank you for the recommendations!"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 314, "sentences": ["Thank you for the valuable feedback!", "We uploaded a revised version of the paper based on the comments.", "- The reason behind using V-GMM is that V-GMM is much faster than KDE in inference and has a better generalization ability compared to GMM.", "We use V-GMM as a proof of concept for the idea \u201cCuriosity-Driven Experience Prioritization via Density Estimation\u201d.", "Other density estimation methods can also be applied.", "We now clarify these reasons in Section \u201c2.3 Density Estimation Methods\u201d of the revised paper.", "- We concatenate the goals and estimate the trajectory density instead of state density because HER needs to sample a future state in the trajectory as a virtual goal for training.", "- For episodes of different length, we can pad or truncate the trajectories into same lengths and apply V-GMM.", "Another method is to use PCA or auto-encoder to reduce the dimension into a fixed size and then apply CDP.", "- Similarly, to handle scaling issues, for very high dimension vectors, we can first apply dimension reduction methods, such as PCA and auto-encoder, and then use CDP.", "- The reference for \"It is known that PER can become very expensive in computational time\u201d is actually the \u201cPrioritized Experience Replay\u201d paper itself.", "On page three of the PER paper, it writes \u201cImplementation: To scale to large memory sizes N , we use a binary heap data structure for the priority queue, for which", "finding the maximum priority transition when sampling is O(1) and updating priorities (with the new TD-error after a learning step) is O(log N). See Appendix B.2.1 for more details. \u201c", "In their Atari case, the memory size N is of 1e4 transitions.", "In our hand manipulation environment cases, the memory size N is of 1e6 trajectories, and each trajectory has 100 transitions.", "Thus, the memory size is 1e4 (theirs) vs 1e8 (ours).", "The complexity of updating priorities is O(log N).", "Therefore, PER is very expensive in computational time, at least in our case.", "The memory buffer size N can be found in OpenAI Baselines link: https://github.com/openai/baselines"], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 315, "sentences": ["Comment: The proposed method seems to be specifically designed for the", "generation of contrastive explanations, i.e.  why the model predicted class", "A and not class B. While the generation of this type of explanations", "is", "somewhat novel, from the text it seems that the proposed method may not", "be able to indicate what part of the image content drove the model to", "predict class A. Is this indeed the case?", "Response: The goal of this paper is not to answer \"why A?\" but rather", "\"why A and not B?\"  The visual answer to the two questions may be similar,", "but it may not.", "We seek to highlight what in the image would need to", "change to make it a B and not an A, as a way of explaining this contrast.", "There are other papers that seek to answer the question of \"why A?\" but", "that is not our focus.", "Comment: Although the idea of generating contrastive explanations is", "quite interesting, it is not that novel. See Kim et al., NIPS\u201916,", "Dhurandhar et al., arXiv:1802.07623.", "Response: Dhurandhar et al. does use the term constrastive explanation.", "However, they look at the question of \"why A?\"  Contrastive in their case", "refers to whether something is or is not present that drives the", "classification of \"A.\"  This is a different constrast than ours", "that", "contrasts \"A\" to \"B\" (rather than \"present for A\" to \"absent for A\").", "We think this is also an interesting form of explanation, but a different", "one.", "Kim et al. also has a different form of model criticism; they look at the", "dataset as a whole for examples that help explain.", "We look at a different", "problem: for a given example (perhaps not even from the training set),", "why is it not class B?", "Comment: The work from Samek et al., TNNLS\u201917 and Oramas et al.,", "arXiv:1712.06302 seem to display similar properties in their explanations", "without the need of explicit constractive pair-wise training/testing.", "The", "manuscript would benefit from positioning the proposed method w.r.t. these", "works.", "Response: The work from Samek et al. is similar to PDA in its essence. We", "will add the comparison with this method to our work for sake of", "completeness.", "In the experiment section of Oramas et al., they proposed", "a synthetic flowers dataset that can be used for our purpose.", "Since it", "is synthetic and fine-grained, we can compare the method qualitatively", "and quantitatively.  We sent a request to the authors for accessing the", "dataset.", "If we granted this access we will add quantitative comparisons", "to our paper.", "Comment: In the evaluation section (Sec.4.1) the proposed method is", "compared against other methods in the literature.", "Three of these methods,", "i.e. Lime, GradCam, PDA, are not designed for producing contrastive", "explanations", ", so I am not sure to what extend this comparison is", "appropriate.", "Response: The only method we found before submitting the paper which was", "able to answer the contrastive explanation was xGems.", "However, other", "methods could be shoe-horned into trying to answer the question of \"why A", "and not B?\" and so we figured we should demonstrate that they were not", "sufficient and that a new method (like ours) was necessary.", "Comment: The reported results are mostly qualitative. I find the set of", "provided qualitative examples quite reduced.", "In this regard, I encourage", "the authors to update the supplementary material in order to show extended", "qualitative results of the explanations produced by their method.", "Response: We have added a supplementary section, adding more qualitative", "results. Thank you for your suggestion."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 316, "sentences": ["We thank the reviewer for their feedback."], "labels": ["rebuttal_social"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 317, "sentences": ["Response to AnonReviewer2", "We would like to thank you for your positive review and comments.", "We would be happy if you have any other questions."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 318, "sentences": ["We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.", "1. Scientific Contribution: Most recent work on disentangling generative modelling tries to obtain an independent/factorised posterior over the latent generative factors without directly addressing the problem of d-separation, which theoretically prohibits factorisation of the posterior in models such as beta-VAE, conditional GAN or stack GAN.", "To further elaborate, due to d-separation, models from prior work that have the same underlying plate notation either fail to disentangle the representations (since $p(c,z|x) \\neq p(c|x)p(z|x)$ ) ) or do so at the cost of lower generative quality\u2014-because their training relies on having an additive information-theoretic penalty term.", "Our method, on the other hand, decouples the problem of learning disentangled latent representations and high fidelity generation into two separate problems by introducing a hierarchical structure (sub-graph c-y) that is trained separate from the rest of the model.", "This allows obtaining a posterior $p(c|y)p(z|x,y)p(y|x)$, which in fact guarantees the disentanglement of the factors c from z while preserving the generative strength of the model.", "2. Supervised Setting: We would argue that the setting where labelled data (C) is available is more natural than the unsupervised setting as we aim to learn physical simulators (such as graphics engines) that have a well-defined control variate structure.", "This setup appears in many previous works, e.g. conditional GAN and its derivatives.", "Testing such models on synthetic datasets (i.e. outputs of graphics engines) where one can control the generative variables is a standard practice in the field and allows for better testing.", "3. Unsupervised results: For the unsupervised setting, in addition to our face dataset and CelebA, we also present the results on the chairs and cars in the Appendix (See Figure 5, Figure 7, Figure 12, Figure 13).", "4. Experiments: We would argue that our qualitative plots and quantitative metrics are in line with the evaluation used in current SOTA work.", "In fact, we provide a very thorough mix of quantitative and qualitative experiments for both supervised and unsupervised settings.", "We would like to point out that there are no accepted measures in the field for the quality of learned disentangled representation (see Locatello et. al. [https://arxiv.org/pdf/1811.12359.pdf](https://arxiv.org/pdf/1811.12359.pdf)) and most previous papers in the field include a similar mix of quantitative and qualitative results in their experiments section.", "Also, we provide all the code so that it can be verified that the reported results are not cherry-picked."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 319, "sentences": ["We would like to thank the reviewer for constructive feedback.", "Results: We would like to clarify that all the results reported in the paper are on test sets (this includes Figures 1, 2, 3, and 5 as well as those in the supplementary).", "We decided to report the test set performance for all epochs instead of just the last epoch to show that: 1- in many cases, LSTM+Time2Vec consistently outperforms LSTM+T, 2- replacing the notion of time with Time2Vec does not deteriorate the performance, and 3- adding Time2Vec makes the model reach its best performance faster.", "Sorry about the confusion, we will clarify this in the paper.", "\u201cIf adding your representation to the SOTA model improved test set performance (or at least sped up training without hurting test set performance), then that would be better evidence.\u201d ->  This is indeed what we did.", "We showed that adding Time2Vec to LSTM+T (the model used in several recent works - see the last paragraph of related works section) and to two variants of TimeLSTM (a recent architecture with remarkable results on asynchronous sequential datasets) improves test set performance.", "\u201ctest accuracy/recall@K with/without your representation for more than one other state of the art algorithm for these datasets\u201d: Upon the reviewer\u2019s request, we are looking to extend one more architecture with Time2Vec. If we managed to obtain results until the end of the rebuttal period, we will post them here.", "Dataset that exhibits seasonality: The hand-crafted dataset has been created to serve that purpose (we could change the frequency from weekly to monthly or quarterly).", "The reason for using a hand-crafted dataset was because we could control the underlying dynamics and verify if the model can learn the correct dynamics.", "Optimization of sine functions: The results we have reported in the paper demonstrate mean and standard deviation across multiple runs.", "In each run, we initialize the parameters randomly.", "The standard deviations provide evidence that the performance of LSTM+Time2Vec doesn't depend on the initialization values more than a standard LSTM+T model.", "Moreover, from Fig 1(b) and 1(c), it can be observed that the standard deviation of LSTM+Time2Vec is even smaller than that of LSTM+T.", "Theory: According to Fourier sine-cosine series, any real-valued function f(t) that is integrable on an interval of length P can be approximated as f(t) = a_0 + sum_{n=1}^{N/2} (a_n cos(2nt\\pi/P) + b_n sin(2nt\\pi/P)) by choosing appropriate weights a_n and b_n.", "Since cos(x)=sin(x+\\pi/2), the cos functions can be replaced with sine functions so f(t) can be approximated with N sine functions.", "By concatenating Time2Vec to the input, as explained in the second paragraph of Section 4, we allow the sequence model to learn a function (or multiple functions) of time based on the data by taking a weighted sum (the weights correspond to a_n and b_n in the formula above) of the sinusoids.", "Learning a function of time from data rather than fixing it to a hand-crafted function can potentially lead to better generalization.", "We will state the theory behind Time2Vec more explicitly."], "labels": ["rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 320, "sentences": ["Thank you for your helpful suggestions and we would like to address your concerns as follows:", "1. Better explanatory texts for natural statistics comparison.", "We have modified the caption for Fig. 2 and text in Sec 2.4 to be more clear about the natural statistics analysis.", "This analysis is intended to contrast the natural statistical differences among the representations, to indicate that different modeling approaches are needed for each of them.", "Models that capture image priors well might not transfer to spectrograms or raw waveforms.", "2. Equation 1 typo fixed.", "3. Complex Coefficient vs Spectrograms.", "Thanks for the suggestion.", "We intentionally use the spectrogram notation as we do not use complex-valued kernels with complex-valued convolution.", "Yet in order to generate the audio signal, we simply generate the real and imaginary parts of the STFT coefficients such that we can convert them to waveform using inverse STFT.", "We have modified the text in the implementation details in Sec. 3 and the setup paragraphs in Sec. 4.2, 4.3, and 4.4 to make this point.", "4. Details in the experiments to clear up the settings.", "We have modified the text in Sec. 4.2, 4.3, and 4.4 to make the details more clear.", "For experiments in Sec. 4.2 and 4.3, the network\u2019s output is the complex STFT coefficient, the raw waveform is then recovered by inverse STFT using the overlap-and-add method.", "For experiments in Sec 4.4, the output of the network is the ratio mask, and the separated audio is generated by an Inverse STFT operated on the input STFT coefficients multiplied by the predicted ratio mask.", "The L1 loss is calculated between the predicted ratio mask and the ground truth ratio mask.", "Please let us know for any questions.", "Thanks again for your suggestions, which have made this submission stronger.", "Thanks,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 321, "sentences": ["Thank you for your review.", "Your summary correctly and comprehensively reflects the gist of our paper.", "One minor correction we would like to make is that our experiments are not only conducted on the Cifar-10 dataset.", "On ImageNet dataset, we were able to compress ResNet models with no or little accuracy loss, but reduce the model size by up to 21.1x and computational cost by up to 103.5x, better than previous baselines.", "Please let us know if you have further questions or concerns that we can help clarify."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 322, "sentences": ["Thank you for your comments.", "We have revised the paper to address the issues you brought up.", "- Contribution", "We developed our approach based on Transformer models.", "We agree with the reviewer that the model novelty is relatively incremental.", "However, the focus of the paper is to contribute a new prediction problem and adapts and applies the Transformer model for this problem to establish a benchmark for future exploration, which we believe has values.", "- Benchmark & Reproducible", "The data that our experiments used is an open dataset:", "https://storage.cloud.google.com/crowdstf-rico-uiuc-4540/rico_dataset_v0.1/semantic_annotations.zip", "We will release our data preprocessing, and model code, including all the eval metrics to ensure the work is reproducible.", "- Technical details", "Thanks for pointing out the issues with our presentations.", "We agree much detail on embeddings can be condensed or moved to Appendix.", "We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.", "We revised the notations in the paper to make formulation clearer.", "In addition, we added more details about the data as you suggested.", "Given a partial tree, there can be more than one way to complete the layout.", "Given a 10%, 50% and 80% BFS partial layout, the mean number of completions of the layout is 2.97, 1.23 and 1.17 respectively.", "Given a 10%, 50% and 80% DFS partial layout, the mean number of completions is 3.63, 1.24, and 1.17 respectively."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_other", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 323, "sentences": ["Thank you for the valuable feedback!", "We uploaded a revised version of the paper based on the comments.", "- To improve the clarity, we clarify why we chose to use V-GMM, among the three basic density estimation methods, including KDE, GMM, and V-GMM.", "(in the revised version of the paper Section \u201c2.3 Density Estimation Methods\u201d)", "The reasons are the following:", "1. GMM can be trained reasonably fast for RL agents.", "GMM is also much faster in inference compared to Kernel Density Estimate (KDE) (Rosenblatt, 1956).", "2. Compared to GMM,  V-GMM has a natural tendency to set some mixing coefficients close to zero and generalizes better.", "3. We only use a basic density estimation method, such as V-GMM, in our framework as a proof of concept for the idea \u201cCuriosity-Driven Experience Prioritization via Density Estimation\u201d.", "Other destiny estimation methods can also be applied in this framework.", "- We move the exact setup (Section \u201c2.1 Environments\u201d in the new version) in early sections to improve the clarity of the paper.", "- We are glad that you like the idea of the paper. Yes, indeed the curiosity mechanism in our context is related to surprise.", "The idea of our method is also related to neuroscience (Gruber et al., 2014).", "- Yes, the entire trajectories need to stored in the replay buffer and the memory size increases as the trajectory length increases.", "However, this is a general issue with off-policy RL methods which uses experience replay, such as DQN and DDPG.", "Our method CDP only uses the trajectories that are already in the memory, so CDP does not introduce additional memory usage."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 324, "sentences": ["Comment: One of the problems highlighted in the paper regarding existing", "explanation modalities is the use of another black-box to explain the", "decisions of an existing deep network (also somewhat of a black-box) which", "the authors claim their model does not suffer from.", "Response: While the GAN is certainly another black box, it is a function", "just of the data (or the data domain), and not a function of the", "discriminator from which we want to extract explanations.", "Therefore,", "training different models, switching prediction tasks on the same", "domain, or any other similar changes would not require changing the GAN.", "Comment: Learning such a model of the input space is an overhead in itself.", "Response: Overhead calculations of some form are almost impossible to", "avoid.", "Whether this is an overly large computational burdon will depend", "on the problem, although we believe the GAN or VAE need only be trained once", "per domain.", "Comment: The paper does not provide any quantitatively convincing results", "to suggest the generator in use is a good one.", "Response: Measuring the reconstruction accuracy (of goodness of the", "generator)", "is difficult, as each measure has its own flaws.", "For instance,", "Norm measures are sensitive to translations in the image.", "Comment: Experiments demonstrating comparisons between GANs and VAEs as the", "reference generative model for explanations would have made the paper", "stronger (as the proposed approach relies explicitly on how good the", "generative model is)", "Response: This is a good suggestion. We have added experiments using", "with variational autoencoders (VAEs) instead of GANs", ".", "We believe these", "address this concern and some of the comments above.", "Thank you.", "Comment: The paper proposes an interesting experiment to show that the", "proposed approach is somewhat capable of capturing slightly adversarial", "biases in the input domain (adding square to the top-left of images of", "class 8). While I like this experiment, I feel this has not been explored", "to completion in the sense of experimenting with robustness with respect", "to structured as well as unstructured perturbations.", "Response: While we could certainly perform more experiments in this vein,", "we are uncertain what type of unstructured perturbations would be useful", "(and how then to measure whether the technique captured the correct", "explanation).", "Comment: typographical errors...", "Response:  Thank you.", "We have fixed the typos.", "Comment: Section 7 in Gradcam (https://arxiv.org/pdf/1610.02391.pdf)", "provides a procedure to generate counter-factual explanations using", "Gradcam. Is there a particular reason the authors did not choose to adopt", "the above technique as a baseline?", "Response: The the proposed counter-factual experiment for GCAM produces", "*any* counter-factual explanation, not a targeted explanation.", "It answers", "\"why A?\" and not \"why A and not B?\" as we do in this paper."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_none", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 325, "sentences": ["1. vs StackGAN: Our method introduces a learning method that allows for training generative models with disentangled latents without compromising on the generative quality (unlike all SOTA mutual information (MI) based disentangled representation learning methods such as beta-VAE or info-GAN).", "More fundamentally, our method provably avoids issues posed by d-separation that theoretically prohibit disentanglement in the current SOTA methods.", "This is completely different from the motivation of StackGAN which aims to use iterative refinement (like several other generative models) to learn a generative map from image captions and does not care about disentanglement.", "2. Unsupervised control variable discovery: Beta VAE (or other MI-based methods) disentangle the latents by compromising the generative quality.", "The more the model forces disentanglement, by giving more weight to a certain information-theoretic regularizer, the worse the generated images become.", "By decoupling the training into two steps, our method allows for far better disentanglement than beta-VAE like methods without compromising the generative quality.", "Novelty: Our method aims to solve the fundamental issue of d-separation in disentangled representation learning.", "It allows for a theoretically consistent way of obtaining factorisation in the posterior without any information-theoretic penalties.", "It is true, that one can describe the method as a (non-trivial) combination of beta-vae + GAN.", "But this description mischaracterizes the fundamental problem that we have identified and proposed a solution for.", "(Please refer to comments for Reviewer 2 under \u2018Scientific Contribution\u2019)"], "labels": ["rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 326, "sentences": ["Dear Reviewer:", "Thanks very much for your comments and questions. We would like to explain them in detail and modify our paper accordingly.", "----------------------------------------------------------------------------", "Q: First, the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 \"graph translator\".", "A: General architecture: The whole framework includes a translator and a discriminator.", "(1) Translator.", "Translator consists of an encoder, a decoder, and a skip network, which first learn the representation of the graph and then decode it back to the target graph.", "See details in the third part of the answer.", "(2) Discriminator.", "Our discriminator aims to classify the generated graphs and the real target graphs given the input graph.", "(3) The translator and discriminator are trained together, and the final goal is that the discriminator cannot distinguish the generated graphs and real target graphs.", "After training such a model, the translator will be used in the test phase.", "The logic behind edge-to-edge convolution:", "(1) Generally speaking, the purpose of edge-to-edge convolution layers is to aggregate the neighborhood information of nodes.", "Specifically, the n-th edge-to-edge convolution layer aggregates the n-th hop connection information of nodes related to each edge.", "(2) Different from image convolution, for each hidden channel, we have two filters, one is a column vector while the other is a row vector.", "To learn the nth hop information of edge <i,j>, row filter aggregates all the (n-1)-th hop information of outgoing edges of node i and column filter aggregates all the (n-1)-th hop information of incoming edges of node j.", "(3)  Edge-to-edge layers are important to extract some higher-level graph features, e.g., the n-hop reachability from a node to another; n-hop in-degree and out-degree, and many other higher-order patterns.", "Different blocks in the graph translator:", "Translator consists of an encoder, a decoder, and a skip network.", "(1) Encoder.", "The encoder does n-hop edge information aggregation from the input graphs using edge-to-edge layers and then uses the edge-to-node layer to learn the latent representation of nodes.", "(2) Decoder.", "Reversely, the graph decoder first uses node-to-edge layers to decode the node representations to aggregated edge information and then further decode that into adjacency matrix, which is the final generated graphs.", "(3) Skip-network.", "Over the encoder-decoder framework, we also added skip-network (the black line of Fig.1) which can directly map the edge aggregation information in every hop from the input graph to the output graph so that can preserve the local information in every resolution (i.e., every hop).", "----------------------------------------------------------------------------", "Q: how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies?", "Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.", "A: (1) L1 norm is applied to the weight matrix.", "Our methodology is still general enough which is achieved by a trade-off between L1 loss and adversarial loss (GAN-D), which jointly enforces Gy and T(Gx) to follow a similar topological pattern but may not necessarily the same.", "Specifically, L1 makes T(Gx) share the same rough outline of sparsity pattern like Gy, while under this outline, adversarial loss allows the T(Gx) to vary to some degree.", "(2) Combining L1 loss and adversarial loss is well-recognized and validated.", "Works on image-translation have proposed and utilized L1 loss and adversarial loss jointly in GAN, for example, reference [1] (with 600+ citations) and reference [2] (with 1300+ citations).", "They have done extensive experiments to show the advantage of such a strategy.", "Furthermore, in our experiments, we found the performance when using L1 loss and adversarial loss jointly is better than using either of them.", "------[1] Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., & Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2536-2544).", "------[2]", "Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.", "-----------------------------------------------------------------", "Q: Third, and slightly related to the previous point, why do you need a conditional GAN discriminator, if you already model similarity by L1?", "Typically one would use a GAN-D() to model \"proximity\" to the source-distribution, and then a similarity loss (L1 in your case) to model \"proximity\" to the actual input sample, in the case of traditional domains.", "Instead, here you seem to suggest using L1 and GAN to do basically the same thing, or with significant overlap anyways.", "This is confusing to me.", "Please explain the logic for this architectural choice.", "A:(1) The logic of using both of them has been explained in the answer to the last question.", "(2) The logic has been well-utilized and verified in the image-translation domain. Again please see the details in the answer to the last question.", "(3)", "Our ablation experiment also demonstrates the similar advantage of using both losses for graph translation than only using L1 loss.", "Specifically, the proposed GT-GAN that uses both loses outperformed the S-Generator that only uses L1 loss on all three datasets by 10% in accuracy on average as shown in Table 2,3 and 4.", "-----------------------------------------------------------------", "Q: Four, could you please explain the setting for the \u201cgold standard\u201d experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behavior, and label accordingly? That said I am not 100% sure of this problem setting.", "A: Yes, \u201cgold standard\u201d method is directly trained based on real target graphs instead of generated ones.", "Specifically, as you know, all the comparison methods in our paper are generative models which generate graphs, and our experiment is to evaluate how real the generated graphs are.", "One way to evaluate this is by \u201cindirect evaluation\u201d, where we use the graphs generated by different comparison methods as training data to train a classifier based on KCNN (see reference (Nikolentzos, et al.,2017) in the paper), and then compare which model generates \u201cmore-real graphs\u201d by testing their corresponding trained classifier on test set which consists of real graphs.", "In \u201cgold standard\u201d method, it directly uses the real graphs to train the classifier (still based on KCNN), so it is expected to get the best performance.", "Therefore, \u201cgold standard\u201d method acts as the \u201cbest-possible-performer\u201d, and is used as a benchmark to evaluate all the different generative models on how \u201creal\u201d the graphs they can generate: the closer (and better) their performance is to the \u201cgold standard\u201d one, the \u201cmore real\u201d their generated graphs are.", "We hope we were able to answer everything to your satisfaction, please let us know if there are any more open points.", "Thank you once again!"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_structuring", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 327, "sentences": ["We thank the reviewer for insight into our paper.", "The reviewer found some points, where we were not clear enough. It is now the time to respond to them.", "1. The reviewer noticed,", "that  \u201cin the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed\u201d.", "According to the best knowledge of the authors, the Cramer-Wold kernel (which defines the Cramer-Wold metric), except for the classical RBF kernel, is the only known characteristic kernel which has closed form for radial gaussians, and we believe the respective computations in other cases (like the inverse quadratic kernel used in WAE-MMD), would be highly nontrivial.", "2.", "The reviewer also points out, that the evidence lower bound ELBO, when used with a notiGaussian prior results in case of VAE in a generally analytic formula.", "It was never the intention of the authors to sneak in that VAE cannot do it.", "Our primary goal was to define a method for training the Gaussian prior generative model using a different closed form formula for the distribution distance.", "At the same time VAE requires encoder to be Gaussian non-deterministic, and random decoder, which is not the case in CWAE (as well as in a WAE model, see Tolstikhin https://arxiv.org/pdf/1711.01558.pdf).", "The kernel used in the derivation is not a Gaussian kernel but has a closed form formula for a product of two Gaussians (see last equation in the current paper), itself not being Gaussian.", "The Gaussian kernel itself is not well suited,", "because it has an exponential rate of decay, and loses much information on the outliers (see also Bi\u0144kowski et al.,  https://arxiv.org/pdf/1801.01401.pdf, section 2.1).", "Our objective was to add a method alternative to the WAE method, but simpler in use (e.g. less parameters to be found).", "We have extended the contribution part (in the introduction) and added Sections A and B to the Appendix, to make things clearer.", "Thank you again for your comments and suggestions. Have our responses and the changes we made to the manuscript addressed all of your concerns?"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 328, "sentences": ["POINTS 1 AND 2 OF THE REVIEW", "The reviewer has noticed that the cw-distance resembles that of a U-statistic MMD estimate, and thus the proposed model very much resembles MMD itself.", "We fully agree with the reviewer, that CWAE is a model based on the kernel as the divergence measure for distributions, and consequently can be seen as a modified variant of WAE-MMD (we have added the respective comments in the paper, see the extended introduction, and added a section B in the appendix, which discusses the comparison in more details).", "However, there are some important, in our opinion, differences between those two models, which also result in an improved training speed and stability of CWAE compared to WAE-MMD (see refined experiments in section 5, as well as figures in the appendix showing comparisons between proposed CWAE and WAE and SWAE models in the Appendix).", "The differences are:", "Due to the properties of the constructed Cramer-Wold kernel, we are able to substitute in the distance the sample estimation d(X,Y) of d(X,N(0,I)) given by its exact formula.", "Consequently, the CWAE has, while being trained, potentially less stochastic perturbation then WAE-MMD.", "CWAE, as compared to WAE-MMD, has no parameters (while WAE-MMD has two).", "We observed that in many cases (like log-likelihood), the logarithm of the probability function works better, since it increases the role of examples with low-probability.", "Thus, instead of using an additional weighting parameter lambda (as in WAE-MMD) whose aim is to balance the MSE and divergence terms, we decided to automatically (independently of dimension) balance the two terms of the loss function, by taking the logarithm of the divergence.", "Moreover, since our kernel is naturally introduced with the sliced approach and kernel smoothing, the choice of regularization parameter is given by the Silverman's rule of thumb, and depends on the sample size", "(contrary to WAE-MMD, where the parameters are chosen by hand, and in general do not depend on the sample size)", ".", "The appropriate clarifications are given in the appendix B.", "Summarizing, in the proposed CWAE model, contrary to WAE-MMD, we do not have to choose parameters.", "Additionally, since we do not have the noise in the learning process given by the random choice of the sample from normal density", ",  CWAE in generally learns faster than WAE-MMD, and has smaller dispersion of the cost-function during the learning process (see Figures 7 and 8, Appendix F).", "POINT 3", "The reviewer notices that the WAE-MMD does not need to sample when used with Gaussian prior and a Gaussian RBF kernel.", "We fully agree that the gaussian kernel has the close formula for the product of two gaussians.", "However, the problem (see Tolstikhin et al\u2019s paper Wasserstein auto-encoders, https://arxiv.org/pdf/1711.01558.pdf, Section 4, also Bi\u0144kowski et al,  https://arxiv.org/pdf/1801.01401.pdf) that Gaussian kernel does not work well with the model, as its derivatives decrease too fast, and the model with Gaussian kernel is unable to learn to modify points which lie far from the center.", "We have added a respective comment in Appendix A. As to the best knowledge of the authors, the introduced Cramer-Wold kernel is the unique characteristic kernel which has the closed form for spherical gaussians, and does not have exponential decrease of derivative (as the case of RBF kernel).", "POINTS 4 AND 5", "As the reviewer accurately and carefully noticed, we have not formally proved that cw-distance is a true distance, and that the definition is introduced partially: first for two clouds of points, then a distribution and a cloud.", "This is true.", "We have added a respective proof in Appendix, Section A, where also the precise mathematical construction of the general form of Cramer-Wold metric is presented.", "We have also added the comment at the beginning of Section 3 of the paper.", "We hope clarifies our unintentionally imprecise original approach.", "POINT", "6", "The reviewer asked \u201cwhat is image(X) in Remark 4.1?\u201d", "By image(X) we understand the set of all possible values the random vector X can attain (we have included the footnote in Remark 4.1 explaining the notation)."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 329, "sentences": ["Thank you for your comments!", "We agree with R3 that it would be ideal to have a one-size-fits-all metric.", "Unfortunately, the complex landscape of the problem doesn\u2019t permit a single recommendation.", "We did our best to conduct a detailed and honest study.", "We believe our experiments to be some of the most extensive in this area, and we hope they will contribute to researchers\u2019 understanding of the problem.", "It\u2019s important to note, though, that BERTScore is an improvement over the commonly used Bleu across the board.", "Our recommendation to use F1, while potentially not optimal in specific cases, generally performs very well and much better than Bleu.", "There are largely two sets of options, (1) Among P, R, F; and  (2) What model to use.", "For (1), as we specify, F-BERT is a reliable metric for MT.", "For (2), Roberta-Large performs consistently well for to-English language pairs.", "The results are less conclusive for from-English language pairs.", "BERTScore computed with Multilingual-BERT is better than most existing metrics except on few low-resource languages.", "We have updated the paper with these recommendations in Section 7.", "We are using word pieces in all experiments, and we compute IDF using word pieces.", "We updated the paper to make this clear in Section 3, under Importance Weighting.", "Regarding unknown words handling, we computed the IDF on the reference sentences in the test set.", "This ensures that the IDF is the same for all MT systems that are tested.", "The candidate sentences generated by MT systems may contain words that never appear in the test set.", "We apply plus-one smoothing to handle such words.", "Following your suggestion, we further studied idf scoring.", "We computed idf scores on the monolingual English corpus released by WMT18 and experimented with BERTScore computed with the Roberta-large model.", "We have found that this leads to worse performance, likely because of the domain shift."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 330, "sentences": ["Thank you for the valuable feedback!", "We uploaded a revised version of the paper based on the comments.", "- We added a mathematical justification paragraph in Section 3.3 \u201cAn Importance Sampling Perspective\u201d.", "We argue that to estimate the integral of the loss function L(\u03c4) of the RL agent efficiently, we need to draw samples \u03c4 from the buffer in regions which have a high probability, p(\u03c4), but also where L|(\u03c4)| is large.", "Since, p(\u03c4) is a uniform distribution, i.e., the agent replays trajectories at random, we only need to draw samples which has large errors L|(\u03c4)|.", "The result can be highly efficient, meaning the agent needs less samples than sampling from the uniform distribution p(\u03c4).", "The CDP framework finds the samples that have large errors based on the \u2018surprise\u2019 of the trajectory.", "Any density estimation method that can approximate the trajectory density can provide a more efficient proposal distribution q(\u03c4) than the uniform distribution p(\u03c4).", "The sampling mechanism should have a property of oversampling trajectories with larger errors/\u2018surprise\u2019.", "- To mitigate the influence of very unusual stochastic transitions, we use the ranking instead of the density directly.", "The reason is that the rank-based variant is more robust because it is not affected by outliers nor by density magnitudes.", "Furthermore, its heavy-tail property also guarantees that samples will be diverse", "(Schaul et al., 2015b).", "- Yes, the experiments are mostly in deterministic domains.", "- In the FetchSlide environment, the best-learned policy of CDP outperforms the baselines and PER, as shown in Table 1.", "Yes, we did not use the last set of parameters but used the best one encountered during training, as described in Section 4 \u201cExperiments\u201d: \u201cAfter training, we use the best-learned policy as the final policy and test it in the environment.", "The testing results are the final mean success rates.\u201c", "- Our implementation is based on \u201cOpenAI Baselines\u201d, which provides HER. We combined HER with PER in \u201cOpenAI Baselines\u201d.", "OpenAI Baselines link: https://github.com/openai/baselines", "- To improve the clarity of the paper, we move the exact set-up into the earlier section, Section 2.1 \u201cEnvironments\u201d.", "In this section, we also redefine the \u201cstate\u201d based on your suggestions.", "We delete the \u201ctroublesome\u201d sentence and also clarify what the goal actually is in Section 2.1.", "For more detail, please read the revised paper, Section 2.1."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 331, "sentences": ["Thank you very much for your thoughtful review.", "We would like to point out that our experiments include multiple architectures (WRN and ResNet for image classification, LSTM and transformers for language modeling) and optimizers (SGD for image classification, SGD and Adam for language modeling).", "These were chosen according to standard implementations in the literature.", "However, we agree that it is important to demonstrate the results on a greater variety of architectures and optimizers and in particular in a manner that allows to assess the stability with respect to changing them for a specified task.", "Following your suggestion, we have therefore added experiments with both VGG and DenseNet, each trained with both SGD and Adam, on CIFAR100.", "The results conform with good agreement to the functional form defined in Eq. 5, with fit quality quantitatively very similar across all the architectures/optimizers settings in these experiments, and in particular reaching small divergences.", "We added a new section (6.2) and figure (Fig. 5) for these experiments.", "We do believe that the variety of architectures/optimizers examined over a variety of tasks (extending to large datasets over both vision and language processing) in this study, augmented with the explicit additions following your valuable feedback, experimentally cover a meaningful chunk of settings, which supports our conclusions.", "We hope you will reevaluate the paper in light of these additions, and welcome any additional feedback."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 332, "sentences": ["We would like to thank you for the review and comments.", "We revised the manuscript to address your concerns.", "Below we summarized your concerns/questions with our answers.", "Q1: It is not justified to go over the 8 page soft limit.", "A1: We removed some redundant information in the paper, moved a few paragraphs and figures to Appendix, and added discussions according to the review comments in the revised manuscript.", "Now, the paper has full 8 pages that is a soft limit.", "Q2: There likely no computational savings when compared to lookup tables.", "A2: As we added in Section 1, if weights are quantized in binary codes, then the number of multiplications is significantly reduced (even though scaling factors have full precision) or most computations can be replaced with bit-wise operations, which have been introduced and discussed as unique advantages of using binary codes in previous works.", "Since we do not suggest new computation methods using binary codes, computational savings using quantized weights become the same as those of previous binary-codes-based quantization techniques.", "FleXOR, however, saves on- and off-chip memory requirements significantly if $N_{in}$ is smaller than $N_{out}$, and reducing memory bandwidth/footprint is crucial to designing energy-efficient inference systems.", "We included this discussion in the evaluation parts.", "Q3: The evaluation section lacks experiments that evaluate the computational savings.", "A3: Since binary codes and lookup table would be associated with vastly different inference architecture, computation methods, and storage design, it is difficult to analyze detailed comparisons on FleXOR and lookup-table methods.", "We chose quantization schemes using binary codes in the experimental results because 1) binary codes are being widely studied and 2) we can focus on the practical issues on binary codes.", "Since all of quantization techniques in Table 1 and Table 2 follow the form of binary codes with the same q bits, comparisons have been made under the same computational savings (thus, model accuracy is emphasized).", "FleXOR, however, provides not only higher model accuracy but also additional storage savings due to the proposed encryption algorithm/architecture using XOR logic.", "We added discussions on the same computational savings and additional storage savings of FleXOR in Section 4 and 5."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 333, "sentences": ["Thank you for your constructive feedback!", "> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.", "More explanations are needed.", "Thank you for this suggestion (shared by other reviewers too): the updated version of the paper will clarify these things, and relegate less of the information to the appendix too.", "Specifically for Figure 4, the performance outcome for each variant is measured on multiple independent runs (seeds).", "All the outcomes are then jointly ranked, and the ranks are averaged across seeds.", "Finally, these averaged ranks are normalized to fall between 0 and 1.", "A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.", "Figure 4 then further aggregates these normalized ranks across 15 Atari games.", "Note that these joining rankings are done separately per subplot (ie modulation class).", "We think we could address all your concerns, but please let us know if you have further questions, the discussion period lasts until the end of the week!"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 334, "sentences": ["Thank you for your constructive feedback!", "Comment 1:", "We acknowledge that our presentation focused more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).", "The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.", "We will also clarify that the little phrase \u201cAfter initial experimentation, we opted for the simple proxy\u2026\u201d implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.", "Comment 2:", "Sorry, our presentation of Figure 4 was not very clear: The performance outcome for each variant is measured on multiple independent runs (seeds).", "All the outcomes are then jointly ranked, and the ranks are averaged across seeds.", "Finally, these averaged ranks are normalized to fall between 0 and 1.", "A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.", "Figure 4 then further aggregates these normalized ranks across 15 Atari games.", "Note that these joining rankings are done separately per subplot (ie modulation class).", "The bandit is not guaranteed to reproduce the performance of the best arm for a couple of reasons: (a) the signal f(z) it obtains is noisy, (b) if is myopic in that it reflects only current performance not future learning, and (c) the dynamics are non-stationary, so the best arm changes over time.", "For all these reasons, the bandit we use is a conservative one that tends to spread the probability mass among decent-looking arms, while suppressing obviously sub-optimal arms.", "The experiment you suggest (picking the best hyper-parameter after the first X episodes) is exactly what we investigated in Figure 5 (left subplot).", "The empirical result is that it works well for some games but not others, and better for some modulation classes than others, but overall it\u2019s not reliable.", "The updated paper will split Figure 5 into two to increase clarity.", "Comment 3:", "Thank you for that suggestion: we will update the organization of the paper to make the main body more self-contained.", "Comment 4:", "The updated paper will discuss related work in more depth, including the suggested [A] and [B].", "We think we could address all your concerns, but please let us know if you have further questions, the discussion period lasts until the end of the week!"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 335, "sentences": ["We thank the reviewer for the positive and constructive feedback.", "Below we answer the questions and concerns:", "Question 1:", "We agree with the referee and will therefore include a visualization of the trained distributions using Gumbel top-k sampling and a realization of the sampling pattern.", "We are currently running experiments to obtain Gumbel top-k results for the \u2018lines and circles\u2019 and CIFAR10 experiments as well.", "Since we did not sufficiently emphasize that leveraging Gumbel top-k sampling for learning signal subsampling matrices is part of the novelty of the present work, we clarified this in the revised manuscript.", "In fact, using Gumbel top-k sampling in this context can be seen as a constrained version of DPS, with shared weights across the M distributions.", "To also include previously-published baselines, we are currently running experiments with the recently proposed LOUPE method by Bahadir et al. (2019).", "Question 2:", "Indeed, the notion of compressed sensing has spurred vast work, ranging from sensing strategies to signal recovery algorithms.", "On the sensing side, sampling strategies are typically designed to satisfy the Restricted Isometry Property (RIP); describing isometry of the sensing matrix given K-sparse vectors, and thereby providing signal recovery guarantees, given an appropriate algorithm.", "On the algorithm side, sparsity in some basis transform is typically exploited, leveraging a wide variety of optimization algorithms spanning from proximal gradient methods to projection-over-convex-set and greedy algorithms.", "More recently, deep learning methods have been proposed for fast signal recovery from CS measurements, yielding state-of-the-art results.", "In this context, DPS adopts current practices in data-driven CS recovery, but extends this to incorporate subsampling (the sensing) in an end-to-end pipeline.", "Such an end-to-end (sampling-to-any-task) learning strategy opens up opportunities for data-driven optimization of sensing strategies beyond theoretically-established results.", "As pointed out by the referee, the shortcomings of disjoint optimization in classical CS are perhaps most evident when high-level tasks such as classification are part of the pipeline.", "As such, we are currently running additional experiments to better illustrate this.", "Question 3:", "We agree with the reviewer that such a comparison might be of interest.", "As such, we are currently running additional experiments to include a comparison to Gumbel top-k (as we did for the MNIST classification case) as well as the method proposed by Bahadir et al. (2019).", "Notably, and unlike our method, the latter approach does not permit setting a specific subsampling rate, with this rate is only being indirectly controlled via hyperparameter settings.", "As a follow-up on our answer regarding the second question, we would like to mention that we added a case in the MNIST classification experiment (DPS-topk), in which we jointly train a reconstruction network with a subsampling pattern.", "We subsequently train the classifier network on the reconstructed images.", "It shows that learning a task-adaptive (classification in this case) sampling pattern outperforms disjoint learning of sampling and the task."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 336, "sentences": ["Thank you for your comments! We appreciate the very detailed review.", "We have included the missing citations and fixed the typos in the revised version.", "Similar to your hypothesis, we suspect that multilingual BERT cannot produce high-quality representations for Turkish and Finnish.", "This can lead to worse performance of BERTScore.", "Based on [1] and [2], YiSi-1 trains word2vec embeddings on the monolingual data provided as part of the WMT translation task, which may explain its comparably higher performance on these languages.", "We believe it is an important future direction to improve the performance of multilingual BERT on low-resource language, but this requires a broader study of training BERT in low-resource regimes.", "We have studied computing the idf on a larger corpus.", "We computed idf scores using the monolingual English corpus released by WMT18, a much larger amount of data then we used before.", "The importance-weighted version of BERTScore using these idf scores performs worse than the original importance-weighted version.", "We hypothesize this is due to the domain shift between the corpora.", "Beyond paraphrase detection, we didn\u2019t try using BERTScore for text similarity tasks.", "The results on paraphrase are definitely promising.", "Given the number of experiments we conducted, we decided to consider this an important direction for future work.", "Indeed, several groups are already following the direction of using BERTScore for other tasks, including [3] and one of the papers R1 points to (\"Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization\").", "We discuss these follow up works in Section 7.", "[1] Chi-kiu Lo. 2017. Meant 2.0: Accurate semantic mt evaluation for any output language. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Tasks Papers, Copenhagen, Denmark, September. Association for Computational Linguistics.", "[2] Chi-kiu Lo. 2018. The NRC metric submission to the WMT18 metric and parallel corpus filtering shared task. Proceedings of the Third Conference on Machine Translation: Shared Task Papers, Belgium, Brussels, October. Association for Computational Linguistics.", "[3] Qin, L., Bosselut, A., Holtzman, A., Bhagavatula, C., Clark, E., & Choi, Y. (2019). Counterfactual Story Reasoning and Generation. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China, November. Association for Computational Linguistics."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 337, "sentences": ["Thank you very much for your comments, which is very helpful for clarifying our contribution and improving the presentation of the paper.", "Please see the inline responses.", "Q1: The paper is easy to follow but the authors are expected to clarify the rationality in integration of the loss function. How the parameter of \\lambda_r, \\lambda_t, and \\lambda_r influence the performance.", "It would be better if the authors could present some analysis.", "A1: There are in general two rules to follow when choosing the lambda for optimization: 1) the loss term provides gradient in similar numerical range, such that no single loss should dominate the training since accuracies in depth and camera pose are both important to reach a good consensus.", "2) we found in practice the camera rotation has higher impact on the accuracy of the depth probably but not the opposite.", "This is presumably because that pose cost volume accumulate depth differences of all the pixels such that is more tolerant to the depth error.", "To encourage better performance of pose, we set a relatively large \\lambda_r.", "Note that all the losses are necessary to achieve good performance.", "On the validation data, some preliminary experiments by grid search values of each lambda, show that the performance of our model is not very sensitive to various values of lambda.", "Therefore we provide a combination of lambda that produces the model for our experiment, and presumably there could be other settings that may potentially further improve the performance.", "We have added some insight to Section 3.5 about the loss function.", "Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.", "A2: We thank the reviewer for the suggestion.", "We add more analysis with the state of the art in Section 4.2, especially about the case that other methods outperforms our method.", "Q3:The experiments in section 4.3 are also expected to be improved.", "It is difficult to draw a conclusion that the method is better than other ones based on such limited experiments.", "A3: Thanks for this point.", "However, the main experiments and the conclusions are in Sec. 4.2; and  thus Section 4.2 included  much more insights and discussion of our model Vs. the other baselines in the revised version.", "In contrast,  Section 4.3 lists the ablation study, where the purpose of experiments is to verify the necessity and sufficiency of some system design options of our model and demonstrate the behavior under controlled experiments, instead of comparing with other methods.", "Specifically, we show the performance of our method with different number of iterations, with and without pose cost volume, and different numbers of the input view.", "At the same time, we found that our method also outperforms other methods in some aspects.", "In Figure 2, the curves are going down, which means that our method can effectively reduce depth and pose error from DeMon.", "The solid curves are consistently lower than dashed curves, which means our pose cost volume outperforms Steinbr\u00fccker et al. (2011) in pose estimation and further benefits depth estimation.", "In Figure 3, the blue curve is significant lower than orange curve, which means that our method is more robust in the situation with fewer views than COLMAP.", "Even though, the main purpose is not to compare to others but provide some analysis on important model components."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 338, "sentences": ["The reviewer observed that \u201cauthors need to highlight at least one practical advance introduced by the CW distance\u201d and suggested the following potential options:", "1) Faster training times.", "2) Stabler training.", "3) Usefulness of the CW distance outside of the autoencoder context.", "Ad. 1) (faster training) The experiments show, that CWAE model approaches best generalization, measured with the FID score, much more rapidly, than it is the case with WAE or SWAE models.", "E.g., when trained on the CelebA problem, the FID-score in case of CWAE drops below 100 after only about 75 batches, while for the WAE model only near 400 batches, it does so.", "The same applies to SWAE.", "Needless to say, the FID score for CWAE is near a common best value (after about 500 epochs) of about 95 (these are results are for a DeConv encoder-decoder architecture, see. Appendix E for details; for a direct comparison with Tolstikhin at al\u2019s paper results for an identical architecture to theirs are given in Table 1 in the paper) after a much shorter processing time.", "This is both thanks to the quicker convergence, but also due to faster batch processing (as it was shown in the paper).", "The MMD-like cloud-to-cloud formula for CW-distance (see equation (3) in the paper) is much more cumbersome than the actual one cloud-to-distribution used in the experiments derived in the paper and shown in equation at page 5 of the paper.", "The proposed Cramer-Wold kernel behaves correctly.", "We have added graphs describing this to the paper, exchanging those on page 8 (as the new are much more clearer).", "Graphs comparing CWAE, WAE and SWAE learning, on both CelebA and CIFAR10 datasets, shall be added to the Appendix.", "Ad. 2) (stable training) We have run repeated experiments with different initializations for all the generative models, as the reviewer has suggested.", "All experiments show that CWAE learning process is stable and repetitive: the standard deviations, for most of the coefficients computed during training are smaller than those of WAE or SWAE models (in particular CWAE minimizes WAE distance faster then WAE-MMD).", "We have added appropriate graphs to the paper.", "Ad. 3) (CW usefulness) We have verified how the Cramer-Wold metric works as a Gaussian goodness of fit,", "however, the results were not satisfactory.", "The tests based on Cramer-Wold metric were, in general, in the middle of compared tests (Mardia, Henze-Zirkler and Royston tests).", "We doubt it can be efficiently applied in this direction.", "However, since Cramer-Wold metric is defined by characteristic kernel, it can be applied in the large field of kernel-based methods in machine learning (where its particular advantage lies in the fact that it can be efficiently computed for the mixture of radial Gaussians).", "The reviewer noted that \u201cbesides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.", "In our opinion sliced approach works well for neural networks, as the neural networks see/process data by applying similar one dimensional projections.", "Also the success of neural networks based on the classical activation functions, as compared to RBF networks, supports this.", "Concerning the closed-form, Cramer-Wold kernel is the only known to the authors, which is given by the sliced approach and has a closed form for radial gaussians.", "The reviewer also noted, that \u201cSilverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?\u201d.", "In our opinion the model works well due to the fact that we compare it to the Gaussian N(0,I), where the Silverman\u2019s kernel is optimal.", "However, if the prior in general would not be standard Gaussian, the situation could possibly be different."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_by-cr", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 339, "sentences": ["We thank reviewer3 for the comprehensive review.", "We would like to address each of the reviewer's concerns individually. (This will include some discussions above only seen by authors, reviewers, area chairs and program chairs.)", "# Is the approach really limited?", "It is true that that the transformation that removes the desired information must be known before hand which is the main assumption in the paper.", "Other conceivable transformations are (i) removal of colour information by converting image to grey scale, (ii) removal of orientation information with random rotation and positional shift, (iii) removal of temporal correlation using shuffling in a time-series data and etc.", "We hope that that this paper could ignite a discussion around what transformation can be created to impose prior knowledge into the model.", "These prior transformation could be something that we observed in biology, for example, we observe global-local information disentanglement in our perception.", "Is there other hard-coded disentanglement in biology?", "This is rather an interesting problem in our opinion.", "Can this method learn more factor than just two?", "It is conceivable that there could be more than one information of interests that get destroyed in a transformation.", "For example, one latent factor could model middle-range correlations if the transformation remove long-range correlations through shuffling process and short-range correlations get destroyed through a blurring process (e.g. local smoothing transformation).", "Another two factors could represent long-rang and short-range correlations.", "What if the desired factors are not clearly disjoint and collectively exhaustive?", "This is an interesting question.", "We do not think that our current approach can disentangle continuous features.", "In a future work, there could be an auxiliary task method that can create continuous latent variables.", "We hope that this paper create interesting open problems for future research.", "# More ablations or experiments with comparable settings would be desirable.", "In our experiments, we found that the disentanglement of global and local information is very robust to different values of beta.", "In experiment 1.2 we use beta=1.0 which is the same as using the original VAE objective.", "However, beta does affect the quality of the generative samples (blurriness).", "For experiment 1.1, different beta produce similar disentanglement results, we use beta=20 to produce the figures as it created nicest looking samples.", "We uses beta=40 for all clustering experiments which had been searched from beta=\\{1, 10, 20, 30, 40, 50, 60\\} for the best digit identity clustering results.", "Thanks to reviewer3, we incorporated this information into the revision.", "Regarding the clustering result, we believe that the resulting accuracy number cannot be used to compare the quality of the clustering methods.", "We observe that the global structure contains more information than just digit identity.", "It also contains information such as whether or not there are distracting digits in the image.", "We are not concern with improving upon baseline but rather to confirm that our method can disentangle global-local information and the further analysis have shown that the grouping corresponds to more than just the digit identity.", "The use of 30 clusters helps us identify the grouping of other types of global information in addition to the digit identity.", "Therefore, the identity clustering performance does not directly translate into the ability to disentangle local and global variables.", "# Related work", "We would like to thank reviewer 3 for suggesting the related works that we have missed. These were incorporated in the revision.", "As discussed in the comments above (visible only to authors and area chairs), there is an overhead regarding grouping of data into batches in DC-IGN approach.", "We agree that DC-IGN could potentially perform the same task as our model or more.", "However, our method can reduce the effort of needing to group the data or use labelled data by instead thinking more about prior knowledge (transformation function) of the entire dataset.", "The contributions of this paper are", "(i) Suggest that there is another method of imposing prior knowledge into algorithmic design of the latent variable model.", "We believe this can be categorised as a self-supervised learning approach (a kind of unsupervised learning) which have not been explored much in the context of the latent variable model.", "(ii) Show that it can be used to disentangle global and local information through experiments."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 340, "sentences": ["Thank you for your careful reading and comments.", "Q1: Missing assumptions about the black-box calibration approaches", "We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.", "The black-box calibration assumes no read/write to model weights or availability training data, but access to the sampling of random seed.", "The black-box calibration is useful for both model user and API owner.", "Model owner: We suppose that the dense mode happens to be close to a specific training image, thus violating privacy.", "The model owner would like to calibrate the model to alleviate the mode collapse.", "In such a situation, training data may no longer be accessible since it contains private information, e.g. human faces or person images.", "Retraining consumes much time and energy, especially for complex models trained on a huge dataset.", "Besides, we empirically validate that the dense mode is not caused by imbalanced data or randomness during initialization/optimization.", "So retraining won't work for dense-mode alleviation.", "Our proposed black-box calibration has an advantage over retraining with minimum time and energy cost and no touching training data.", "Moreover, the calibration can target any dense mode for alleviation.", "API owner: For enterprise users having access to the face image generation service via cloud API, they are given the ping service for a huge number of times or not even restricted.", "Black-box calibration enables the API owner to customize the model's sampling process to meet the users' needs.", "Q2: Missing key experiments that will provide more motivation that 1. face identity can be used as a proxy for face image diversity; 2. applying our proposed metric to the training datasets should show no gap between $\\mathcal{R}_{obs}$ and $\\mathcal{R}_{ref}$:", "1. Face identity as a proxy for face image diversity", "We would like to clarify that we are not using the identity label as a proxy.", "Instead, we are using the embedding features obtained from the neural network trained on the face recognition task.", "We claim that the embedding features have rich semantics of all kinds of facial attributes, e.g. age, gender, race and so on.", "The rich semantics of the face embedding feature can be validated by its strong transferability on other visual tasks, e.g. gender/race classification and age regression.", "Prior studies [Savchenko, Andrey V, \"Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output ConvNet\" (2019)] have shown that transfer learning using neural networks pretrained on face recognition can produce highly effective results for gender recognition and age estimation.", "2. Applying our metric on the training set of FFHQ", "FFHQ is a public face dataset contains $56,138$ images, without repeating identities.", "We first randomly pick $1k$ images to form the S set and sort the S set according to the number of neighbors within distance 0.3.", "We choose the sample at percentile $0.01\\%, 0.1\\%, 1\\%, 10\\%, 20\\%, 30\\%, 40\\%, 50\\%, 60\\%, 70\\%, 80\\%, 90\\%$. We conduct the neighboring analysis on these selected samples.", "We still observe a gap between $\\mathcal{R}_{obs}$ and $\\mathcal{R}_{ref}$, which demonstrates that FFHQ dataset has dense mode, even without repeating identities.", "Furthermore, we would like to clarify that our metric is proposed to measure the collapse of GAN's learned distribution.", "We have empirically shown in the paper that the mode collapse still occurs despite balanced training data.", "You can check the details in the appendix of the paper, the paragraph of \"Applying Our Proposed Metric on FFHQ\".", "Q3: Minor improvements", "1. Proof or citation for the flaws of FID", "There is a recently published survey paper that can back our claim. It is [Ali Borji, \"Pros and Cons of GAN Evaluation Measures\" (Arxiv 18)]", "2. The contradiction between the two statements", "We use the word \"loss of diversity\" since IS's measuring of diversity is limited.", "E.g., on ImageNet with 1000 classes, it can not rule out the case when then generator simply repeating the same image for each different class.", "3. We take your advice and will address this piece of work as a \"pilot study\" in the final version."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 341, "sentences": ["We thank the comments with cares and insights, which are helpful for improving the quality and readability of our paper.", "We are glad that you support our paper.", "We have addressed all the comments as follows:", "Response #1: In the revision, we had added a new experiment to zoom in on two categories for clearer utility visualization.", "In particular, we show the DNN\u2019s deep features and RAN\u2019s Encoder output to illustrate how they push the features to cluster with the \u201ccar with/without road\u201d & \u201csailboat with/without water\u201d images in the feature space.", "Response #2: We agree that we should provide more details about the decoders.", "Generally, we set the Decoder to mirror the Encoder's architecture.", "That is, we assume a powerful adversary that knows the Encoder in training.", "Because the Encoders are different for different tasks, the Encoders are different too.", "In particular, we select the architecture of Encoder plus Classifier to be LeNet for MNIST, Ubisound and Har, to be AlexNet for CIFAR-10, and to be VGG-16 for ImageNet.", "The architectures of Encoder in four cases are different, so the Decoder is varied as well.", "In the revision, we have added above explanations about Decoder in Section 2.3 and in experiment settings of Section 3.", "Response #3: We agree that the description of three baselines should be more precise, especially the DNN and DNN(resized) baseline.", "In the revision, we have added explanations on the difference/similarity between DNN (resized) and DNN baselines. And explain why we include them as baselines to compare RAN against in Section 3.1.", "Response #4: We have added more explanations in Section 3.1 about how \u201cthe proposed algorithm works as an implicit regularization leading to better classification accuracy than the original model which completely ignores privacy\u201d.", "As shown in Figure 3, the utility of RAN\u2019s Encoder output is higher than that of DNN.", "Here the DNN model stands for the non-private feature extractor followed by a non-private classifier.", "Response #5: We agree that it is necessary to conduct experiments to compare RAN\u2019s performance concerning privacy and accuracy with/without a different kind of layers so that we can back up the argument mentioned in Section 2.2.", "On the one hand, we have already conducted exhaustive micro-benchmark experiments to determine the current design of RAN.", "For example, we select different model architectures (layers and building blocks), weight updating schemes of different parts (when and how to update Encoder, Decoder and Classifier) and settings of some important hyper-parameters (the setup of \u201cn\u201d epochs and \u201ck\u201d steps, learning rate) to select the empirically optimized one.", "However, we only present the most important results in this paper due to the space limit.", "On the other hand, for all the arguments in Section 2.2, we have added the citation to support them.", "Response #6: We agree that it is important to justify how the reconstruction error works as a measure of privacy in this paper.", "In the revision, we have added the following explanation and justification on privacy quantification in Section 1, Section 2, Section 4 and Section 5.", "First, there is no single standard definition of data privacy-preserving and corresponding adversary attacks.", "And a fundamental problem is the natural privacy-utility tradeoff which is affected by different data privacy-preserving methods.", "We note that our principal contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attacker and privacy quantification.", "Second, finding the right measurement for privacy is an open problem in itself.", "To evaluate RAN, one has to pick some quantifications.", "In the present paper, we chose the \u201creconstructive error\u201d because it is the most intuitive one to measure the risk of original data disclosure given perturbed data (Encoder output).", "Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a defined application.", "For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN\u2019s Encoder output, and the sensitive patterns founded from the raw data, in an object recognition task.", "Response #7: Thanks for pointing out the citation problem in Section 3.1.", "In the revision, we have added explanation and cited more articles about several attacks for how the raw data can cause privacy risks in Section 1.", "For example, underlying correlation detection, re-identification and other malicious mining.", "As for the \u201cNoisy Data\u201d method, we have added the citation on differential privacy in Section 3.1.", "Response #8: We have re-plotted Figure 3 and Figure 4 to improve the readability."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_reject-request", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 342, "sentences": ["We thank Reviewer 1 for their insightful questions and suggestions.", "We agree that Product Quantization (PQ) is key to get \u201cimpressive compression ratio\u201d while maintaining competitive accuracy, provided that there is some special structure and redundancy in the weights and the way we quantize them.", "Which kind of redundancy does our method capture?", "As rightfully stated by Reviewer 1, choosing which elementary blocks to quantize in the weight matrices is crucial for the success of the method (what the Reviewer calls \u201chorizontal/vertical/other\u201d correlation)", ".", "In what follows, let us focus on the case of convolutional weights (of size C_out x C_in x K x K).", "As we state in our paper: \u201cThere are many ways to split a 4D matrix in a set of vectors and we are aiming for one that maximizes the correlation between the vectors since vector quantization-based methods work the best when the vectors are highly correlated", "\u201d", ".", "We build on previous work that have documented the *spatial redundancy* in the convolutional filters [1], hence we use blocks of size K x K. Therefore, we rely on the particular nature of convolutional filters to exploit their spatial redundancy.", "We have tried other ways to split the 4D weights into a set of vectors to in preliminary experiments, but none was on par with the proposed choice.", "We agree with Reviewer 1 that the method would probably not yield as good a performance for arbitrary matrices.", "Using row permutations to improve the compressibility?", "This is a very good remark.", "Indeed, redundancy can be artificially created by finding the *right* permutation of rows (when we quantize using column blocks for a 2D matrix).", "Yet in our preliminary experiments, we observed that PQ performs systematically worse both in terms of reconstruction error and accuracy of the network that when applying a random permutation to a convolutional filter.", "This confirms that our method captures the spatial redundancy of the convolutional filters as stated in the first point.", "[1] Exploiting linear structure within convolutional networks for efficient evaluation, Denton et al."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 343, "sentences": ["We are grateful to the reviewer for their time and effort in reading our paper and providing feedback.", "Generative model assumptions: our model is an expansion of the original RAND-WALK model of Arora et. al., with the purpose of accounting for syntactic dependencies.", "The additional assumptions we include and the concentration phenomena we prove theoretically are verified empirically in section 5, so our results do hold up on real data.", "Use on downstream tasks: we believe that capturing syntactic relationships using a tensor can be useful for some downstream tasks, since our results in the paper suggest that it captures additional information above and beyond the standard additive composition.", "However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.", "Interaction between arbitrary word pairs: our model introduces the tensor in order to capture syntactic relationships between pairs of words, such as adjective-noun and verb-object pairs.", "While it might be interesting to try to capture interactions between all pairs of words, that is not justified by our model and we didn't explore it.", "However, we also trained our model using verb-object pairs, and we have updated section 5 as well as the appendix to include these additional results.", "Comparison to Arora, Liang, Ma ICLR 2017: we appreciate the suggestion to include a comparison with the SIF embedding method of Arora et. al., as this method is also obtained from a variant of the original RAND-WALK paper.", "We have updated Table 2 and the discussion in section 5 to include these additional results.", "As reported in their paper, the SIF embeddings yield a strong baseline for sentence embedding tasks, and we find the same to be true in the phrase similarity task for adjective-noun phrases (not so for verb-object phrases).", "However, we find that we can improve upon the SIF performance by addition of the tensor component from our model. (We note that we have just used the tensors trained in our original model; it is possible that combining the model in SIF and syntactic RAND-WALK more carefully could give even better results.)", "Additional citations: we have updated the paper to include both additional citations."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 344, "sentences": ["Hello,", "We would like to thank you for reviewing our paper.", "Also, thank you for your comment about the clarity of the writing, we spent a lot of effort ensuring the paper was easy to read.", "Regarding the suggested ablation,", "This comment was also made in the official blind review #2.", "We also responded to this suggestion in the public comment. For your convenience, we have copied our response here:", "In our model, BERT is more than a source of contextual word embeddings as we fine-tune all of its ~110M parameters during training.", "Simply replacing BERT with distributed embeddings and a character-CNN or LSTM wouldn\u2019t allow us to determine the effect of contextualized embeddings because we would simultaneously be removing the majority of our model\u2019s trainable parameters.", "Nevertheless, we performed the suggested ablation by swapping BERT for GloVe embeddings (300 dimensional) and found that NER performance dropped from 89.46% to 40.33% and RE performance fell from 66.83% to 14.44% on the test set of the ConLL04 corpus (note that we had to increase the learning rate by 10X to get the model to converge).", "If you were to somehow control for this drop in model capacity, say by adding in an LSTM network, the ablated model would closely match this paper [1], whom we outperform by ~3% overall on the CoNLL04 corpus.", "This paper is not cited in Table 1 as they report macro-averaged F1 scores, while most other papers (including the current state-of-the-art [2]) report micro-averaged F1 scores, as we did.", "Finally, it is well known that contextual embeddings outperform distributed embeddings on a wide range of NLP tasks, including NER [3].", "The aim of our study wasn\u2019t to compare contextual vs. distributed embeddings but on how to successfully integrate BERT into a state-of-the-art joint NER and RE architecture.", "Regarding your comments:", "\u201cI think the main source of improvement comes from the BERT representations used as input.\u201d", "\u201c[...] the contributions of this paper are to show that using BERT representations as input [\u2026]\u201d", "We would like to clarify that we are not simply using BERT representations as input.", "We are integrating BERT as part of our model architecture and fine-tuning it along with the task-specific parameters (as stated in the second to last paragraph of the introduction).", "For the particular problem of joint NER and RE, we found this to be critical.", "For example, early on in our experiments we tested using BERT as a feature extractor vs. fine-tuning the entire architecture and found that performance dropped to ~42.82% (from 78.15%) on the CoNLL04 corpus.", "Integrating BERT as part of our model (as opposed to simply using its embeddings as inputs) allowed us to swap recurrent architectures common in joint NER and RE models in favour of simple and shallow task-specific architectures composed of feed forward neural networks.", "This reduced training times while improving performance (see our response to official review #1 for more details).", "Again, thanks for your constructive comments!", "[1] https://link.springer.com/chapter/10.1007/978-3-030-15712-8_47", "[2] https://arxiv.org/abs/1905.05529", "[3] https://arxiv.org/abs/1802.05365"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_other", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 345, "sentences": ["Thank you for your comments.", "Please refer to the joint response in regards to training stability and mode collapse."], "labels": ["rebuttal_social", "rebuttal_other"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label"]}
{"abstract_id": 346, "sentences": ["We thank the reviewer for their insightful and constructive feedback.", "Re: (W1 & W2) Adversely affected by rotations", ">", "While the CFS is not invariant to rotations, it is a surprising, and empirically noteworthy, finding that all 4 different ways of producing representations consistently encode a dozen tasks very succinctly.", "This is in line with some early work that observed that certain characteristic properties like length [1][2], sentiment [3], presence/absence of brackets [4] are encoded in a single dimension in the space.", "Some of these findings can be attributed to the additive property of the cell state of the LSTMs c_t = f_t c_{t-1} + h_t {c\u2019}_{t}, which is free from matrix rotations.", "As previously noted, this empirically also results in single cells of the LSTM being interpretable.", "To just give one example, LSTM cell state can increment by a fixed amount at every time step and can count the number of tokens reliably (i.e the string length) [1][4].", "Re: (W3) Baselines for transfer learning:", "> The random baseline (i.e a random ordering of candidate task) is compared in figure 3 (and all the plots in the appendix), where we plot the accuracy boost using the best task till now in the produced recommendation of candidate tasks using different methods.", "We can clearly see that the random ordering is much worse compared to informed metrics that use representations.", "Upon your suggestion, we would also add this random baseline in table 2 as well.", "Re: (W4) Metrics for ranking of transfer don't make sense (and some are missing).", "How is precision and NDCG calculated", "> To compute the gold set, we first train a neural network for each of the candidate tasks and then use the pre-trained sentence encoder (part of the network) from the candidate task to fine-tune on the target task.", "The ranked list (in the decreasing utility of transfer learning gain) is then considered the \u2018gold\u2019 set.", "We further compare our recommendations of candidate tasks generated using CFS and classifier weight difference methods against the gold ranked list.", "Precision@K, Reciprocal Rank and NDCG are among the popular information retrieval metrics to compare a recommended list against a gold ranked list.", "These metrics are meaningful in our case, for instance, Reciprocal Rank tells us how many tasks we need to consider as per our recommendation before we hit the highest performing candidate task.", "Figure 3 presents the accuracy boost using the best task till now in the produced recommendations for the candidate tasks using different methods.", "Regarding missing values:", "As we explain in the paper, classifier weight difference metric is only applicable in cases", "where the number of features between the tasks are of the same size.", "Thus, 2 sentence input tasks and 1 sentence input tasks cannot be compared using the metric.", "Re: (W5) Multi-task learning", "> Our goal is somewhat orthogonal to the multitask learning setting where all the tasks are jointly trained.", "We, instead, focus on how the task-specific information is present in popular sentence representations, and how this could be used to assess transfer potential among tasks.", "Re: (W6) Motivation for CFS", "> There is a rich literature concerning what information is captured in the representations.", "Further, there are a few initial works that show that certain characteristics like length [1][2], sentiment [3], presence and absence of tokens like brackets [4] are densely captured in a single dimension of the representation space.", "In a similar spirit, we wanted to quantitatively study this surprising phenomenon, and we were curious about how densely is information encoded in representations.", "Re: (W7) Alternatives to CFS / Computational concerns", "> We agree that LARS/LASSO could act as potential ways to attain reduced dimensions.", "For our use case, we found the greedy forward selection computationally fast enough (of the order of a few minutes), and we observed a significant portion of accuracy captured in a very few dimensions.", "We would definitely explore this further, and add a detailed analysis on computational efficiency of different methods to reduce dimensions.", "Re: (W8) The proposed  CLF weight difference method has some concerning aspects as well. For example say we had two task with exact opposite labels.", "They would have a very low weight difference score though they are ideal representations for each other", "> You are right. For the very same reason, we take the inverse of the difference of normalized absolute classifier weights (Section 4.2).", "References:", "1.\u201cWhy Neural Translations are the Right Length\u201d :http://www.aclweb.org/anthology/D16-1248.pdf", "2. On the Practical Computational Power of Finite Precision RNNs for Language Recognition: https://arxiv.org/abs/1805.04908", "3. Learning to Generate Reviews and Discovering Sentiment: https://arxiv.org/abs/1704.01444", "4. Visualizing and Understanding Recurrent Networks : https://arxiv.org/abs/1506.02078"], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 347, "sentences": ["Thank you very much for the feedback.", "We have updated the paper and included a new section (4.3) showing how pGAN attacks bypass 4 different defence mechanisms, including outlier detection (as in Paudice et al. 2018a), the PCA-based defence in Rubinstein et al. 2009 (Antidote), Sever (Diakonikolas et al ICML 2019), and label sanitization", "(Paudice et al. 2018b)", ".", "From the reviewer\u2019s comments we noticed that, perhaps, the submitted paper, may not have sufficiently clearly explained that the approach is already targeting defences based on outlier detection and in particular that proposed in Paudice et al. 2018a.", "We already assume that the defender is in control of a fraction of trusted (clean) data points to train the outlier detector, which is a strong assumption in favour of the defender.", "To make this point clearer, we have also updated Figure 2 in the paper, showing the performance of pGAN for alpha = 0, i.e. when no detectability constraints are considered.", "In the Figure, we can observe that both for MNIST and FMNIST the outlier detection is capable of detecting many poisoning points and the effect of the attack is reduced compared with the results for alpha = 0.1.", "Different outlier-detection-based defences have already been proposed in the literature, such as Steinhardt et al. 2017 (\u201cCertified defenses for data poisoning attacks\u201d), Koh et al. 2018 (\u201cStronger data poisoning attacks break data sanitization defenses\u201d) or Paudice et al. 2018a, to cite some.", "In our experiments we chose the scheme proposed by Paudice et al. 2018a, as it assumes a stronger model for the defender (as mentioned before), which, in our opinion helps to validate the effectiveness of pGAN to craft successful poisoning attacks even in cases where the defender is in control of a fraction of trusted (clean) data points.", "Label sanitization (as proposed in Paudice et al. 2018b) completely fails to defend against pGAN attack, as shown in Figure 8 (right).", "As pGAN produces poisoning points that are correlated, the KNN-based algorithm proposed to do the relabelling is not capable of detecting the poisoning points.", "Moreover, some of the genuine points from the target class are incorrectly relabelled, making the problem even worse.", "The PCA-based defence proposed by Rubinstein et al. 2009 (Antidote) is also not capable of mitigating pGAN attack.", "The detectability constraints included in our model prevents this defence to detect the generated poisoning points.", "In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.", "We can observe that the error increases as we increase this threshold.", "The \u201cSever\u201d defence (Diakonikolas et al. 2019 ICML) is also not robust against pGAN attack.", "In Figure 8 (left) we can observe that the defence performs worse than the outlier detector and that, when the algorithm is not under attack, the performance slightly decreases, as the algorithm is removing genuine data points that are significant for the training process.", "For FMNIST, Sever outperforms the outlier detector when the number of poisoning points is reduced, although the degradation of the algorithm as we increase the fraction of poisoning points is faster compared to the outlier detector and the PCA-based defence.", "In the supplement we included the sensitivity analysis w.r.t. the parameter that controls the fraction of points to be discarded.", "We can observe that, in this case, the difference in performance is not significant for the different values explored for this threshold.", "In summary, the revised paper (see the new version uploaded) now provides a comprehensive comparison of different defence mechanisms and shows the effectiveness of pGAN to bypass all of them.", "First in Figure 2 we show the effect of the attack for different values of alpha tested against the outlier-detection-based defence.", "Then, we have provided an empirical evaluation of pGAN against 4 different defence mechanisms both in MNIST and FMNIST, showing how our attack bypasses all of these defences.", "We thank the reviewer for this valuable comment, which has certainly helped us to improve the paper.", "We hope that the score can be revised to reflect this improvement.", "Thank you very much."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 348, "sentences": ["We thank the reviewer for the comments and appreciation, and would like to answer the reviewer\u2019s questions as follows:", "Q1. The use of the word \u201cguarantees\u201d is imprecise:", "Thanks for pointing out this.", "We have adjusted the word.", "A theoretical analysis will be an interesting future work.", "Q2. Whole sequence reconstruction results:", "Our current implementation only allows up to 5 images in a single 2015 TITANX GPU with 12GB memories.", "This is because we implemented the whole pipeline using tensorflow in python, which is memory inefficient, especially during training.", "Each image takes about 2.3GB memory on average, and most of the memory is consumed by the CNN features and matrix operation.", "But it is straightforward to concatenate multiple 5-frame segments to reconstruct a complete sequence, which is demonstrated in the comparison with CodeSLAM in Figure 7 of the revised version.", "It is also straightforward to implement our BA-Layer in CUDA directly to reduce the memory consumption of matrix operation and push the number of frames."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 349, "sentences": ["Thank you for your comments and suggestions. Please see our responses below.", "1. Related work", "Thanks for pointing out this. We have added discussion about the optimal teaching and active IRL.", "2. More implementation details", "We have provided more details in the revision and plan to release our code.", "Regarding your questions: i) demonstrators policies are implemented by search algorithms; ii) the behavior tracker is an LSTM with 128 hidden units; iii) fusion module produces a 32-dim attention vector corresponding to 32 feature maps from the state encoder, and each element of that vector is used to reweight one of the feature map in order to reshape the state feature.", "3. I am not sure why it was submitted to ICLR and not the Annual Meeting of the Cognitive Science Society", "We think this is appropriate for ICLR as we propose a novel deep RL approach to improve representation learning for agent modeling.", "Having said that, it could be an interesting future work to study how humans perform probing in the perspective of cognitive science.", "4. Typos", "Thanks for point out the typos. We have fixed them in the revision."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_future_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 350, "sentences": ["We thank the reviewer for reading and evaluating our submission.", "Additive composition vs. tensor: as discussed in our introduction (and illustrated by the qualitative results in Tables 1 and 2), we believe that linear addition of two word embeddings may be an insufficient representation of the phrase when the combined meaning of the words differs from the individual meanings.", "Syntactically related word pairs such as adjective-noun and verb-object pairs can have this property."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 351, "sentences": ["Thank you for your careful reading and comments!", "Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.", "Q2: The main contribution is listed as follows:", "1. A pilot study of mode collapse existence in GAN.", "2. Metric to detect mode collapse in GAN models without any labels (ground truth or pseudo-labels).", "2. Black-box plug-and-play model collapse calibration.", "Thank you again for your careful reading and kindly identifying the typos in our paper! We will fix these typos and meticulously proofread our article."], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 352, "sentences": ["We thank the reviewer for his comments.", "We address the comments about novelty in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ), for instance concerning the relationship to previous work, and the regularization penalty ||f||_M we propose.", "More detailed comments are addressed below.", "**", "weakness of adversarial training", "As noted in our general response, our ||f||_M regularization approach empirically yields models with a more useful certified generalization guarantee in the presence of adversaries on Cifar10, while PGD adversarial training would likely require local verification of robustness around each test example, and we are not aware of useful guarantees on adversarial generalization for such models.", "We agree that this aspect is not clear in the current submission, and we will improve it in the next version.", "*", "* relationship with traditional RKHS regularization", "There is indeed no question that kernel methods/RKHSs have been widely used for regularization of non-linear functions, for over 20 years now, however these methods typically rely on solving convex optimization problems using the kernel trick, or various kernel approximations (such as random Fourier features).", "Separately, defining RKHSs that contain neural networks has indeed been the study of previous work, such as Bietti and Mairal (2018) or Zhang et al. (2016; 2017), however these only study theoretical properties of the kernel mapping and the RKHS norm, or derive convex learning procedures to replace training neural networks.", "Our approach is quite different, in that we leverage these insights to obtain practical regularization strategies for generic neural networks.", "** new regularization methods", "In addition to the ||f||_M lower bound penalty discussed in our general response, we note that combined approaches based on lower bound + upper bound methods are also novel to the best of our knowledge, and in particular we found combining robust optimization techniques with spectral norm constraints to be quite successful in many of the small data scenarios considered (see Table 1).", "We will happily clarify some of these points in an updated version of the paper."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 353, "sentences": ["We would like to thank the reviewer for the positive feedback.", "We reply to the the two questions below.", "Q1: For the motivation of this method, why would the graph be constructed within each class? If there is a correlation between different classes, how could the model use such class-wise correlation to clean the label?", "R1: The most general graph would be constructed based on image and text similarities combined.", "Here, we pre-filter with text similarity, i.e., label names, and then build the graph based on visual similarities.", "This permits (a) to significantly reduce the size of the graph and hence the complexity and (b) to reduce the noise during the cleaning task.", "We agree that operating on the more complex graph could be the subject of future research, but a significantly different method would be required and the gain of the correlation is not granted.", "Q2: Maybe I missed it, but how is the relevance score / predicted label determined for testing data given the graphs constructed in each class of training data?", "R2: There is no relevance score assigned to the test data.", "Relevance scores are only used during training.", "In particular, we build per-class graphs using the training data, assign each training example a relevance score (Section 4), and train a classifier using the training data and the corresponding relevance scores (Section 5).", "Now given a test image, a prediction is simply made by the classifier; no data or relevance scores are used.", "See also pseudo-code in response R1 to reviewer 3."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 354, "sentences": ["We thank you for your comments and hope that the following response will address your concerns.", "1. We did stated both in the Theorem statements and Remark 3.4 that the a large batch size $B_t=T$ is used for the convergence proof.", "This means the effective rate of convergence is $O(1/\\sqrt{T})$ as pointed out by the reviewer.", "This rate matches the currently best known rate of convergence for SGD (see, e.g. Ge et al., COLT'15).", "We have now made this very clear in both Remarks 3.3 and 3.5.", "Please see changes highlighted in blue and also our response to Reviewer 2 on novelty of the convergence analysis.", "If our response addresses your main concern, we sincerely hope you that you can reconsider your score.", "For your other points, we have made the following changes in the paper.", "2. We have checked and fixed a few typos in the paper.", "Please note that we wrote  $\\nabla f(x)\\cdot \\sigma (\\nabla f(x))$ in eq. (7) as a dot product.", "So it is the same as $\\nabla f(x)^{T} \\sigma(\\nabla f(x))$. This notation was explained in the notation section.", "If you have any remaining concerns, please let us know.", "3. We have added the values for chosen $\\gamma$ in the updated version (see caption of Figure 1).", "4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.", "We promise to do so in the final version.", "5. We were not aware of this at the time of submission.", "We have changed this to PoweredSGD. If you have any alternative suggestions, please let us know.", "We summarize the main contributions of the paper as follows:", "- In the theoretical part, we provided more concise convergence rate analysis for stochastic momentum methods in the non-convex setting.", "This was made possible by a sharp estimate of the accumulated momentum terms (Lemma B1).", "We believe this is an important but under-explored topic (Yan et al., 2018).", "- In the experimental part, we empirically showed that the proposed optimisation algorithms have potential to solve realistic problems.", "We are not claiming these variants will outperform all other methods in all training cases, but we sincerely believe that the results are promising.", "In particular, we have demonstrated their potential benefits of mitigating gradient vanishing and combining other techniques for accelerating optimization.", "We do admit the gap between our theoretical analysis and experiments in the sense that the analysis does not account for the initial acceleration observed in many experiments.", "We think this is a very interesting question for future research and hope that this paper can motivate further research in this area.", "We agree with your intuition that this may have something to do with $\\gamma\\in (0,1)$ boosting the gradients."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_done", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label"]}
{"abstract_id": 355, "sentences": ["Thanks for your comments.", "Below we address the detailed comments.", "In particular, we clarify the potential misunderstanding on the linearity of the discriminator and added new state-of-the-art results by applying negative feedback to SN-GAN.", "Q1: About novelty and analysis:", "A1: As agreed by both Reviewer #2 and Reviewer #3, in this paper, our contributions are twofold: (1) a unified and promising framework to model the stability of GANs using control theory, (2) we propose to use the negative feedback to stabilize GANs.", "First, using control theory, the dynamics of GANs can be modeled as transfer functions with Laplacian transformation, and various existing methods (e.g., Negative Momentum and Reg-GAN) can be considered as certain controllers that are widely used in control theory.", "Moreover, through control theory, the stability of GANs can be easily inferred from the poles of the transfer function, instead of analyzing the complicated jacobian matrix of the dynamics as discussed in Sec. 4.1 and Appendix A&C. We argue that our method is distinct from existing method, which is well-discussed in our response to Q4.", "Second, our perspective also provides a promising direction that can further benefit the training dynamics of GANs using advanced control methods (e.g., nonlinear control and  modern control theory [*2]) to improve both the stability and the convergence speed of GAN.", "As a concrete example, we propose to use the most widely-used negative feedback control method to stabilize GAN's dynamics and the empirical results demonstrate the effectiveness of NF-GAN as shown in Sec. 4&6.", "Exploring advanced control methods is our important future work.", "We updated the empirical results on the state-of-the-art model in the revision, where we applied our proposed NF-GAN to the SN-GAN [*6].", "We can see that NF-GAN can successfully address the potential unstable issues of SN-GAN and achieve state-of-the-art inception score on CIFAR-10.", "More details can be found in our response to the common concern and our revised paper.", "Q2: Linear discriminator and extending the analysis to realistic settings:", "A2: Thanks.", "We indeed extended the analysis of Dirac GAN to the more realistic setting in Sec. 3.2, where the discriminator is NOT linear.", "In this part, we analyzed the dynamics of WGAN in the function space following [*1], i.e., we directly modeled $D(t, x)$ and $G(t, z)$ for all $x$ and $z$. It avoids the nonlinearity issue caused by the neural network, and both G and D are linear dynamics, at least locally around the equilibrium, as discussed in Sec. 3.2 and Appendix D in the revision.", "Fig. 2 (right) provides a diagram of the unregularized WGAN.", "In practice, we use the gradient descent method in the parameter space to approximate the dynamics in the functional space to efficiently solve the optimization problem.", "Recent advances in modeling GAN in the functional space [*5] provide powerful tools to bridge the gap and we leave it as our future work.", "We updated the discussion in Sec. 3.2 in the revision to make this clearer.", "Q3: The Lip constraints on the discriminator:", "A3: Actually, our method also applies to WGAN with Lipschitz constraints (vanilla WGAN).", "Existing work [*3] states that vanilla WGAN diverges and we provide theoretical and empirical evidence that our method helps vanilla WGAN converge.", "Theoretically, to address this comment, we added Theorem 1 (See in the Appendix D) that states the dynamics of $D$ with Lipschitz constraint follows Eqn. (10) *around the equilibrium*. Therefore, the stability analysis and our proposed method in Sec. 4 still applies to vanilla WGAN because control theory mainly focuses on the stability *around the equilibrium* [*2].", "Empirically, as suggested by R#3, we built a vanilla WGAN baseline using the SN-GAN [*6] framework, whose Lipschitz constraints are satisfied through spectral normalizations.", "We compared SN-GAN (WGAN loss) and NF-SN-GAN (WGAN loss) and obtained a significant improvement on both the stability and the final results (IS from 3.29 to 8.28, See details in our post for common concerns).", "It demonstrates that our method helps vanilla WGAN converge, which is consistent with our theoretical analysis.", "Q4: Related work:", "A4: Thanks for pointing out the related work. In fact, our method is distinct from these methods.", "For the first paper (i.e., Gradient descent GAN optimization is locally stable) analyzed the stability of GANs using the Jacobian matrix and adopted a regularization term to stabilize GANs similarly to [*4].", "Instead, we adopted a different method to model the dynamics from control theory.", "The difference has been discussed in Sec. 1 and Sec. 5", "For the second paper, the authors used the Lyapunov function, which is different from our framework, to analyze the stability of GANs.", "Besides, their method fails to scale-up to large datasets such as CIFAR-10 because of computational issues.", "Q5: Empirical results:", "A5: Theoretically, Reg-GAN is also a stable training method for GANs but it is computationally less efficient than NF-GAN (ours), as illustrated in Fig. 4.", "Empirically, we can achieve better results compared to Reg-GAN as illustrated in Table 1 (top).", "Moreover, we also also advanced the state-of-the-art results based on practical GANs (SN-GAN).", "The inception score on CIFAR-10 is improved from 8.22 to 8.45.", "See details in Table 1 (bottom) in the revision and our post about common concerns.", "[*1] Johnson, Rie, and Tong Zhang. \"Composite Functional Gradient Learning of Generative Adversarial Models.\" International Conference on Machine Learning. 2018.", "[*2] Khalil, Hassan K. \"Nonlinear systems.\" Upper Saddle River (2002).", "[*3] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. \"Which Training Methods for GANs do actually Converge?.\" International Conference on Machine Learning. 2018.", "[*4] Mescheder, Lars, Sebastian Nowozin, and Andreas Geiger. \"The numerics of gans.\" Advances in Neural Information Processing Systems. 2017.", "[*5] Johnson, Rie, and Tong Zhang. \"Composite Functional Gradient Learning of Generative Adversarial Models.\" International Conference on Machine Learning. 2018.", "[*6] Miyato, Takeru, et al. \"Spectral Normalization for Generative Adversarial Networks.\" (2018)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_future_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 356, "sentences": ["We thank the reviewer for their comments and suggestions.", "We answer below:", "1. As the reviewer accurately points out, we choose to always employ the hinge loss for DFW in this paper because it gives an optimal step-size.", "In the new version of the paper, we have included additional baselines on the SNLI data set.", "This provides more empirical comparisons between the performance of CE and SVM for different optimizers.", "2. In appendix B.2 of the paper, we have added the convergence plot for all methods on the CIFAR data sets.", "In some cases the training performance can show some oscillations.", "We emphasize that this is the result of cross-validating the initial learning rate based on the validation set performance: sometimes a better behaved convergence would be obtained on the training set with a lower learning rate.", "However this lower learning rate is not selected because it does not provide the best validation performance (this is consistent with our discussion on the step size in section 6)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 357, "sentences": ["We thank the reviewer for the comments and appreciate that the reviewer likes our idea of including optimization in the network. But our contribution is beyond adopting Levenberg-Marquardt instead of Gauss-Newton.", "We would like to clarify several things to address the reviewer's concerns:", "Q1. The advantages of Levenberg-Marquardt over Gauss-Newton is unclear (the main reason for rejection):", "Firstly, we want to clarify that our contribution is beyond improving the Gauss-Newton optimization to Levenberg-Marquardt.", "More importantly, our contribution is the combination of conventional multi-view geometry (i.e. joint optimization of depth and camera poses) and end-to-end deep learning (I.e. depth basis generator learning and feature learning).", "This contribution is achieved by our differentiable LM optimization that allows end-to-end training.", "Secondly, we agree with the reviewer that comparing with the Gauss-Newton algorithm will be interesting and have updated such a comparison in Appendix B in the revised version according to the reviewer\u2019s suggestions:", "1. We retrained the whole pipeline with Gauss-Newton, to make sure the features are learned specifically for Gauss-Newton.", "2. We compared with various constant lambda values to see how the performance varies along with lambda.", "Note that we also fine-tune the network to make sure the features fit different lambda.", "In Table 4 of the revised version (Appendix B), our method outperforms the Gauss-Newton algorithm in the last column.", "This is because the objective function to be optimized is non-convex, and the vanilla Gauss-Newton method might get stuck at saddle point or local minimum.", "This is why the Levenberg-Marquardt algorithm is the standard choice for conventional bundle adjustment.", "In Figure 6 of the revised version (Appendix B), our method also consistently performs better than different constant lambda values.", "This is because the value of lambda should be adapted to different data and optimization iterations.", "There is no \u2018optimal\u2019 constant lambda for all data and iterations.", "Q2.", "Comparison with CodeSLAM:", "We have included that in Figure 7 of the revised version (Appendix E).", "Since there is no public code for CodeSLAM, we cite its results directly from the CodeSLAM paper.", "Q3. The state vector Chi is not defined for the proposed method.", "The Chi is defined in Section 3 as the vector containing all camera poses and point depths.", "Since our method also solves for these unknowns as in classic methods, we did not redefine the Chi.", "But in the revised version we have recapped the definition of Chi when introducing our method at the beginning of Section 4.", "Q4. Should the paper be called Bundle Adjustment?:", "The term \u2018Bundle Adjustment\u2019 is originally used to refer to the joint optimization of 3D scene points and camera poses by minimizing the reprojection error.", "The keyword Bundle comes from the fact that a bundle of camera view rays pass through each of the 3D scene points.", "Multiple recent works, e.g. [Engel et al., 2017,Delaunoy and Pollefeys, 2014], have generalized it to \u201cphotometric BA\u201d where scene points and camera poses are optimized together by minimizing the photometric error.", "Our method is along this line.", "But we further improve the photometric error to featuremetric error.", "Each 3D scene point is still constrained by a bundle of camera view rays, though the error function has been changed.", "So we believe it is justified to call this method feature-metric BA.", "But we agree with the reviewer that the word \u2018reprojection\u2019 is misleading when we introduce our feature-metric BA and the photometric BA.", "So we use the word \u2018align\u2019 as the reviewer suggested and use \u2018reprojection\u2019 only for the geometric BA.", "Q5. Is B the same for all scenes?:", "In the revised version, We added Figure 8 to visualize of the term B in Equation 7 (Page 6) for different scenes.", "We can clearly see that it is scene dependent.", "Q6.Typos:", "We have fixed all the typos as suggested in the revised version.", "We thank the reviewer for raising the score.", "We submitted the response and the revision until the last minute because a lot of extra works have been done for the revision, and we want to ensure the correctness and completeness.", "But we will have a better-planned schedule for the next ICLR to fit the purpose of openreview."], "labels": ["rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_none", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_social", "rebuttal_social", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 358, "sentences": ["1. The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.", "A: While ReMixMatch comprises many components (some of which are new), we believe our ablation study justifies the reason why each component exists. If there are additional ablation experiments that you think would be helpful for us to run, please let us know.", "2. As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.", "A: We actually found that using stronger augmentations in MixMatch resulted in divergence.", "We mention this in the paper (\"Since MixMatch uses a simple flip-and-crop augmentation strategy, we were interested to see if replacing the weak augmentation in MixMatch with AutoAugment would improve performance but found that training would not converge.\") but will emphasize this more in the next draft.", "We also found in our ablation study that using only strong augmentation (i.e., replacing weak augmentations with strong augmentations) resulted in very poor performance for ReMixMatch, suggesting that anchoring towards a weaker augmentation is important.", "We will update the labels in the ablation table to make this more clear.", "3. What about enforcing the marginal distribution on the annotated data and the marginal distribution of the model to be similar with KL divergence?", "A: We tried this approach in initial experiments and found that it performed poorly.", "Using the KL loss also introduces a scalar multiplier hyperparameter for this loss term.", "We spent some time tuning this hyperparameter and were unable to obtain good results, so we chose to use the proposed version which does not introduce such a hyperparameter.", "It may be that further investigation into this form of a loss could be fruitful."], "labels": ["rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 359, "sentences": ["Thank you for your feedback. We are glad to know that you find the problem inherently interesting and important.", "Re: no exploration of encoder architectures is performed", "> We are not sure if we understand this completely.", "Just to clarify, we do compare 4 different sentence encoders [1][2][3][4] which display a fair amount of variety in ways which sentence representations can be computed.", "For instance, SkipThought vectors [1] use bi-GRU based encoder-decoder model to reconstruct the surrounding sentences.", "ParaNMT [2] and InferSent [3] use different LSTM based architectures to perform back-translation and textual entailment respectively.", "Lastly, SIF [4] is a tf-idf based weighted average of individual GloVe word representations.", "One of the key findings of our work is that task-specific information is captured succinctly for a majority of 13 different NLP tasks across 4 different choices of encoder architectures.", "1. Skip-Thought Vectors (https://arxiv.org/pdf/1506.06726.pdf)", "2. PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (https://arxiv.org/pdf/1711.05732.pdf)", "3. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data (https://arxiv.org/pdf/1705.02364.pdf)", "4. A Simple but Tough-to-Beat Baseline for Sentence Embeddings (https://openreview.net/forum?id=SyK00v5xx)", "Re: Utility of the methods is a bit unclear", "> We agree that our approach to estimate transfer potential reaps true benefits only when n is large.", "However, this is not uncommon in scenarios like machine translation, where there are hundreds of potential language pairs that could be used as candidate tasks.", "Furthermore, we believe (although acknowledge that this is subjective) that curiosity-driven questions about how the information is encoded are interesting: while they might not be useful in a way that is easily measurable by quantifiable metrics, they provide insights that can help guide future work.", "Re: CFS metric depends on a hyperparameter (the \"retention ratio\")", "> Sorry about the lack of clarity!", "To clarify, we used the elbow method (used to find an appropriate number of clusters for clustering) and observed that the \u2018elbow\u2019 in the accuracy vs dimensions plot was around the 80% accuracy mark for most tasks, and hence, we used 80% as the retention ratio.", "We will discuss this process and test with different retention ratios in the final version.", "Re: motivation for the restriction to linear models?", "> Our motivation to use linear models is to keep the setup simple and fast.", "As the classifiers are able to extract task-specific information and reliably estimate transfer potential; changing to a different classifier like MLP, we believe, shouldn\u2019t affect our results in a significant way.", "However, we will empirically verify this, and discuss this in the camera-ready/future versions of the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 360, "sentences": ["We thank the reviewer for pointing out the potential similarity between our pipelined approach and the asynchronous update approach.", "Pipelined backpropagation is similar to model parallelism but it addresses the resource underutilization issue in model parallelism.", "However, asynchronous update (e.g., asycn-SGD in Dean et al. [1]) usually utilizes a parameter server to keep track of model parameters (weights) while our pipelined method does not use any parameter server.", "Furthermore, each accelerator obtains a replica of a full model in asycn-SGD training while each accelerator contains only a part of the model in our pipelined method, on the assumption that the full model does not fit into the memory of a single accelerator.", "The async-SGD in Dean et al. [1] still falls into data parallelism because each accelerator has a replica of the full model.", "On the other hand, our approach falls into pipelined parallelism.", "Thus, we focused our comparison to related work on two similar approaches: PipeDream and GPipe, both utilizing pipelined parallelism.", "Nonetheless, we will expand the related work section to more explicitly compare to data parallelism and non-pipelined approaches to model parallelism (i.e., expand on the first paragraph of related work).", "[1]  Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. 2012. Large scale distributed deep networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems"], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_other_label"]}
{"abstract_id": 361, "sentences": ["U^m in Eq 1:", "This term is clearly defined at the beginning of section 3, first paragraph, line 6 \u201cU is the unit of hyper-volume in ...\u201d .", "U^m is simply U powered by the cardinality variable.", "In section 3.3, in the paragraph after Eq.7 line 2, we explain the mechanism to obtain U. For each experiment, we also report the tuned value for U.", "- The term p(w) in Eq 2:", "Note Eq. 2 calculates the posterior, i.e., p(w|D) and according to the Bayes theorem, it is p(w|D)\u00a0\\propto\u00a0p(D|w)p(w)", "which\u00a0is\u00a0simply\u00a0Eq. 1.", "- Eq 5 confusion :", "To explain Eq.5 and Eq. 6, the training works as follows:", "At each iteration k-1, the network predicts the outputs, i.e., O1, O2 and \\alpha.", "We first solve a discrete optimization to find the permutation (matching) between the predictions at k-1 and the ground truth (GT).", "Then, we use this permutation to order GT and back-propagate the losses to update w at iteration k. Please note that cardinality loss does not depend on this permutation variable.", "- The network architecture :", "Note that our described methodology can be applied to any network architecture.", "In ALL our experiments, we use Res-net 101 (mentioned several times on page 6 and 8).", "We\u00a0only\u00a0need to define the number of outputs and use the set loss defined in Eq. 5 and 6.", "For example, for the set size with maximum cardinality 4, we need 5 outputs\u00a0( \\alpha)\u00a0for cardinality m = {0, 1,...4}.\u00a0If the state of each set is 5 for the detection experiment, we need 4*5=20 outputs for the state  loss (O1).", "For the permutation (O2), we need 4!=24 outputs.", "For each output we have a loss defined for each experiment\u00a0in the text.", "- the permutation to benefit training:", "We refer the reviewer to the experiment we have already included in Appendix titled \u201cAn additional baseline experiment\u201d, which unfortunately we could not include in the main manuscript due to space constraints.", "We use a baseline model with no permutation, which is exactly same as [21], to train the network for the detection task.", "The results show the model is not able to learn this task, hence highlighting the need for permutation prediction for a complete set prediction network.", "Even if we remove the permutation head, O2, from our model, we still need to calculate the permutation using Eq. 5 and use it for backpropagation in Eq. 6.", "However the model in [21] completely ignore the permutation in its formulation.", "Therefore, it cannot learn the detection task.", "- Term f2 in Eq5 uses w~ estimates:", "Your interpretation is indeed correct.", "Given the predictions of O1 and O2 using statistics from past SGD runs, we want to find the best permutation.", "There are indeed m! way to assign GT set elements to the predictions.", "We solve this optimization to find the best one.", "- the significance of the permutation:", "Even if we don\u2019t use f2 for the estimation of the best permutation, we can use \\pi* as ground truth for updating its loss in Eq. 6.", "- classifying permutations:", "Classifying the permutations provides the extra information about the structure of the problem, e.g. there exist a single order which matters or it can be several different orders or the problem is orderless.", "We simply do not ignore the permutations from Hungarian by allowing the network to learn them.", "We refer you to the experiment we have already included in Appendix titled \u201cDetection & Identification results\u201d, where we used the predicted permutations to identify the bounding boxes for similar looking objects across different test images", "-  larger images and many instances:", "We agree.", "We leave this as future work, as it would require an engineering effort that departs from the main purpose of our paper, which is to show theoretically how to construct a network that can work with sets instead of tensors.", "- sensitivity to seeing a certain cardinality:", "During the training, we do the data augmentation (by cropping, flipping etc) to ensure the network sees enough sample for each cardinality.", "We will include this detail in the text.", "- Related work", "We are happy to include these references.", "But these works are orthogonal to the main subject of our paper.", "In our paper the goal is to introduce a framework to output a set using neural networks and we used the detection task as one of the set prediction examples.", "We would like to add few comment about these two works:", "These approaches try to learn a pairwise relationship between the boxes outputted using the conventional proposal based detection approaches.", "First a) they need to introduce extra pairwise network or heavier computation to learn these pairwise relationship b) they assume the relationship is pairwise between bounding boxes.", "Out framework is a single stage approach which uses a conventional convnet backbones with no extra computation.", "Since it is end-to-end prediction of boxes, we don\u2019t enforce any pairwise or higher order relationships between the outputs.", "We all rely on the layer of neural nets to capture high level relationship between outputs before predicting them."], "labels": ["rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_done", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 362, "sentences": ["Thank you very much for your feedback. We have revised and updated the paper following your suggestions.", "Here are our reply to your comments:", "(Q1) Thank you very much for the suggestion.", "We have included experimental results evaluating pGAN on CIFAR-10.", "The results are shown in Figure 4 in the updated version of the paper.", "(Q2) Following your recommendation, we have structured the section with the experimental results in different subsections.", "(Q3) The observation from the reviewer is correct: pGAN aims to inject poisoning points in regions that are close to the decision boundary, especially in those where the data distributions overlap more.", "Points that are far away from the decision boundary may be detected by the discriminator, outlier detection or other defensive algorithms that could be used.", "Defences aiming to remove points that are close to the decision boundary could be effective to remove poisoning points generated with pGAN, but these defences will suffer from a significant loss in performance, especially when the algorithm is not under attack.", "For example, in SVMs, support vectors are points that are close to the decision boundary.", "If we remove these points, as suggested by the reviewer, we would obtain different support vectors that would lead to suboptimal solutions with significantly degraded performance.", "With regards to addressing the more general point of the performance of pGAN when state-of-the-art defences are used, we have updated the paper to include a new section (4.3) where we show that pGAN is capable of bypassing 4 different defence mechanisms.", "This supports the effectiveness of our attack.", "(Q4) Munoz-Gonzalez et al. (2017) showed an experiment using a Convolutional neural network with 450,000 parameters, trained with 1,000 training points and injecting 10 poisoning points.", "In our case, for the experiment with MNIST in Figure 2, we used a deep neural network with more than 40,000,000 parameters, 1,000 training points, injecting up to 400 poisoning points.", "As the reviewer can observe the scale of the experimental evaluation is significantly different.", "The computational complexity of the attack in Munoz-Gonzalez et al. (2017) makes the experimental evaluation intractable for the settings considered in our experiments.", "On the other side, Paudice et al. (2018a) showed that, in many cases, if we don\u2019t consider appropriate detectability constraints, the attack points generated by optimal attack strategies formulated as bilevel optimization problems can be effectively filtered out with appropriate outlier detection, resulting in blunt attacks.", "This is not the case for pGAN, which is capable of bypassing different defences, including the outlier detection scheme proposed by Paudice et al. (2018a).", "Although defences based on outlier detection can be bypassed, as shown by Koh et al. (2017) (Stronger poisoning attacks break data sanitization defences), the complexity of the bilevel problem significantly increases compared to Munoz-Gonzalez et al. (2017).", "Thus, applying the attack strategy proposed by Koh et al. (2017) is also computationally intractable in our experimental settings.", "One of the main advantages of pGAN is the possibility of generating poisoning attacks at scale with detectability constraints capable of targeting large deep networks, where strategies relying on bilevel optimization have a limited applicability.", "Please, let us know if there are aspects that remain unclear or that require further clarification.", "Thank you very much."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 363, "sentences": ["The best part about this paper is that the size of the models are much smaller; but the paper does offer any explanation of the value of this.", "For example, even a 1% drop in accuracy can be unacceptable; but in some applications (like cell phones and IOT devices) model size is critical.", "The authors' should add some wording to explain this value.", "R) Thank you this good observation (Added to the paper)", "The \"adaptive\"kernels the the authors talk about", "are really a new class of nonlinear kernels.", "It would be very interesting to see a discussion of the class of functions these nonlinear kernels represent.", "This kind of discussion would give the reader  motivation for the choice of function, ideas for how to improve in this class of functions, and insight into why it works.", "The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.", "It would be nice if the authors pointed to a git repository with their code an experiments.", "R) Now we have a pytorch version of the code at https://github.com/adapconv/adaptive-cnn", "With MNIST and CIFAR.", "More importantly, the results presented are quite meager.", "If this is a method for image recognition,", "1)\tit would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.", "R) We added a new experiment for a real life application; testing different topologies.", "2)", "And the analysis of the \"dynamic range\" of the algorithim is missing.", "R) New data was added to the paper exercising multiple topologies, in a wider range of applications.", "3) How do performance and model size trade off?", "R)     A new experiment added to the paper shows the accuracy degradation vs model compression", "4) How were the number of layers and kernels chosen?", "R)We started with the original topology replacing convolutional kernels by the Adaptive kernels, then we reduced kernel by kernel, retraining the model each time to match the accuracy (with small drop).", "But our proposal is not the topology is the new type of filters, so many topologies can be improved using this type of filters, for instance an Adaptive ResNet.", "5) Was the 5x10x20x10 topology used for MNIST the only topology tried?", "R)We tested many, and we think that we can continue reducing the model, but our purpose is not to present a topology, our purpose was to show the advantages of Adaptive convolutions, having a model 66X smaller, 2X less MAC operations and trained 2X faster give us the clue that many researchers can explore on their own topologies and get benefits of it.", "6) That would be very surprising.", "What is the performance on all of the other topologies tried for the proposed algorithm?", "R) A table comparing different topologies is included in the new version of the paper.", "7) Was crossvalidation used to select the topology?", "If so, what was the methodology.", "We started with the reference topology like: ResNet18, LeNet, etc. then we reduce the number of kernels and layers keeping similar accuracy.", "Additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in Section 3; (2) omitted details, e.g.,", "1)how much overlap exists between kernels (Figs. 1, 2, and 4 suggests there is no overlap - this should be made clear);", "R)That is right, there is not overlap.", "and (3) poor grammar and nonstandard terminology, e.g., the authors' use of the word \"energy\" and the phrase \"degradation problem\".", "All of these issues should be addressed in a future version of the paper.", "R) Terms like \u201cenergy\u201d  were removed from the paper.", "We didn\u2019t invent the terminology \u201cdegradation problem\u201d it was used here https://arxiv.org/pdf/1512.03385.pdf, you want us to remove it?", "(Fixed on the paper)", "Not sure why Eqns. 2 and 9 need any parentheses", ".  They should be removed.", "(Fixed on the paper)"], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_followup", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 364, "sentences": ["We appreciate AnonReviewer3\u2019s recognition of our work.", "- Network details", "We only replace the last fc layer of ResNet-101 with a new fc layer mapping to 49 (5+20+24 = 49) outputs for calculating cardinality, states and permutation (the choice of these numbers explained in our response to R2).", "- inference time", "We also performed extra experiment on accuracy and inference time between different detectors (on the same machine and GPU) reported here:", "Faster R-CNN: AP=0.68, Inference time=101 ms", "YOLO\u00a0v2: AP=0.68, Inference time=12.3 ms", "YOLO\u00a0v3: AP=0.70, Inference time=18.2 ms", "Our network: AP=0.75, Inference time=15.1 ms", "- test on PASCAL VOC", "We observed PASCAL VOC dataset include many images with more than 4 persons.", "Considering the images include up to 4 persons only, we might not have enough training data to train ResNet-101 network."], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 365, "sentences": ["Thanks for your attention to our work.", "1) For the first problem that the duality gap is only an upper bound of F-distance.", "Our logic is that: a) There exists a condition s.t. duality gap = 0.", "b) If duality gap = 0, then the generator is the best one that can generate the true distribution.", "May be in the algorithm, we will miss the best generator because we do not get the equilibrium.", "2) Our method may encounter the same problem as the traditional algorithm.", "It is a kind of Markov chain to train the Loss. And the essence of the algorithm is in fact to solve $\\sup_f \\inf_{g^*} V(f, g^*)$ and $\\inf_g \\sup_{f^*}V(f^*, g)$. We should consider some better algorithm to solve it.", "3) For the experiments, we will do some modification and improve our network."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 366, "sentences": ["We thank the reviewer for their comments.", "We provide answers below:", "* \u201cThe DFW linearizes the loss function into a smooth one, and also adopts Nesterov momentum to accelerate the training.\u201d", "We would like to clarify this statement: one of the key ideas of the DFW algorithm is not to linearize the loss function $\\mathcal{L}$, but only the model $f$.", "* \u201cBoth techniques have been widely used in the literature for similar settings\u201d.", "We wish to clarify the main technical contributions of this paper, since the SVM smoothing and the application of Nesterov acceleration are not the main novelty of this work.", "We discuss the summary of contributions (available at the end of section 1 of the paper) in the context of technical novelty.", "- Employing a composite framework allows us to use an efficient primal-dual algorithm.", "As stated by Reviewer 1, this is novel in the context of deep neural networks: \u201cTo my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network [..]\u201d.", "- Crucially, our approach yields an update at the same computational cost per iteration as SGD and with the same level of parallelization.", "In contrast, in the closest approach to ours, the algorithm of Singh & Shawe-Taylor (2018) can only process a single sample at a time.", "This results in an approach whose runtime is virtually multiplied by the batch-size (it would be slower by two orders of magnitude in typical classification settings, including for the experiments of this paper).", "- We do not mean to claim that the application of Nesterov acceleration is a technical novelty in itself.", "However, its use is subtle in our case (see appendix A.7) and it is empirically crucial for good performance, hence its mention in the paper.", "- To the best of our knowledge, the hyper-parameter free smoothing approach that we propose in this work is novel (but is not the main contribution).", "We have adapted the abstract and summary of contributions to focus on the main novelty, which is an optimization algorithm for deep neural networks with an optimal step-size at the same computational cost per iteration as SGD.", "If the reviewer remains concerned by a lack of novelty, we would be grateful if he/she could provide specific references so that we can compare them in detail with the DFW algorithm."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 367, "sentences": ["Thanks for the detailed and encouraging feedback! We reply all comments below (relevant ones are put together):", ">> Comments #1, #11", "We mainly account the success of this simple training strategy to the simplicity of the model, the relatively low dimensionality of our input features, and the simplified action space (though all  three suffice to obtain a good controller in the current settings).", "They make the training of the controller much easier compared to other RL tasks with higher dimensional features or larger output space.", "We have added the detailed PPO-based training algorithm in Appendix A.1.", "While AutoLoss is amenable to different policy optimization algorithms, we empirically find PPO performs better on NMT, but REINFORCE performs better on GANs.", "As to the online setting, thanks for pointing us to the \u201cshort-horizon bias\u201d paper.", "We have indicated in the revision the existence of this bias -- this bias was observed on the GAN task -- overtraining G can increase IS in a short term, but may lead to divergence in a long term as G becomes too strong.", "On the other hand, we didn\u2019t observe it harms on NMT task noticeably.", "We hypothesize the tradeoff is insignificant on NMT, as in our multi-task setting, slightly over-optimizing one task objective usually does not have irreversible negative impact on the MT model (as long as the other objectives are optimized appropriately later on).", ">> Comments #2, #3", "We\u2019d like to clarify that S=1 is consistent in the overhead section and Algorithm.1.", "S controls how many sequences to generate to perform a (batched) policy update (i.e. S is the batch size), and we set S=1 for all tasks.", "Only T differs across tasks, but we always update \\phi whenever a reward is generated.", "Back to comment #2: for regression and classification, we have experimented with larger S and found the improvement marginal.", "As each reward is generated via an independent experiment, the correlations among gradients are unobvious.", "For large-scale tasks, we use memory replay to alleviate correlations in online settings (please see Algorithm 2 in Appendix A.1 in our revised version).", "Performing batched update with a larger S might help reduce correlations; However, a large S, as a major drawback, requires performing ST (S>>1) steps of task model training, in order to perform one step of controller update.", "This yields better per-step convergence, but longer overall training (wallclock) time for the controller to converge.", "There might exist sweet spots for S where one can achieve both good per-step convergence and short training time, but we skip the search of S and simply use S=1 as it performs well.", "It is worth noting that some recent literature uses a stochastic estimation of the policy gradient with batch size 1 as well, and report strong empirical results [1].", "[1] Efficient Neural Architecture Search via Parameter Sharing.", "ICML 2018", ">> Comment #4", "We observe the controller performance on all 4 tasks are insensitive to initialization.", "A good initialization (e.g. in NMT, equally assigning probabilities to each loss at the start of the training) indeed leads to faster learning, but most experiments with random initializations manage to converge to a good optima,", "thanks to \\epsilon-greedy sampling used in training.", ">> Comment #5", "They are the same -- there is a typo leading to confusion in the sentence \u201c...in Figure 1 where we set different \\lambda in l_2 = \\lambda |\\Theta|_2...\u201d; which should be \u201c...in Figure 1 where we set different \\lambda in l_2 = \\lambda |\\Theta|_1...\u201d.", "We have fixed it in the latest version.", ">> Comment #6", "Please see the last paragraph in page 5.", "For regression, classification and NMT, we split data into 5 partitions D_{train}^C, D_{val}^C, D_{train}^T, D_{val}^T, D_{test}. AutoLoss uses D_{train}^C and D_{val}^C to train the controller.", "Once trained, the controller guides the training of a new task model on another two partitions D_{train}^T, D_{val}^T. Trained task models are evaluated on D_{test}. Baseline methods use the union of D_{train}^C, D_{val}^C, D_{train}^T, D_{val}^T for training/validation.", "For GANs that do not need a validation or test set, we follow the same setting in [1] for all methods.", "[1] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ICLR 2016.", ">> Comment #7", "Thanks for pointing out -- we apologize for misusing \u201cexploding or vanishing gradients\u201d and have revised the paper to be accurate.", "We simply intended to clip the reward to reduce variances, and fount it effectively improved training.", ">> Comment #8, #9", "Thanks for pointing us to these two works.", "In [1], the authors investigate several features and develop a controller that can adaptively adjust the learning rate of the ML problem at hand, similarly in a data-driven way.", "In [2], the authors propose to manually balance the training of G and D by monitoring how good G and D are, assessed by three quantities and realized by simple thresholding.", "By contrast, AutoLoss offers a more generic way to parametrize and learn the update schedule.", "Hence, AutoLoss fits into more problems (as we\u2019ve shown in the paper).", "We have appropriately revised the two claims and cited them in the latest version.", ">> Comment #10", "Empirically, IS^2 or IS do not make much difference on the performance.", "The scaling term is a flexible parameter that controls the scale of the reward which we do not tune very much though.", ">> Comment #12", "Yes, in WGAN, it is preferable to train the critic till optimality.", "We have revised the statement for accuracy -- we observe in our experiments, for DCGANs with the vanilla GAN objective (JSD), more generator training than discriminator training generally performs better (but this may not be an effective hint for other GAN objectives as they behave very differently).", ">> Comment #13", "We have added Appendix A.8 to disclose all hyperparameters.", "All code and model weights used in this paper will be made available.", ">> Comment #14", "We\u2019ve revised our statements to be more accurate: for all GANs and NMT experiments, we observe AutoLoss reaches better final convergence; For GAN 1:1, GAN 1:9, AutoLoss trains faster; for NMT experiments, AutoLoss not only trains faster but also converges better.", "We\u2019d like to clarify that for all our GANs and NMT experiments, the stopping criteria of an experiment is either divergence or when we don\u2019t observe improvement of convergence for 20 continuous epochs.", "This is why in Fig.2, Fig.3(L) and Fig.4(c), it looks like that different methods are given different training time.", ">> Comment #15", "We have update Figure.4(b) to a scatter plot, and fixed mentioned typos in the current version."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_contradict-assertion", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 368, "sentences": ["We thank the reviewer for the comments!", "Q: \u201cADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4)\u201d", "The differences in training time are due to the size of the models: Weight tying saves a lot more parameters for the Billion Word model due to the larger vocab compared to the WikiText-103 models which have a smaller vocab.", "On WikiText-103, tying saves 15% of parameters (Table 3, ADP vs ADP-T, 291M vs 247M) and training time is reduced by about 13%.", "On Billion Word, tying saves 27% of parameters (Table 4) and training time is reduced by about 34%.", "The slight discrepancy may be due to multi-machine training for Billion Word compared to the single machine setup for WikiText-103.", "Q1: \"I am curious about what would you get if you use ADP on BPE vocab set?\"", "We tried adaptive input embeddings with BPE but the results were worse than softmax.", "This is likely because 'rare' BPE units are in some sense not rare enough compared to a word vocabulary.", "In that case, the regularization effect of assigning less capacity to 'rare' BPE tokens through adaptive input embeddings is actually harmful.", "Q2: \"How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?\"", "For WikiText-103 (Table 3) we measured 24.92 on test with a full softmax model (a 5.2 PPL improvement over the previous SOTA).", "This corresponds to a Transformer model including our tuned optimization scheme.", "Adding tied adaptive input embeddings (ADP-T) to this configuration reduces this perplexity to 20.51, which is another reduction of 4.4 PPL."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 369, "sentences": ["Thanks for your constructive feedback.", "We have modified the paper to include some of the experiments you have suggested.", "Please find our detailed response below:", "[R4: First, the proposed attack method can yield adversarial perturbations to images that are large in the \\ell_p norm.", "Therefore, the authors claim that the method can attack certified systems.", "However, attack in Wasserstein distance and some other methods can also do so.", "They can generate adversarial examples whose \\ell_p norm is large.", "I think the author should have some discussions about these related methods.]", "Thank you for pointing us out to the missing related work which we have included in the revision.", "Indeed, the Wasserstein attack and the other previously mentioned non-$\\ell_p$ bounded attacks are alternatives for producing quasi-imperceptible non-$\\ell_p$ bounded adversarial examples.", "Any of these methods can alternatively be used for generating non $\\ell_p$ bounded attacks.", "However, one major advantage of our attack method over the Wasserstein attack may be its simplicity and scalability.", "Per your suggestion, we ran experiments using the Wasserstein attack.", "The authors of [1] suggest that the Wasserstein PGD attack works best when the attacker takes PGD steps in $ell_p$-norm directions and then project the noise back onto the Wasserstein ball.", "We used their official implementation and adapted it to attack the Randomized Smoothed classifier.", "Based on the official implementation, after every 10 iterations, if the attack is not successful, we increase the radius of the wasserstein ball in which the noise is projected back onto.", "Consequently, the attack is always able to reach a comparable, but slightly weaker, spoofed certified radii (~ 67% that of the shadow attack) at the cost of slightly more perceptible adversarial noise in difficult cases.", "Note that the reason that the examples are more perceptible than those from [1] is that they are made to produce large certified radii and not only cause misclassification (i.e., the entire Gaussian augmented batch needs to get misclassified.) A comparison of the resulting images and average certified radii of those images can be found in the following anonymized link:", "https://docs.google.com/spreadsheets/d/1F0P8aOD_5aiVjW3CrR49fudz4EgrORz7v4t0ZIJEBAo/edit?usp=sharing.", "[1] Wong et al., \u201cWasserstein Adversarial Examples via Projected Sinkhorn Iterations\u201d.", "[R4: Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].", "I hope to see some discussions about this. Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.]", "Per your request, we have attacked the work of [2] and reported results of attacking the pre-trained SmoothAdv classifiers (available in [3]) in Appendix B.", "Similar to the non-adversarially trained smooth classifier included in the original submission, we can produce adversarial examples for the SmoothAdv classifier which on average produce larger certified radii than their natural example counterpart.", "Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.", "[2]. Salman et al., \u201cProvably Robust Deep Learning via Adversarially Trained Smoothed Classifiers\u201d, NeurIPS 2019", "[3]. https://github.com/Hadisalman/smoothing-adversarial"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 370, "sentences": ["Thank you very much for the comment.", "We believe that functional gradient based methods have much room to explore, and it is our hope that many aspects of GANs can be analyzed using this philosophy."], "labels": ["rebuttal_social", "rebuttal_summary"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_reject-criticism_label"]}
{"abstract_id": 371, "sentences": ["We would like to thank Reivewer 1 for your time and the review.", "We agree that more experiments based on other types of dataset will make the result stronger which we hope to perform in a follow up work.", "However, we believe that the current results already give substantial evidences that the method successfully disentangle local and global structure.", "While the performance of VAE+Auxiliary in digit identity clustering is not higher than two of the other methods, we found that the grouping can corresponds to other global features such as how many digits are in the image and the global-colour style.", "Therefore, the lower clustering accuracy does not mean that the method poorly disentangle local and global information but rather suggesting that the digit identity clustering is an incomplete evaluation metric for unsupervised clustering.", "We hope that reviewer 1 see that, in the context of this paper, the experiments have substantially fulfilled their proposed of showing that the method can disentangle global and local information as intended."], "labels": ["rebuttal_social", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 372, "sentences": ["Due to the overlap between reviewer comments, we decided to address all concerns in a single response (please see above)."], "labels": ["rebuttal_other"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label"]}
{"abstract_id": 373, "sentences": ["Thank you for your review.", "Please also see our high-level clarification above which we believe can help in better interpretation of our contribution.", "Some specific responses below:", "-- \u201cproposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.\u201d", "Please refer to our overall comments on this question (and also a few more details in reply to Reviewer#1\u2019s similar question).", "-- Comment on scale / speed for large instances of combinatorial optimization:", "The point of this work is only to see if ML can find optimal algorithms, and not about doing it faster than the known theoretical algorithms.", "Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers, since our problems are online, i.e., the instance is not known beforehand, so there is no comparison to such \u201cgeneral-purpose\u201d solvers.", "Thus we don't compare to the running time of offline solvers, but to the worst-case competitive ratio of the optimal online algorithms.", "As mentioned in the comment, this approach may eventually lead to finding optimal or near-optimal algorithms for a problem (not an instance of a problem) for which no algorithm is known -- but this is outside the scope of this work future work.", "Again drawing the analogy of playing Go, the objective is mostly on training an agent that can make competitive moves rather than very fast moves, and there is no known \u201cgeneral-purpose\u201d strategy to accomplish this.", "*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*", "-- \u201cSki Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, i.e. problem instances are not generated by use of a machine learning model, which is one of the main claims the authors are making.\u201d", "Please see our high-level clarification on top.", "(4) We agree with the reviewer that in many cases there is a gap between solving the discrete problem and the fractional problem.", "In general it is an established approach to solve the fractional problem and use additional techniques such as rounding to fill the gap.", "As to AdWords, although the discrete problem naturally corresponds to the real world scenario, we do not consider fractional AdWords below the bar", "compared to discrete AdWords in terms of difficulty", ".", "The optimal CR bound and the adversarial distribution are the same for both cases, and the optimal algorithms basically have the same structure.", "One may arguably say that the optimal algorithm for the fractional problem has richer structure as in the fractional problem the action space is much larger as we can fractionally assign each ad to many advertisers.", "As to the shortcomings of our techniques and why we pick the fractional problem, note that the GAN framework needs the computation of the discriminator network (i.e. the algorithm agent in our context) to be differentiable in order to update the generator network (i.e. the adversary in our context) during training.", "This poses difficulties if we ask the algorithm agent to make discrete decisions via sampling or rounding since it will not be differentiable.", "This doesn\u2019t mean that our high-level framework (i.e. training the algorithm and adversary networks simultaneously) is doomed, since we can use other ML techniques (e.g. reinforcement learning) to implement our framework, but in general sampling and rounding will lead to much more work during training, so we pick the GAN structure in this work.", "(5) We know from theory that if the algorithm player runs a no-regret dynamic (e.g. MWU) and the adversary player responds with the worst input for the algorithm in each round, then the algorithm player converges to the optimal algorithm, and the uniform distribution over the adversary player\u2019s responses gives the adversarial distribution.", "However, we cannot really follow this approach as the space of algorithms is infinite and we cannot run a MWU on this space, and in general it is also hard or impossible to find the absolute worst input in each round.", "In the practical framework, the algorithm player uses a neural network, and the adversary network tries its best to come up with a bad (but not necessarily worst) input each round.", "Thus we don\u2019t have all the clean theoretical guarantees anymore, but the intuition should still largely hold (as our empirical result suggests).", "(6) We updated the appendix to address this. See \u201cTraining convergence\u201d in Appendix D.2", "(7) We updated the appendix to address this. See \u201cAdversarial distribution\u201d in Appendix D.2"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_reject-request", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_none", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 374, "sentences": ["We thank the reviewer for reading the paper and the comments.", "As already stated in the comments below, our claim of state-of-the-art in the original manuscript pertains to models with a single softmax, which we clearly state in section 4.1.", "We will update the abstract to remove any confusion.", "As suggested by multiple reviewers, we have performed further experiments by incorporating our Past Decode Regularization (PDR) in the mixture-of-softmax (AWD-LSTM-MoS) model of (Yang et al. 2017).", "We use the same model sizes as used in the paper.", "As shown below, we observe gains of 0.4 and 1.0 perplexity points for PTB and WT2, while with dynamic evaluation the gains are 0.4 in both cases.", "AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)", "Penn Treebank with finetuning -", "56.2/53.8", "||  56.5/54.4", "Penn Treebank with dynamic evaluation -", "48.0/47.3", "||  48.3/47.7", "WikiText-2 with finetuning -", "63.0/60.5", "||  63.9/61.5", "WikiText-2 with dynamic evaluation -", "42.0/40.3", "||  42.4/40.7", "Note that, we performed very limited hyperparameter tuning in the vicinity of the hyperparameters used by (Yang et al. 2017) and a more exhaustive search is likely to lead to better gains.", "Thus, the gains due to PDR generalize to more complex models like AWD-LSTM-MoS+PDR.", "We can justify PDR theoretically as an inductive bias on the language model.", "The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform.", "Similarly, the distribution of the first word given the second word will be far from uniform.", "A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one.", "In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the \"true second word\" and decode the distribution of the first word.", "Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram.", "We believe language modeling is a fundamental problem in NLP and our work continues a long stream of papers that have achieved steadily lower perplexities over the past few years.", "We evaluated our approach on two standard datasets that have been used as a benchmark in most of these papers.", "As suggested by multiple reviewers, we have conducted further experiments on the Gigaword corpus to test PDR on larger corpora.", "Specifically,  we use a 2-layer LSTM with hidden dimension 1024 and a word embedding dimension of 1024.", "We truncated the vocabulary by keeping approximately 100k words with the highest frequency and used the same validation and test sets as (Yang et al. 2017).", "We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 points in the test perplexity.", "Note that we tuned the PDR loss coefficient very coarsely and tuning it further could lead to higher gains.", "We will update the manuscript with these additional results and discussion and post it shortly.", "Yang et al. 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv:1711.03953."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_other_label"]}
{"abstract_id": 375, "sentences": ["We thank Maxwell for some clarification.", "We believe AnonReviewer2 misunderstood some of the concepts and we will try to clarify them here and update the paper accordingly.", "- predicting unordered sets", "The assumption is what is available as GT is a set.", "This means we cannot infer any specific ordering from GT.", "The proposed framework is very flexible as we don\u2019t need to enforce the problem to be necessarily orderless  (although it can be).", "The reason we would like to learn  p_m(\\pi | x_i, w) is to infer the nature of the problem.", "However, excluding the main experiment in supplementary material, we did enforce the problem to be orderless by removing O2 and the permutation loss.", "This is equivalent to assume p_m(\\pi | x_i, w) is uniform (order does not matter) in Eq.2 and you can see O2 and its loss will be eliminated from Eq. 5 and 6.", "However, we still require to solve Eq. 5 to find the best permutation based on f1 only, which is equivalent to use Hungarian to solve the assignments.", "We also disagree with R3 that the problem is either unordered sets or there exist only one order to be correct.", "There can exist multiple orders to be true, but not all.", "This can be inferred by learning p_m(\\pi | x_i, w) from samples derived during training by Eq. 5.", "- permutation in the likelihood (2) does not make sense:", "In addition to what is explained by Maxwell, I add this clarification:", "p_y(y_1 | x, w, (1, 2)) means the first output is assigned to the first ground truth, while p_y(y_1 | x, w, (2, 1)) mean the first output is assigned to the second ground truth.", "These two scenarios are acctally generate very different gradient.", "The same argument can be extended to p_y(y_2 | x, w, (1, 2)) and p_y(y_2 | x, w, (2, 1)).", "- the dependence on \\pi drops out when getting a MAP estimate of outputs:", "The permutation takes into the account when there is loss and a GT to compare as GT  annotations are permutated to be assigned to the outputs.", "During inference, we don\u2019t have loss and GT.", "We just have the predicted outputs, e.g. cardinality, states and premutation and the order which we want to show the states will not change the value of the states.", "We hope to have clarified all the technical misunderstandings.", "We would like to point the reviewer again to our impressive results in the detection problem and ask him/her to reconsider his/her rating if the technical concerns are now clear."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 376, "sentences": ["We thank the reviewer for taking the time to consider our paper and appreciate that you are excited to use our counterfactually-augmented dataset.", "While degrees of novelty and the relevant sorts of novelty are a matter of opinion we respectfully assert our view that new ideas, the new resource that we present, and the scientific insights derived from our experiments, are precisely the sorts of novelty that should be sought by conferences.", "We respectfully disagree with the reviewer\u2019s suggestion that a fundamentally distinct resource warrants only a whitepaper.", "We politely point out that many conferences have entire dedicated tracks, and even best paper awards for resources, and that many seminal papers of pivotal importance to the field make precisely this sort of contribution (e.g. ImageNet).", "Additionally we point out that the resource is not the only novel idea here.", "Of chief importance here is the intellectual contribution casting the problem of learning \u201csuperficial associations\u201d coherently in the language of intervention, and producing a dataset that addresses counterfactuals in a real sense (as pointed out more eloquently by R1).", "Moreover, our experiments shed insights about the price to be paid for relying less on spurious associations and our updated experiments (inspired by R3\u2019s suggestions) show that our methods result in improved performance out-of-sample on a variety of datasets.", "We hope that you might be willing to reconsider our contributions in light of the significance and uniqueness of the dataset, the insights of our experiments and the demonstrated out-of-domain robustness."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_summary", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 377, "sentences": ["1", ".", "We thank the reviewer for pointing out papers [1] and [2].", "We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.", "In essence, our scheme is different than [1] in two key aspects: (1) we pipeline both the forward and backward passes of the backpropagation while [1] pipelines only the backward pass.", "Further, equation (9) in [1] suggests that while weight updates use delayed gradients, the delayed weights (W^(t-K+k)) are used for the weight gradient calculation.", "This is essentially similar to weight stashing used in PipeDream, which we compared to in our paper.", "Thus, our scheme has the advantage of a smaller memory footprint.", "The follow up work in [2] attempts to reduce the memory footprint through feature replay (i.e., re-computing activations during backward pass, similar to GPipe).", "Our scheme saves the activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization of the accelerators (GPUs).", "We will edit the related work section to include the above discussion.", "2.\tThe method proposed in our paper stores immediate activations, which is mentioned in Section 3 of the submission.", "3.", "We appreciate the pointer to the better performance of ResNet-110.", "We trained the network for only 164 epochs with a batch size of 100, which is probably the reason that its inference accuracy is lower than expected.", "Should we adopt the hyperparameters (a batch size of 128) and more training epochs (200 epochs) as shown at https://github.com/akamaster/pytorch_resnet_cifar10 , our ResNet-110 baseline reached 93.59% in inference accuracy, and the pipelined ResNet-110 reached 92.88% in inference accuracy.", "The speedup obtained is 1.73X, slightly higher than the 1.71X obtained in our paper, which could be caused by the batch size increase that makes the GPU process more efficient.", "The exact inference accuracy of the model is somewhat orthogonal to our study.", "It is the trend of the decline in inference accuracy with pipelining is what we study and this trend exists with both our hyperparameters and those at https://github.com/akamaster/pytorch_resnet_cifar10.", "Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.", "4.", "Indeed, comparisons to the results in [1][2] would be interesting.", "However, since the scheme in [1] employ weight stashing as PipeDream does and in [2] utilizes re-computing activations, as in GPipe, our comparisons to PipeDream and GPipe subsume comparisons to [1][2], particularly given the space limitations of submission.", "5.", "We appreciate such detailed and rigorous convergence analysis provided in [1] and [2].", "The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing or micro-batching, is simpler and does converge.", "The paper does achieve this goal, on a number of networks.", "Given the limited space provided, it would be difficult to fit a convergence analysis in our paper."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 378, "sentences": ["Thank you for your review.", "We believe it captures the essence of what we are proposing and are delighted with the overall very positive assessment.", "As regards the weakness mentioned, we accept the characterisation of our work as incremental in the sense that it draws together a number of known techniques from areas such as multi-task and adversarial learning.", "We argue, however, that the contribution of our submission lies exactly in the unification of these approaches into the stethoscope framework, which lends itself to targeted representation analysis and modification.", "We showcase stethoscopes to provide insights into a particular application domain, stability prediction in intuitive physics, but believe that the methods presented here will provide a ready toolkit for researchers addressing a variety of challenges in network interpretability and (de)biasing."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 379, "sentences": ["Thank you for the time. We would like to take this opportunity to correct some factually incorrect statements below.", "[Q] The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.", "[A]  We respectfully disagree.", "To our knowledge, this is only the second work which attempts to fairly and systematically compare GANs in a large-scale setting.", "The main conclusions of our work (about NS-GAN, spectral normalization, and gradient penalty) hold across several datasets and architectures.", "[Q] But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.),", "[A] We again respectfully disagree -- both LSUN and CelebaHQ are used for the first time in such a large-scale evaluation.", "In fact, none of the techniques were previously evaluated on CelebaHQ.", "Furthermore, even if some data sets, such as LSUN, were used previously, the comparison to other works was always done by the authors of the new method usually with additional changes, such as architectural decisions and optimization tricks.", "[Q] Not clear if the improvement in performance is statistically significant, how robust it is to changes in other parameters etc.", "[A] In this we take care of systematically evaluating various design decisions.", "While the space of design decisions is too large to search over, we focus on the main design choices and provide some conclusions in this context.", "Performance improvements obtained by both spectral norm and gradient penalty are statistically significant as seen in the plots -- the performance with respect to the baseline is far outside of the two standard errors of the median in most settings.", "[Q] The authors also rely mostly on the FID metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.)", "[A] FID was shown to correlate well with perceived image quality (e.g. precision) and mode coverage (recall).", "The evidence can be found in [1], and [2].", "As such, a reduction in FID corresponds both to improved image quality, as well as improved mode coverage.", "IN practice, a 10% drop in FID is visible to a human, and samples can be seen in the Appendix.", "While it is not a perfect metric, it is arguably useful for sample-based relative comparison of generative models.", "[1] https://arxiv.org/abs/1711.10337", "[2] https://arxiv.org/abs/1806.00035", "[Q] The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them, give mathematical formulations, making it hard to the non-expert reader to understand what are these techniques and why are they introduced.", "[A] Most of these are described in Section 2 (in particular, discussion on regularization and penalties is in Section 2.2).", "Describing all aspects of these techniques would require substantially more space and hence we refer to the original work for precise formulation.", "[Q] With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.", "[A] We respectfully disagree: we believe that for GAN practitioners our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most resnet tricks do not matter, etc.", "All of these insights are supported by a fair and unbiased rigorous experimental process.", "On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 380, "sentences": ["We thank Reviewer 3 for raising important questions.", "We answer them below.", "Using \\tilde x in the E- and M-steps.", "We agree with Reviewer 3 that \u201cthe error arising from quantizing v into c is only affected by a subset of rows of \\tilde x\u201d.", "However, we solve Equation (2) with this proxy algorithm for two reasons.", "First, using the full \\tilde x matrix allows to factor the computation of the pseudo-inverse of \\tilde x and thus allows for a much faster algorithm, see answer to Reviewer 2 and the details of the M-step in the paper (as well as footnote 2).", "Second, early (and slow) experiments suggested that the gains were not significant when using the right subsets of \\tilde x in this particular context.", "Minimizing the reconstruction error", "Our method results in both better reconstruction error and better training loss than na\u00efve PQ *before* any finetuning.", "As we state in the paper, applying naive PQ without any finetuning to a ResNet-18 leads to accuracies below 18% for all operating points, whereas our method (without any finetuning) gives accuracy around 50% (not reported in the paper, we will add it in the next version of our paper).", "Choosing the optimal number of centroids/blocks size", "There is some rationale for the block size, related to the way the information is structured and redundant in the weight matrices (see in particular point 1 of answer to Reviewer 1).", "For instance, for convolutional weight filters with a kernel size of 3x3, the natural block size is 9, as we wish to exploit the spatial redundancy in the convolutional filters.", "For the fully-connected classifier matrices and 1x1 convolutions however, the only constraint on the block size if to be a divisor of the column size.", "Early experiments when trying to quantize such matrices in the row or column direction gave similar results.", "Regarding the number of centroids, we expect byte-aligned schemes (256 centroids indexed over 1 byte) to be more friendly for an efficient implementation of the forward in the compressed domain.", "Otherwise, as can be seen in Figure 3, doubling the number of centroids results in better performance, even if the curve tends to saturate around k=2048 centroids.", "As a side note, there exists some strategies that automatically adjust for those two parameters (see HAQ for example).", "Comparison with pruning and low-rank approximation", "We argue that both pruning and low-rank approximation are orthogonal and complementary approaches to our method, akin to what happens in image compression where the transform stage (e.g., DCT or wavelet) is complementary with quantization. See \u201cDeep neural network compression by in-parallel pruning-quantization\u201d, Tung and Mori for some works investigating this direction."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 381, "sentences": ["We thank the reviewer for raising useful points which helped us a lot improving the paper.", "The main point of the reviewer is that the novelty of our approach is limited with respect to the Evolutionary RL (ERL) algorithm, and that improvement is sometimes small.", "These remarks helped us realize that we had to better highlight the differences between our approach and ERL, both in terms of concepts and performance.", "We did so by replacing Figure 1, which was contrasting CEM-RL to CEM, with a figure directly contrasting CEM-RL to ERL.", "We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.", "By the way, the ERL paper is now published at NIPS, but it was not the case yet when we submitted ours. We updated the corresponding reference.", "The reviewer seems to consider that each actor in our CEM-RL algorithm comes with its own critic (the reviewer says value function), which would raise a value function initialization issue.", "Actually, this is not the case: there is a single TD3 critic over the whole process, and gradient steps are applied to all the selected actors from that single critic.", "This has been clarified in the text by insisting on the unicity of this critic.", "We agree with the reviewer that the importance mixing did not provide the sample efficiency improvement we expected, and we can only provide putative explanations of why so far.", "Nevertheless, we believe this mechanism still has some potential and is currently overlooked by most deep neuroevolution researchers, so we decided to keep the importance mixing study in Appendix B rather than just removing it."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label"]}
{"abstract_id": 382, "sentences": ["We thank the reviewer for their time and comments on the work.", "Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.", "See Figures 2,3,5 for more learning curve results and baseline comparisons and Figure 6 for qualitative metric analysis.", "We show that our method outperforms the baselines across multiple environments.", "In the paper, we include many details on the environment rewards and design as we consider these simulation tasks part of the contribution of the work.", "The simulation tasks contain robotic humanoid characters that need to learn how to navigate given egocentric vision.", "No other simulation is available that combines these challenges.", "The simulation will be released with the work for others to use and build on multi-agent learning methods.", "We have reviewed the provided references and have included them in the paper."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label"]}
{"abstract_id": 383, "sentences": ["* models that get scores in the ~80 ppl range for Penn Treebank are important.", "we agree with the advice but not with the justification.", "We explain why in the general response: our goal is not to get good language models, but to use language modelling as a setting to test a property of a mechanism that is proposed.", "The perplexity becomes a way to observer the effect of a mechanism and not the goal itself.", "Moreover, (not in this case but) the architectures used to achieve better scores on given datasets are so over-parametrized that it's hardly reasonable to assume that the improvement justifies the cost of accommodating huge models overfitted to a particular dataset (and sometimes to a particular dataset configuration)", "That said, we agree that using different architectures would strengthen our point and make the paper more convincing.", "Also, using different datasets would help us demonstrate that the effect of the proposed mechanism is data-independent.", "We are also considering it's application to a different set of tasks in the future.", "We did follow reviewer recommendations and performed experiments with LSTMs and QRNN (slightly faster) along with WikiText (which is larger but not intractable), unfortunately we couldn't accommodate all the analysis and changes in time.", "* its parameter-reduction approaches against other compression and hyperparameter optimization techniques.", "We recognize that the focus on parameter reduction was perhaps counter productive to making the goal or this work clear.", "It is a byproduct of the technique, but modelling discrete distributions without prior knowledge of how many classes one might encounter is the main issue we are trying to solve. We could do that by using character-level or sub-word tokens, but again, the goal is not --solely-- language modelling as a task.", "The mechanism is applicable to settings where the number of possible input patterns is too large to instantiate as a parameter table (embeddings), but where the number of patterns that actually occur could actually more \"reasonable\". Meaning that as long as the \"world\" is not random uniform, we can make predictions."], "labels": ["rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_future", "rebuttal_future", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 384, "sentences": ["Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:"], "labels": ["rebuttal_social"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 385, "sentences": ["We thank the reviewer for their comments.", "We address their comments individually below.", "> The work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.", "The main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN", "Response: The main contribution and novelty of this paper is the demonstration of the transferrability and power of the learned representation, and thus is a good fit for ICLR.", "We use the application of protein interface prediction as a test case for this, but applications can range widely from drug discovery, to RNA folding, to small molecule quantum mechanical calculations.", "As many of these tasks are very data-poor, this demonstrated transferrability opens up novel avenues through which these problems can be tackled.", "> - Their method improves over prior deep learning approaches to this problem.", "However, the results are a bit misleading in their reporting of the std error.", "They should try different train/test splits and report the performance.", "Response: We do use different subsets of the train set for different replicates.", "However, the train and test sets cannot be mixed as they come from different data distributions (P_r and P_p) and we are trying to show we can transfer with no retraining from P_r to P_p.", "Thus our reported metrics are correct and justified for this problem, though we have clarified the exact nature of the replicates in the text to ensure this is not misleading.", "> - The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)", "Response: We do provide citations for these choices.", "See the second and third paragraphs of section 3 on page 4 for motivation/citations for sequence identity and cut-offs used, respectively.", "> -", "The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred", "Response: Fout et al. and Sanchez-Garcia et al. are feature engineering approaches -- they both use high-level features as inputs to their models (not atomic coordinates).", "Sanchez-Garcia et al. use a tree ensemble model that has no end-to-end learning aspects at all.", "Another popular pure feature engineering approach is PAIRPred (Minhas et al., Protein 2014), which uses an SVM trained on high-level features.", "However, we do not compare to them as their performance on C_p^{test} (0.863)  was already superseded in Fout et al.\u2019s work.", "IntPred [Northey et al., Bioinformatics 2017] addresses the binding site prediction problem (given one protein, which residues can be interfacial with any other protein), which is different than the problem we present.", "> - The authors use a balanced ratio of positive and negative examples.", "The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones.", "Can they show performance at various ratios of positive:negative examples?", "In case there is a consistent improvement over prior methods, then this would be a clear winner", "Response: We can demonstrate consistent performance at different ratios of positive:negative examples.", "Running tests on C_p^{test} at 1:3, 1:5, and 1:10 ratios demonstrate no significant impact on performance (0.889 [0.882 +/- 0.012], 0.889 [0.882 +/- 0.011], and 0.895 [0.886 +/- 0.015], respectively).", "The AUROC metric we use is insensitive to class imbalance, and thus is a good measure to use when evaluating on datasets with varying amounts of imbalance."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_done", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 386, "sentences": ["Thank you for your reviews.", "Here are our responses to your questions:", "1. Clarify how the skills of agents play a role in the problem setup", "We clarify the definition of skills and how it influences the manager\u2019s decision as follows.", "i) As defined in Section 3, an agent\u2019s skill depends on its state transition probabilities and its policy.", "The state transition probabilities define if a resource can be collected by an agent (i.e., whether the \u201ccollect\u201d action executed by this agent will have real effect), and it is equivalent to a binary value for each resource in Resource Collection.", "The agent\u2019s skill also depends on its policy because it affects how fast an agent can achieve a goal.", "E.g., when the agent has a suboptimal policy, it may not be able to reach a goal within the time limit even though it actually can collect the resource if given more time.", "ii) The skills are completely hidden from the manager.", "It can be inferred by the manager based on the performance history, and also on the estimated worker policies by IL.", "However, only checking whether a goal is reached is not sufficient to determine skills.", "Failing to reach a goal may be a result of several reasons -- it may be because i) the bonus in the contract is too low, ii) the contract terminates prematurely before the agent can reach the goal, or iii) the assigned task depends on another task which has not been finished yet.", "So the manager needs to infer agents\u2019 skills, preferences, and the task dependency jointly through multiple trials.", "2. Is maximizing utility justified?", "Maximizing utility is actually the setup in similar problems in economics.", "Just like those problems (e.g., mechanism design), this paper focuses on scenarios where agents won\u2019t truthfully or clearly reveal its skills and preferences to the manager, and do not always behave optimally.", "As we stated in the paper, maximizing utility is more realistic, and typically the span of the decision making process of the manager is much shorter than the time needed for improving worker agents.", "Let\u2019s consider a simple scenario.", "An agent is unable to collect a certain kind of resource.", "By maximizing its utility, it may still accept the contract and go to that resource.", "Once a resource is occupied by this agent, other agents can no longer collect it according to our setting.", "This means that the resource will never be really collected.", "As an empirical evidence,  you may compare the S2 and S3 settings with S1 in Resource Collection.", "In S2 and S3, workers may prefer a task that it can not perform, which should never happen in the case of maximizing return.", "As a result (shown in Figure 4b and Figure 4c), the training difficult significantly increases.", "3. Are there alternate ways to overcome maintaining the UCB explicitly, especially for the number of time-steps?", "Yes, there are ways to overcome this.", "First, we can define small time intervals instead of maintaining statistics for each step (i.e., combining statistics in every dT consecutive steps will reduce the complexity to 1 / dT of the original size).", "Note that this has been done in results shown in Appendix C.1, where dT also means that for every dT steps, the manager can only change the contracts once.", "Second, we may define a maximum number of steps to be considered in the performance history, which can be determined by the upper bound of the execution time for a subtask, and can be smaller than the step limit of the whole episode.", "4. What are the units for rewards in the plots?", "It is the average per episode.", "The reward is defined as in Section 5.1.1 and Section 5.1.2 without any rescaling.", "We have added this in the caption.", "5. Typos", "Thank you for pointing out these typos. We will fix them in the next revision."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 387, "sentences": ["We thank you for your useful feedback and suggestions for additional experiments, and are glad you found the connection we draw between verification and rare event estimation to be an interesting idea.", "1. \"How does the performance of the proposed method scale wrt scalability? It will be useful to do an ablation study, i.e. keep the input model fixed and slowly increase the dimension.\"", "This is a great question and something we have been looking into.", "As a first step, we have run a new experiment at a higher scale with the CIFAR-100 dataset and a far larger DenseNet-40/40 architecture as discussed in the response to Reviewer 1.", "We see our approach still performs very effectively on this larger problem, for which most existing verification approaches would struggle due to memory requirements (see also our new comparisons in Section 6.4).", "We are now working on doing an ablation study on the size of the input dimension x, but it is unlikely we will be finished with this before the end of the rebuttal period due to the fact that it will require a very large number of runs to generate.", "2. \"Did you experiment with other MH proposal beyond a random walk proposal?\"", "That\u2019s an excellent idea and a topic for future research.", "We didn\u2019t experiment with a MH proposal beyond a random walk because this was the simplest thing to try and it already worked well in practice.", "As well as different proposals, we have also been thinking about the possibility to instead use a more advanced Langevin Monte Carlo approach to replace the MH, which we expect to mix more quickly as the chains are guided by the gradient information.", "3. \"What is the performance of the proposed method against 'universal adversarial examples'?\"", "\u201cUniversal adversarial examples\u201d refers to a method for constructing adversarial perturbations that generalize across data points for a given model, often generalizing across models too.", "Our method does not give a measure of robustness with respect to a particular attack method - it is attack agnostic.", "It measures in a sense the \u201cvolume\u201d of adversarial examples around a given input, and so if this is negligible then the network is robustness to any attack for that subset of the input space, whether by a universal adversarial example or another method.", "All the same, investigating the use of our approach in a more explicitly adversarial example setting presents an interesting opportunity for future work.", "4. \"The most interesting question is whether this method gives reasonable robustness estimates even for large networks such as AlexNet?\"", "This is an important point to address.", "As previously mentioned, we have extended the experiment of section 6.3 to use the much larger DenseNet-40/40 architecture on CIFAR-100 and we see that our method still performs admirably.", "See the updated paper and our response to Reviewer 1 above.", "5. \"Please provide some intuition for this line in Figure 3: 'while the robustness to perturbations of size epsilon=0.3 actually starts to decrease after around 20 epochs.'\"", "The epsilon used during the training method of Wong and Kolter (ICML 2018) is annealed from 0.01 at epoch 0 to 0.1 at epoch 50.", "It\u2019s interesting from Figure 5 that the network is made robust to epsilon = 0.1 and 0.2 by training to be robust using a much smaller epsilon.", "The network appears to become less robust for epsilon = 0.3 as the training epsilon reaches 0.1.", "So this a counterintuitive result that training using a smaller epsilon may be better for overall robustness.", "One hypothesis for this is that the convex outer adversarial polytope is insufficiently tight for larger epsilon.", "Another hypothesis may be that training with a lower epsilon has a greater effect on the adversarial gradient at an input, as the training happens on a perturbation closer to that input.", "6. \"A number of attack and defense strategies have been proposed in the literature.", "Isn't it possible to use the proposed method to quantify the increase in the robustness towards an attack model using a particular defense strategy? If it is possible to show that the results of the proposed method match the conclusions from these papers, then this will be an important contribution.\"", "It is possible to quantify the increase in robustness using a particular defense strategy, as we do in section 6.4 for the robust training method of Wong and Kolter (ICML 2018).", "We find that our method is in agreement with theirs.", "To quantify the increase in \u201crobustness\u201d with respect to a particular attack method, you can simply record the success of the attack method over samples from the test set as the training proceeds.", "This will not, however, be a reliable measure of robustness as the network can be trained to be resistant to the attack method in question while not being resistant to attack methods yet-to-be devised (the adversarial \u201carms race\u201d).", "We believe that what we really desire is an attack agnostic robustness measure, such as the method in our work."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 388, "sentences": ["We thank the reviewer for their encouraging feedback and thoughtful comments on our work.", "Regarding the permutation learning experiment, in response to the feedback, we have revised the main text to clarify the setup.", "The core of the experiment is the ability to denoise permuted images using some representation of the permutation set.", "In order to do this successfully, it is necessary for such a representation to have certain properties such as inducing a distribution over permutations.", "We have implemented and added a comparison to the Gumbel-Sinkhorn method (Mena et al., 2018), which is a customized representation for permutations with these properties, and requires similar techniques (unsupervised objective, permutation sampling, etc.) in order to learn the latent structure.", "The ResNet classifier on top can be viewed primarily as a way to evaluate the quality of the learned permutation; both of these representations are capable of learning the right latent structure, with test accuracies of 93.6 (Kaleidoscope) and 92.9 (Gumbel-Sinkhorn) respectively.", "The highlight of this experiment is that the K-matrix representation also comes with the requisite properties for this learning pipeline, despite not being explicitly designed for permutation learning.", "Regarding comparison to a dense matrix for the speech experiment, in Table 5 (Appendix B.1.2), we compare the use of K-matrices in the raw-features speech model with several other classes of matrices, including dense matrices.", "For instance, we find that, while using a trainable dense matrix slightly outperforms just using the fixed FFT (0.3% drop in test phoneme error rate), using a K-matrix instead of a dense matrix yields a further improvement of 0.8% in the phoneme error rate.", "Regarding ease of training and hyperparameter tuning, we would like to re-emphasize that for all experiments, all hyperparameters for training were kept the same as those for training the default model architecture, other than those we explicitly mentioned as being tuned.", "In particular, we did not modify any hyperparameters (such as number of epochs, optimizer, or learning rate) for the ShuffleNet and DynamicConv experiments.", "For the TIMIT speech experiment, we tune only the \u201cpreprocessing layer\u201d learning rate.", "This is because the default speech pipeline already uses different learning rates for different portions of the network, so there is no clear choice a priori for the learning rate of the \u201cpreprocessing layer\u201d (note that most methods, including K-matrices, do not seem to be overly sensitive to the choice of this learning rate).", "Thus, in these experiments, K-matrices can be used as a drop-in replacement for linear layers without significant tuning effort.", "Regarding structure and sparsity: We use \u201cstructure\u201d in the context of structured matrices to mean matrices with a fast (subquadratic) multiplication algorithm.", "Structured matrices have a sparse factorization with total NNZ on the order of the number of operations required in the multiplication.", "This connection was known in the algebraic complexity community, and formalized by De Sa et al. (2018).", "Regarding the inductive bias encoded by K-matrices: the building block of K-matrices is a butterfly matrix, which encodes the recursive divide-and-conquer structure of many fast algorithms such as the FFT.", "Analyzing the precise effects of the inductive bias imposed by K-matrices is an exciting question for future work."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 389, "sentences": ["Thanks again, Reviewer #1, for your thoughtful comments.", "We respond to your other comments below.", "1.  \u201cIt seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?\u201d", "This is an interesting idea, but we are not sure it is applicable.", "If one looks closely at Figure 2 (b), there are still blue and black histogram bars (denoting CIFAR-10 train and test instances) covering the entirety of SVHN\u2019s support (red bars).", "2.  \u201c[The constant input]\u2019s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn\u2019t seem applicable.\u201d", "See general response #2.", "3.  \u201cHow much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images.\u201d", "We have not tested non-image data, since images are the primary focus of work on generative models, but this is an interesting area for future work.", "4.  \u201cSamples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.\u201d", "This is a very good point.", "See our response to Shengyang Sun\u2019s comment below.", "We see think this phenomenon has to do with concentration of measure and typical sets, but we do not yet have a rigorous explanation.", "5.  \u201cThere seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)\u201d", "We have fixed the spacing in the latest draft :)"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_future", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 390, "sentences": ["Thanks for your efforts in reviewing our paper and the valuable comments, but we have different opinions about your comments.", "1. Comments about the contributions and novelty", "As we emphasized many times in our paper, the success of DNN in domains such as image, speech and text, is built on the comprehensive exploration of the locality-based patterns, which motivates us to first find such patterns of features in tabular data automatically and then build up NN architecture based on these discovered patterns.", "This is the core idea of this paper.", "Thus, GBDT is just a tool we adopt to mine the patterns and do feature grouping since GBDT is an efficient and convenient method for these pre-processing tasks: 1) GBDT is very fast.", "In most experiments, the total time cost of GBDT part in TabNN is about several minutes, while the NN part often needs several hours for training.", "2) the learning of GBDT is just based on statistical information over full dataset.", "Thus, GBDT can learn the stable and robust feature combinations.", "We can definitively replace GBDT with other methods, such as feature correlations, as long as they can achieve better performance then GBDT.", "Regarding the comments asking for the comparison with GBDT, we consider that they are not comparable since we are not inventing a model to beat GBDT, instead, we are developing a model to cover the scenarios not suitable for GBDT such as some applications need online updating.", "This point has also been emphasized in our paper.", "2. Heavy feature engineering and ad-hoc practical steps", "We are not sure why you conclude this point.", "TabNN is a fully end-to-end learning approach with no need of an extra feature engineering step.", "And as stated in the paper, the design of TabNN follows two principles: \\emph{to explicitly leverages expressive feature combinations} and \\emph{to reduce model complexity}. We cannot agree there are ad-hoc parts in the proposed model. Could you explain this with more details?", "3. Benchmark Dataset and Compared with Deep and Wide (D&W) NNs", "As stated in Section 2, D&W NNs and many related models can work well with high dimensional sparse features, which are usually in the form of one-hot encoding converted from categorical features.", "Actually, these NNs perform very well in such datasets, even better than GBDT.", "In contrast, the proposed TabNN works better on another kinds of tabular data, with numerical features and low-cardinality categorical features.", "Since there are many dummy dimensions in one-hot encoding, TabNN is hard to learn the useful features combinations from them.", "Therefore, TabNN and D&W NNs are orthogonal with each other.", "We can use them independently according to the feature types of data. And they can be used together for the data with mixed feature types.", "Therefore, we did not conduct any experiment on data with high-cardinality categorical features.", "We will state this clearer in the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_reject-request", "rebuttal_done", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 391, "sentences": ["4. \"The experiments of this paper lack comparisons to certified verification", "methods. There are some scalable property verification methods that can give a", "lower bound on the input perturbation (see [1][2][3])", ".", "These methods can", "guarantee that when epsilon is smaller than a threshold, no violations can be", "found.", "On the other hand, adversarial attacks give an upper bound of input", "perturbation by providing a counter-example (violation).", "The authors should", "compare the sampling based method with these lower and upper bounds.", "For", "example, what is log(I) for epsilon larger than upper bound?\"", "The three references and the follow-up work that you cite give different methods for obtaining a certificate-of-guarantee that a datapoint is robust in a fixed epsilon l_\\infty ball, with varying levels of scalability/generality/ease-of-implementation.", "For those datapoints where they can produce such a certificate", ", the minimal adversarial distortion is lower-bounded by that fixed epsilon.", "This is important work to be sure, but we view it as predominantly orthogonal to ours, for which we define robustness differently, as the \u201cvolume\u201d of adversarial examples rather than the distance to a single adversarial example.", "We actively argue that the minimal adversarial distortion is not a reliable measure of neural network robustness in many scenarios, as it is dictated by the position of a single violation, and conveys nothing about the amount of violations present.", "Despite these being two different definitions of robustness, to try and demonstrate some comparisons between the two, we extended experiment 6.4 (already using Wong and Kolter (ICML 2018) [3]) and compared the fraction of samples for which I = P_min to the fraction that could be certified by Wong and Kolter for epsilon in {0.1, 0.2, 0.3}. We found that it wasn\u2019t possible to calculate the certificate of Wong and Kolter for epsilon = 0.2/0.3 for all epochs, or epsilon = 0.1 before a certain epoch, due to its exorbitant memory usage.", "This significant memory gain thus indicates that our approach may still have advantages when used as a method for approximately doing more classical verification, even though this was not our aim.", "Please see the updated paper for full details.", "5. \"Additionally, in section 6.4, the results in Figure 2 also does not look very", "positive - it unlikely to be true that an undefended network is predominantly", "robust to perturbation of size epsilon = 0.1. Without any adversarial training,", "adversarial examples (or counter-examples for property verification) with L_inf", "distortion less than 0.1 (at least on some images) should be able to find.\"", "You are correct that without any robustness training it is possible to find adversarial examples with distortion less than 0.1 for some inputs.", "This is indeed what our results show in Figure 5 in the appendices, illustrating our metric for individual samples.", "You can see for several samples that were not initially robustness to eps=0.1 perturbations (log(I) > log(P_min)), the value of log(I) decreases steadily as the robust training procedure is applied.", "It does appear, however, that the network is predominantly robust to perturbations smaller than 0.1 before robustness training.", "The curves in Figure 3 plot the values of our measure log(I) between the 25th and 75th percentile for a number of samples.", "This shows that the network is already robust to perturbations of size eps=0.1 for more than about 75% of samples before the training procedure of Kolter and Wong is applied.", "All the same, we agree that the original Figure 3 was confusing in this respect, and have rerun this experiment with a lower minimum threshold for log(I) to make the point clearer in the graph.", "With this lower value of log(P_min), we see the 75 percentile of log(I) over the samples quickly decrease as robustness training proceeds for eps=0.2.", "Notably, however, log(I) is incredibly small before any of this training for eps=0.1, demonstrating how it is important to not only think in terms of whether any violations are present, but also how many: here less the proportion of violating samples is less than 10^-100 at eps=0.1 for most of the datapoints.", "We thank Reviewer 1 for their critical appraisal and helpful suggestions."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 392, "sentences": ["Thanks for your efforts in reviewing our paper and the valuable comments.", "We attempt to address your concerns using the following points and hope they can help you better understand our work.", "1. the number of benchmark datasets", "Currently, there are 5 datasets in our experiments.", "Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.", "Due to the space restriction, however, we cannot present them all in the paper.", "We can provide more experiment results in appendix to eliminate this concern.", "2. XGBoost", "We use LightGBM to learn GBDT model in the experiment part.", "LightGBM is proven comparable (even better) with XGBoost in many Kaggle competitions (refer to https://www.kaggle.com/shivamb/data-science-trends-on-kaggle and https://github.com/Microsoft/LightGBM/tree/master/examples).", "Therefore, we think using LightGBM is sufficient for comparison.", "3. Two shortages of tree-based models", "Let us describe these two shortages with more details here.", "1) Hard to be integrated into complex end-to-end frameworks.", "In such framework, there are many modules, each of which may correspond to one sub-task with a global optimization goal.", "The outputs of modules can serve as the inputs of other modules.", "Therefore, to train such a framework in an end-to-end way, the module should be able to propagate the errors from its outputs to its inputs.", "NN can naturally support this, as its learning algorithm is the back-propagation.", "In contrast, tree-based models do not support this as the tree learning process is not differentiable and therefore cannot propagate the errors to its inputs.", "As stated in the Section 2, although there are some works targeting to address this problem, these solutions will lose the automatic feature selection ability and cannot work well on the tabular data.", "2) Hard to learn from streaming data.", "For NN's learning, we can use Stochastic Gradient Descent (SGD) or mini-batch SGD to naturally learn from streaming data, since the NN model could be updated per data sample or per mini-batch of samples.", "However, it is not effective for tree-based model to support this as its learning needs the global statistical information.", "Using the partial statistical information may produce the sub-optimal split points and results in worse models.", "There are some works addressed this problem, like Hoeffding trees, which stores the statistical histograms into leaf nodes.", "However, most of these solutions are designed for the single decision tree.", "Although there are ensemble versions of them, most of them are based on bagging (like Random Forest), which is proven not as good as GBDT.", "In short, NN does not suffer from these two problems due to its mini-batch back-propagation learning process.", "In contrast, tree-based model is hard to solve these two problems due to its learning algorithm is based on global statistical information.", "Therefore, TabNN is a better general solution for tabular data."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 393, "sentences": ["We thank the reviewer very much for reading the paper carefully and providing us with constructive comments.", "We have conducted further experiments applying our Past Decode Regularization (PDR) to the mixture-of-softmax (AWD-LSTM-MoS) model of (Yang et al. 2017).", "We use the same model sizes as in the paper.", "Even with the very limited hyperparameter search in the vicinity of those used in the paper and fixing the PDR loss coefficient to 0.001 (as used in the other models in our paper), we see consistent gains on the Penn Treebank and WikiText-2 datasets.", "The validation/test perplexities are as follows -", "AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)", "Penn Treebank with finetuning -", "56.2/53.8", "||  56.5/54.4", "Penn Treebank with dynamic evaluation -", "48.0/47.3", "||  48.3/47.7", "WikiText-2 with finetuning -", "63.0/60.5", "||  63.9/61.5", "WikiText-2 with dynamic evaluation -", "42.0/40.3", "||  42.4/40.7", "Thus we observe gains of 0.6 and 1.0 points in test perplexity for PTB and WT2.", "With dynamic evaluation, the gains for both datasets is 0.4 points.", "Note again that we did a very limited hyperparameter search and more exhaustive experiments will likely lead to even better gains by using PDR.", "We will update and reorganize the experiments section in the paper accordingly.", "The updated manuscript will be posted shortly.", "Yang et al. 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv:1711.03953."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_other_label"]}
{"abstract_id": 394, "sentences": ["Response: We thank the reviewer for their comments.", "We would like to clarify one of the central points of this paper, as the cons presented are built upon a misunderstanding of this point.", "We are not proposing a new transfer learning model -- we are demonstrating the transferrability of the atomic features we have learned.", "We train our structural features on C_r and show that with no re-training they can achieve state-of-the-art results of C_p.", "Applying a classical transfer learning algorithm might improve performance even further, as then we could fine-tune results on C_p.", "Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.", "Thus, instead of comparing transfer learning methods, we evaluate the transferrability of both our own structural features as well as those of competitors."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 395, "sentences": ["We thank the review for their positive comments.", "We agree that our main contribution is to cast a graph convolution network as a binary classifier learning to discriminate clean from noisy data and show its excellent results for few-shot learning.", "Q1: In future, I would like to see a joint approach to such training, where the function g(), the nearest neighbour graph loss and the classification loss are all tied in the same objective function and are optimized jointly.", "R1: We fully agree that this is an interesting direction for future research, which should result in a further increase in performance.", "It would be interesting to see if the feature extractor can benefit from the cleaning of noisy images during learning, resulting in more robust feature descriptors."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_future", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 396, "sentences": ["We thank the reviewer for the detailed and thoughtful review.", "We address the reviewers main concerns.", "\u201cA more general function is $P(X)_i=Ax_i+\\sum_{j\\in N(x_i,X)} B(x_j,x_i) x_j + c$, where $N(x_n,X)$ is the set of index of neighbors within the set\u2026 Then, can the function family the authors used in the paper approximate this function?", "No.\u201c", ">> We respectfully disagree.", "The function P(X) described by the reviewer can be approximated arbitrarily well using a continuous equivariant function (by using bump functions to approximate indicators functions of neighbors).", "As such it can be approximated with the universal models considered in this paper.", "(e.g. PointNetST, DeepSets etc.)", "Reiterating the main result of this paper: *Every* continuous equivariant function defined solely on a set of feature vectors can be approximated with PointnetST over a compact domain.", "We are happy to include a discussion about this in the revised paper.", "\u201cOn the Knapsack test, the metric of interest is not the accuracy of individual prediction. Rather, whether the network has successfully predicted the optimal solution, or how close the prediction is to the solution.\u201d", ">> Thank you for this comment.", "We will revise the Knapsack graph to show the success metric suggested by the reviewer.", "We remark that the goal here was not to construct a network that solves the Knapsack problem but to demonstrate the difference between universal and non-universal models.", "\u201cSpare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference\u201d", ">> The definition of DeepSets appears in Equations 7 and 8.", "The PointNetSeg model is described in detail in the discussion before the proof of Corollary 1.", "We will make the definitions clearer.", "\u201cLemma 3 is too trivial.\u201d", ">> We agree it is trivial (and indeed the proof is a one-liner).", "If the reviewers feel strongly about this, we can move it to appendix, however we feel it helps to provide a complete picture.", "\u201cP.2 power sum multi-symmetric polynomials. \"For a vector and a multi-index", "...\" I think it was moved out of the next paragraph", "since  the same is defined again as again in the next sentence.\u201d", ">> Thank you. We will make this clear."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 397, "sentences": ["Thank you for the comments, please find our responses to specific points below.", "[Q] \u201cAs far as I can see the most important take home message of the paper can be summarized in \"one should consider non-saturating GAN loss and spectral normalization as default choices [...] Given additional computational budget, we suggest adding the gradient penalty [...] and train the model until convergence.\"", "[A] While we want this study to be approachable by non-experts, some level of formalism is required as our main audience are researchers working on or interested in GANs.", "The summary you provided is indeed correct -- coupled with our open-sourced code, it allows a non-expert to train a GAN with state-of-the-art methods without needing to understand the details.", "On the other hand, for more experienced researchers, we provide more details on which design choices generalize to new settings and identify the biggest obstacles towards fair and unbiased quantitative evaluation of generative models.", "[Q] Limited amount of new insight.", "[A] Our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most Resnet tricks do not matter, etc.", "All of these insights are supported by a fair and unbiased rigorous experimental process.", "On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.", "[Q] Clarification and exposition of plots.", "[A] Say that you had access to a GPU and had to train a model (loss+penalty+architecture).", "How many hyperparameter settings would you need to consider to achieve a certain quality?", "The FID from the plot is the estimate of the min FID computed by bootstrap estimation and the line-plots show this relationship.", "In other words, given a computing budget, which model should you pick?", "We will provide additional details in the caption of the plot.", "[Q] Bayesian optimization and variance.", "[A] We agree and will provide more details.", "When the sequential Bayesian optimization chooses the next set of hyperparameter combinations to test we run the model once (per hyperparameter combination) and report the scores to the optimizer.", "Then, the optimization algorithm takes these scores into account when selecting the next set of hyperparameters.", "The algorithm itself trades-off exploration and exploitation and it can explore hyperparameters \"close\" to the existing ones if they seem promising.", "Hence, the averaging happens implicitly during the search.", "[Q]: Studies and experiments. Stating that lower is better in the plots.", "[A]: Study is a set of experiments (say a study on the impact of the loss).", "Experiment is a concrete run with certain hyperparameters.", "Stating lower is better is a good idea, we will add this to the captions."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 398, "sentences": ["We thank the reviewer for the comments.", "We justify the novelty and significance of the contributions made by this paper as follows.", "1) Novelty of the convergence analysis: The paper by Yuan et al. did not present proof of convergence in the discrete-time setting.", "The authors only provided convergence of the ODE models.", "On the other hand, convergence analysis of momentum methods in non-convex setting is an important but under-explored area  (Yan et al., 2018).", "In the current paper, the convergence results are proved for non-convex objective functions satisfying mild assumptions.", "Appropriate use of some sharp estimates allowed us to obtain concise bounds on convergence of the entire class of PoweredSGD methods for $\\gamma\\in[0,1]$ and the bounds continuously depend on parameters $\\gamma$ and $\\beta$. In the special cases ($\\gamma=0,1$, $\\beta=0$), these bounds matches the best known bounds for GD/SGD/SGDM in the non-convex setting.", "More specifically, we would like to draw the reviewer's attention to the following two papers:", "*", "[Yan18] Yan, Y., T. Yang, Z. Li, Q. Lin, and Y. Yang. \"A unified analysis of stochastic momentum methods for deep learning.\" In IJCAI International Joint Conference on Artificial Intelligence. 2018.", "*  [Bernstein18] Bernstein, Jeremy, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. \"SIGNSGD: Compressed Optimisation for Non-Convex Problems.\" In International Conference on Machine Learning, pp. 559-568. 2018. (Theorem 3)", "We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.", "Please take a look at Theorems 1 and 2 in [Yan18] and Theorem 3 in [Bernstein18].", "We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).", "2) Novelty of experiments: The current paper presents substantially more comprehensive experiments for benchmarking the proposed class of optimizers against other popular optimization methods for deep learning tasks.", "In particular, we highlight the experiments on vanishing gradients and learning rate schedules.", "This, in addition to the potential to accelerate initial convergence, makes the proposed PoweredSGD methods useful in many potential applications."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 399, "sentences": ["Thank you for the thoughtful comments. We are glad that you found our problem interesting, and problem formulation/applications of this research well explained.", "Regarding the competing algorithms:  Both algorithms that we compare to, Count-Sketch and Count-Min, are state-of-the-art hashing-based algorithms (see e.g., Cormode & Hadjieleftheriou (2008)).", "Further, they are widely used in practice for processing internet traffic, large databases, query logs, web document repositories, etc.", "To the best of our knowledge, our paper is the first to use machine learning to design better sketches for any streaming problem.", "We tried to cover related work thoroughly in section 2."], "labels": ["rebuttal_accept-praise", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 400, "sentences": ["1. One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.", "For example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from Section 4.", "A: We actually ran this experiment, where we monitored the KL divergence between the marginal distribution of model predictions and the true marginal distribution of labeled data over the course of training (with and without distribution matching).", "We added the results of this experiment to the appendix of the latest revision.", "2. For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.", "A: For space reasons we provided only a short description of CTAugment, and how it differs from AutoAugment. We will include a longer treatment in the appendix.", "3. On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it\u2019s being reported as 6.24.", "What is the reason for the difference?", "A: The 4.95 error rate in the MixMatch paper is in Table 1 which is the result when using a larger model (26 million parameters).", "Our results are comparable to the WRN-28-2 results (as used in the \"Realistic Evaluation of Semi-Supervised Learning Algorithms\" paper), as seen in Table 5 of the Appendix of the original MixMatch paper.", "4. Another paper, [2], reports very competitive results on CIFAR-10 for 4k labels.", "A: We will include a discussion of this paper in the revised manuscript.", "Similar to the comment on MixMatch above, we only use small models with 1.5 million parameters compared with the 26 million parameters in SWA.", "We chose this experimental setting because it simplifies comparison to existing results, as argued in \"Realistic Evaluation of Semi-Supervised Learning Algorithms\"."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 401, "sentences": ["We would like to thank you for reviewing our paper.", "[Unguided Case and Disentanglement] Please refer to our general comment above on why our unguided case performs better now.", "We also updated our \u201cProbabilistic Interpretation\u201d section with analysis on how the contrastive loss helps us to learn a disentangled representation.", "Evidence and comparison to other methods on disentanglement is provided in  Table 9 in Appendix G, where we visualize the correlations between our embedding dimensions.", "[Experiments Section] We have significantly updated qualitative and quantitative results in our \"Experiments\" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label"]}
{"abstract_id": 402, "sentences": ["Thank you for the time and effort spent reviewing our paper.", "We are glad you liked the paper.", "We want to emphasize one point that we perhaps did not highlight enough in our paper: there are other existing algorithms that fall into the marginal policy gradients framework.", "Specifically, researchers and practitioners both almost always clip actions for use in robotics control environments (read: MuJoCo tasks).", "Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces.", "Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten existing analyses of variance reduction that can be achieved for clipped actions.", "To respond to your question, yes it is possible (e.g. the example given above), but their is no general procedure that we know of to derive such methods.", "Rather, this would be done on an action space by action space basis"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 403, "sentences": ["We thank the reviewer for their detailed review and for their suggestions.", "We answer point by point:", "*FW vs BCFW*", "The (primal) proximal problem is created for a mini-batch of samples, and not for the entire data set (details in section 3.2).", "In other words, the primal problem consists of the proximal term which encourages proximity to the current iterate, the linearized regularization, and the average over the mini-batch of the losses applied to the linearized model.", "As a result, we can compute the Frank-Wolfe update for all dual coordinates simultaneously, and we do not need to operate in a block-coordinate fashion.", "We have included this clarification in the new version of the paper.", "*", "Batch-Size*", "We thank the reviewer for this suggestion.", "We have adapted the description of Algorithm 1 accordingly.", "*Convex-Conjugate Loss*", "In order to compare the DFW algorithm to the strongest possible baselines, we choose the baselines to use the CE loss in the CIFAR experiments.", "Indeed we have generally found CE to help the baselines in this setting.", "In addition, the hand-designed learning rate schedule of SGD and the l2 regularization were originally tuned for CE.", "In the case of the SNLI data set, we allow the baseline to use either CE or SVM because using the hinge loss can increase their performance.", "Finally, we choose to always employ the multi-class hinge loss for DFW because it gives an optimal step-size in closed form for the dual, which is a key strength of the formulation.", "*BCFW vs BCD*", "We thank the reviewer for this recommendation.", "It would be interesting indeed to explore how to exploit such updates in the context of the composite minimization framework for deep neural networks.", "In our case, we emphasize that for speed reasons, it is crucial to process the samples within a mini-batch in parallel, and this does not look straightforward with the algorithm in [3, E.3].", "Therefore we believe that for this setting the FW algorithm permits faster updates thanks to an easy parallelization over the mini-batch on GPU.", "*Hyper-parameter*", "Counting a single hyper-parameter for SGD implicitly assumes that SGD can employ a constant step-size.", "Using such a constant step-size for SGD would incur a significant loss of performance (e.g. at least a few percents on the CIFAR data set).", "Therefore in order to obtain good performance, SGD requires a manual schedule of the learning rate, which involves many hyper-parameters to tune in practice."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 404, "sentences": ["Thanks for your insightful comments.", "1. How or why is the benefit.", "This comment is insightful and we also considered about it.", "Intuitively, we would easily fall into the connections between each sentence and image.", "However, it is nearly impossible to pair sentence with images with completely the same meaning all the time.", "According to our investigation, we conclude that the major contribution would be more effective contextualized sentence encoding for better representation from the visual clue combination instead of single image enhancement for encoding each individual sentence or word.", "According to Distributional Hypothesis (Harris et al., 1954) which states that \u201cwords that occur in similar contexts tend to have similar meanings\u201d, we are inspired to extend the concept in multimodal world, \u201cthe sentences with similar meanings would be likely to pair with similar even the same images\u201d, where the consistent images (with similar topic) could play the role of topic or type clues for similar sentence modeling.", "For your example, the topic words are {private, courts, table}, which can be paired with relevant images and other sentences with the same (similar) topics will be paired with the same (similar) group of images.", "This is also very similar to the idea of word embedding by taking each image as a \u201cword\u201d.", "Because we use the average pooled output of ResNet, each image is represented as 2400d vector.", "For all the 29,000 images, we have an embedding layer with size (29000, 2400).", "The \u201ccontent\u201d of the image is just like the embedding initialization.", "It indeed makes effects, but the capacity of the neural network is not up to it.", "In contrast, the mapping from text word to the index in the word embedding is critical.", "Similarly, the mapping of sentence to image in image embedding would be essential, i.e., the similar sentences (with the same topic words) tend to map the similar images.", "To verify the hypothesis, we shuffle the image embeddings but keep the lookup table, to only exchange the features of each image but maintain the sentence-image mapping.", "Unsurprisingly, the BLEU score (EN-RO) is 33.53, which is very close to the reported one (33.78).", "In addition, we randomly initialize the image embedding instead of ResNet, the result is 33.28.", "In comparison, if we randomly retrieve unrelated images to break the lookup, the result is 32.14.", "These results verify the necessity of the lookup table.", "We have added a detailed discussion in the paper (please see Analysis 6.1).", "We believe this finding would be suggestive for the future research since most previous work focused on the content of the image itself.", "As a different research line, we highlight the consistency among the mono-modality to bridge the gap of language and image modeling.", "2. Why stop words are ignored.", "According to the explanation above, we think the spatial relations or grammatical nuances would not be so important in this task if we take the images as topic guidance.", "Ignoring the stopwords can help us get rid of the disturbance of unnecessary high-frequency words (such as function words) being the topic, as the standard practice for TF-IDF topic extraction.", "3. Comparison of different feature extractors.", "Yes. We compared with ResNet101 and ResNet152 on EN-RO.", "The BLEU scores are 33.63 and 33.87.", "It seems deeper ResNet indeed gives better results but the difference is not very significant."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 405, "sentences": ["Thank you for your comments and suggestions.", "We respond to your questions and concerns as follows.", "1. Connection with principal-agent problems.", "Thank you for pointing this out.", "We really appreciate it.", "The problem we address is indeed closely connected to principal-agent problems, or moral hazard problems in economics, which considers whether the agent makes the best choice for what the principal delegates (e.g., a plumber might make more money by suggesting an overhaul rather than a short-term fix).", "In this setting, there are a lot of issues to be modeled, e.g., information asymmetry between principals and agents, how to setup incentive cost, how to infer agents\u2019 types and how to monitor their behaviors, etc.", "Traditional approaches [1] in economics build mathematical models to address these issues separately, leading to complicated models with many tunable parameters.", "In comparison, our paper provides a practical end-to-end computational framework to address this problem in a data-driven way, once the agents\u2019 utility function is written down as a combination of principal\u2019s request and its own preference (Eqn. 1).", "Moreover, this framework is adaptive to changes of agents\u2019 preferences and capabilities, which very few papers in economics have addressed.", "Because of the connection to principal-agent problems and the data-driven nature of the proposed method, there could be a broad number of practical applications.", "We will incorporate a more thorough literature reviews in the next revision.", "[1] The theory of incentives: the principal-agent model, Jean-Jacques Laffont, 2001", "2. More details should be given on the mind tracker module.", "We will explain more implementation details in the appendix in the next revision.", "We will also release the code.", "3. Is it necessary to use deep reinforcement learning for contract generation?", "As stated in the introduction, one of the main points of this work is about incomplete information.", "I.e., we do not know the true agent models and their mental states, and also do not assume that the task dependency is known.", "In real world problems, we indeed can not assume that a manager knows the exact nature of other agents.", "So we want to train a manager that can quickly model worker agents through observations and simultaneously generate optimal contracts.", "In contrast, traditional methods do not consider task dependency, and usually assume agent types are either known or follow a given distribution.", "Also, deep models are flexible enough to handle complicated interactions between agents and changes of settings.", "Thus, deep RL is a more suitable approach than traditional methods under the incomplete information setting."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_other", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 406, "sentences": ["We thank the reviewer for the comments.", "We have revised the paper according to the suggestions and would like to clarify several things:", "Q1. Evaluation Time:", "We have added the detailed running time for each component in Table 3 in Appendix A of the revised version.", "Q2. Implementation Details:", "We will share all the source code to make sure it is reproducible.", "Meanwhile, we have included more details as suggested in Appendix A, including a visualization of all layers of the different parts of the network. If 1-2 extra pages are allowed, we can include those details to the paper.", "Q3. Figure 1 is too abstract:", "We have updated the figure to make it more intuitive and contains more details.", "Q4. The top row of Figure 2b is confusing:", "We apologize for the confusion caused.", "Shown at the top row of Figure 2b are not three consecutive frames.", "They are the R, G, B channels of a single frame.", "To avoid confusing, we use different colors for them and explained that in the figure.", "Q5. How the first camera pose is initialized?:", "All the camera pose including the first camera are initialized with identity rotation and zero translation, which are aligned with the coordinate system of the first camera.", "We clarified this at the end of Section 4.3 in the revised version.", "Q6. Evaluation metrics are not clear:", "To facilitate comparisons with other methods, we use the evaluation metrics in previous works in Table 1 and 2, so that we can cite the results of previous methods.", "As we described in the paper, the depth metric are the same as Eigen and Fergus (2015).", "The translation metrics(ATE) are the same as [Wang et al. 2018, Zhou et al. 2017].", "In the revised version, we briefly introduce the definition of these metrics at the beginning of each paragraph in Section 5.2.", "Q7. Attention should be given to the notation in formulas (3) and (4):", "We changed the parameters from \u2018d\u2019 to \u2018d \\cdot p\u2019 which is a 3D point.", "We also removed the redundant subindex \u20181\u2019, because all points \u2018q\u2019 are on the first frame.", "Q8. Terminology consistency through the paper:", "Thanks for the suggestion.", "We consistently use the term \u201cfeature-metric BA\u201d and \u201cbasis depth maps\u201d through the paper now.", "Q9. Typos, Grammar, Format, and Bibliography:", "Thanks for pointing them out. We have revised the paper to fix these problems."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 407, "sentences": ["*All or at least some of these decisions would need to be relaxed to make a convincing paper.", "you are right, even if the focus of the paper is not on getting the best possible score on language modelling, different settings would make this point not only more convincing, but clearer.", "*The reasons for the use of the energy-based formulation are not clear to me.", "Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?", "This formulation is fundamental for the next step of the work in which we are removing the restrictions from the output layer and learning word probability distributions without prior knowledge of the vocabulary size.", "That said, the formulation is just the re-use of the embedding layer transposed.", "It removed an entire set of m x V parameters and got us better results in all our experiments so we decided to use it.", "* It looks like all the results are given on the test set. Did you not do any tuning on the validation data?", "Yes, all the parameters were tuned on the validation data.", "All the models were selected according to their validation data evaluation.", "The early stop criterion is also based on the validation data evaluation.", "We consider the model to converge when it cannot improve further on validation data.", "The models never saw the test set during training or tuning, otherwise we would be cheating and these scores would be irrelevant to compare different settings."], "labels": ["rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 408, "sentences": ["Thank you for the detailed review and constructive remarks.", "Below are answers to the main points that were commented as well as updates on the current work.", "* Sound quality is disappointing and with artifacts:", "We are working on Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks, arXiv:1808.06719, Sercan O. Arik et al. to replace Griffin-Lim inversion ; two possible improvements we expect are much faster (towards real-time) sound rendering and better audio quality.", "We are also working on mini-batch MMD latent regularization (Wasserstein-AE) instead of per-sample KLD regularization (VAE) which may result in improved generalization power and generative quality.", "* Not suited to transfer from audio without label:", "If the audio carries a note information, it can be easily/automatically extracted in the form of pitch tracks as we did for transferring on instrument solos.", "Some audio data do not have note qualities, which are out of the current training setting.", "For that we have been training unconditioned one-to-one models or solely instrument conditional many-to-many models that do not require any note information.", "But we are working on models which incorporate an unconditioned processing option (eg. training while zeroing the one-hot conditioning or adding an entry in the input embedding of FiLM which is the unconditional state) to be trained on a dataset that mixes conditional and non conditional audio (eg. adding instrument solo sections which in parts have a clear pitch track and in others none).", "* A fully convolutional model would process arbitrary length of audio:", "We use the linear layers to set the latent space dimensionality, when processing various length audio sequences, each encoding amounts to about 120ms context and we resynthesize with overlap-ad that mirrors the short-term input analysis ; this process was used when transferring on the instrument solos (a task that was beyond the training setting).", "* Insufficient justification of the 3D latent space:", "At first we validated that our models could perform well in term of training/test spectrogram reconstructions with only 3 latent dimensions, some reasons that we found interesting to enforce this are more related to a possible music/creative application of the model: less synthesis/control parameters for the user (and controls which may then be more expressive), direct visualization of the latent space which is turned into a 3D synthesis space from which users may draw and decode sound paths or create other interaction schemes, a denser latent space that may be better suited for random sampling/interpolations.", "The direct interaction with 3D latent space becomes even more interesting when we pipeline our model with fast-spectrogram inversion.", "* Interesting incremental comparison in one-to-one transfers:", "We keep working on more detailed benchmarks/comparisons that would equally cover one-to-one and many-to-many model variations and that would integrate the new features we are testing.", "* All claims about running time should be corroborated by controlled experiments:", "Indeed we didn\u2019t benchmark yet our models on Nsyth and our approach differs from others such as Mor et al. that report using \u00ab\u00a0eight Tesla V100 GPUs for a total of 6 days", "\u00bb", ".", "From the beginning of our experiment we aim at a much lighter-weight system that could be trained/used more broadly (eg. with a single mid-range GPU).", "The computational cost difference is not rigorously estimated on a same given dataset/task to learn but still we think it is relevent to point that the results we report can be achieved in less that a day on a single Tesla V100 GPU.", "* Why does the MMD version constitute an improvement? Or is it simply more stable to train?", "It is more stable to train, it does not require the extra \u2018cost\u2019 of an auxiliary network training and it can generalize to many-to-many transfer without requiring as many adversarial networks.", "About the significance of score differences, we agree that it needs more details and comparisons, it was also noted by \"AnonReviewer1\" and we should make alternative tests to scale or give a few more references to the benchmark.", "* \"FILM-poi\" .. is this a typo ?", "Thank you for pointing this as well as your other remarks on the writing and use of precise terms/phrases.", "Indeed this is right, we mixed poi/pod but both refer to many-to-many conditioning on pitch+octave+instrument/domain classes.", "We also thank you for pointing more literature to improve our references and discussions to related works."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_none", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 409, "sentences": ["Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:", "4.", "Note that the training data is not constrained with respect to ratios and number of objects addressed by the caption, so the learned behavior should be independent of these aspects.", "Moreover, note that for most ratios there is only one combination of numbers with at most 15 objects in total, but larger images fitting a greater total number of objects would definitely be an option here.", "For the less close-to-balanced ratios 1:2, 2:3, 3:4 where there are multiple possibilities, performance generally is (close-to-)perfect, indicating that there is no increased difficulty of learning multiples in the presence of more close-to-balanced ratios (for instance, 6:9 vs 7:8).", "We hope this clarifies your concern.", "5. We fully agree that it would be very interesting to investigate these models.", "For this paper, we decided to focus on the methodology of investigating such questions in detail (the evaluation for FiLM alone comprises around 100 experiments) as opposed to focusing on the comparison of behavior of different models, and leave the latter to future work.", "We added a few additional sentences to section 3.2 regarding that.", "6. We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer.", "We didn't think about the fact that one may want to control which strategy is learned, which would indeed be interesting, but that's why we considered FiLM as is and didn't experiment with changing architecture details.", "At the same time, considering that understanding \"most\" is only one of many capabilities a VQA model is supposed to learn, these results are unlikely to be an important influencing factor for architecture choice, while at least knowing about the properties of a model is nonetheless interesting.", "7. The evaluation is supposed to show what an architecture is capable of learning under \"ideal\" conditions.", "It's an interesting question whether/how this changes when gradually shifting towards \"less ideal\" setups.", "An advantage of using a controlled setup like ours is that this is possible to investigate, to some degree at least (for instance, add more types of captions to the training data, not just quantifier statements).", "At some point we may be interested in actually investigating the same for real-world data, but we think it's unclear right now what exactly such evaluation data should ideally look like, what problems are most interesting, what details to pay attention to. Artificial data allows us to investigate these questions while avoiding the elaborate and expensive process of obtaining real-world data.", "8. Note that the training data is far less constrained than the evaluation data, including various distracting aspects like additional shapes/colors.", "The evaluation data doesn't contain such distractors, but it would of course be possible (and potentially interesting) to add such.", "We didn't do so since we considered instances with only \"relevant\" attributes to be the most difficult setup, like a minimal pair, where a model is required to focus on all objects and both their shape and color attribute to decide correctly.", "9. We incorporated the changes as you suggested.", "Thanks!", "10. We didn't think about this interpretation -- our intention was to signal that we take the \"The Meaning of 'Most'\" setup and methodology of Pietroski et al. from psychology, and implement a deep learning version for visual question answering models."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_future", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_concede-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label"]}
{"abstract_id": 410, "sentences": ["We thank the reviewer for a careful reading of the paper and the constructive comments.", "Although we proposed Past Decode Regularization (PDR) with language modeling in mind to exploit the symmetry between the input and output vocabulary (and the corresponding word embedding and softmax layer), any model/task that has this symmetry can potentially use a PDR term.", "As suggested by the reviewer, models for tasks like text summarization and neural machine translation (using a byte-pair encoding vocabulary as in Ofir & Wolf 2016) that use an encoder/decoder seq2seq architecture can benefit from PDR and is a topic of future research.", "We will incorporate this discussion in the updated version of the paper.", "We can justify PDR theoretically as an inductive bias on the language model.", "The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform.", "Similarly, the distribution of the first word given the second word will be far from uniform.", "A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one.", "In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the \"true second word\" and decode the distribution of the first word.", "Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram.", "Finally, we have conducted further experiments on larger corpora, specifically the Gigaword corpus.", "We use a 2-layer LSTM with a word embedding dimension of 1024 and hidden dimension of 1024.", "We truncated the vocabulary by keeping approximately 100k words with the highest frequency.", "We compare the performance of the model with and without PDR and using no other regularization.", "We used the same validation and test sets as (Yang et al. 2017).", "We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 in the test perplexity by using PDR.", "We will incorporate these results in the experiments section and post the updated manuscript shortly.", "Press, Ofir, and Lior Wolf. \"Using the output embedding to improve language models.\" arXiv preprint arXiv:1608.05859 (2016).", "Yang, Zhilin, et al. \"Breaking the softmax bottleneck: A high-rank RNN language model.\" arXiv preprint arXiv:1711.03953 (2017)."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_future", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 411, "sentences": ["We thank the reviewer for their comments.", "We address their comments individually below.", "> My overall concern is that the experiment result doesn\u2019t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn\u2019t really fit in the \u201ctransfer\u201d learning scenario.", "Response: Our work is indeed not classical transfer learning -- it is in fact an even stricter variant.", "We do not re-train the parameters of the neural network at all using C_p, which is typically done as a \u201cfine-tuning\u201d step in the transfer learning scenario.", "So while we do use C_p^{val} for model selection (i.e., hyperparameter tuning), this is still much less use of the data-poor dataset than in the common transfer learning setting of actually fine-tuning the parameters of the neural network using a subset of the data from the data-poor dataset.", "The use of C_p^{val} for hyperparameter tuning was incidental and not a central point of our paper.", "To really make this clear, we have updated the paper to demonstrate that even if we do not use C_p^{val} for model selection, and instead select from the same class of models we previously generated by using a randomly selected held-out set C_r^{val}, we still obtain state-of-the-art performance (0.892 [0.885 +/- 0.009]).", "In this formulation, C_p is not used at all by our method until test time.", "> Also, the compared methods don\u2019t really use the validation set from the complex data for training at all.", "Thus the experiment comparison is not really fair.", "The competing models do make use of validation set C_p^{val} from the complex data to select amongst the most important hyperparameters of their model -- which is equivalent to what we did in our initial formulation, and favors the competing methods compared to if we use C_r^{val} for hyperparameter search instead.", "> 2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn\u2019t include any significance of the sampling.", "Specifically, the testing dataset is fixed.", "A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.", "Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.", "While it is true that the hyperparameter validation set was initially fixed, the switch to use C_r^{val} as above resolves this.", "The testing data C_p^{test} is that which has been used in the prior works we compare to (Fout et al. 2017; Sanchez-Garcia et al. 2018).", "Furthermore, use of this subset for performance evaluation is justified as as C_p^{test} corresponds to latest released structures in C_p, leading to a more accurate assessment of how such methods would perform on unreleased structures (as they do no sequence identity pruning).", "Thus our experimental set up", "is rigorous and justified.", "> Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.", "Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can\u2019t capture while SASNet can.", "Response: As we discuss above, we believe our experimental setup and analysis is sufficient to demonstrate that our atomic representation transfers much better across atomic tasks.", "We have also added to our discussion, making clear that our method represents a significant advantage over competing methods when detailed atomic information is available.", "Competitors rely on amino acid-level features that fail to capture specific atomic positions but can be better when the structural is less detailed or accurate.", "> Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.", "Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.", "Response:", "As our paper is primarily about the power and transferrability of our structural features for atomic tasks, we believe a detailed investigation of non-structural features is mostly outside of the scope of this work.", "To show that we can easily include these features, we have included in our appendix some results including non-structural features.", "When adding in the sequence features used by Fout et al. via a simple linear model combining our final hidden layer and the additional sequence features, we are able to achieve a superior performance of 0.921 (0.914 +/- 0.009) versus their performance of 0.896 (0.894 +/- 0.004).", "While BIPSPI (Sanchez-Garcia et al. 2018) does achieve the best combined performance at 0.942, they also use additional sequence correlation features (note their structure-only performance is comparable to that of Fout et al).", "> Some discussion on why the \u201cSASNet ensemble\u201d would yield better performance would be good; could it be overfitting?", "Response: We have removed the SASNet ensemble from the paper, as it was based on C_p^{val} and confuses the point we are making about minimally relying on C_p for training and validation.", "We could definitely investigate further why this mild ensembling yields a small performance increase, but we see this as tangential to the overarching points of the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 412, "sentences": ["Thank you for your review and the overall positive assessment.", "In particular, we are delighted that you see the potential of the stethoscope framework lending itself to much broader applications.", "We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.", "This is a key consideration in our belief that it makes for a valuable contribution to the community.", "We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.", "In particular, we demonstrate the efficacy of neural stethoscopes in interpreting, promoting and suppressing specific information in the context of the complex interplay between visual clues and physical properties in stability prediction.", "Our work is primarily motivated by the question as to what extent neural networks learn about physical principles or whether they merely follow visual clues and how we can guide the learning process.", "We showcase the stethoscope framework here to that effect and, based on its application, provide some interesting and novel [according to Reviewer 1] insights into representations for visual stability prediction.", "As regards the implications of the paper, we would like to address this in the context of the reviewer\u2019s comment that increased performance is not surprising given the additional supervision provided.", "Our submission argues that the manner in which this information is provided really does matter.", "Figure 6 addresses this point in that multi-task learning fails to leverage the potential of the additional training labels (and, indeed, leads to a detrimental effect, Fig 6b) whereas the stethoscope framework allows the specification of whether the information considered should be promoted or suppressed.", "This leads to the performance gains shown in Fig 6a."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_future", "rebuttal_other", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 413, "sentences": ["Due to the overlap between reviewer comments, we decided to address all concerns in a single response (please see above)."], "labels": ["rebuttal_other"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label"]}
{"abstract_id": 414, "sentences": ["We thank the reviewer for the review and the comments.", "Below we address the main concerns.", "\u201cone may study the simple case of having single input channel, for which the output at index \"i\" of an equivariant polynomial is written as the sum of all powers of input multiplied by a polynomial function of the corresponding power-sum.", "This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.", "Generalizing this to the multi-channel input as the next step could make the proof more accessible \u201d", ">> We will highlight the connection to the case of single input channel and DeepSets permutation invariance universality.", "\u201cThe second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model. ... if there were any other objectives beyond this in the experiments could you please clarify? \u201c", ">> We agree it is trivial (and indeed the proof is a one-liner).", "If the reviewers feel strongly, we can move it to appendix, however we feel it helps to provide a complete picture.", "We included it in the experiments as a naive baseline and to show that adding a single transmission layer indeed provides a significant improvement.", "\u201cFinally, could you give a more accurate citation (chapter-page number) for the single-channel version of Theorem 2.?\u201c", ">> This result is Proposition 2.27 in Golubitsky&Stuart(2002).", "We will update the paper to give a more accurate citation."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_concede-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label"]}
{"abstract_id": 415, "sentences": ["Thank you very much for your comments and your feedback.", "We provide our reply to your questions below:", "(1) In equation (3) we are using scalarization, a well-known technique to solve multi-objective optimization problems (see for example Boyd\u2019s book \u201cConvex optimization\u201d Ch. 4).", "In this case, the maximization problem is a multi-objective optimization problem including both the parameters of the discriminator and of the classifier.", "The parameter alpha controls the importance/priority of each of the objectives.", "The parameter alpha also allows to control the detectability constraints for the attack, which allows us to test the robustness of learning algorithms and defences in different settings, considering more or less aggressive adversaries.", "This is common in most security settings to test system\u2019s robustness and resilience in different attack scenarios.", "(2) In pGAN the discriminator allows to model detectability constraints for the poisoning points.", "In other words, to evade detection or removal of points by algorithms that defend against poisoning attacks, such as the defences we used in our experiment, we want our attack points to be close to the distribution of the genuine data.", "However, please, note that the discriminator\u2019s loss is decoupled from the classifier\u2019s loss.", "In contrast, the generator is the element that competes with both the discriminator and the classifier.", "On the other side, the discriminator does not exclude poisoning data or select any data point but helps to guide the generator to craft poisoning points that are difficult to detect.", "In other words, the discriminator does not filter out the points that are used to train the classifier during the training of pGAN.", "It is not clear to us what the reviewer refers to when mentioning measuring the classification error from the data the discriminator selects, as the discriminator does not \u201cselect\u201d any data point, but just aim to classify genuine from fake data points.", "We would be happy to provide further clarifications on this point if needed.", "(3)-(4) To some extent pGAN can control the specific errors produced in the system, as shown both in Figures 5 and 6.", "But the changes produced in the system may also depend on the characteristics of the dataset and the learning algorithms used.", "pGAN produces poisoning attack points that are close to the decision boundary, \u201cpushing the decision boundary away\u201d from the source class (i.e. the same class as the labels of the poisoning points) towards the samples of the target class.", "Then, we can expect an increase of the false positive rate, which is shown in Figure 6 (centre).", "At some point, when the fraction of poisoning points increases significantly the decision boundary starts to change in a different (and possibly more abrupt way), so that the false negatives also start to increase.", "In Figure 6 (right) this happens when the fraction of poisoning points is larger than 25%.", "In contrast, the label flipping attack is less subtle as it does not consider detectability constraints.", "The attack points are therefore not necessarily close to the decision boundary, and thus, the changes produced in the algorithm are more unpredictable and affect the errors for the two classes.", "If there are points that, in your view, require further clarification, please let us know.", "Thank you very much."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 416, "sentences": ["Thank you for approving our contribution to understanding the transferability of adversarial examples.", "We agree with that the smoothing gradient idea, especially the Gaussian smoothing technique, is not new, since the smoothing strategy could be used in many different scenarios.", "However, we motivate and derive the idea of smoothing the gradient based on our novel understanding on the transferability of adversarial examples between two models.", "To the best of knowledge, we are the first to derive and apply this technique to enhance the transferability of adversarial examples, whose significant improvement is also confirmed by our intensive experiments."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 417, "sentences": ["Thank you for the comments.", "We provide the feedbacks below.", "- Indeed there are a huge number of papers on adversarial examples, but specifically only a small fraction  of them are", "about the  transferability of adversarial examples .", "Understanding why adversarial examples can transfer from one model to another model is a much harder problem, which is a rather unexplored area.", "Indeed we do not provide a perfect explanation in this submission, however the factors we have considered and the well-designed numerical investigations could be helpful for future studies on this topic.", "In addition, the works Fawzi'15 and Athalye'18 did not talk about the issue of adversarial transferability.", "- Could you be more specific on what do you expect for \"larger studies\" and \"general study\u201d?  This will be helpful for improving our work."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 418, "sentences": ["Thanks again, Reviewer #2, for your insightful feedback.", "We respond to your other comments below.", "1.  \u201cWhy investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.\u201d", "See general response #3.", "2.  \u201cFor instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood.\u201c", "We do not believe our models are necessarily underfit.", "In fact, we found that Glow had a tendency to *overfit,* and that one must carefully set Glow\u2019s l2 penalty and choose its scale parametrization (exp vs sigmoid, see Appendix D) in order to prevent it from doing so.", "We thought this overfitting to the training data could be a reason for the phenomenon and therefore we tuned our implementations to have reasonable generalization.", "3.  \u201cIt would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.", "For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting.\u201d", "See general response #1 in regards to data sets and additional results.", "Thank you for the suggestion of looking at data sets with similar statistics.", "We do this, in a way, with our second order analysis and the \u2018gray-ing\u2019 experiment in Figure 5 (b) (formerly Figure 6 (b) in the original draft).", "Gray CIFAR-10 (blue dotted line) nearly overlaps with original SVHN (red solid line) in terms of their log p(x) evaluations.", "Figure 12 (formerly Figure 13) then shows the latent (empirical) distribution of the gray images, and we see that the gray CIFAR-10 latent variables nearly overlap with the SVHN latent variables.", "This is to be expected though, given the overlapping p(x) histograms, since the probability assigned by CV-Glow (in comparison to other inputs) is fully determined by the position in latent space.", "4.  \u201cThe second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.\u201d", "See general response #2."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label"]}
{"abstract_id": 419, "sentences": ["Thank you for your comments!", "* We included a table showing accuracy numbers for different values of beta and M (see p. 6, Table 1) for the latent bottleneck sizes K=256 (Figure 2) and K=2 (Figure 3).", "*", "In relation to the figures, we have improved these in the revision.", "We are added a figure tracing the mutual information between representation and output I(Z;Y) vs. the minimality term I(Z;X) for different values of beta (see Figure 2, lower right panel), when training with our loss function.", "This is the usual information bottleneck curve.", "This contrasts with the deficiency bottleneck curve (Figure 2, upper right panel) which traces the corresponding sufficiency term J(Z;Y) (which is just the entropy of the labels minus our loss) vs. I(Z;X) for different values of beta.", "Note that for M=1, J(Z;Y) = I(Z;Y).", "We apologize for the confusion.", "The text now makes this more explicit (see p.7, first paragraph).", "*", "In response to your question about how we estimate the mutual information", ".", "Yes, we minimize an upper bound on both the deficiency and the rate term (see p.3, equation 3 and discussion leading up to the VDB objective in equation 4).", "The estimation of this upper bound is simplified by our choice of the prior and the encoding distribution which are diagonal Gaussians.", "The KL term can be computed and differentiated without estimation.", "We estimate the expected loss term using Monte Carlo sampling.", "We draw samples from the encoder using the reparameterization trick and leverage automatic differentiation (in Tensorflow) to compute the gradients.", "Since the expectation is inside the log, gradient updates may have higher variance for larger values of M.", "Our model is a classifier and our loss term is a tighter bound on the misclassification error (bias) than the usual cross-entropy loss as in the VIB (see p. 12, equation 13).", "Trading bias for variance has been investigated in some recent works (see, e.g., Bamler, Robert, et al. \"Perturbative black box variational inference.\" NIPS 2017).", "See last paragraph in p. 18 for the related discussion in the unsupervised setting.", "* In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.", "The method is different from ours, except in the limiting case where M = 1 and beta =1, in which case it coincides with the beta-VAE and also with our method.", "After taking a close look, we make the following observations:", "For M > 1, the IWAE bound does not admit a decomposition like the standard ELBO (see equation 29 and 36) into a reconstruction loss term and a regularization term.", "In particular, this implies we cannot trade-off reconstruction fidelity for learning more meaningful representations by incorporating bottleneck constraints.", "See ensuing discussion in p.18 following equation 36.", "In contrast, our method has a tuning parameter beta.", "The IWAE bound is known to be equivalent to the ELBO in expectation with a more complex approximate posterior qIW (see p.17, equation 34 and 35 and references therein in Appendix E.1).", "For beta values other than 1, a naive trick would be to plant qIW in liue of qphi in equation 37 (p. 18) to get a beta-IWAE of sorts.", "It is not entirely clear however, why we would want to do so when modulating beta already suffices to tune the VAE towards autoencoding (low beta) or autodecoding behavior (high beta) depending on the requirement at hand.", "A similar argument goes in the direction of an \"Importance weighted Variational Information Bottleneck\".", "We have not explored if and how using more expressive posteriors such as the qIW (p. 17, equation 35) can help the supervised bottleneck formulations in VDB or VIB.", "This remains a scope for future study.", "We are now also citing the paper Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label"]}
{"abstract_id": 420, "sentences": ["Thank you for your insightful comments to help us improve our paper.", "First of all, we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods, and observe that they are also vulnerable to our proposed attacks.", "Please see our reply to all reviewers.", "Here are our responses to your concerns in \u201cCons\u201d and \u201cMinor comments\u201d.", "Although we were not able to provide theoretical analysis in this paper, our proposed attacks are very effective on state-of-the-art adversarial training methods, and we believe our conclusions", "Currently, there is relatively few theoretical analysis in this field in general, and many analysis makes unpractical assumptions."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 421, "sentences": ["1.", "However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.", "As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.", "A: While ReMixMatch comprises many components (some of which are new), we believe our ablation study justifies the reason why each component exists. If there are additional ablation experiments that you think would be helpful for us to run, please let us know.", "2. The objective of the update equation of CTAugment\u2019s learned weights seems contradicted with the purpose of how data augmentation is used", "A: It is true that CTAugment at any point in time will only perform augmentations that the model correctly predicts.", "However, we select augmentations where the probability the model output will change is less than 1.", "As such, the augmentation boundary will grow progressively as the training process converges.", "(We experimentally observe this fact: for example, rotation is initially only invariant up to +/- 13 degrees but throughout training becomes invariant to +/- 30 degrees.)", "We don\u2019t aim to maximize the output variation at any instant, but instead ensure that by the end of training the model is invariant to large perturbations.", "3. The authors should provide ablation study and analysis of their CTAugment.", "A: As also discussed with reviewer 2, for space reasons we provided only a short description of CTAugment, and how it differs from AutoAugment.", "We updated the draft to include a longer treatment in the appendix.", "4. The authors should provide more detail of the setting in the ablation study", "A: We agree with the reviewer the details are sparse.", "We will include more details.", "To answer the reviewer\u2019s specific questions: \u201cNo strong aug.\u201d means that all augmentations were weak (as is done in MixMatch) and \u201cNo weak aug.\u201d means that all augmentations were strong. If there are other questions we will clarify any.", "5. The authors hypothesize that \u201cstronger augmentation can result in disparate predictions, so their average may not be a meaningful target.\u201d", "A: See above, where we found that the experiment diverged in the \u201cNo weak aug.\u201d ablation (using strong augmentations only)."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 422, "sentences": ["We really appreciate your comments.", "The main purpose of this paper is to introduce a new method to solve global optimization problem via replica exchange Langevin diffusion.", "We quantify the acceleration effect from the viewpoint of continuous time process.", "Although this work is inspired from Dupuis's work, their setting is MCMC and they only investigate by large deviation.", "We quantify the acceleration effect by both large deviation and chi^2 divergence.", "Besides, the large deviation rate function in our paper is different with that of Dupuis's since we use an alternative approach.", "We choose such a form of rate function because it is connected to the Dirichlet form, and hence, the convergence of chi^2 divergence.", "We acknowledge that our analysis tools is standard and not fancy in mathematics.", "However, this is not a mathematics conference after all.", "One of our contribution is applying standard mathematical tools to a specific machine learning problem.", "Finally, another contribution is that we propose a discretized algorithm.", "Although Dupuis& et.al's work establishes beautiful and complicated mathematical theory for replica exchange Langevin diffusions, they does not consider the discretization at all.", "In practice, we can only use the discretized one instead of the ideal continuous process to solve problems.", "Our theory quantifies the discretization error and convergence rate and hence, ensures the validity to use the discretized algorithm."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 423, "sentences": ["Thank you for your detailed review and the constructive comments on our work.", "We note the remarks on the paper writing that we will correct and answer below the main points that were commented.", "* In-depth evaluation of MoVE and comparison of with/without conditioning:", "We agree and this was also pointed by 'AnonReviewer2', we are working on new incremental benchmarks, more detailed on both one-to-one and many-to-many models.", "Moreover, the need of pitch/octave conditioning limits the applicability of our model to transfer only on audio carrying such note features.", "Hence we trained models without conditioning mechanism and, as answered to 'AnonReviewer2', we are planning experiments on models which are conditional but integrating an unconditioned state to be trained in parallel of the note-conditional state.", "**", "*", "Interpretability of the generative scores:", "We agree on this remark, the idea of scaling scores is right and would improve the interpretability of our benchmarks.", "For that purpose, we should define a set of reference scores as you recommended to.", "* Incomplete definition of the metrics:", "We gave references to the papers that introduced such metrics.", "Discussing a set of reference scores should also come with a better explanation of these.", "* Criteria for bolding: we intended to highlight the best scores", "**", "* Pairing generated and real examples by instrument and note to compare:", "In addition to the spectral descriptor distribution plots, we used sample-specific scatter plots to visualize how the transfer maps them individually.", "On the overlap of each instrument tessitura, we can make such pairing.", "We can also transfer and transpose to the target instrument tessitura if needed.", "Remains the question of which metric can be used here to evaluate generation at the sample-level (?), as our model does not aim at reconstructing an hypothetical corresponding sample in the target domain but rather at blending in features from the other domain so that it sounds like the input note (pitch, octave but also some dynamics/style qualities relative to the input instrument) played by the target instrument.", "We later aim at experimenting on mechanisms to control the amount of target feature blending in the process of transfer.", "* Invertible ? Decodable ? Approximate inversion ?", "We agree that the current state of the research should be stated as using approximate spectrogram inversion.", "We plan on replacing the iterative slow spectrogram inversion with Griffin-Lim by faster decoding with Multi-head Convolutional Neural Networks, arXiv:1808.06719, Sercan O. Arik et al.", "*", "**", "Definition of the RBF kernel:", "The summation is on the alpha parameter which can be a list of n values (or a single float value).", "The trainings were done with n=3 and alpha=[1. , 0.1 , 0.05].", "Depending on the kernel and bandwidth definitions, we may link both as", "alpha = 1 / (2 x bandwidth**2).", "* Calculation of reconstruction errors:", "All scores are computed on NSGT magnitude spectrogram slices.", "No evaluation (except listening) is done on the time-domain waveforms.", "The points marked with *** are highlighted as we would gratefully receive further remarks from your review.", "How would you recommend making reference scores to the MMD/kNN evaluations ?", "How would you recommend comparing pairs of generated and ~ corresponding target domain samples ? (at the sample level)", "Is the definition of the RBF kernel correct to you given that clarification (that should be added to the paper) ?", "Thanks again for the interesting feedbacks !"], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_contradict-assertion", "rebuttal_by-cr", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_none", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_none", "rebuttal_none", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_followup", "rebuttal_followup", "rebuttal_followup", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_summary_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 424, "sentences": ["Thank you for your comments.", "We have added a pseudo-code description of TTS-GAN training algorithm to the updated submission.", "We believe that, together with other architectural details present in the paper, it makes our work reproducible."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label"]}
{"abstract_id": 425, "sentences": ["We would like to thank you for reviewing our paper.", "[Experiments Section] We have significantly updated qualitative and quantitative results in our \"Experiments\" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN.", "[InfoGAN] Compared to InfoGAN, our method is novel in two ways: First, we use separate networks to obtain the image embeddings, which enables us to guide some of these networks with simple functions.", "The guidance allows more control over the latent space, even in lack of data.", "Second, we use pairwise similarity/dissimilarity in order to perform disentangling, which is different from InfoGAN's approach of maximizing the label likelihood.", "This point is now addressed in our \"Related Work\" section."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 426, "sentences": ["We thank the reviewer for their helpful feedback on our work.", "Regarding the IWSLT translation result, the key claim we aim to validate is that the theoretical efficiency of K-matrices translates to practical speedups on real models as well.", "We agree that there are other approaches that may offer different model quality vs. inference speed tradeoffs; we simply highlight that K-matrices are one promising method, especially given their important theoretical properties.", "We have added a performance comparison of K-matrices with other structured replacements such as circulant, Fastfood, ACDC, and Toeplitz-like in Appendix B.4.3, showing that K-matrices yield faster inference with similar BLEU score.", "We also point out that our DynamicConv model with K-matrices in the decoder attains a comparable BLEU score with the state-of-the-art from two years ago \u2013 the Transformer model, which continues to enjoy widespread use today \u2013 while having over 60% higher sentence throughput and 30% fewer parameters than this model.", "As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.", "Exploring how to continue to improve these structured compression approaches, while retaining the efficiency and theoretical benefits of K-matrices, is an exciting question for future investigation."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_future_label"]}
{"abstract_id": 427, "sentences": ["Indeed, GPipe [2] incurs less memory footprint than our pipelining scheme and PipeDream [1] because it only saves the activations at the boundary of each model partition and re-computes the activations of the model during the backward pass.", "However, the re-computation still incurs pipeline bubbles during training.", "Our scheme saves all activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization for the accelerators (GPUs).", "Our scheme has less memory footprint than PipeDream because it does not stash weights.", "The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing [1] or micro-batching [2], is simpler and does converge.", "The paper does achieve this goal, on a number of networks.", "It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.", "Thank you for pointing out paper [3].", "We notice that it is submitted to arXive after the submission deadline of ICLR, thus we were unaware of it at the time of submission.", "Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 428, "sentences": ["Thank you for your detailed reviews.", "Here are our responses to your questions and concerns.", "1. The authors should provide more details on how the hand-crafted demonstrator agents were made.", "We have added more details, and plan to release the code.", "We indeed implemented search algorithm with simple heuristics for acceleration for all grid-world tasks.", "In Maze Navigation, the state space is extended to the combination of map status and the agent's inventory.", "By this definition of states, an efficiency search can still be achieved.", "2. Scalability?", "We focus on simpler domains to provide proof-of-concept results as the first step on this direction.", "We are definitely interested in studying how our approach can be applied to more complex tasks as future work.", "3. A deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.", "Thanks for the suggestion.", "We have included a more detailed analysis with new visualizations in the updated paper.", "i) We visualize the latent vectors obtained from demonstrations with probing and without probing.", "It indeed shows that with probing, we are able to find new behaviors that correspond to the new latent vectors.", "ii) We also show the correlation between the distance of two consecutive latent vectors m^{t-1} and m^t and, the KL divergence between the two corresponding policies KL(\\pi(a|s^{t+1},m^t) || \\pi(a|s^{t+1},m^{t-1})), i.e., how different the policy would have been if m^t didn\u2019t change.", "The correlation is significant, and thus validates the idea.", "4. 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?", "We focus on improving modeling machine agents, and applying the improved agent models for multi-agent tasks.", "The current form of our approach is not designed for learning from human demonstrations.", "However, there are ways to modify our approach towards that direction: i) learning probing policy with model-based RL; ii) incorporating inductive bias from humans (e.g., the learner knows a specific set of possible goals of the demonstrator and probes the demonstrator to test which goal it has).", "This seems to be a good direction for future work, but we also think that the current research has provided promising results in simpler domains, and hopefully incites more research where human demonstrators are also involved by introducing this problem to the community.", "5. I think this method demonstrates a method for improved collaborative and/or competitive performances given the fact that you already have a single agent with a learned policy.", "Yes, in our experiment, we do assume that the opponent has a learned policy which is unknown to us.", "We think that this is a quite general setting where multiple machine agents are interacting with each other but do not know each other\u2019s true policies and intentions."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 429, "sentences": ["Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:", "- We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer, and we do indeed think that a pairing-based strategy is plausible for convolution-based networks.", "When talking in more general terms about \"deep learning models\", we refer to the proposed methodology for \"investigating deep learning models\", and don't want to claim that we actually evaluate a representative number of models.", "We see this methodology, as illustrated by our experiments for one model, as the central part of our contribution.", "The first paragraph of section 3.2 describes this FiLM model and, given the focus on methodology, we considered the description (plus reference to the paper) sufficient here.", "- There are a few points here:", "*", "Since it was shown that humans seem to follow a cardinality-based strategy, the pairing-based one would be not human-like, but nonetheless cognitively plausible.", "We use \"cognitively\" in the sense of \"algorithmically\" plausible, so a procedure that makes sense for solving the problem.", "An example for an implausible method would be to rely on color/shape cues to solve instances involving \"most\", which doesn't make sense for the abstract meaning of \"most\".", "*", "Regarding the question whether comparability to human behavior is indispensable: On the one hand, acceptance of and reliance on systems which follow vastly different principles can be problematic; on the other hand, if the information a system uses to arrive at its conclusion doesn't make any sense to humans (in cases where humans have an intuition what is relevant, like the above example of \"most\" and color/shape cues), we doubt that good performance alone will justify using such a model, as opposed to instead doubting the quality of the underlying benchmark data.", "* The question whether we want a VQA system which returns approximate answers is an interesting one, but we don't intend to claim that this is a desired property, just that it is desirable to know whether (and how exactly) our systems currently solve such tasks approximately.", "A conclusion from our findings may well be that it is worth improving VQA models with respect to their counting capability, as they seem to rely on an approximate as opposed to an exact number system.", "- We added a longer footnote to section 2.4 about the \"one-glance feed-forward-style networks\" for clarification.", "In summary, general precise counting is an inherently recursive ability, and models which don't have an architectural module for recursive computations are not expected to be able to learn this capability, while they may stil learn to subitize or represent numbers approximately (which doesn't require recursion).", "- You're right, this sentence was a bit vague, we rephrased it to: \"these models can indeed learn and utilize more abstract concepts (approximate numbers) than mere superficial pattern matching (\"red square\" etc)\".", "The differences we want to point out is that, on the one hand, (approximate) numbers are a more abstract concept than, for instance, object types like \"cat\", \"chair\", etc as they can be combined with any object type. On the other hand, being able to utilize such representations to answer practical questions like whether \"most\" applies is more interesting than just being able to classify which representation applies.", "- Good point, our reasoning here was mostly influenced by Pietroski et al.'s work and our intuition about the ability of convolutions to learn a local pairing strategy (see addition at the end of section 2.4).", "Presumably, it could be possible to observe the behavior in our paper based on a pairing-based mechanism which works approximately, independent of clustering, as predicted by Weber's Law.", "It's probably impossible to ultimately rule out a pairing-based strategy via experiments evaluating extrinsic behavior only, but we note that there is evidence for Weber's Law in other approximate systems where pairing-based strategies are no alternative, thus suggesting that similar mechanisms are at work here.", "We added a footnote on this to section 2.4.", "- There are a few papers focusing solely on \"most\" in psycholinguistics (like the ones cited) and linguistics in general (e.g., formal semantics), many of which talk about cardinality as a \"core concept\" of human cognition, and many of which contrast a more subconscious concept of cardinality (like the approximate number system) with the conscious algorithmic ability for precise and infinite counting.", "- We consistently use \"interpretation\" in the new version.", "- Fixed.", "- We think that our approach and particularly its inspiration by experiments from psychology are a substantial contribution to evaluation methodology in the context of powerful deep learning models, which are not infrequently described by anthropomorphizing words like \"understand\" and compared to \"human-level\" performance (added a sentence to the introduction).", "The reason for exceeding 8 pages in length is likely due to the more elaborate introduction of the various concepts from Pietroski et al.'s work and experimental methodology in psychology in general, which we assume the audience of this conference is not very familiar with."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_summary_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 430, "sentences": ["We would like to thank you for reviewing our paper.", "[Unguided Case]", "Please refer to our general comment above on why our unguided case performs better now.", "The main usefulness of our guided approach is to directly capture some of the desired variations in the data.", "This is now clearer on our quantitative and visual results in the \u201cExperiments\u201d section.", "[Heuristic Guidance]", "The main premise behind guiding our siamese networks is to find very simple, yet effective ways to capture some of the variation in the data, through weak supervision.", "For more complex semantics, we discuss the possibility of using a pre-trained network as guidance.", "Please refer to our \u201cDiscussion\u201d section for more details.", "[Differentiable Guidance]", "The transformations need to be differentiable in order to backpropagate the gradients into our generator.", "This is now pointed out and discussed in our \"Discussion\" section.", "Although this limits the function families, we can still use differentiable relaxations of more complicated functions.", "[Gaussian Prior on Latents] In our new experiments, we used uniform distributions to model the generative factors.", "We had experiments with categorical variables, however, we faced training stability issues with them.", "We now point this out in our \"Discussion\" section.", "[Similar Latent Factors] We now use an adaptive margin that depends on the distance between two latent samples.", "So, if samples are close to each other, the margin is smaller, and vice versa.", "[Experiments Section] We now compare our method against Beta-VAE, DIP-VAE, and InfoGAN, both qualitatively and quantitatively.", "Please refer to our updated \"Experiments\" section.", "[Information of Guidance] In Figure 3, we visualize which part of an image was visible to a siamese network.", "In addition, we show how changing the corresponding guided knob affects the generated images.", "[More Than Two Attributes] We now use 32 dimensions for the CelebA dataset and 10 dimensions for the 2D shapes dataset."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 431, "sentences": ["We appreciate the Reviewer's comments, which help us to improve the paper.", "In the final version of the paper we will take them into consideration.", "In the following we reply to the main concerns of the reviewer.", "Q1 - \"... how general this approach would be? ...if rules contain quantifiers, how would this be extended?\"", "The extendibility of the Neural LP framework is a very important and relevant question, which we also mentioned explicitly as a possible future work direction.", "In the rules that we support in our framework all variables are universally quantified.", "While learning rules with existential quantifiers in rule heads is a difficult endeavor in general, even for classical relational learners, the Neural LP framework in principle can be extended to support them as follows: For every relation p, we can create a fresh diagonal Boolean matrix $M_{\\exists p}$, which has 1 at the position (i,i) iff there exists an entity j, such that p(i,j) is in the KG (similar as for classification operators discussed on p. 5).", "Incorporating these matrices into the framework and filtering rules that have the respective relations in the head should allow us to extract the target rules.", "Yet analysing how well such approach performs in practice is still an open problem, which we leave for future work.", "In any case, we will discuss the extendability of the framework in the paper.", "Minor comment 1) - 4.1,", "\"O(n^2/2) -- just put O(n^2) or simply write as n^2/2\".", "This is correct, thank you. We will fix this in the final version.", "Minor comment 2) - \"How are the rules from in Eq (2)? i.e., how is \\beta_i selected for each i? In the extreme case it would be all the permutations.\"", "To avoid exponential enumeration of the predicate orderings sophisticated transformation of the rules has been applied in the Neural LP framework (see [Yang et al. 2017]).", "Minor comment 3) - \"I would suggest a different name other than Neural-LP-N...\"", "Thanks for this suggestion.", "We will certainly consider renaming the approach and fixing this in Table 2."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 432, "sentences": ["Thank you for the comments and suggestions.", "The technical comments are addressed below:", "Extending result to other target functions:", "We agree that the problem might be significantly more difficult for different target functions, and would like to make the following remarks:", "1.", "Note that in our bias-variance decomposition, only the bias term depends on the target function.", "In other words, our result on the variance (including Theorem 4) would still be valid for other targets, such as two-layer neural network.", "One caveat is that for general target function, the output needs to be properly scaled since our current analysis in Section 5 relies on linearizing the network.", "2. When the target function is a multiple-neuron neural network, deriving the bias term can be challenging.", "However, we note that under the same setup, the bias may be obtained when the teacher is a slightly more general single-index model, i.e. $y=\\psi(\\beta^\\top x)$ with Lipschitz link function $\\psi$, equivalent to a single-neuron network.", "For instance, the bias under vanishing initialization is the same as that of least squares regression on the input, which can be solved under isotropic prior on $\\beta$ via decomposing the activation function similar to Appendix C.5.", "Parameter count:", "To clarify our statement in the discussion section, our current result requires $n,d,h$ to grow at the same rate, and thus $n = O(dh)$ is beyond the regime we consider.", "This is also true for previous works on double-descent in random feature model [Hastie et al. (2019)][Mei and Montanari (2019)].", "When $h \\ll n$, it is not clear if the same analysis still applies (for instance approximating the network with a kernel model), and thus the instability of the inverse may not be the complete explanation of double-descent (if it appears).", "Characterizing the generalization in this regime would be an interesting direction.", "Training both layers:", "Thank you for the suggestion; we have included training both layers simultaneously as a future direction.", "We would like to briefly mention that under certain model parameterization and initialization, gradient flow on both layers may reduce to one of the three models we analyzed (see [Williams et al. (2019)]).", "More generally, our current result may be extended to cases where the dynamics of training both layers can be linearized (for instance initialization in the \"kernel regime\"), for which the learned model can be written down in closed-form."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 433, "sentences": ["Thank you for the thoughtful comments.", "We are glad that you found our topic interesting and appreciated our theoretical analysis and experimental results.", "We address other comments below:", "[Results are only given for the Zipfian distribution]", "Many real-world data naturally follow the Zipf\u2019s Law, as we showed in Figure 5.1 and Figure 5.3 for internet traffic and search query data.", "Thus, our theoretical analysis assumes item frequencies follow the Zipfian distribution.", "While our analysis makes this assumption, our algorithm does not have any assumption on the frequency distribution.", "[Assuming query distribution is the same as data distribution]", "As the reviewer pointed out, the query distribution we use is a natural choice.", "There might be other types of query distributions, such as the one pointed out by the reviewer.", "Intuitively, our overall approach that separates heavy hitters from the rest should still be beneficial to such query distribution.", "[Algorithm design]", "We agree that our algorithms are relatively simple.", "We believe this is a feature not a bug: as we showed in Sec. 4.1, our algorithm does not need to be more complex.", "Specifically, our Learned Count-Min algorithm achieves the same asymptotic error as the \u201cIdeal Count-Min\u201d, which is allowed to optimize the whole hash function for the specific given input (Theorem 7.14 and Theorem 8.4 in Table 4.1).", "The proof of this statement demonstrates that identifying heavy hitters and placing them in unique bins is an (asymptotically) optimal strategy.", "(In fact, our first attempt at solving the problem was a much more complex algorithm which optimized the allocation of elements to the buckets (i.e., the whole hash function h) to minimize the error.", "This turned out to be unnecessary, as per the above argument.)", "[Novelty compared to Mitzenmacher\u2019 18]", "Our paper, as well as the works of Kraska et al \u201918, Mitzenmacher \u201918,  Lykouris &", "Vassilvitskii \u201918, Purohit et al, NIPS\u201918, belong to a growing class of studies that use a machine learning oracle to improve the performance of algorithms.", "All such papers use a learned oracle of some form.", "The key differences are in what the oracle does, how it is used, and what can be proved about it.", "In Kraska\u201918 and Mitzenmacher\u201918, the oracle tries to directly solve the main problem, which is: \u201cis the element in the set?\u201d An analogous approach in our case would be to train an oracle that directly outputs the frequency of each element.", "However, instead of trying to directly solve the main problem (estimate the frequency of each element), our oracle is a subroutine that tries to predict the best resource allocation --i.e., it tries to answer the question of which elements should be given their own buckets and which should share with others.", "There are other differences.", "For example, the main goal of our algorithm is to reduce collisions between heavy items, as such collisions greatly increase errors.", "This motivates our design to split heavy and light items using the learned model, and apply separate algorithms for each type.", "In contrast, in existence indices, all collisions count equally.", "Finally, our theoretical analysis is different from M'18 due to the intrinsic differences between the two problems, as outlined in the previous paragraph.", "[The analysis is relatively straightforward]", "There are three main theorems in our paper: Theorem 8.4, Theorem 7.11 and 7.14.", "Our proofs of Theorem 7.11 and 7.14 are technically involved, even if the techniques are relatively standard.", "On the other hand, the proof of Theorem 8.4 uses entirely different techniques.", "In particular, it provides a characterization of the hash function optimized for a particular input.", "[The machine learned Oracle is assumed to be flawless at identifying the Heavy Hitters]", "Actually, this is not the case.", "The analysis in the paper already takes into account errors in the machine learning oracle.", "Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.", "In summary, our results hold even if the learned oracle makes prediction errors with probability O(1/ln(n)).", "We will revise the text to make it clearer."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 434, "sentences": ["We thank the reviewer for the detailed review.", "Below we address the main concerns.", "\u201cI think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area... For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly\u201d", ">> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.", "\u201cWhat would have been nicer would be to perform some sort of ablation study, where the authors studied how the representational capacity of the network changed as a result of them introducing the universal linear transmission layer.\u201d", ">> We refer the reviewer to Figure 2 where one can see that the PointNet model underperforms across various tasks compared to PointNetST that is identical to PointNet except the addition of a single linear transmission layer.", "Furthermore, PointNetQT, PointNetSeg, and DeepSets can be seen as different versions of PointNet variations.", "\u201cA direct theoretical and experimental comparison between PointNet and PointNetST would have been useful for me to understand the impact of the change that the authors introduce..\u201d", ">> In Theorem 1 we state that PointNet is not equivariant universal, but PointNet with a single transmission (PointNetST)  layer is.", "In the experiments section we compare these two models on three different learning tasks.", "\u201cWhere is the conclusion section?\u201d", ">>We felt it is unnecessary but see we were wrong, we will add one."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label"]}
{"abstract_id": 435, "sentences": ["We appreciate the reviewer\u2019s positive comments about our work.", "Regarding the convergence and speed of training, we would like to stress that all hyperparameters for training were kept the same as those for training the default model architecture, other than those we explicitly mentioned as being tuned (e.g. learning rate for the speech experiment).", "In particular, for all experiments, the number of epochs is the same for both the baseline approach and the K-matrix approach.", "Additionally, for the speech preprocessing and ShuffleNet experiments, we compare the total wall-clock training time of our K-matrix approach to that of the baseline approach, in both cases finding that the training time required by our approach is at most 20% longer than that of the baseline approach.", "In our updated revision, we also include the training time comparison for the DynamicConv model in Appendix B.4.2 (in this case, the modified model with K-matrices actually trains slightly faster than the baseline).", "We agree with the reviewer that a training plot can help provide a better understanding of how our proposed approach performs, and therefore have included an example plot (for the ShuffleNet experiment) in our updated revision (in Appendix B.2.3).", "Regarding empirical comparisons to dense matrices, in Table 5 (Appendix B.1.2), we compare the use of K-matrices in the raw-features speech model with several other classes of matrices, including dense matrices.", "We find that, while using a trainable dense matrix slightly outperforms just using the fixed FFT (0.3% drop in test phoneme error rate), using a K-matrix instead of a dense matrix yields a further improvement of 0.8% in the phoneme error rate.", "Another empirical comparison of K-matrices and dense matrices is in Section 3.3, in which we replace the linear layers in the decoder of a DynamicConv model with K-matrices; these linear layers are by default dense (fully-connected) matrices.", "Theoretically, in Lemma E.3 we show that arbitrary dense matrices are contained in the BB* hierarchy \u2013 in particular, that any n x n matrix is in (BB*)^{2n-2}, which implies that its K-matrix representation requires at most (4n log n)*(2n-2) = O(n^2 log n) parameters and thus is tight up to a logarithmic factor in n."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 436, "sentences": ["We thank reviewer2 for the kind and constructive review.", "We agree that broader analysis beyond global-local disentanglement is desirable and we hope to perform more experiments in a follow up work."], "labels": ["rebuttal_social", "rebuttal_future"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label"]}
{"abstract_id": 437, "sentences": ["The primary goal of the projections is to project all embeddings into the model dimension d so that we can have variable sized embeddings.", "Our goal was not to make the model model expressive.", "Compared to the rest of the model, these projections add very little overhead compared to the rest of the model.", "Doing without them is an interesting future direction though!"], "labels": ["rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 438, "sentences": ["We appreciate the comments of the reviewer.", "Please see our reply below.", "1) - \"... the current proposed method can only deal with one form of numerical predicate, which is numerical comparison.\"", "Apart from simple numerical comparison we are also able to deal with complex classification operators that aggregate numerical attributes using linear functions, where the threshold value is selected in a systematic fashion, (see Classification Operators) as well as negated atoms (see Negated Operators on p. 6).", "We note that such rules are indeed limited to some extent, but they still capture a rather expressive fragment of answer set programs with restricted forms of external computations", "[Eiter et al., 2012].", "Below we present examplar rules learned by our framework, which are not restricted to numerical comparisons.", "2a) - \"The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.\"", "With the rapid development of industrial and scientific knowledge graphs, we believe (and agree with the Reviewer #2) that learning rules that involve multiple modalities is an important and relevant problem.", "Indeed, such rules can not only be used for data cleaning and completion, but they are also themselves extremely valuable assets carrying human-understandable structures that support both symbolic and subsymbolic representations and inference.", "2b) -  \"The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.\"", "To the best of our knowledge Freebase and DBPedia are the only standard KGs with numerical values [Garcia-Duran et al., 2018] used for the evaluation in state-of-the-art works.", "This is the reason why we have selected and used them for our experiments.", "The impact of our approach might appear to be rather modest, since these KGs still have only a limited amount of numerical information.", "Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.", "We would be happy to learn about other datasets suitable for our experiments.", "3) - \"The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.", "A good place to start with is to visualize (print out) the learned numerical rules and see if they make any sense.\"", "According to the Reviewer's comment we will extend Section 5 on experimental results by showing more detailed analysis.", "In particular, we will present the following examples of the learned rules from the considered (real-world and synthetic) datasets:", "- FB15K:", "disease_has_risk_factors(X,Z) :- f(X), symptom_of_disease(X,Y), disease_has_risk_factors(Y,Z)", "The rule states that symptoms with certain properties (described by the function f) typically provoke risk factors inherited from diseases which have these symptoms.", "Here, the function f is the sigmoid over a linear combination of numerical properties of X.", "- DBPedia:", "defends(X,Z) :- primeMinister(Z,Y), militaryBranch(Y,X), f(Y)", "This rule states that prime ministers of countries with certain numerical properties (described by the function f), are supported by military branches of the given country.", "The function f is the sigmoid over a linear combination of numerical properties of Y.", "- Numerical1:", "prefer(X,Y) :- isNeighbourTo(X,Y), hasOrder(X,Z1), hasOrder(Y,Z2), Z1>Z2, max{Z2:hasOrder(Y,Z2)}", "This rule with a comparison operator states that a person X prefers neighbours with the maximal order that is less than X's.", "- Numerical2:", "prefer(X,Y) :- isNeignborTo(X,Y), hasBalance(Y,Z1), borrowed(Y,Z2), f(Y)", "This rule states that neighbours with the largest difference between the balance and the borrowed amount are preferred.", "More precisely, here f selects among all X those entities, for which the difference between the balance and the borrowed amount is maximal."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_followup", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 439, "sentences": ["Thank you very much for the comment.", "We have revised the script to reflect the suggestions, and we would like to articulate on the changes we have made.", "Because we are out of space, we will provide the  answers over several comments."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 440, "sentences": ["We really appreciate your comments.", "Replica exchange Langevin diffusion is widely used in classic MCMC over the years.", "Our work also uses this methodology in the setting of nonconvex optimization problem, which arises in many machine learning applications such as training neural networks.", "There are also many interesting questions in this direction, for example, how to choose the best temperature based on the structure of specific problems.", "That is why we still submit it for ICLR.", "As for the comments on math side.", "First, when a->infty, the exchange process should be defined in another way.", "Our current definition, which swapping particles with some rate, is only valid for finite a.", "This extension is not totally trivial and in Dupuis&et. al's work, some results are established.", "In our paper, we only discuss finite swap rate.", "This brings convenience for the discussion of discretization error.", "Otherwise, we need to use a different approach to analyze.", "Moreover, we point out that in discretization, the swapping intensity a should be smaller than the step size.", "This also reflects the nontrivial connection between infinity swapping and discretization.", "Second, the kappa in (3.10) is related to the Poincare inequality and it is also a lower estimate of the spectral gap of Markov process.", "Kappa is can be defined as the solution of a variational problem involved Dirichlet form.", "However, although our result shows that swapping boosts the Dirichlet form, we still cannot obtain an analytical formula of kappa depending on a, since the variational problem makes this relation extremely complicated.", "Even in the field of pure math, it is still very hard to obtain an explicit formula of kappa for a general Markov process.", "However, for this special case, we will keep trying to solve it in the future."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label"]}
{"abstract_id": 441, "sentences": ["We thank the reviewer for positive feedback and for championing our paper.", "We are also grateful for your constructive suggestions to improve the paper and would like to report on how we have incorporated your feedback.", "Inspired by your suggestion, we conducted additional experiments on Amazon Reviews, Yelp Reviews, and Semeval (Twitter) datasets, and found that the counterfactually-augmented data resulted in across-the-board gains.", "These experiments are featured in the updated draft."], "labels": ["rebuttal_accept-praise", "rebuttal_social", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 442, "sentences": ["Due to the overlap between reviewer comments, we decided to address all concerns in a single response (please see above)."], "labels": ["rebuttal_other"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label"]}
{"abstract_id": 443, "sentences": ["We thank the reviewer for many positive comments about our paper.", "The typos explicitly mentioned in the review have been corrected, and we did our best to spot other typos not mentioned.", "Besides, all the acronyms have been explained.", "We added the tutorial from Hansen (2016) as the reference for the common choices for setting \\lambda_i in Equation 1, 2.", "We agree with the reviewer that our paper is not theoretically oriented, nor does it address any real world application like robotics or other challenging domain.", "Our point is rather to provide a practical method performing well with respect to the state of the art, which is most often evaluated with the same widely used benchmarks.", "With respect to initialization of hyperparameters, as explicitly mentioned in the \"experimental setup\" section, \"Most of the TD3 and DDPG hyper-parameters were reused from Fujimoto et al. (2018).\" The justification for this choice is to facilitate comparison with previously published work."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 444, "sentences": ["Thank you for the encouraging review.", "[R3: Weakness: It would be good to see some comparison to the state of the art ]", "With regards to your comment on attacking the current state of the art method for smoothed classifiers, we have added new results to the resubmission (Appendix B), in which we attack the adversarially trained smooth classifier [1].", "[1]. Salman et al., \u201cProvably Robust Deep Learning via Adversarially Trained Smoothed Classifiers\u201d, NeurIPS 2019"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_other_label"]}
{"abstract_id": 445, "sentences": ["We are glad you found our work interesting and novel, and thank you for your helpful suggestions for improving the writing. We have taken them on board in the revised paper, making a number of edits.", "1. \"In the introduction, \"the classical approach\" is mentioned but to be the latter is", "insufficiently covered. Some more detail would be welcome.\"", "We have added a reference to what we mean by the classical approach in the related works section.", "2. \"page 2, \"predict the probability\": rather employ \"estimate\" in such context?\"", "We have changed \u201cpredict\u201d to \u201cestimate\u201d.", "3. \"'linear piecewise': 'piecewise linear'?\"", "This was a typo and we have corrected this phrase.", "4. \"What is 'an exact upper bound'?\"", "We mean that it is a true upper bound instead of just being a stochastic estimate of an upper bound (while, on the other hand, Weng et al\u2019s approach is stochastic estimate of a lower bound).", "However, we agree that the \u201cexact\u201d is superfluous and have removed it.", "5. \"I am not an expert but to me 'the density of adversarial examples' calls for further", "explanation.\"", "We think perhaps \u201cthe prevalence of adversarial examples\u201d would be a better phrase and have corrected this.", "We mean that the input model density is integrated over for our metric to calculate the volume of counterexamples in a subset of the input domain, relative the overall volume of that input domain.", "6. \"From page 3 onwards: I was truly confused by the use of [x] throughout the text", "(e.g. in Equation (4))", ".", "x is already present within the indicator, no need to add yet", "another instance of it.\"", "In retrospect, we agree that this was confusing and have removed the [x] notation from the indicator function.", "7. \"In related work, no reference to previous work on \"statistical\" approaches to NN", "verification. Is it actually the case that this angle has never been explored so far?\"", "As far as we are aware this is correct: we have not been able to find any prior work which aims to estimate the statistical prevalence of counterexamples.", "8. \"In page 6, what is meant by 'more perceptually similar to the datapoint'?\"", "We mean that the minimal adversarial distortion for models on CIFAR-10 is known to typically be much smaller than for MNIST.", "The result of this is that an adversarial example on MNIST will often have visual salt-and-pepper noise, whereas an adversarial example for CIFAR-10 typically is indistinguishable to the naked eye from its unperturbed datapoint.", "9. \"In the appendix: the MH acronym should better be introduced, as should the notation", "g(x,|x')", "if not done elsewhere (in which case a cross-reference would be welcome).", "Besides this, writing \"the last samples\" requires disambiguation (using \"respective\"?).\"", "We have added to this description so that it is less terse and more carefully introduces the notation, including changing \u201clast samples\u201d to \u201cfinal samples\u201d and adding in a reference for further reading."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 446, "sentences": ["We thank the reviewer for his comments.", "We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.", "Further comments are addressed below.", "**", "controlling the amount of deformations", "The stability bounds of B+M provide upper bounds on ||Phi(x') - Phi(x)|| (where x' is a deformation of x) based on quantities related to the corresponding diffeomorphism, i.e. the maximum norm and the maximum jacobian norm.", "For simple classes of deformations these can be computed precisely in terms of the parameters of the deformation, e.g. for translations, rotations, scaling or simple parametric warps.", "When bounding these away from zero by a certain constant, ||Phi(x') - Phi(x)|| is then included in a centered ball of the RKHS with a radius growing with this constant.", "This constant then acts as a regularization parameter, just like the size of additive perturbations in the case of adversarial perturbations, and can be tuned by cross-validation.", "** tightness of the lower bounds", "This is something that we verify empirically in our experiments at the end of training by checking the values of spectral norms as a proxy of the upper bound, and looking at the gap with the lower bound.", "In particular, when using the ||f||_M penalty, lower and upper bounds seem to be controlled together in our experiments (Figure 2), making the bound useful, in contrast to PGD, for which spectral norms grow uncontrolled when the lower bound decreases.", "We will further clarify this in the paper.", "eqn (8), (12): thanks for pointing these out, we will fix this in the paper."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_summary_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_done_label"]}
{"abstract_id": 447, "sentences": ["Thank you for the time and effort spent reviewing our paper.", "We mostly agree with your characterization of our work, but we think there are two important points we perhaps did not sufficiently emphasize in our paper and that we would like to mention:", "(1) There are other existing tasks and algorithms that fall into the marginal policy gradients framework.", "For example, researchers and practitioners both almost always clip actions when using policy gradient algorithms for robotics control environments (read: MuJoCo tasks).", "Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces.", "Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten the existing analysis of their algorithm.", "(2) To the best of our knowledge, our work is the first to apply such variance reduction techniques to RL.", "To summarize, our work consists of two components: (a) a new algorithm for directional control and (b) a variance reduction framework that can be applied to directional action space and clipped action spaces.", "While directional action spaces are not very common at this time, clipped action spaces are extremely common.", "We also anticipate that in the future, many additional environments will be available that feature directional actions (many console or PC games, for example).", "For these reasons, we feel that our work is not incremental at all, and is actually quite novel."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label"]}
{"abstract_id": 448, "sentences": ["We thank the reviewer for the positive assessment of our work.", "We would like start by stating that we did not mean to claim that the rate of convergence proved in this paper is better that than of Yan et al.", "We have modified the Remarks to clarify the statements.", "In the stochastic gradient setting, the number of gradient evaluation is indeed $T^2$. This is consistent with the result in Bernstein et al. (2018).", "The main point we would like to make is that the bounds are very concise and exactly reduce to that of gradient descent/stochastic gradient descent in the special cases.", "We thank you for pointing out that the bounded variance assumption may also be restrictive and only satisfied on bounded domains.", "It is nonetheless a standard assumption made in the literature.", "We have modified Remark 3.4 (and added Remark 3.5) to make this clear in the updated version.", "Response to other minor points:", "Our convergence analysis is done for non-convex objective functions (similar to that of Yan et al. and Bernstein et al.).", "In the non-convex setting, to the best of our knowledge, there are no theoretical results that show benefits of momentum methods over SGD.", "For experiments, we speculate that the reason is that the batch size used is too small for (Powered)SGDM to gain an advantage over (Powered)SGD.", "We plan to add SGD as a reference algorithm (as suggested by another reviewer).", "Once the experiments are complete, we should be able to see how SGDM compares with SGD in the experiments.", "This may take a while for the ImageNet experiments, but we promise to do so in the final version.", "We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 449, "sentences": ["Thanks for acknowledging our novel perspective as well as giving the valuable comments.", "Below, we address the detailed comments.", "Particularly, we clarify some potential misunderstandings and provide new results to show state-of-the-art results.", "Q1: About the main concern on \u201cnovelty, improvement relative to the current state of the art implementations,  and non-linearity of G and D\u201d:", "A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.", "Indeed, as agreed by R#2, this is a major novel contribution that provides a unified and promising framework to model the stability of GANs, which includes some recent developments (e.g., Negative Momentum and Reg-GAN, See Sec. 4.1 and Appendix A&C) and also provides us a possibility to explore advanced tools in control theory (e.g., nonlinear control and modern control theory [*3]) to improve both the stability and convergence speed of GANs.", "Then, as some useful examples, in this paper, we particularly showed that the technique of negative feedback can be leveraged to stabilize GANs and developed NF-GAN, which was proven to be effective in our experiments.", "Finally, we added new results in Table 1 in the revision, which shows that the same technique of negative feedback can further improve the state-of-the-art method of SN-GAN [*5].", "Specifically, we apply NF-GAN to SN-GAN [*5] and NF-GAN provides a significant improvement on the state-of-the-art inception score on CIFAR-10 (from 8.22 to 8.45).", "Such results indicate that our technique of NF-GAN can still benefit the state-of-the-art variants of GANs (e.g., SN-GAN).", "Overall, our perspective is novel and it indeed sets new state-of-the-art results as compared to the current variants of GANs.", "As for the statement \u201cthe WGAN analysis does not take into account that G and D are non-linear\u201d, this is a potential misunderstanding.", "In fact, in the WGAN analysis, we do not put any constraints on the G and D networks, which can be any well-defined nonlinear models.", "This confusion may arise from the linearity of the dynamics.", "In Sec. 3.2", ", as we model the dynamics of G and D in the functional space instead of the parameter space, they can both denoted as integral parts (See Eqn. (10)&(11)), thereby both are linear.", "However, this doesn\u2019t influence the nonlinearity of G and D with respect to the weights.", "Technically, we provide an approximate solution to deal with such nonlinearity.", "As discussed in Remark 3 and Section 7 of the revised paper, we also note that the recent analyses of GANs on the functional spaces [*4] can provide a promising solution to solve this approximation and we leave it as our future work.", "Q2: About \u201ccomparison with the baselines\u201d:", "A2: For fairness, we tried our best to fairly compare all methods.", "See details in the Concern 1 of our post for common concerns.", "For the state-of-the-art results, we indeed provided a significant improvement on the inception score of CIFAR-10 (from 8.22 to 8.45) using SN-GAN\u2019s architecture as suggested.", "Q3: How does negative feedback (NF) influence the training of stable dynamics and further evaluation:", "A3: As stated above in our response to Q1, we added the new results of applying negative feedback to SN-GAN, which is a state-of-the-art variant of GANs with empirically stable performance.", "Our results (See Table 1 (bottom) in the revision) indeed show that NF can further improve to reach new state-of-the-art results.", "In general, as NF is essentially a penalty term that regularizes $D$ to the zero-function, we can expect it to be effective for most dynamics [*3].", "We also included the suggested related work (Balduzzi et al. 2018) in Section 5.", "Finally, as for the further evaluation (such as multiple seed runs and 2nd-momentum estimates), we agree it is interesting, but it is very demanding in computational resources, and we leave it for a systematic future investigation.", "In summary, this paper proposes a unified framework to model the dynamics of GANs which is a powerful tool to stabilize and improve GANs.", "The experimental results on SN-GANs demonstrate that NF-GAN can further improve the performance of stable GANs and provide an improvement to the state-of-the-art models.", "Finally, it is a promising direction to follow where further advanced control methods can benefit the training of GANs.", "[*1] Gidel, Gauthier, et al. \"Negative Momentum for Improved Game Dynamics.\" The 22nd International Conference on Artificial Intelligence and Statistics. 2019.", "[*2] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. \"Which Training Methods for GANs do actually Converge?.\" International Conference on Machine Learning. 2018.", "[*3] Khalil, Hassan K. \"Nonlinear systems.\" Upper Saddle River (2002).", "[*4] Johnson, Rie, and Tong Zhang. \"Composite Functional Gradient Learning of Generative Adversarial Models.\" International Conference on Machine Learning. 2018.", "[*5] Miyato, Takeru, et al. \"Spectral Normalization for Generative Adversarial Networks.\" (2018)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_summary", "rebuttal_answer", "rebuttal_done", "rebuttal_future", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 450, "sentences": ["We would like to thank you for reviewing our paper.", "[Principled Guidance] The design of guidances is heuristic, but as illustrated in Figure 2 and in Table 2, they are easy to design and are effective.", "Further, we added our unsupervised analyses to show that the method works even without explicit guidance on all tested datasets.", "In this paper, we propose the idea of guidance itself and show that it is imposing a desired semantics on the latent space without having labeled data.", "In our future work, we plan to investigate more principled ways of deciding guidances.", "We now address this point in our \"Discussion\" section.", "[Experiments Section] We have significantly updated qualitative and quantitative results in our \"Experiments\" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."], "labels": ["rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_other_label"]}
{"abstract_id": 451, "sentences": ["We thank you for your thoughtful review. We are happy to learn that you believe it is an interesting direction that holds potential for high impact.", "Re: simpler methods (like clustering, BoW etc.) might work equally well", ">", "To assess transfer learning potential reliably, we require both the X and y for the target task (i.e supervision).", "Consider the case where the target task is sentiment analysis, and one of the candidate tasks is finding sentence length (SentLen).", "For the sake of the argument, let us assume that the X for both sentiment analysis and sentence length is exactly the same set of movie reviews.", "In such a case, unsupervised metrics like clustering, BoW etc. would indicate maximum transfer potential, whereas the actual transfer potential would be close to zero (assuming the lengths of reviews aren\u2019t correlated with the sentiment).", "This is a fundamental problem of measures that look directly at the input data X without considering the nature of the labels y.", "For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.", "Re: not clear if the classifier weight difference is well defined", "> You are right in noting that the classifier weights might capture dissimilar yet useful features for two similar tasks, and hence the classifier weight difference might under-predict the transfer potential.", "We discuss this issue in the paper (section 4.1), which is why we avoid the set overlap metric.", "Owing to similar concerns, we recommend using CFS information transfer metric over classifier weight difference (which is also supported by results in Table 2 and Figure 3).", "Re: thoughts on how this could be applied outside the context of sentence representations and classification", "> It is easy to adopt our approach to study the information encoded in the encoders for other problems involving structured prediction (say POS Tagging).", "Instead of using a decoder that takes in all the dimensions of the encoded input token, one could iteratively select dimensions that provide the highest gains in decoding the right target sequence (say POS tags).", "Our formulation is very general, and it could potentially also be applied to other modalities like images for tasks like image classification and captioning."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 452, "sentences": ["We appreciate your time and comments on the work."], "labels": ["rebuttal_social"], "confs": [1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label"]}
{"abstract_id": 453, "sentences": ["We are really thankful for the positive feedback. Here we give detailed answers to the Reviewer's concerns.", "1) - \"... in Table 2, AnyBurl ... yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.\"", "Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.", "2) - \"... the expressiveness of the learned rules can be somehow limited,...\"", "We remark that our framework supports rules with negation, comparison among numerical attributes and classification operators, where linear functions over attributes can be expressed.", "Such rules capture a fragment of answer set programs, where a limited form of aggregation [Faber et al., 2011] and restricted external computation functions [Eiter et al., 2012] are allowed.", "While these rules might not cover all possible knowledge constructs, they are still valuable and rather expressive for encoding correlations among numerical and relational features.", "Moreover, to the best of our knowledge they have not been directly supported by previous works on rule learning.", "3) - \"Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 ...\"", "Thanks for referring us to this important work! We will certainly add this reference to the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label"]}
{"abstract_id": 454, "sentences": ["Thank you for the thoughtful review and positive assessment. We are glad to see that you appreciate the genuine flavor of causality in our paper and support our paper\u2019s acceptance.", "We agree that a formal exposition introducing an NLP/deep learning audience to the basics of interventions and counterfactuals and expressing a toy DAG to explain the spurious associations between the review sentiment and the manifestation in text of other attributes of the review, including but not limited to the genre, actors, budget, etc. We are actively working on preparing this exposition and while it is not yet in the draft we plan to have it prepared in advance of the camera-ready version.", "We thank the reviewer for pointing out that we should have been more thorough in explaining that while genre is a clear example of such a spurious association, it is far from the only one captured in Figure 4.", "Indeed, many other words, including \u201cwill\u201d, \u201cmy\u201d, \u201chas\u201d, \u201cespecially\u201d, \u201clife\u201d, \u201cworks\u201d, \u201cboth\u201d, \u201cit\u201d, \u201cits\u201d, \u201clives\u201d, \u201cgives\u201d, \u201cown\u201d, \u201cjesus\u201d, \u201ccannot\u201d, \u201ceven\u201d, \u201cinstead\u201d, \u201cminutes\u201d, \u201cyour\u201d, \u201ceffort\u201d, \u201cscript\u201d, \u201cseems\u201d, and \u201csomething\u201d, appear to be spuriously associated with sentiment and are captured by the original-only and revised-only classifiers as highly-weighted features.", ", Notably all of these features fall out from the highly-weighted features when our classifier is trained on counterfactually-augmented data.", "Regarding the sensitivity of BERT models, Table 9 shows the ability of a model explicitly trained to differentiate between the original and the revised data.", "This is to shed some insight on how much the two differ (on account of our intervention).", "Because the two indeed are different, we expect that a model should be able to differentiate them to some degree.", "We note that a model class\u2019s ability to differentiate between the original and revised data when explicitly trained to do so may not necessarily be correlated with how susceptible that model is to breaking when evaluated out of sample.", "We\u2019re grateful for your comments on exposition and will continue to address these points as we improve the draft."], "labels": ["rebuttal_accept-praise", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 455, "sentences": ["Thank you for your comments.", "With respect to your concern over scalability, the need to input the actions and observations of all agents in the value function (i.e. centralized value function) limits scalability only during training time, and it is a necessary measure to reduce the non-stationarity of multi-agent environments, as discussed in previous work [1].", "We would also like to re-emphasize the fact that our final trained policies are decentralized and do not require any information exchange between agents.", "This trait makes our approach (and other centralized-critic/decentralized-policy approaches) useful in situations where one can train in a simulation where communication is less taxing, but deploy in the real world, where communication may be more challenging.", "We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.", "Your thinking of \u2018semantically probable\u2019 exchange of information is interesting.", "We note that it is possible to compress each agent\u2019s actions/observations before they are sent to a central critic.", "Our setup naturally allows for this.", "Consider a case with high-dimensional image observations.", "In our approach, each agent needs to embed these observations (along with their actions) before sharing with other agents.", "In a situation where information exchange between agents is expensive, even during training, we can select a sufficiently small embedding space such that performance and efficiency are balanced.", "This notion of compressing embeddings prior to sharing across agents does not fit as naturally into the competing methods.", "Our experiments were especially designed to have two contrasting environments, so that we can illustrate two different aspects of multi-agent RL where we felt like the current approaches have not been able to address at the same time.", "Thus, it is by design that different baselines perform differently on them, as every approach has its own strengths and weaknesses.", "Our experiments demonstrate that our approach handles both environments well, which none of the baselines is able to do.", "Our experiments on Cooperative Treasure Collection demonstrate that the general structure of our attention model (even without considering dynamic attention as in our uniform attention baseline) is able to handle large observation spaces (and relatively larger numbers of agents) better than existing approaches which concatenate observations and actions from all agents together.", "Furthermore, our experiments on Rover-Tower demonstrate that the general model structure alone is not sufficient in all tasks, specifically those with separately coupled rewards for groups of agents, and dynamic attention becomes necessary.", "We have added a new section 6.3  to the supplement that includes visualizations of the attention mechanism both over the course of training and within episodes.", "Our code is available online and a link will be included in the paper once the anonymized review period is over.", "[1] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6382\u20136393, 2017."], "labels": ["rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_done", "rebuttal_by-cr", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_other_label"]}
{"abstract_id": 456, "sentences": ["Thank you for the comments and suggestions.", "As you pointed out, our current result in Section 5 does not apply to non-smooth activations -- understanding the generalization of ReLU networks would be interesting future work.", "We have updated the manuscript with a few minor modifications: 1) Figure on the population risk of sigmoid network (first layer optimized) in addition to SoftPlus; 2) additional remarks on the population risk of network in the kernel regime in Section 5.2; 3) corrected typos."], "labels": ["rebuttal_social", "rebuttal_future", "rebuttal_done"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 457, "sentences": ["-- \u201cThis work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms. It is hard to support this motivation when no experiments are done in its favor.\u201d", "The long-term agenda / research program is indeed two-fold:", "1. Investigate whether known optimal worst-case algorithms can be reproduced without any domain knowledge (i.e., \u201cCan ML learn Algorithms\u201d).", "This is the case in which the optimal distribution of inputs is also known.", "2. Discover new/better worst-case algorithms for problems with the aid of ML, when neither a good algorithms or input distribution is known.", "#2 is a long-term goal, and not tackled in this paper, but we believe #1 (tackled in this paper) is itself of strong interest (and difficult) -- would ML be able to discover the same \u201cpen-and-paper\u201d algorithms that computer scientists invented?", "The problems we study (ski-rental and Adwords) fall into the first category of problems.", "Note that the algorithms in the two cases are very different in structure.", "Further, please note that even though the optimal distribution of input is known in these two problems, we do not use it at all in training.", "Indeed, this is the main point of this paper -- the previous work of Kong et al. used these distributions to train the algorithm network (and hence that technique still needed the prior theoretical \u201cpen-and-paper\u201d work), while this work starts with ZERO knowledge.", "We follow this approach even in case #1 when the optimal input distribution is known exactly because we have the ultimate goal #2 in mind, that is, we want to design a framework that can eventually also work without knowledge of optimal input distribution (but that goal is outside the scope of this paper).", "-- \u201cNo comparison has been made between their approach and other previous approaches.", "We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio.", "It is thus very hard to know if this new approach brings any improvement to previous work.\u201d", "We believe there is some misunderstanding here as to the contribution.", "As such, there are no previous approaches to \u201clearn algorithms\u201d (besides Kong et al.).", "To be more explicit (in case we didn\u2019t understand the comment)", ", previous work for algorithmic problems could fall into a few buckets:", "(1) The original algorithms papers which found optimal worst case algorithms [Karlin et al. 1986, Mehta et al. 2007].", "These give the analytical benchmarks.", "E.g., [Mehta et al. 2007] proposes the algorithm to solve Adwords, and proves that it achieves the optimal CR of 1-1/e ~ 0.63 (i.e., no matter what the online input sequence is, you get >= 1-1/e of the optimal solution in hindsight if you knew the instance offline).", "Thus the difference of 0.01 CR is a direct comparison to that work.", "(2) One may imagine there could be some kind of optimization (IP / LP) technique or some ML technique to solve specific instances of the problem (a specific instance of Adwords e.g.).", "But this is in fact not a feasible possibility, for two reasons: (a) Our problems are online problems where the full instance itself is not known in advance, and (b) we are looking for worst case competitive algorithms, i.e., a policy that does well no matter how the instance unfolds in the future.", "Thus there can not be previous work to compare in such a bucket.", "(3) Kong et al., 2018 is the closest previous work since it shows how to learn algorithms in the online setting.", "As mentioned above, the critical difference is that our paper learns the algorithms without any prior knowledge of the worst input distribution, but evolves both the distribution and the algorithm jointly (with some parallels to GANs, AlphaZero, self-play, etc. as we have stated).", "Quantitatively, the CR results are equally good; our main objective is to see if the learned algorithm is close in policy to the theoretical algorithm, and whether we are reasonably close to the optimal CR.", "Thank you for your review.", "Please also see our high-level clarification above which we believe can help in better interpretation of our contribution.", "Some specific responses below:", "** Addressing comments on the write-up:", "Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).", "Details on architecture: Agreed, and thanks. We have added some details on the specific network architectures to Appendix C (for ski rental) and Appendix D (for AdWords).", "New suggested structure and related suggestions: These are nice suggestions and explain why the structure was confusing. We\u2019ve worked on these to come close to the suggested structure.", "\u201cMSVV\u201d reference.", "Thanks for pointing out! This is the same algorithm described above in Mehta et al., but we realize that must have been confusing. Fixed.", "** Addressing Technical Comments:", "-- \u201chyperparameter searching\u201d:", "The networks we used in this work are fairly simple: dense layers with standard ReLu activation, and we use standard Adam optimizer.", "Simply choosing commonly recommended values for the parameters turn out to work well for the problems we looked at.", "In general, we agree with the reviewer\u2019s point that hyperparameter searching can be important.", "For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.", "We also clarify that it is not the case that \u201cwe have no interests in extending ML techniques\u201d in general.", "Indeed, we believe that for the future success of our approach on more open problems in online algorithms, it very much relies on the advances of ML in terms of neural network structure, optimization algorithms and training techniques.", "We also hope our work can motivate the design of new tools/techniques tailored for this direction."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label"]}
{"abstract_id": 458, "sentences": ["* the fact that the random projections preserved the inner product (centered at zero) was probably not desirable.", "It might be more fruitful if these linear combinations were learned or sub-senses of words", "(e.g. [1])", ".", "Preserving the inner product means that the distribution of the features is not biased, if we keep adding words to the dictionary, the performance would degrade gracefully with the amount of compression.", "Perhaps a non-orthonormal basis would also work if the network compensates for the different distortions in the inner products.", "You are correct in assuming that other discrete building blocks could be more fruitful, but, we chose language modelling as a setting, not a task (see general response) as such, the building block chosen was the word. We could have chosen sub-words, or characters but the goal here is not the get the best possible language model but to understand a property of the mechanism.", "An interesting idea would be to actually use other information and encode it as random projections (e.g. syntactic dependency patterns).", "The amount of possible patterns is simply too large to be enumerated and as such the random projections would serve as unique \"fingerprints\" for unique \"dependency patterns\" that would be used as inputs.", "1. I do not get the point of bringing up NCE...", "Approximations like NCE (in conjunction with random projections) would allow us to remove the restriction in the output layer.", "We want to imply that our proposal is not incompatible with NCE, but we did not yet explore it so, to make the paper more self-contained it is probably best to leave this out.", "2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).", "we were trying to cram different experiments (with different regularization) in the same figure which is understandably confusing and needs to be corrected.", "3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?", "yes but not necessarily. Alpha can be used to control the expected proportion of non-zero entries, but as long as the probability of a sparse configuration is random uniform, our mechanism guarantees that any sampled index is almost orthogonal to any other sampled index, so it's easier achieve the same while guaranteeing the sparsity in the inputs."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 459, "sentences": ["Thanks for your efforts in reviewing our paper and the valuable comments.", "We attempt to address your concerns in the following.", "1. Response to the \"Weaknesses\" part and the comparison with GBDT", "As stated in the response to review 1, our goal is not inventing a model to beat GBDT but developing a model to cover the scenarios not suitable for GBDT such as some applications need online updating.", "\"The next contender\" model in your comment is the GBDT, which indeed works well for tabular data.", "However, GBDT suffers from two shortages, as stated in Section 2 and the responses to reviewer 3.", "These 2 shortages make GBDT very hard to be used in many real-world scenarios.", "For example, in an online recommender system, we need to update the model frequently to achieve the satisfying real-time performance.", "In this case, GBDT will be very inefficient as it needs to be re-trained from scratch.", "In contrast, NN can be learned by mini-batch fashion and therefore can learn from streaming data naturally.", "The proposed TabNN can overcome these shortages and achieve comparable accuracy with GBDT.", "Moreover, compared with previous NN based solutions for tabular data, TabNN outperforms them significantly.", "Therefore, TabNN is a better general solution for tabular data as it can cover more scenarios.", "2. Difference between \"implicit feature combinations\" and \"explicit feature combinations\"", "The main difference lies in whether the feature combination information is explicitly introduced into model structure or not.", "For example, in FCNN, as all features are connected to the neurons in the next layer, there are no feature combination information in the model structure.", "Although the feature combination information are not explicitly provided, one neuron in FCNN can learn a linear combination of its input features.", "Thus, we say there are \"implicit feature combinations\" in FCNN.", "In TabNN, we leverage GBDT to find feature combinations and then construct model structure according to them.", "Thus, we say there are \"explicit feature combinations\" in TabNN.", "\"Implicit feature combinations\" is not efficient as it introduces much more trainable parameters, and has a risk of over-fitting.", "In contrast, \"explicit feature combinations\" let model focus on the more important feature combinations and is more efficient.", "The successful CNN model also uses \"explicit feature combinations\", as it only combines the local pixels.", "3. About \"encourage parameter sharing\".", "Yes, we use parameter sharing in the one cluster of feature groups.", "We will clarify this in the paper.", "4. Benefits brought by the \"Structural Knowledge\"", "We had compared the benefit brought by the 'Structural Knowledge' in the experiment.", "The difference between TabNN (S) and TabNN (R), as shown in Table 3, implies that that the structural knowledge from GBDT yields a large contribution to the performance of TabNN.", "The \"Structural Knowledge\" is in TabNN by default. We will clarify this in the paper."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 460, "sentences": ["Thank you for the valuable and encouraging feedback! Below, please see our replies.", ">> What are the key limitations of AutoLoss? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?", "More discussions on these questions can be very helpful to further understand the proposed method.", "These are indeed good questions.", "We list several limitations we discovered during the development of AutoLoss:", "- Bounded transferability", "We observe AutoLoss has bounded transferability -- while we successfully transfer a controller across different CNNs, we can hardly transfer a controller trained for CNNs to RNNs.", "This is slightly different from some related AutoML works, such as in [1], where auto-learned neural optimizers are able to produce decent results on even different families of neural networks.", "We hypothesize that the optimization behaviors or trajectories of CNNs and RNNs are very different, hence the function mappings from status features to actions are different.", "We leave it as a future work to study where the clear boundary is.", "- Design white-box features to capture optimization status", "Another limitation of AutoLoss is the necessity of designing the feature vector X, which might require some prior knowledge on the task of interest, such as being aware of a rough range of the possible values of validation metrics, etc.", "In fact, We initially experimented with directly feeding blackbox features (e.g. raw vectors of parameters, gradients, momentum, etc.) into controller, but found they empirically contributed little to the prediction, and sometimes hindered transferability (as different models have their parameter or gradient values at different scales).", "- Non-differentiable optimization", "Meta-learning discrete schedules involves non-differentiable optimization, which is by nature difficult.", "Therefore, a lot of techniques in addition to vanilla REINFORCE are required to stabilize the training.", "Please also see our answer to the next question for more details.", "As a potential future work, we will seek for continuous representations of the update schedules and end-to-end training methodologies, as arisen in recent works [2].", "We haved add the above discussion to the latest version as Appendix A.9.", ">> As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.", ">> Any plan for open source?", "We acknowledge the difficulties of training controllers using vanilla REINFORCE.", "During our development of the training algorithm (See Eq.2, the \u201cdiscussion\u201d section in Sec.4, and Appendix A.1), we found the vanilla form of REINFORCE algorithm leads to unstable training.", "We therefore have made many improvements and adaptations by either referring to existing literature, or depending on the specific tasks.", "They include:", "- Substitute from the reward a baseline term, which is a moving average (see section 3, Eq.2)", "- Reward clipping (see section 3, under Eq.2)", "- Use different values of T for different tasks (see \u201cdiscussion\u201d in section 4)", "- Use improved training algorithms (e.g. PPO) for more challenging tasks, and slightly adjust reward generation schemes (see \u201cdiscussion\u201d in section 4, and Appendix A.1).", "We have also revised the submission to disclose more details on how we make these improvements.", "We will make all code and models trained in this paper available for reproducibility.", "[1] Neural optimizer search with reinforcement learning. ICML 2017.", "[2] DARTS: Differentiable Architecture Search. Arxiv 1806.09055."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_by-cr", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 461, "sentences": ["We thank Reviewer 4 for stating that \u201cthe proposed method has a good compression ratio while maintaining competitive accuracy\u201d.", "We provide clarification for the two main questions of the Reviewer below.", "Novelty of the paper", "As we state in our introduction, using codebooks to compress networks is not new, as well as using a weighted k-means technique.", "However, as we state in the paper: \u201cThe closest work we are aware of is the one by Choi et al. (2016), but the authors use a different objective (their weighted term is derived from second-order information) along with a different quantization technique (scalar quantization).", "Our method targets a better in-domain reconstruction, as depicted by Figure 1\u201d.", "Note that we already cite two of the suggested references by Reviewer 4, namely \u201cTowards the limit of network quantization\u201d and \u201cThiNet: A filter level pruning method for deep neural network compression\u201d in our work.", "We will further clarify our positioning in an updated version of the paper.", "Compression ratio", "We provide an example of the computation of compression ratio in Section 4.1, paragraph \u201cMetrics\u201d.", "Let us detail it further here.", "The memory footprint of a compressed layer is split between the indexing cost (one index per block indicating the centroid used to encode the block) and the cost of storing the centroids.", "Say we quantize a layer of size 128 \u00d7 128 \u00d7 3 \u00d7 3 with 256 centroids and a block size of 9.", "Then, each block of size 9 is indexed by an integer between 0 and 255: such integer can be stored using 8 bits or 1 byte (as 2^8 = 256).", "Thus, as we have 128 x 128 blocks, the indexing cost is 128 x 128 x 1 byte = 16,384 bytes = 16 kB.", "Finally, we have to store 256 centroids of dimension 9 in fp16, which represents 256 x 9 floats (fp16) = 256 x 9 x 2 = 4,608 bits = 4.5 kB.", "The size of the compressed model is the sum of the sizes of the compressed layers.", "Finally, we deduce the overall compression ratio which is the size of the compressed model divided by the size of the non-compressed model."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 462, "sentences": ["Thanks for your attention to our work.", "1) For the presentation, we apologize for our typos and unclear statement in the paper. And your advice is so helpful. We will modify it.", "2) For the experiment, we will train our experiments longer and modify our network.", "Thanks for your advice."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_social_label"]}
{"abstract_id": 463, "sentences": ["Thanks for your valuable comments. Below we address the detailed comments.", "Q1: Connection to the Jacobian matrix:", "A1: Thanks for the interesting suggestion.", "Indeed, the proposed regularizer can be interpreted as certain constraints on the Jacobian at the equilibrium point.", "Since at the equilibrium, $D(x)=0$ for all x, indicating that the equilibrium is a global optimal point of the negative feedback regularization $L = \\lambda \\int D^2(x)dx$. Therefore, the Hessian matrix $J = \\frac{\\partial^2 L}{\\partial \\phi^2}$ is positive-semidefinite.", "Otherwise, $\\phi$ is a stationary point instead of the global optimal point of the regularization term.", "Therefore, introducing the $L$ is equivalent to adding a negative-semidefinite matrix to the jacobian matrix of the original dynamics, which do help to stabilize the dynamics.", "We added the related discussion in Appendix E in the revision.", "Q2: The linear assumption on the dynamics:", "A2: Yes. The Laplacian transformation and the following discussions in Sec. 2.2 rely on linear dynamics, but it does not put any restriction on defining the discriminator as a nonlinear neural network.", "In Section 3, we can see that in the function space, the discriminator $D(x)$ and the generated samples $G(z)$ can be considered as integral parts which are also linear dynamics.", "The two non-linear operations are clearly denoted in Fig. 2(right), and we make an approximation to ignore these two non-linear operations which is widely adopted in control theory [*1].", "Q3: Eqn. (7):", "A3: Actually, Eqn. (7) comes from Eqn. (6).", "Letting $e = c - \\theta$, and taking Laplacian transformation on both side of Eqn. (6), we have $s\\mathcal{F}(D(t, x)) = x \\mathcal{F}(c-\\theta)$, which induces Eqn. (7).", "We made this clearer in the revision.", "Q4: The input and output of dynamics:", "A4: For a dynamic, both the input and the output are functions of time $t$. We take $D$ as an example.", "Since the dynamics of $D$ is equivalent to the integral part, the output of $D$ can be formulated as $D(t, x) = \\int_0^{t} g(u, x)du$, where $g$ is also a function of time t. In this setting, we say that the input of the dynamic is $g(t, x)$ and the output is $D(t, x)$, for all $x$. In Eqn. (10), we ignore the $x$ to emphasize that we modeling $D$ in the time-space.", "We added an example for a better presentation in Sec. 3.1 in the revision.", "Q5: The stability of previous methods:", "A5: Indeed, many existing methods can *generate realistic images*, which, however, does not necessarily imply that these methods *are stable*. For example, in Fig. 4 (left), the inception score of SGAN and LS-GAN is high at the beginning but finally diverges after a period.", "The early stopping in GAN's training is widely adopted otherwise the image quality will decrease, which indicates that these methods are not actually stable.", "Besides, NF-GAN can also boost the performance of SN-GAN to achieve new state-of-the-art performance (see details below).", "It indicates that improvement on the stability also benefits the state-of-the-art methods.", "Q6: Experiments:", "A6: For the experiments, we directly used the officially released code of Reg-GAN for fair comparison and it uses the ResNet instead of DCGAN architectures.", "We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to \"The Common concerns about experimental setting and results\".", "[*1] Khalil, Hassan K. \"Nonlinear systems.\" Upper Saddle River (2002)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_other_label"]}
{"abstract_id": 464, "sentences": ["[Q] My one ask would have been a survey of how activations might affect performance.", "I sense that everyone has settled upon LeakyReLUs for internal layers, but a survey of that work and experimentation within the authors' framework would have been nice.", "[A] We agree that this is an interesting question in it\u2019s own right and should and will be explored more rigorously in future work.", "At this point, it seems like the number of parameters and whether skip-connections are used is much more impactful.", "[Q] It would be interesting to see what these metrics would reveal when applied to other types of data (e.g. scientific images).", "[A] We are aware of several works in the area of scientific images, such as [1] and [2], where GANs were successfully applied on 2D image snapshots from N-body simulations.", "The main issue for us at this point is having access to such data sets.", "Nevertheless, as these data sets become available for public, we will happily include them within our framework and investigate whether the conclusions extend to data sets beyond natural images.", "[1] https://arxiv.org/abs/1702.00403", "[2] https://arxiv.org/abs/1801.09070", "[Q] I feel the discussion on loss was rushed, and I gained no insight on what the authors thought was a prominent difference between the three losses studied.", "[A] The theoretical differences between these losses were studied in detail in the corresponding publications.", "From the practical side, it\u2019s unclear which statistical divergence to optimize, in particular whether to pick (i) an f-divergence such as Chi-squared implemented by LS-GAN, or (ii) an integral probability metric such as Wasserstein distance, or (iii) a loss function which doesn\u2019t correspond to any statistical divergence, such as NS-GAN.", "Hence, we wanted to provide some insight on how do these perform within different setups, not necessarily the ones used in the original publications.", "To this end we uncover that on the considered data sets it's hard to outperform the non-saturating loss combined with regularization and normalization.", "Apart from this, the empirical evidence doesn\u2019t allow us to say more and we will clarify this in the manuscript.", "[Q] For architectures to be a main pillar of the paper, I feel that this area could have been explored in greater detail.", "[A] We agree with this assessment and we are indeed focusing on regularization and normalization.", "Our main question here was whether swapping Resnet with SNDCGAN leads to the same insights which is indeed the case.", "On the other hand, architectures are such a rich area enabling various design choices that they possibly merit a paper on their own.", "We will clarify the precise goal of the architecture exploration in this work.", "This being said, one major question we wanted to understand is which Resnet tricks from the literature (all 7 of them) are meaningful in practice and we present an ablation in the Section D of the appendix to conclude that the only relevant one is the number of channels which makes sense as it drastically changes the number of trainable parameters.", "[Q] The graphs were difficult to parse.", "I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.", "In addition, different charts presenting only one loss function, with their spectral normalization and gradient penalty variants, would have made the effects of the normalization more obvious on the FID distribution graphs.", "If this can be changed before publication, I would strongly suggest it.", "[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.", "Furthermore, in Figure 1, for the FID distribution plots, we can group the methods visually (according to the loss function) by drawing a slightly shaded rectangle around results with the same loss (e.g. https://goo.gl/6YeUL1).", "If you have a specific proposal we would be happy to consider it and update the submission.", "[Q] In the future, the authors should be careful to provide an anonymous repository for review purposes.", "[A] This is a good point and we will address this issue in the future.", "[Q] I would suggest changing the title to be more appropriate and accurate (the researchers are primarily focused on showing the positive and negative effects of normalization across various loss functions and architectures).", "[A] Given the architecture discussion stated above, this is a valid point.", "Our current candidate is:", "\u201cThe GAN Landscape: The effect of Regularization and Normalization across various Losses and Neural Architectures\u201d.", "However, if you have a specific proposal we would be happy to consider it."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_future", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_followup", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label"]}
{"abstract_id": 465, "sentences": ["Hello,", "Thank you for reviewing our paper.", "Regarding your comments about the triviality of our paper,", "We undertook an extensive initial phase of experiments where we discovered non-trivial contributions for achieving SOTA performance and fast convergence.", "In particular:", "- To the best of our knowledge, our specific implementation of entity pre-training is novel.", "Our technique can be ported to other neural, multi-task setups with a strong dependence between tasks (e.g. where a model must learn to perform some task before attempting to learn one or more dependent tasks jointly).", "For example, we have already begun porting this scheme to a neural cross-lingual summarization project of ours.", "Entity pre-training accounted for a 0.63% increase in our ablation experiment.", "On the CoNLL04 test set, it accounts for a 1.26% boost in performance, which is large relative to historic improvement on this corpus [1].", "- As opposed to previous models (e.g. [2], [3]) we were able to drop all recurrent architectures, which reduced training times substantially.", "Our work is one of the first architectures for joint NER and RE to do this.", "Because previous papers do not report their training times, we contacted the authors of a comparable method [4] for their training times and found that our method converged between 3-35X times faster for the ACE04, ADE, and CoNLL04 corpora (keeping in mind that we did not train on the same hardware).", "The large range in our estimate is because [4] trained for a wide range (60-200) of epochs.", "Our model serves as a strong baseline for future studies on joint NER and RE architectures and provides guidance on how to best integrate a pre-trained language model into such an architecture.", "For the ADE corpus in particular, we advance SOTA RE performance by >10%, which is substantially larger than improvements have been historically [5].", "It is also complementary to [6] (published in ACL this year), by demonstrating similar performance without the need for templated queries, which, as pointed out in our introduction, may become a limiting factor where domain expertise is required to craft such questions (e.g., for biomedical or clinical corpora).", "Regarding your comments on a System Demonstration submission to ACL,", "Our paper is a methodological advancement, not a system, tool, or demonstration [7] and is not suitable for submission to System Demonstrations at ACL.", "[1] https://paperswithcode.com/sota/relation-extraction-on-conll04", "[2] https://link.springer.com/chapter/10.1007/978-3-030-15712-8_47", "[3] https://arxiv.org/abs/1804.07847", "[4] https://www.sciencedirect.com/science/article/pii/S095741741830455X", "[5] https://paperswithcode.com/sota/relation-extraction-on-ade-corpus", "[6] https://arxiv.org/abs/1905.05529", "[7] https://aclweb.org/portal/content/acl-2020-call-system-demonstrations"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 466, "sentences": ["Thank you very much for the comment.", "We believe that the core novelty of our work is in introducing a type of regularization based on a functional gradient.", "Because we were able to conduct additional experiment in time, we also conducted feature matching as well and confirmed the superiority of DC-regularization over the method (Table 8)."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 467, "sentences": ["Thank you for the helpful comments and suggestions.", "Regarding the example, the goal of it is to help the reader understand", "the algorithm more easily.", "The wrong assignment with 80% probability", "is used to illustrate the situation where some predictions are wrong.", "\"if we try here $x_2 = 1$\" is based on the original formula, which is", "\"$x_1 \\lor x_2) \\land (\\lnot x_1 \\lor x_2) \\land ( x_1 \\lor \\lnot", "x_2)$\"", ".", "Therefore, the assigned formula is $x_1$.", "We tested PicoSAT, MiniSAT, Dimetheus and CaDiCaL and reported the", "results in the updated paper.", "CNNSAT outperformed all these solvers", "by", "at least two orders of magnitude over the \"Long Range\" dataset."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 468, "sentences": ["We thank a lot for the comments with cares and insights, and appreciate your efforts in reviewing our paper, which is helpful for improving the quality and readability of our writing. We are also glad that you support our paper.", "We agree that it is essential to justify how the reconstruction error works as a measure of privacy in this paper.", "In the revision, we have added the following justification on privacy quantification in Section 2, Section 4 and Section 5.", "We also note that the proposed reconstructive adversarial network (RAN), is not an extension of GAN but only borrows GAN\u2019s thoughts on adversarial training several neural networks, for the data privacy-uniquely problem.", "First, there is no single standard definition of data privacy-preserving problems and corresponding adversary attacks.", "And a fundamental problem in it is the natural tradeoff between privacy and utility, which is affected by different data privacy-preserving methods.", "Our key contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attackers and privacy quantification.", "Second, finding the right measurement for privacy is an open problem in itself.", "To evaluate RAN, one has to pick some quantifications.", "In the present paper, we chose the \u201creconstructive error\u201d as the quantification of privacy because it is the most intuitive one to measure the risk of disclosing sensitive background information in the raw data for the given perturbed data (Encoder output).", "Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a definitely defined application.", "For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN\u2019s Encoder output, and the sensitive patterns founded from the raw data, in the object recognition application."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label"]}
{"abstract_id": 469, "sentences": ["We thank the reviewer for their time and response to our paper.", "Phrase similarity results: the tensor component T(v_a,v_b,.) does yield improvement over all other weighted additive methods in 5 out of 6 cases, as shown in Table 3.", "We have also updated that table with additional results, which show that adding in the tensor component improves upon the strong baseline of the SIF embedding method.", "We also added Table 4, which repeats the phrase-similarity task for verb-object pairs, and shows that the tensor component leads to improvement in most cases."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 470, "sentences": ["We have fixed the footnote and capitalization problems.", "Below are replies to other comments.", ">> Comment #1", "We agree vanilla REINFORCE can exhibit high variance.", "However, as we have elaborated in the text below Eq.2, to reduce the variance and stabilize the training, we have made the following adaptations referring to previous works [1,2]:", "- Substitute a moving average B (defined in text) from the reward", "- Clip the final reward to a given range", "We empirically found the two techniques significantly stabilize the controller training.", "Moreover, AutoLoss is not restricted to REINFORCE, but open to any off-the-shelf policy optimization method, e.g. for large-scale tasks such as NMT, we introduce PPO to replace REINFORCE, and adjust the reward generation scheme accordingly (see the paragraph \u201cDiscussion\u201d).", "We\u2019ve also revised Appendix A.1 to cover details of how PPO is incorporated.", "Empirically, with random parameter initialization most experiments manage to converge and give fairly good controllers.", "Almost all main results are averaged over multiple runs as explicitly indicated in the main text and the table or figure captions (e.g. see captions of Table.1 and Fig.2).", "See Fig.2 and Fig.3(R) where vertical bars indicate variances.", "We have also updated Table.1 to show the variance.", "We will release all code and trained models for reproducibility.", ">> Comment #2", "We have provided substantial analysis and visualizations on what AutoLoss has learned in our *initial submission*. Below, we summarize them for your reference:", "- d-ary regression and MLP classification", "*See sec 5.1, the 3rd paragraph in P6 for analysis, and Table.1 for comparisons to handcrafted schedules*: we observe AutoLoss optimizes L1 whenever needed during the optimization.", "By contrast, linear combination objectives optimize both at each step while handcrafted schedules (e.g. S1-S3) optimize L1 strictly following the given schedule, ignoring the optimization status.", "We believe AutoLoss manages to detect the potential risk of overfitting using designed features, and combat it by optimizing L1 only when necessary.", "- GANs", "Per our observation, AutoLoss gives more flexible schedules than manually designed ones.", "It can determine when to optimize G or D by being aware of the current optimization status (e.g. how G and D are balanced) using its parametric controller.", "- NMT", "*See sec 5.1, the 3rd paragraph in P7 and Fig.3(M)*: we have explicitly visualized in Fig.3(M) the softmax output of a learned controller and explain in text: \u201c...the controller meta-learns to up-weight the target NMT objective at later phase\u2026resemble the \u201cfine-tuning the target task\u201d strategy...\u201d.", ">> Comment #3", "We experimented with S>1 and found the improvement marginal.", "However, a large S requires more task model training steps to perform one PG (or PPO) update, meaning longer overall wallclock time for the controller to converge.", "We hence use S=1 as it performs satisfactorily.", "Note that some recent meta-learning literature uses policy gradient with batchsize 1, and report strong empirical results [3].", ">>", "Comment #4", "We\u2019d like to clarify that we have *not* claimed that \u201cAutoLoss can resolve mode collapse in GANs\u201d.", "AutoLoss improves the performance of GANs by enabling an adaptive optimization schedule than a pre-fixed one.", "Our point is better and faster convergence of the model training.", "In the GAN experiments we *qualitatively* observed the generated images are of satisfying quality and exhibit no mode collapse.", "But we never claimed we aim to or can resolve mode collapse.", ">> Comment #5", "We respectfully disagree with this comment.", "The NMT experiments aim to verify that AutoLoss can guide the multi-task optimization toward faster and better convergence on the target task, i.e. our interest is to see how the optimization goes instead of how the MT performs.", "Held-out PPL is the direct indicator of the quality of convergence, while BLEU evaluates the MT performance.", "Hence we believe PPL suffices as a metric to evaluate the performance of AutoLoss.", ">> Comment #6", "We acknowledge that there may exist DCGAN implementations that achieve higher IS on CIFAR-10, but note the following facts:", "- The link verifies in a table that the best official IS (reported in literature) is 6.16 (the number we report).", "- The self-implemented DCGAN 1:1 baseline used in our paper (see Fig.4(c)) achieves an IS=6.7, higher than 6.16.", "- Still, AutoLoss-guided DCGAN achieves IS=7, higher than 6.16 reported in literature, our own implementation, and the result from your link.", "Thanks again for mentioning spectral norm.", "However, these techniques are *completely orthogonal* from the scope of this paper, where we focus on whether AutoLoss can improve the convergence instead of resolving mode collapse.", "[1] Device Placement Optimization with Reinforcement Learning. ICML\u201917", "[2] Neural Optimizer Search with Reinforcement Learning. ICML\u201917", "[3] Efficient Neural Architecture Search via Parameter Sharing. ICML\u201918"], "labels": ["rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 471, "sentences": ["Thanks so much for your constructive feedbacks. Please see our response below.", "1. Influence of the number of images:", "Yes. The reason might be the higher chance of noise.", "It would be very important to provide a group of images that share similar patterns or topics.", "However, too many images for a sentence would have greater chance of noise.", "2. Impact of paired sentence-image dataset:", "Yes. We add the external MS COCO image caption training set and evaluate on the EN-RO task for quick evaluation.", "The BLEU scores are 33.55 and 33.71 respectively for COCO only and Multi30K+COCO.", "In addition, we are also interested in the influence of the number of sentence-image pairs inspired by your suggestion.", "We randomly split the pairs of Multi30K into the proportion in [0.1, 0.3, 0.5, 0.7, 0.9], the corresponding BLEU scores are [33.07, 33.44, 34.01, 34.06, 33.80] respectively.", "These results indicate that a modest number of pairs would be beneficial.", "3. The extra computation:", "The extra computation is negligible.", "The time of obtaining image data for MT sentences for EN-RO dataset, for example, is approximately less than 1 minute by tensor operation in GPU.", "The lookup table is formed as the mapping of token (only topic words) index to image id.", "Then, the retrieval method is applied as the tensor indexing from the sentence token (only topic words) index to image ids, which is the same as the procedure of word embedding.", "The retrieved image ids are then sorted by frequency.", "Learning image representations takes only about 2 minutes for all the 29,000 images in Multi30K using 6G GPU memory for feature extraction and 8 threads of CPU for transforming images.", "The extracted features are formed as the \u201cimage embedding layer\u201d with the size of (29000, 2400) for quick accessing in neural network.", "4. Missing BLEU scores & the number of parameters:", "Because those missing numbers (N/A) are not reported in the corresponding literature."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label"]}
{"abstract_id": 472, "sentences": ["Thank you for your reviews and comments.", "We address your questions as follows.", "1. Scalability of the proposed solution", "From our current results, you may see that our approach has a decent scalability -- even though we doubled the subtasks and also introduced additional dependency in Crafting compared to Resource Collection, it does not need much more episodes for converging to optimal policies, where our agent-wise exploration plays an important role.", "Generally speaking, deploying more present workers coupled with our agent-wise exploration should significantly improve the learning efficiency and overcome the challenges introduced from more substasks or a larger dependency graph.", "In addition, the computational complexity is linear in terms of the number of agents, so our approach is also scalable when there are more agents.", "2. What is the reason for using rule-based agents in all the experiments?", "We have actually used RL agents as well (Appendix C.3), and it showed that our approach also works when workers are RL agents.", "In the main results, we focus on rule-based agents because it is computationally demanding to train a large population of RL agents, and our focus was not about the worker policies but rather how the manager assesses the workers\u2019 mental states and encourages an optimal collaboration accordingly.", "In this paper, using a cheap rule-based implementation with randomness has demonstrated the effect of different components of our approach.", "3. Are the authors willing to release the code?", "Yes, we do plan to open source our implementation.", "Specifically, the game environment and the worker agents were implemented in Python and it runs at a speed of more than 300 steps per second.", "We used PyTorch as the framework for implementing all the network modules.", "Typically it took < 10 hours to get a converged result by our approach on a single Nvidia Tesla V100 GPU.", "4. Typos", "Thanks for pointing out these typos. We will fix them in the next revision."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 473, "sentences": ["Thank you for your review.", "Please also see our high-level clarification above which we believe can help in better interpretation of our contribution.", "Some specific responses below:", "Reviewer #1 is absolutely right -- we don\u2019t know yet how to scale this to more difficult combinatorial problems.", "But let\u2019s clarify that statement a bit more:", "The ski-rental problem is often the first problem studied when teaching online algorithms, but it is certainly far from a \u201ctoy problem\u201d when we wish to learn an algorithm from scratch.", "We apologize if we painted an incorrect picture by calling it a \u201csimple example\u201d and a \u201cstaple introductory problem\u201d.", "It is easy to describe in that it has a single hidden parameter (the length of the ski season) and a single revealed parameter (the cost of buying).", "It is a staple introductory problem because it is elegant and illuminates the essential difficulties in designing online algorithms: there is a nearly-trivial factor-2 competitive algorithm (rent until you\u2019ve spent $B, then buy, so even if the ski season ends the next day, you\u2019ve not spent more than twice the least possible amount), but the 1-1/e competitive ratio algorithm is quite creative and subtle, and serves as an introduction to the richness of the field of online algorithms.", "In fact, the Karlin et al. (1986) paper also introduced the notion of competitive analysis of online algorithms, and  is probably the most-cited paper in this field.", "In some sense, this poses us the ideal challenge: can ML approaches discover creative and subtle \u201csolutions\u201d (in our case, an algorithm)?", "On a more technical note, please note that our \u201cmachinery\u201d of solving the two-player game is needed to discover an algorithm for the ski rental problem: if we don\u2019t allow the players to alternate and reach an equilibrium, for any fixed distribution on the ski rental instances (B, K), there is a deterministic algorithm that is optimal (among all online algorithms), and the worst-case performance of that (or any) deterministic algorithm is *provably* limited by a factor of 2 (i.e., there exists some distribution on instances where it will fail badly).", "Also refer to our discussion on this in the high-level clarification at the top.", "The AdWords problem considered is actually a difficult combinatorial problem, and is an archetypal online combinatorial optimization problem that captures the class of problems solvable by one of the most powerful techniques in this area -- primal-dual algorithms, which have led to the state-of-the-art approximation algorithms for numerous hard online (and offline) optimization problems.", "In particular, it generalizes bipartite matching, historically one of the most significant combinatorial optimization problems (led to the development of the classic Hungarian method, see https://en.wikipedia.org/wiki/Hungarian_algorithm).", "We did water down our ambition in a few ways:", "Instead of producing an algorithm that works for inputs of all sizes, we focus on the case of 9x3 (three advertisers, nine slots) -- a fixed finite size!", "This choice was arrived at based on the following criteria: what can we learn in a few hours of computation that\u2019s still *well beyond* what can be achieved through exhaustive search (for an algorithm).", "Think of our task roughly as learning to play a very hard game on a 9x3 board -- we would, of course, love to learn how to play the same game on arbitrary size boards, but the fact is that the game is mighty hard even at this \u201cboard size\u201d (since in each round, one player plays a 0-1 assignment to each cell in the board, and the other player picks a subset of the columns, in fact a weight vector on the columns).", "Instead of producing an algorithm that works for the 0-1 version of the problem, we produce an algorithm that works for the fractional version of the problem.", "This is, once again, motivated by making something work with modest amount of computation.", "Our explorations indicated that producing an algorithm for the 0-1 version needs reinforcement learning, and producing an algorithm that works on all 9x3 instances using this approach would still take several days of computation.", "On calling it Yao\u2019s Principle: as Reviewer #1 correctly noted, this is an application of the classic von Neumann minimax principle to the \u201cgame\u201d between an \u201calgorithm player\u201d and an \u201cinput player\u201d.", "We call it Yao\u2019s principle primarily in accordance with tradition in theoretical CS (see https://en.wikipedia.org/wiki/Yao%27s_principle and also https://blog.computationalcomplexity.org/2006/10/favorite-theorems-yao-principle.html, where it is noted that \u201cYao observed [the result]\u201d and commentators note that it\u2019s called Yao\u2019s principle because this observation has significant consequences for many central problems in TCS).", "We are happy to add text to reflect this."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 474, "sentences": ["Thank you for your comments.", "With regard to the structural choices of the attention model, our decision was based on a survey of attention-based methods used across various applications and their suitability for our problem setting.", "Our mechanism was designed such that, given a set of independent embeddings, each item in the set can be used to both extract a weighted sum of the other items as well as contribute to the weighted sums that other items extract.", "When applied to multi-agent value-function approximation, each item can belong to an agent and the separate weighted sums can be used to estimate each agent\u2019s expected return.", "Some other choices of attention mechanisms such as RNN-based ones (widely used in NLP), while interesting, do not naturally extend to our setting as our inputs (ie embeddings from agents) do not form a natural temporal order.", "We have updated our draft to provide more insight into our choices.", "We have included a new section 6.3 in the appendix of our revised draft that visualizes the behavior of our attention mechanism, as well as how it evolves over the course of training.", "While our approach does not significantly outperform the best individual baseline in each environment, it consistently performs near the top in all environments --- other methods falter in at least one of the two settings.", "Our experiments on Cooperative Treasure Collection demonstrate that the general structure of our attention model (even without considering dynamic attention as in our uniform attention baseline) is able to handle large observation spaces (and relatively larger numbers of agents) better than existing approaches which concatenate observations and actions from all agents together.", "Furthermore, our experiments on Rover-Tower demonstrate that the general model structure alone is not sufficient in all tasks, specifically those with separately coupled rewards for groups of agents, and dynamic attention becomes necessary."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 475, "sentences": ["Thank you for the encouraging comments.", "First of all, we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods, and observe that they are also vulnerable to blind-spot attacks.", "Please see our reply to all reviewers.", "We agree that the K-L based method is complicated and computationally extensive.", "Fortunately, we only need to compute it once per dataset.", "To the best of our knowledge, currently, there is no perfect metric to measure the distance between a training set and a test set.", "Ordinary statistical methods (like kernel two-sample tests) do not work well due to the high dimensionality and the complex nature of image data.", "So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.", "As suggested by the reviewer, we added a new metric based on the mean of \\ell_2 distance on the histogram in Section 4.3.", "The results are shown in Table 1 (under column \u201cAvg. normalized l2 Distance\u201d).", "The results align well with our conclusion: the dataset with significant better attack success rates has noticeably larger distance.", "It further supports the conclusion of our paper and indicates that our conclusion is distance metric agnostic.", "We hope that we have made everything clear, and we again appreciate your comments.", "Let us know if you have any additional questions.", "Thank you!", "Paper 1584 Authors"], "labels": ["rebuttal_accept-praise", "rebuttal_done", "rebuttal_other", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_social_label"]}
{"abstract_id": 476, "sentences": ["Thanks for your effort in providing this detailed and useful review!", "We present our clarification in the following:", "Q1: the feasibility of using neural networks to learn cumulative quantities:", "A: In each iteration, only a small subset of information sets are sampled, which may lead to the neural networks forgetting values for those unobserved information sets.", "To avoid such catastrophic forgetting, we used the neural network parameters from previous iterations as initialization, which gives an online learning/adaptation to the update.", "Furthermore, due to the generalization ability of the neural networks, even samples from a small number of information sets are used to update the new neural networks, we find that the newly updated neural networks can produce very good value for the cumulative regret and the strategy mixture.", "(we give related discussion in section 3.1 and add much more experimental results in Figure 5, further details please see the revised paper.)", "Q2: It does not seem necessary to predict cumulative mixture policies (ASN network)?", "A: As you say, any information nodes I_i would be sampled proportionally to \\pi^{\\sigma^t}_i(I_i), which is the same probability as in the definition of the mixture policy (Eq.4).", "Actually, if we have a large enough buffer to save all the sampled nodes, it\u2019s easy to inference the mixture policy accordingly.", "However, in the large game, this large memory is expensive and impossible.", "Another method called reservoir sampling was used in NSFP to address a similar problem.", "We borrow this idea to our method, however, the achieved mixture policy cannot converge to a low exploitability.", "Actually, the third possible solution could employ the checkpoint of each current strategy, and mixture this current strategy accordingly.", "Q3: It would help to have a discussion about how to implement (7), for example do you use a target network to keep the target value R_t+r_t fixed for several steps?", "A: The optimization problem for the double neural networks is different from that in DQN, where the target network is fixed for several steps and only one step of gradient descent is performed.", "In our setting, both RSN and ASN perform several steps of gradient descent with stochastic mini-batch samples.", "Furthermore, in DQN, the Q-value for the greedy action is used in the update, while in our setting, we do not use greedy actions.", "Algorithm E gives further details on how to optimize the objectives in Equation 7 and Equation 8 (Further discussion please see the revised paper.)", "Q4: It is not clear how the initialisation (10) is implemented.", "Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?", "A: Generally, Eq.10 is an idea of behavior cloning algorithm.", "Clone a good initialization, and then continuously update the two neural networks using our method.", "In the large extensive game, the initial strategy is obtained from an abstracted game which has a manageable number of information sets.", "The abstracted game is generated by domain knowledge, such as clustering similar hand strength cards into the  same buckets.", "(refer to section 3.3 in the revised paper.)"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label"]}
{"abstract_id": 477, "sentences": ["Thanks for your effort in providing this detailed and useful review!", "We present our clarification in the following:", "Q: I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed.", "A: To address this problem, we add three different kinds of experiments.", "Use small batch size, only a small subset of infosets are sampled in each iteration.", "In this case, we can present the generalization ability of the neural network.", "Use small embedding size and let the number of parameters is much fewer than the number of infosets of the whole game tree.", "In this case, we can present the compression ability of the neural network.", "Use the larger stack to increase the size of the game tree.", "In all these three kinds of experiments, we find the neural CFR can still converge to a good strategy.", "Further details please see Figure 5(A), 5(B) and 5(C) in the revised paper.", "We fixed the typos in the revised paper accordingly."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 478, "sentences": ["Thank you for your reviews and comments.", "We respond to your questions as follows.", "1. Scalability?", "While we agree that the tasks in this paper are not real world problems, we think, as a first step towards this direction, the evaluations in this paper have provided some promising proof-of-concept results. Applying the approach to more realistic and more complex tasks could be a good future research direction.", "2. It would be beneficial to provide more information about the baselines", "We have added details of baselines including their reward functions in Appendix E.", "3. For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs", "We show the standard deviation of multiple runs in Figure 7,8,9 in the revision.", "We have done our best to evaluate the robustness given the limited time and will continue to improve the evaluation.", "4. The strong dependence of performance on the dimensionality of the latent, internal state (Fig.4).", "The network architecture design is not the focus of our paper.", "Generally speaking, a higher dimensionality of the latent vector provides a more powerful network to model agents.", "However, as we show in Figure 4, with probing, the network with lower dimensionality can even outperform the baselines trained with latent vectors that have higher dimensions.", "And with the same architecture, probing clearly provides a significant improvement.", "5. Minor issues.", "Thanks for pointing out these issues. We have fixed them in the revision."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 479, "sentences": ["The authors appreciate the reviewer\u2019s time and efforts for reviewing this paper and would like to respond to the questions in the following paragraphs.", "[Comment]", "Compare FICM against simpler exploration baselines such as epsilon-greedy or entropy regularization.", "[Response]", "We would like to thank the reviewer for raising this interesting question, and would like to bring to the reviewer's kind attention that in the original paper of our baseline \"ICM\" [1], the authors had provided a comparison against an \u2018A3C\u2019 baseline (using entropy regularization) with epsilon-greedy exploration method (Section 3 of [1]).", "According to the experimental results presented in Section 4 of [1], it has been demonstrated that ICM is superior to that baseline in a number of environments.", "This is the reason why we omit that baseline in our paper.", "As our primary interest and focus is prediction-based exploration methods using intrinsic reward signals (as discussed in Section 1 of our paper), we only compare our FICM with ICM [1], RND [2] and large-scale [3], concentrating on analyzing the pros and cons between our proposed method and the other prediction-based ones.", "However, we would still be glad to include additional comparisons against the suggested methods in the final version of our paper, if the reviewer considers that is informative for the readers to comprehend the paper.", "[Comment]", "More extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.", "[Response]", "We appreciate the suggestions from the reviewer and would like to share with the reviewer our additional experimental results of ICM using the same hyper-parameter settings described in Section 4.1 in the following figure.", "(figure link: https://imgur.com/5pPl8PV )", "It is observed that ICM is only able to deliver comparable performance to our method in Atari game \"Seaquest\". We would definitely be glad to incorporate these new results in our manuscript in the revised version.", "[Comment]", "Reproducibility.", "[Response]", "Thank you very much for the suggestions.", "We have already uploaded our source codes as well as the demonstration videos to the following sites.", "Our experimental results and statements presented in the manuscript are fully reproducible and verifiable.", "Github: https://github.com/IclrPaperID2276/iclr_paper_2276", "Demo Video: https://youtu.be/JL68QFNj_N8", "We hope that we have adequately responded to your questions, and would be very glad to discuss with you if you have any further comments or suggestions.", "[1] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. In Proc. Int. Conf. Machine Learning (ICML), pp. 2778\u20132787, May 2017.", "[2] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. In Proc. Int. Conf. Learning Representations (ICLR), May 2019b.", "[3] Y. Burda, H. Edwards, D. Pathak, A. J. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity-driven learning. In Proc. Int. Conf. Learning Representation (ICLR), May 2019a."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 480, "sentences": ["Thanks for the detailed and thoughtful review.", "We are glad that you think of this paper as a timely contribution addressing an important problem that must be addressed in order to build more robust NLP systems.", "We agree with your point that it would be great to have a practical takeaway guiding practitioners for what to do in practice.", "We believe that the first step here is to characterize the problem coherently and that having laid this groundwork, one immediate next step is, as you suggest, to develop a more practical solution that requires a less expensive/onerous annotation effort.", "The key contribution of our paper is to provide a clear characterization of a variety of concerns in the language of interventions and to demonstrate that indeed, they can be addressed by acquiring interventional data.", "The knowledge that (i) NLP models trained on counterfactually augmented data suffer less from these problems and (ii) transport better out of sample (see new results in the updated draft, per R3\u2019s suggestion) validates this.", "As you mentioned, our solution requires significant expenditure (both financial and human capital)", "compared to simply labeling data", ".", "As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.", "In preliminary work, we have been investigating how to use humans in the loop more effectively.", "One approach involves using generative models to propose candidate substitutions and relying on humans only accept or reject the revisions (vs having to write them from scratch).", "Our experience with crowdsourcing suggests that this feedback would be significantly cheaper to collect (provided that a reasonable fraction of suggestions were appropriate).", "We additionally note that for some tasks, such as NLI, creating new datasets already requires annotators to synthesize examples de novo and the fractional increase for soliciting counterfactually-augmented data might not be as onerous as compared to tasks where the default is to rely on annotators only for tags.", "We are also appreciative of your constructive suggestions to improve the paper, and have taken several steps to improve the draft.", "These include updating the draft to include (i) a detailed analysis of edits performed on SNLI, (ii) results on various datasets using an ELMo based classifier; (iii) concerning your question about larger Bi-LSTMs, we had tried a large Bi-LSTM but it overfit badly.", "We have updated the draft to include this detail.", "Thanks also for catching several typographic errors. We have addressed them in the new draft."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_reject-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 481, "sentences": ["I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.", "R)We believe as future work our algorithm can be combined with Winograd techniques for optimization.", "For instance winograd is designed to use a batch of images to convolve with a kernel, here an image convolves with a \u201cbatch of kernels\u201d.", "There is no reason why those two techniques can be merged.", "In our implementation we perform a set of convolutions with the input image where FFT can be applied too.", "p2-3, Section 3.1 - I found the equations impossible to read. What", "are the subscripts over?", "In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??", "Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?", "Very good Catch", "it should be (N)x(N) instead of (N+1)x(N+1). (Fixed on the paper)", "Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?", "{k,l} locate the convolving window inside of the input image", "Experimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost.", "It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.", "R)We compared now against: mobileNet, ShuffleNet, HENet, SqueezeNet, we have less number of parameters or better accuracy or both.", "For instance our method has 4X les parameters than shuffleNet and better accuracy (Added to the paper)", "It would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions.", "R)In this paper we focus on the reduction of parameters, we didn\u2019t focus on the speed, we notice that in our experiment our models were trained using half of the epoch used for the conventional models.", "In terms of the number of operations the LeNet as in the tutorial has 2.29M MAC operations, while our method has 1.23M MAC operations for MNIST.", "(Added to the paper)"], "labels": ["rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_future_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 482, "sentences": ["Thank you for your comments!", "* We have enhanced Appendix E with a discussion on some variants of the VAE that generalize the standard evidence lower bound (ELBO) by incorporating different bottleneck constraints to learn better representations.", "In particular, we discuss how our unsupervised objective (p. 18, equation 38) relates to the beta-VAE and the importance weighted autoencoder (Appendix E.1, p. 17,18).", "* Please note that our unsupervised objective (p. 18, equation 38) contains the beta-VAE as a special case when we use only one sample from the encoding distribution (M=1).", "This means that we are naturally comparing with that method.", "* Appendix E.2 (p. 18) now discusses implementation details of the unsupervised objective.", "Finally, we have included some visualizations in Appendix E.3 for the MNIST and FMNIST datasets for different values of M and beta.", "We agree that more comparisons will be beneficial in investigating the properties of the proposed method.", "This is something we are actively working on."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_concede-criticism_label"]}
{"abstract_id": 483, "sentences": ["1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?", "R)It is possible to change all the layers to use Adaptive convolutions, we replaced only one to measure the unitary contribution.", "We chose the first one because it is where the feature extraction is performed, in addition fully connected layers can use this technique.", "One Adaptive layer can replace two traditional layers.", "(Added to the paper)", "2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?", "R) We can use any Activation function.", "We actually tested ReLu with good results, but we chose tanh because it generates weights in the range of (-1,1) avoiding large values given by ReLu.", "(Added to the paper)", "3. Traditional convolutional kernels together with max pooling operations ensures some degree of translation invariance.", "How big is the generalization gap for the tested models when adaptive kernel is used?", "R)we added an experiment to test the generalization", "4. How sensitive are the results to the number of adaptive kernels in the layers.", "R) As in traditional CNNs, the increment of the number of kernels in a layer produces some saturation, with a marginal increment of accuracy.", "In our experiments it was observed that 5 dynamic kernels generates comparable level of abstraction than 30 traditional convolutional kernels.", "(Added to the paper)", "5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?", "yes we have test several layers with adaptive kernels. but we focus on report the results on the first layer to highlight the contribution", "6. On CIFAR10 the results seem to be worse that other methods.", "However, it is important to note that the Adaptive Kernels CNN has way less parameters.", "It would be interesting to see how the performance of adaptive kernels based CNNs scales with the number of parameters.", "R)We added a new experiment where we show how the performance of adaptive kernels improves with the increment of parameters.", "In order to make a fairer comparison we also added another experiment where we compare the accuracy of ResNet18 with 1 adaptive layer against ResNet18, ResNet50 and ResNet101 and it can be seen that the adaptive one performs even better than ResNet50.", "7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.", "R) Added another experiment, where we use DroNet as base to show the benefit of combine Adaptive layers with ResNet, in this experiment we test different configurations to compress the network up to 32X  (Added to the paper)", "8. The authors acknowledge the similarities (and some differences) with Brabandere et al (2016).", "It might be beneficial to include comparison to this approach in the experimental section.", "Moreover, given the similarities, it might be good to discuss the differences in the approaches in the introduction section.", "R)they train a NN to generate a model of another NN, we  have a ACNN that learns how to generate its filters.", "(in the intro)", "Some typos:", "1. the difficult to train the network", "2. table 2: Dynamic -> Adaptive?", "Very Good Catch", "\uf04a", "Overall, the paper presents interesting ideas with some degree of originality.", "I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.", "We compared now against: mobileNet, ShuffleNet, HENet, SqueezeNet, we have less number of parameters or better accuracy or both (Added to the paper)"], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_none", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 484, "sentences": ["Hello,", "Thank you for your review of our paper.", "We appreciate the positive assessment of the clarity of our writing.", "Regarding the suggested ablations,", "1. For reasons outlined in our response to the public comment you reference, we do not believe this ablation (as suggested) would be meaningful.", "For convenience, we have copied that response here: In our model, BERT is more than a source of contextual word embeddings as we fine-tune all of its ~110M parameters during training.", "Simply replacing BERT with distributed embeddings and a character-CNN or LSTM wouldn\u2019t allow us to determine the effect of contextualized embeddings because we would simultaneously be removing the majority of our model\u2019s trainable parameters.", "Nevertheless, we performed the suggested ablation by swapping BERT for GloVe embeddings (300 dimensional) and found that NER performance dropped from 89.46% to 40.33% and RE performance fell from 66.83% to 14.44% on the test set of the ConLL04 corpus (note that we had to increase the learning rate by 10X to get the model to converge).", "If you were to somehow control for this drop in model capacity, say by adding in an LSTM network, the ablated model would closely match this paper [1], whom we outperform by ~3% overall on the CoNLL04 corpus.", "This paper is not cited in Table 1 as they report macro-averaged F1 scores, while most other papers (including the current state-of-the-art [2]) report micro-averaged F1 scores, as we did.", "Finally, it is well known that contextual embeddings outperform distributed embeddings on a wide range of NLP tasks, including NER [3].", "The aim of our study wasn\u2019t to compare contextual vs. distributed embeddings but on how to successfully integrate BERT into a state-of-the-art joint NER and RE architecture.", "2. Thank you for this suggestion.", "We are currently performing the ablation, and will comment again once we have the results.", "We will be performing the same ablation as used in [2] (see section 6.2).", "Just note, because our manuscript is already at the page limit, we may have to place the results of this ablation in the appendix.", "Regarding predicted entity label embeddings,", "Before training, all unique entity labels (e.g. B-PER, I-PER, ... etc.) are embedded by assigning them to randomly initialized, continuous vectors of 128 dimensions (this hyperparam is mentioned in Table A.2 of the appendix).", "The embeddings are then updated along with the rest of the models' parameters during training.", "Practically speaking, this is handled for us via the embedding layer in PyTorch [4].", "This is the same method used in the works we compare to ([1], [5], [6]).", "We have updated the text in the manuscript (under section 2) to make this more clear.", "Thank you again for taking the time to review our paper.", "[1] https://link.springer.com/chapter/10.1007/978-3-030-15712-8_47", "[2] https://arxiv.org/abs/1905.05529", "[3] https://arxiv.org/abs/1802.05365", "[4] https://pytorch.org/docs/stable/nn.html#embedding", "[5] https://www.aclweb.org/anthology/P16-1105/", "[6] https://www.sciencedirect.com/science/article/pii/S095741741830455X"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_by-cr_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label", "rebuttal_other_label"]}
{"abstract_id": 485, "sentences": ["Thanks for your constructive feedback.", "We have modified the paper to clarify some of the terms per your suggestion.", "Please find our detailed response below:", "[R1: In Table 1, for ImageNet, Shadow Attack does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?]", "During attack/crafting, we need to make an adversarial example that gets misclassified even after perturbations drawn from a Gaussian distribution centered at zero with scale sigma.", "During evaluation, while the augmentations are drawn from a similar distribution, the realized random variables are not identical to those used for crafting the adversarial perturbation.", "In ImageNet, where the dimensionality is high (224X224X3) and for larger sigmas, to have a relatively dense and representative sampling, we need to sample a lot more perturbations during adversarial example crafting.", "However, in our experiments, we could only sample up to 400 instances per example (the maximum batch-size that could fit on our machine with 4 GPUs is 400).", "This results in having a sparse sample when the standard deviation is higher.", "One can potentially improve these results by using larger batch-sizes (i.e., sampling more) or a more powerful GPU or even a TPU, however we do not have the resources for such experiments at this time.", "[R1: In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.", "In my opinion, to support the above claim, shouldn\u2019t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?]", "In the original submission, we tried to produce tables that look like the tables in papers that we compare", "to", ".", "The randomized smoothing paper reports certified radii and also accuracy (1-error) under various perturbation bounds.", "However, the CROWN-IBP paper and the improved randomized smoothing paper based on adversarial training of smoothed classifiers (SmoothAdv) only report *error rates* using a fixed distance to the decision boundary.", "This is done because, unlike the Randomized Smoothing method, the radii are not directly calculated in the CROWN-IBP method and cannot be accessed directly;  CROWN-IBP takes a fixed radius chosen by the user, and either produces or fails to produce a certificate for that radius.", "This is in contrast to randomized smoothing, which outputs different radii for different images (a larger radius means a stronger certificate).", "In regards to why we compare the errors on natural images and those of our adversarial images:", "Please see the (updated) last paragraph of Section 5, which explains this comparison in detail.", "In short -  we are comparing the rate at which natural images certify to the rate at which adversarial images certify.", "For the case of large perturbations, we find that our adversarial image produce certificates more often than natural images!", "For small perturbations, our attack still produces certificates reasonably often, although not quite as frequently as natural images.", "This shows that certificates alone cannot be used to reliably discern between natural images, and adversarial images produced by the proposed shadow attack.", "[R1: From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?]", "This happens because, for CIFAR-10, the smoothed classifier is very \u201cconfident\u201d on a subset of the validation images which correspond to that right peak.", "Here, our use of \u201cconfidence\u201d should not be confused with the confidence of a network (output of the softmax layer).", "For the purpose of the certified radii, the \u201cconfidence\u201d we are interested in is related to the prediction of the network on the Gaussian perturbed images (i.e., a very high \u201cconfident\u201d example is an example where all of the perturbed images get the same label).", "[R1: Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.", "A smaller dissimilarity suggests a greater similarity between channels.]", "Good point! We have updated this in the revised document, and we think it enhanced clarity.", "[R1: Lambda sim and lambda s are used interchangeably. Please make it consistent. ]", "Fixed. Thank you.", "[R1: The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.]", "In the revision, we have described what the numbers are representing in more detail."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 486, "sentences": ["Pipelined backpropagation is similar to model parallelism but it addresses the resource underutilization issue in model parallelism.", "Our pipelined method might look like async-SGD on surface.", "However, async-SGD (e.g. Dean et al., pointed out by Reviewer 4) utilizes data parallelism (as indicated in Dean el al.) and a parameter server to keep track of model parameters (weights).", "In contrast, our pipelined method does not use any parameter server.", "Furthermore, each accelerator obtains a replica of a full model in asycn-SGD training while each accelerator contains only a part of the model in our pipelined method, on the assumption that the full model does not fit into the memory of a single accelerator.", "The accuracy drops for some models in a pure pipelined training.", "However, hybrid training is able to bring the accuracy of most networks studied in our paper up to a comparable level of the non-pipelined baseline as shown in the evaluation section of our paper.", "Our pipelined method is different from data parallelism in the following way (for a 2-GPU example).", "For data parallelism, a model is duplicated and placed onto 2 GPUs, each GPU containing a full copy of the model.", "On the other hand, for pipelined parallelism, a model is divided into two partitions (on the assumption that it cannot fit in a single device): one is mapped onto GPU 0 while the other is mapped onto GPU 1, each GPU obtaining only a part of the model.", "Communication between these two partitions is necessary to enable activation and gradient transfers.", "Regardless of the parallelization techniques, the maximum speedup of a 2-GPU system is 2X compared to a 1-GPU system.", "To obtain a close to perfect speedup of 2X, the communication overhead must be almost non-existent and the workload needs to be perfectly balanced between the 2 GPUs.", "In our implementation, we obtained a speedup of 1.81X for ResNet-362, which is equivalent to 90% utilization of each GPU.", "Thus, our sentence the reviewer refers to.", "Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).", "Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.", "It is the trend of the decline in inference accuracy with pipelining is what we study.", "This trend exists with both our hyperparameters and those at, for example, https://github.com/akamaster/pytorch_resnet_cifar10.", "The use of these set of hyperparameters, obtains an inference accuracy of 91.65% (better than the accuracy stated in the original ResNet paper) for ResNet-20 non-pipelined baseline and 91.21% for pipelined version.", "We are not aware of any reports of an accuracy of ResNet-20 at 92% (perhaps this is approximate).", "Please kindly let us know a pointer.", "It is relatively easy to update our results in the paper with new hyperparameters."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 487, "sentences": ["Thank you for the time and effort spent reviewing our paper, and for the detailed suggestions.", "Below we repeat the questions/comments from the review and respond to each in turn.", "\u201cThe paper positions itself generally as dealing with arbitrary transformations T, but really is about angular transformations (e.g. Definition 3.1).", "The generalization is relatively straightforward and was not too surprising given the APG theory.", "The paper would gain in clarity if its scope was narrowed.\u201d", "Our MPG framework not only supports the angular transformation but also covers the recently proposed clipped transformation in CAPG [Fujita and Maeda, 2018].", "The theoretical result is tighter than the one in [Fujita and Maeda, 2018], and it supports general transformations instead of only clipped actions.", "\"I am not convinced that the measure theoretic perspective is always necessary to convey the insights, although I appreciate the desire for technical correctness.\" / \"Generally speaking it seems like a lot of technicalities for a relatively simple result: marginalizing a distribution onto a lower-dimensional surface.\"", "We agree that the measure theoretic approach is not always necessary (indeed for angular actions, it is not needed), but it is necessary for a very common scenario -- clipped actions.", "Researchers and practitioners both almost always clip actions when using policy gradient algorithms for robotics control environments (read: MuJoCo tasks).", "Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces.", "Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten the existing analysis of that algorithm.", "\"It's hard for me to judge of the experimental results of section 5.3, given that there are no other benchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.\"", "For the results in Section 5.3, the issue is that currently, there are no benchmark environments for directional control.", "We anticipate that in the future this may change (e.g. console and PC games often have directional controls).", "\u201cWhat does E_{pi|s} refer to in Eqn 4.1?\u201d", "The expectation is taken with respect to the policy \\pi conditioned on the current state s (s here is arbitrary, but fixed).", "Stated differently, we are taking the expectation with respect to the distribution $\\pi(\\cdot | s,\\theta)$.", "\u201cCan you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)\u201d", "We have now removed this part of the statement because we are no longer absolutely certain of its correctness, and because it is not used anywhere else in the paper.", "\u201cExperiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian on the angle?\u201d", "Because using a 1D Gaussian requires either (1) clipping the angle to [0,2\\pi) before execution in the environment and making updates using the clipped output or (2) using the sampled angle for updates and perform the clipping in the environment.", "In the first case, this approach is asymmetric in that does not place similar probability on $\\mu_{\\theta}(s) - \\epsilon$ and $\\mu_{\\theta}(s) + \\epsilon$ for $\\mu_{\\theta}(s)$ near to $0$ and $2\\pi$. In the second case, this requires approximating a periodic function.", "We include both these reasons at the start of Section 3.", "Lastly, thank you for the concrete suggestions:", "\"Def 4.4: \"a notion of Fisher information\" -- maybe \"variant\" is better than \"notion\", which implies there are different kinds of Fisher information", "Def 3.1 mu is overloaded: parameter or measure?", "4.4, law of total variation -- define \"", "We have addressed these and uploaded a new draft to reflect the changes.", "For the last suggestion, we currently define the law of total variance(variation) in the preliminaries so we did not repeat the definition in Section 4.4.", "We now write \"law of total variance\" instead of \"law of total variation\" to avoid any ambiguity."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_reject-request", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 488, "sentences": ["Thank you for the comments and suggestions.", "We agree that characterizing the generalization properties of neural network under different scalings is an important future direction.", "We have updated the manuscript with a few minor modifications: 1) Figure on the population risk of sigmoid network (first layer optimized) in addition to SoftPlus; 2) additional remarks on the population risk of network in the kernel regime in Section 5.2; 3) corrected typos."], "labels": ["rebuttal_social", "rebuttal_future", "rebuttal_done"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_concede-criticism_label", "rebuttal_done_label"]}
{"abstract_id": 489, "sentences": ["Thank you for your review, below we answer the points that were questioned.", "* Missing implementation steps and optimization details:", "In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.", "Moreover, we will ultimately release codes on Github.", "* Non-matched experiment to practice environment:", "The evaluation of generative models and unsupervised domain translations remains an open question, even less covered in the field of sound.", "We didn't apply our models yet to datasets previously covered in the related works, such as Nsynth, which is planned and would give some more direct comparisons.", "* How to avoid the negative knowledge transfer:", "As we defined our purpose, the resulting generation is a blending of both domains that renders a target timbre while retaining some of the input features.", "It amounts to note class (that is explicitly controlled for the note-conditional model states) together with timbre.", "We plan on experiments on controlling the amount of timbre transfer in between the input and target domains."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 490, "sentences": ["Thank you for your detailed reviews and constructive suggestions.", "We have added the suggested baselines in the revision.", "Here are our responses to your questions and comments:", "1. Equation 1 typo?", "It is not a typo.", "Our reward function is different from existing curiosity reward.", "We are using the change of the real time m^t and m^{t-1} as the reward for inciting behavioral change from the demonstrator.", "We have shown more analysis and visualization to explain why this works in the new revision (Appendix B.2 & B.3).", "Our \u201cself-supervised\u201d baseline is actually using the prediction loss as reward, and it has a worse performance compared to ours.", "2. Baseline missing: Random actions from expert", "Figure 5 shows the results where 10% actions from the demonstrator are purely random.", "With the randomness, our approach is still be able to find meaningful probing policy.", "We have also evaluated the success rate when we use the policy learned from the suboptimal demonstration (10% random actions).", "As reported in the updated Table 1, this policy is comparable to the one learned from optimal demonstrations, and it still outperforms baselines which are all trained from optimal demonstrations.", "3. Baseline missing: Simple RNN policies that communicate hidden states", "We have evaluated this baseline in the revision (i.e., the \u201c2-LSTM\u201d baseline).", "The network architecture is illustrated in Figure 16.", "It indeed performs much worse than our full model.", "4. Ablation study for the importance of fusion", "We have added the result of this baseline (i.e., the \u201cours w/o fusion\u201d baseline), where we concatenate the state feature and the latent vector m^t together.", "The results have validated the importance of using the attention-based fusion layer.", "5. Generalizability argument", "Our main idea is to show as many configurations as possible to the learner by learning a good probing policy.", "Since the probing always starts from a single setting, there is indeed a limit in terms of how different the new settings could be.", "E.g., in Maze Navigation, it is impossible for the learner to change the room layout drastically in the time limit, so the learned policy won\u2019t make sense in a very different room layout (e.g., 8 rooms instead of 4 rooms).", "To obtain a better generalization, we may need to use a better imitation learning approach to replace the current one (behavioral cloning), and possibly using multiple starting configurations.", "But we think that it is somewhat orthogonal to our main contribution.", "The objective of our approach is to discover more diverse settings/configurations and consequently improve whatever imitation learning approach we actually use."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 491, "sentences": ["Thank you for the detailed comments.", "1. We did not do experiments with such generator architecture.", "Although we have considered other architectural choices for generator and ways of conditioning, our early experiments showed that our residual-upsampling scheme is more efficient than parallel wavenet\u2019s full-resolution scheme.", "The correspondence between temporal dimensions of the conditioning and the waveform also seemed important and hence we decided to keep the proposed generator architecture throughout.", "2. Indeed we believe that the use of the ensemble of random window discriminators was the main factor behind the performance we obtained.", "This, however, breaks down to three steps:", "(a) switching from full discriminator to random-window discriminator(s),", "(b) including unconditional random window discriminator(s),", "(c) including several different window sizes in the ensemble.", "As can be seen in Table 1.", ", (a) already brings a huge improvement (from ~1.9 to ~3.4 MOS).", "(b) and (c) also seem to be important; we have considered fixing the window size or using only conditional RWDs, but all of such trials turned out considerably worse.", "Only models combining all of (a) - (c) made it past MOS of 4.1.", "3. Indeed D^c_k and D^u_k should have been clearly defined there; we clarified this notation in the updated version of the submission.", "4. For the training stability, please see our joint response.", "As for the role of the batch size, we fixed it throughout all experiments, but we will include analysis of model stability with smaller batch sizes in the final version of the paper.", "5. Thank you for pointing out this related work. We refer to it in the updated version of the submission."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_other", "rebuttal_by-cr", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_done_label"]}
{"abstract_id": 492, "sentences": ["We thank a lot for the comments with cares and insights, which are helpful for improving the quality and readability of our writing.", "We have addressed all the comments as follows:", "Response #1: In the revision, we have added the following justification and explanations on privacy quantification in Section 2, 4 and 5.", "First, there is no single standard definition of data privacy-preserving problems and corresponding adversary attacks.", "And a fundamental problem in it is the natural tradeoff between privacy and utility, which is affected by different data privacy-preserving methods.", "Our key contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attackers and privacy quantification.", "Second, finding the right measurement for privacy is an open problem in itself.", "To evaluate RAN, one has to pick some quantifications.", "In the present paper, we chose the \u201creconstructive error\u201d as the quantification of privacy because it is the most intuitive one to measure the risk of disclosing sensitive background information in the raw data for the given perturbed data (Encoder output).", "Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a definitely defined application.", "For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN\u2019s Encoder output, and the sensitive patterns founded from the raw data, in the object recognition application.", "Response #2: Great help.", "In the revision, we have added more experiments (with more \\lambda settings in RAN) to plot the full Pareto Front of three baselines and RAN, and revised the explanations in Section 3.1.", "And we also noted that the parameter \\lambda can be fine-tuned, e.g., exponentially varied, to read a better tradeoff.", "Response #3: Thanks for pointing out the problems in Eq. 1.", "The utility is evaluated as the accuracy of a Classifier, i.e., the probability Yi=Yi\u2019, which is a commonly used metric .", "And we adopted some randomness, e.g., dropout, in the parametric discriminative models (Encoder and Classifier).", "Response #4: We have added more clarifications on the privacy definition in Section 2.1.", "In particular, the privacy of Max Min |Ii-Ii\u2019|^2 is defined for each data rather than a dataset, which is different from any anonymization based data privacy-preserving techniques.", "Response #5: Although we can plug in any adversary architecture (Decoder) and privacy quantification in RAN, this paper adopts the worst possible Decoder to mirror the Encoder\u2019s architecture.", "That is, we assume a powerful adversary that knows the Encoder in the training.", "\u201cAn exactly reversed model\u201d stands for a layer-by-layer deconvolutional model (Decoder) with known Encoder\u2019s convolution filter number and size, pooling size and each layer\u2019s connection relationship.", "In the revision, we have added above clarification in section 2.3.", "Response #6: Thanks for the comments.", "We call Eq. 3 and Eq.4 adversarial, as explained in out intuition, they need not be opposite all the time.", "And we agree that the resulting model is highly affected by the setting of hyper-parameters n and k. In particular, we have compared the settings of k=1, k=2, k=3, and k=4 for each task and finally select the best overall value k=3.", "As for the number of epoch n, it depends on the usual practices of developers for an acceptable converged result.", "In our experiments, we use n=10,000 for MNIST, UbiSound and Har with batch size=128, and adopt n=20,000 for CIFAR-10 and ImageNet with batch size=256 and batch size=512, respectively.", "In fact, we have already conducted exhaustive micro-benchmark experiments to determine the current design of RAN.", "For example, we adopt different options of model architectures, nine weight updating schemes on when and what order to update Encoder, Decoder and Classifier,  and several settings of the important hyper-parameters (e.g., \u201cn\u201d and \u201ck\u201d) to select the empirically optimized one.", "However, we didn\u2019t present the micro-benchmark results in this paper due to the space limit.", "In the revision, we have added more explanations on the selection of n and k in Section 2.4."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 493, "sentences": ["We are grateful for your time and comment on the work.", "We start by further explaining the contributions in the paper.", "Our main contribution is the combination of MARL with HRL to enable the decentralized learning of controllers that can navigate and seek goals in a robotic humanoid simulation.", "The unique combination of methods allows us to learn these sophisticated controllers with far less data than methods without hierarchy (cite openAI Emergent Tool Use from Multi-Agent Interaction).", "Second, we also consider the environment in the paper another contribution.", "Few multi-agent environments simulate dynamics, and none have articulated humanoid robots that observer their world using egocentric vision.", "We plan to release this environment with the work to allow other researchers to pursue and make progress on important complex tasks.", "Many multi-agent problems have been studies using simple robot models (point-mass, etc), where more complex and realistic models have used the problem because significantly more challenging.", "However, often, an assumption can be made that the robots in the environment share similar morphology.", "We propose a method that uses a form of goal-conditioned RL to learn task agnostic low-level policies that can simplify the share control structure across robots.", "In most HRL methods, the lower level can be viewed as part of the environment, yet this restructuring of the environment enables faster and more capable learning.", "Here we clarify some of the proposed advantages of the method.", "First, the use of HRL enables temporal correlation in action exploration that helps reduce the non-stationarity challenge.", "The advantage of this temporal correlation is shown empirically in Figures 2 and 3 where the PPO policies do not improve on learning the tasks.", "This property can be understood to reduce the variance in the policy gradient.", "Instead of having the policy sample an action every step instead, the low-level policy is triggered for $k$ timesteps with a goal proposal.", "For these $k$ timesteps, no noise is added to the low-level policy outputs.", "Similarly, this $k$-step structured exploration enables learning.", "If we think of the policy as a type of VAE that is learning an encoder (high-level) that is trying to learn a good latent goal (z) that will result in the low-level performing the desired sequence of actions.", "The HRL structure is reducing the dimensionality of the control problem given a low-level designed to perform diverse behaviour wrt to the goal (cite Heess and DIAYN).", "Last, the partial parameter sharing appears to make the learning problem easier.", "We know it is challenging to learn Q functions, which implies that the centralized methods that use Q functions will not scale well.", "We compare our method to MADDPG, a popular centralized method that works by treating the problem as a single agent problem with complete information.", "In our case, this method results in a significant increase in network parameters for the Q function, which leads to poor learning performance, as can be seen in Figures 2 and 3.", "Our particular configuration allows our method to be decentralized, making the individual network for each agent more straightforward.", "We are also interested in generalization to different numbers of agents after training, which is also problematic for centralized methods.", "In short, decentralized learning will allow for more general methods, and HRL enables the learning of sophisticated controllers."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 494, "sentences": ["We thank the reviewer for the comments!", "Q: \u201ccomparing directly to Merity et al.'s approach\u201d", "Merity et al.", "share the input and output embeddings via an adaptive softmax where all words have the same embedding size.", "We reimplemented their approach and found that it did not perform very well in our experiments (25.48 PPL; Appendix A, Table 6, last row).", "We found that sharing fixed size input and output embeddings for a flat softmax performs better (22.63 PPL; second to last row of Table 6).", "This is likely because we train all words at every time step, which is not the case for an adaptive softmax with fixed size embeddings.", "Q: \u201cThe discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing\u201d", "We updated the paper and hope that the discussion is clearer now.", "Thank you for the feedback!", "Q: \u201cthoughts as to why full-softmax BPE is worse than adaptive softmax word level\u201d", "Full-softmax BPE is worse because we measure perplexity on the word-level.", "This involves multiplying the probabilities of the individual BPE tokens.", "BPE token-level perplexity itself is actually significantly lower than word-level PPL (around 21.5 for GBW and around 18 for WikiText-103 for the models presented in the paper) but the two are not comparable."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 495, "sentences": ["We thank the reviewer for his/her positive evaluation of our paper and for raising many very useful points which helped us getting to a clearer picture of our contribution.", "A few of these points deserve discussion beyond the changes made in the paper.", "Due to a mistake on page 2, we got the reviewer confused believing we are using importance sampling while we are using importance mixing instead.", "This has been fixed.", "The reviewer mentions it may be possible to construct counter-examples where the gradient updates will prevent convergence.", "This is a very important point.", "There are many RL problems (see e.g. Continuous Mountain Car, Colas et al. at ICML 2018) where at some point the gradient computed by the critic is deceptive, i.e. it drives the policy parameters into a wrong direction.", "In that case, applying that gradient to CEM actors as we do in CEM-RL is counter-productive.", "But the fact that we only apply this gradient to half the population makes it that CEM-RL should nevertheless overcome this issue:  the actors which did not receive a gradient step will be selected and the population will continue improving.", "However, admittedly, in this very specific context, CEM-RL is behaving as a CEM with only half a population, thus it is less efficient than the standard CEM.", "Besides, ERL even better resists than our approach to the same issue: if the actor generated by DDPG does not perform better than the evolutionary population due to a deceptive gradient issue, then this actor is just ignored, and the evolutionary part behaves as usual, without any loss in performance.", "This deceptive gradient issue certainly explains why CEM is the best approach on Swimmer.", "Finally, it may also happen that the RL part does not bring benefit just because the current critic is wrong and provides an inadequate gradient, in a non-deceptive gradient case.", "All the above points have now been made much clear in the new version of the paper, in particular we added an appendix dedicated to the swimmer benchmark.", "The reviewer also raises doubts about the fact that the method of Khadka & Tumer (2018) cannot be extended to use CEM.", "After second thoughts, this is absolutely right.", "As the reviewer says, in both this work and Khadka & Tumer, the RL updates lead to policies that may differ a lot from the search distribution and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.", "But if the RL actor shows good enough performance, this does not prevent from computing a new covariance matrix which includes it.", "The corresponding ellipsoid in the search space may be very large, leading to a widespread next generation, but the process should tend to converge again towards a population of actors where evolutionary and RL actors are closer to each other.", "A result of these second thoughts is that one could definitely build an ERL algorithm where the evolutionary part is replaced by CEM.", "We corrected the paper according to this new insight.", "Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.", "Despite the very interesting points above, the reviewer is wrong when saying that the main distinction between our approach and the ERL approach is that only in ours the information flow is from ES to RL and vice-versa.", "Actually, in ERL, if the RL actor added to the population performs well, it will steer the whole evolutionary population to the right direction just by generating offsprings, so RL and ES also benefit from each other.", "A lot of our effort during the rebuttal stage has been focused on better highlighting the often subtle differences between ERL and our approach.", "For doing so, we replaced Figure 1 with a figure directly contrasting CEM-RL to ERL.", "We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies described either in the main text or in appendices.", "The next point of the reviewer is that a good deal of the strong performance of our method and RL may just be due to the fact that we are using multiple actors, thus benefiting from an \"ensemble method\" effect already mentioned in several papers such as Osband et al., 2016 for DQN.", "This point is absolutely valid.", "The reviewer thus suggests a relevant control which would be to keep a population (ensemble) of policies, but only update using RL while sharing experience across all actors.", "This would isolate the ensemble effect from the evolutionary search effect.", "We performed the suggested control.", "The resulting algorithm is a multiple-actor version of TD3.", "Results show that CEM-TD3 actually outperforms this multiple-actor TD3, thus the CEM part actually brings performance improvement.", "About replacing the ReLU non-linearity in DDPG and TD3 prior work with tanh, we spotted that we could get much better results on several environments with the latter.", "This explanation is now clearly mentioned in the paper, and motivates a future work direction which consists in using \"neural architecture search\" for RL problems, the performance of algorithms being a lot dependent on such architecture details.", "Finally, to keep our paper shorter than the hard page limit for ICLR while addressing all the reviewers points, we had to move several studies into appendices, starting with the importance mixing study."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_contradict-assertion", "rebuttal_done", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_concede-criticism", "rebuttal_contradict-assertion", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_done", "rebuttal_future", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 496, "sentences": ["Thanks for your constructive feedbacks! Please see our response below.", "1. About image captioning.", "Yes. Image captioning dataset is absolutely available for creating the lookup table.", "As you suggest, we use MS COCO Image captioning dataset to learn a lookup table and apply it to the EN-RO translation task to do the quick evaluation.", "As a result, the BLEU score is (33.55), which is comparable to the current lookup table (33.78) based on Multi30K, and outperforms the Trans. (base) (32.66).", "Regarding the performance of the standard image captioning system, we train a caption model (Show, Attend, and Tell (Xu et al., 2015b)) with fine-tuned encoder (ResNet101) on the COCO dataset to encode the images.", "The result on EN-RO is 33.58.", "We are a little bit uncertain if we have well understood this request because our task is text to text translation while image captioning is image to text.", "If not, we are glad to address further.", "2. About the minor comments.", "(1)\tThis is typo. It is Q.", "(2)\tYes. We will remove it following your suggestion."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label"]}
{"abstract_id": 497, "sentences": ["Thank you for your review and we would be delighted to address your concerns, but do require some clarifications.", "While a learnable lambda could be considered we would argue that the learning of this parameter beyond the grid-search applied in the submission is somewhat tangential to our primary contribution: a unified framework which lends itself to targeted representation analysis and modification.", "1)", "The notion of a map of \\lambdas sounds interesting.", "However, at present, it is not clear to us what this refers to as \\lambda is a weighting on a loss term.", "Clarification would be much appreciated so we can fully engage with this point.", "As far as the existing approach is concerned, Figure 6 illustrates the influence of \\lambda on the accuracy and correlation of global and local stability prediction.", "2)", "The inconsistent correlations between the two tasks are exactly the scenarios where stethoscopes come into their own: testing positive and negative regimes of lambda (corresponding to auxiliary and adversarial training, respectively) reveals the interplay between the two tasks and potentially allows for de-biasing the algorithm as shown in Figure 6a.", "Therefore, in contrast to the design not considering these relationships, it explicitly addresses them.", "Could you please elaborate on the comment \u2019the current design [\u2026] simply sums them up\u2019?", "The stethoscope module has its own trainable parameters and a separate loss function.", "Only the encoder shares weights between main and secondary task."], "labels": ["rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_future", "rebuttal_reject-request", "rebuttal_followup", "rebuttal_summary", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_followup", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 498, "sentences": ["We thank Reviewer 2 for their support and questions.", "We answer them below.", "Quantization time", "As we state in our paper, quantizing a ResNet-50 (quantization + finetuning steps) takes about one day on one Volta V100 GPU.", "The time of quantization is around 1 to 2 hours, the rest being dedicated to finetuning.", "Thus, the time dedicated to quantization is relatively short, especially compared with the fine-tuning and even more with the initial network training.", "This is because we optimized our EM implementation in at least two ways as detailed below.", "-\tThe E-step is performed on the GPU (see file src/quantization/distance.py, lines 61-75) with automatic chunking.", "This means that the code chunks the centroids and the weight matrices into blocks, performs the distance computation on those blocks and aggregates the results.", "This falls within the map/reduce paradigm.", "Note that the blocks are automatically calculated to be the largest that fit into the GPU, such that the utilization of the GPU is maximized, so as to minimize the compute time.", "-\tThe M-step involves calculating a solution of a least squares problem (see footnote 2 in our paper).", "The bottleneck for this is to calculate the pseudo-inverse of the activations x. However, we fix x when iterating our EM algorithm, therefore we can factor the computation of the pseudo inverse of x before alternating between the E and the M steps (see file src/quantization/solver.py and in particular the docstring).", "We provided pointers to the files in the code anonymously shared on OpenReview.", "To our knowledge, these implementation strategies are novel in this context and were key in the development of our method to be able to iterate rapidly.", "Both strategies are documented in the code so that they can benefit to the community.", "Incorporating the non-linearity", "As the Reviewer rightfully stated, optimally we should take the non-linearity in Equation (4) into account.", "One could hope for a higher compression ratio.", "Indeed, the approximation constraint on the positive outputs would stay the same (they have to be close to the original outputs).", "On the other hand, the only constraint lying on the negative outputs is that they should remain negative (with a possible margin), but not necessarily close to the original negative outputs.", "However, our early experiments with this method resulted in a rather unstable EM algorithm.", "This direction may deserve further investigation."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 499, "sentences": ["We appreciate your valuable comments.", "As you have said, these methods are popular in practice and achieve good performance.", "However, most of them are done in the setting of MCMC, and people rarely use them in nonconvex optimization.", "One contribution of this paper is to apply these techniques to optimization problems.", "Our paper also tries to understand the acceleration effect of replica exchange.", "We quantify it in both LDP and the convergence of chi^2 divergence.", "Although in Dupuis\u2019s work, he also quantifies the acceleration effect via LDP, the LDP theory we use in this paper is different from that.", "Specifically, his approach is based on the LDP variational theory, and ours is based on the theory of Donsker-Varadhan.", "As a result, our rate function has different form from his.", "In our paper, we also analyze the acceleration in the convergence of chi^2 divergence.", "It is a new perspective and not discussed by Dupuis.", "We emphasize that LDP and chi^2 divergence are two different approached to quantify convergence.", "They have different meanings.", "The first one characterizes the decay rate of the probability that the empirical measures deviate from the stationary measure and the second one characterizes the decay rate of the discrepancy between the transit distributions and limiting distribution.", "Although the theory of LDP and convergence of chi^2 divergence for a general Markov process are well established and standard, to the best of our knowledge, our paper is the first to apply these tools in this specific problem.", "In our paper, one contribution is that we demonstrate the acceleration effect of replica exchange mathematically.", "We first show that the LDP rate function is boosted by replica exchange.", "Dupuis\u2019s work includes similar results but in a different form.", "We also show that the derivative of chi^2 divergence is boosted.", "Specifically, we demonstrate that a strict positive term caused by the replica exchange is added, if the density ratio between current distribution and limiting distribution is not symmetric.", "We say that a function is symmetric if we swap the positions of variables, the function value does not change.", "In this case, the derivative of chi^2 divergence is strictly boosted, and hence, the convergence is accelerated strictly.", "It reflects the benefits of replica exchange.", "To the best of our knowledge, this phenomenon has never been observed by previous literature, including Dupuis\u2019s paper.", "We think it is interesting and useful.", "Another contribution of our paper is the discretization algorithm.", "In practice, it is impossible to simulate the continuous process directly, and discretization is necessary.", "To the best of our knowledge, no one has discussed the discretization of replica exchange Langevin diffusion before.", "Our paper is the first one to analyze the discretization theoretically.", "In this paper, we establish the linear convergence rate for the discretization error, which is highly trivial since the process has state-dependent jumps.", "This result, combined with the acceleration effect, justifies the empirical success of the replica exchange Langevin diffusion in practice."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 500, "sentences": ["We would like to thank the reviewer for the positive feedback.", "We reply to the the two questions below.", "Q1: The learning procedure is confusing. It is highly recommended to provide the pseudocode of the proposed method.", "R1: We will provide the pseudocode in the future versions of the paper:", "Training:", "X_L : clean set", "C_L : class set", "X_Z : noisy set", "# For each class name", "For c in C_L:", "#Take the clean examples belonging to this class", "X_L^c : subset of X_L with label c", "#Only consider noisy examples with the class name in the text", "X_Z^c = filter_by_text(X_Z)", "# Build the graph for this class, and learn the GCN for cleaning", "A^c = build_graph(X_Z^c)", "M^c = GCN_model(X_L^c, X_Z^c, A^c)", "#Clean examples always get weight 1", "for i in X_L^c:", "r_i = 1.0", "#Noisy examples get the learned weight", "for i in X_Z^c:", "r_i = assign_relevance(M^c(X_Z^c(i))", "#Add the noisy examples to the list of training images for this class", "X_L^c = concatenate(X_L^c, X_z^c)", "#Learn a classifier jointly for all classes.", "Use the relevance weights for noisy examples when learning the classifier", "W = train_classifier(X_L^c, r)", "Testing", "Given test image Q", "v = extract_feature(Q)", "scores = W^T v", "prediction = argmax(scores)", "Q2: Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?", "R2: The complexity is linear in the number of classes, since classes are processed independently.", "Furthermore, text filtering is applied before cleaning, which reduces the number of images to be considered for a given class.", "Please also see the response R1 to reviewer1."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_social_label"]}
{"abstract_id": 501, "sentences": ["Thank you for your comments!", "* In our method, \"``more informative\" means \"``less deficient\".", "We have added a figure tracing the mutual information between representation and output I(Z;Y) vs. the minimality term I(Z;X) for different values of beta (see Figure 2, lower right panel), when training with our loss function.", "This is the usual information bottleneck curve.", "The deficiency bottleneck curve (Figure 2, upper right panel) traces the corresponding sufficiency term J(Z;Y) (which is just the entropy of the labels minus our loss) vs. I(Z;X) for different values of beta.", "The text now makes this more explicit (see p.7, first paragraph).", "Note that for M=1, J(Z;Y) = I(Z;Y).", "We can see that when training with our loss, we achieve approximately the same level of sufficiency (measured in terms of I(Z;Y)) while consistently achieving more compression (note the log ordinate for I(Z;X) in the lower left panel in Fig. 2) for a wide range of beta values.", "* We included two new figures plotting the representation for MNIST (p. 19, Figure 7) and Fashion-MNIST (p. 19, Figure 8) in Appendix E.3 for an unsupervised version of the VDB objective (p. 18, equation 38)."], "labels": ["rebuttal_social", "rebuttal_contradict-assertion", "rebuttal_done", "rebuttal_answer", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 502, "sentences": ["Thank you for the thoughtful comments.", "We are glad that you found our algorithmic approach original, and our experiments promising.", "Regarding the notation, given that the topic of our paper is inherently interdisciplinary -- spanning machine learning and algorithm theory -- we need to use notions and notation from both communities.", "This can lead to misunderstandings, but there is no easy way around it.", "In the paper we tried to follow the notation used in heavy-hitter analysis in algorithm theory to make it easy to compare the analysis to past work.", "But since there is no standard notation across both fields, it is difficult to find a notation that is easily accessible to both communities.", "In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.", "We discuss this in more detail below, and hope this should clarify any misunderstandings.", "Regarding our proofs, they are all self-contained.", "- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.", "Starting with S and i: I guess S and i are both simply varying-length sequences in U.", "To clarify, the input S is a sequence *of elements* from some universe U.", "To give an example, we could have U={0...65535}, in which case the sequence S would consist of integers in the range 0...65535.", "For example, S = 10101, 21222, 10222, 1, 10, 1, 52233, 62223 is an example sequence of length 8 whose items belong to U.", "The remainder of the problem definition is as described in the introduction: a frequency estimation algorithm reads the sequence S in one pass, and after that, for any element i from U, reports an estimate of  f_i,  the number of times element i occurs in S. In the above example, we have, e.g., f_1=2.", "- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).", "Thanks for the suggestions.", "We will include more explanation in the introduction and condense related work while keeping it thorough.", "- In describing Eqn 3 there are some weird remarks, e.g. \"N is the sum of all frequencies\". Do you mean that N is the total number of available frequencies? i.e.", "should it be |D|? It's not clear to me that the sum of frequencies would be bounded if D is not discrete.", "N is the sum of all frequencies; i.e., N = \\sum_{ i \\in U }", "f_i.", "- Your F and \\tilde{f} are introduced as infinite series.", "Maybe they should be {f1, f2,..., fN}, i.e. N queries, each of which you are trying to be estimate.", "The series are indeed finite, we skipped the last index for simplicity.", "Formally, it should be F = {f_1, \u2026, f_|U|} and ~F = {~f_1, \u2026, ~f_|U|}", "- In general, you have to introduce the notation much more carefully.", "Your audience should not be expected to be experts in hashing for this venue!! 'C[1,...,B]' is informal abusive notation.", "You should clearly state using both mathematical notation AND using sentences what each symbol means.", "As stated, C[1...B] is a one-dimensional array.", "Equivalently, it is a B-dimensional vector.", "We refer to C as an \u201carray\u201d as opposed to \u201cvector\u201d for the sake of consistency with prior work on frequency estimation, and to avoid nested subscripts.", "C[b] indeed denotes the b-th element/bin of C. Regarding the notation h: U -> [B] : we use [B] to denote the set {1...B}. We define it in Section 7, but we should have defined it earlier.", "The formula h: U->[B] indeed denotes a function h that maps elements of U to {1...B}.", "- Still it is unclear where 'fj' comes from. You need to state in words eg \"C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \\in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.\"", "We hope that after the earlier clarifications, the equation C[b] = sum_{j:h(j)=b} f_j  is more clear now.", "- What I don't understand is how fj is dependent on h. When you say \"at the end of the stream\", you mean that given S, we are analyzing the frequency of a series of sequences {i_1,...,i_N}?", "f_j does not depend on h, only on the input sequence S. Since an element j can occur anywhere in S, the equation C[b] = sum_{j:h(j)=b} f_j  holds only after the algorithm scans the whole sequence S.", "- The term \"sketch\" is used in Algorithm1, like 10, before 'sketch' is defined!!", "As explained in the description, items not stored in unique buckets \u201care fed to the remaining B \u2212 Br buckets using a conventional frequency estimation algorithm SketchAlg\u201d.", "The word \u201csketch\u201d in Algorithm 1 refers to the storage used by SketchAlg.", "To avoid confusion, we will shorten line 10 to \u201cfeed i to SketchAlg\u201d."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_concede-criticism_label", "rebuttal_social_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label"]}
{"abstract_id": 503, "sentences": ["Thank you for the appreciation on the novelty of our paper.", "- We have carefully proofread the manuscript and fixed the typos in the revised version. Could reviewer be more specific about the odd formulations, so that we can improve them?"], "labels": ["rebuttal_social", "rebuttal_done"], "confs": [1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_done_label"]}
{"abstract_id": 504, "sentences": ["Thanks again, Reviewer #3, for your thought-provoking critique.", "We respond to your other comments below.", "1.  \u201cIn particular, Section 4 is a series of empirical analyses, based on one dataset pair\u2026.However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.\u201d", "See general responses #1 and #3.", "2.  \u201cIt is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper\u2026", ".Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.\u201d", "See general response #2.", "We emphasize that we are not trying to approximate the density function, only approximate the difference and characterize its sign.", "Moreover, the special structure of CV-Glow makes these derivative-based approximations better behaved and more tractable than an expansion of a generic deep neural network.", "3.  \u201cSome parts of the paper feel long-winded and aimless\u2026.In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.", "Section 2 background takes too much space.", "Section 3 too much redundancy", "-- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.\u201d", "We will attempt to make the writing more concise.", "But we believe that most, if not all, of Section 2 is necessary in order to make the paper self-contained and accessible to someone who has never before seen invertible generative models.", "While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.", "4.  \u201cI don't think Glow necessarily is encouraged to increase sensitivity to perturbations.", "The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.\u201d", "We are not saying that the model will totally disregard the latent density and attempt to scale the input to very large or infinite values.", "Our point is made in the context of volume term which is only one of the terms in the change-of-variable objective.", "The log volume term in the change-of-variable objective is maximizing the very quantity (the Jacobian\u2019s diagonal terms) that the cited work on derivative-based regularization penalties has sought to minimize.", "The maximization of the derivatives in the objective directly implies increased sensitivity to perturbations.", "5.  \u201cFigure 6(a) [Figure 5(a) in revised draft] clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.\u201d", "We are not sure how you are drawing this conclusion; perhaps from the scale of the x-axis?", "The histogram in Figure 6 (a) (original draft) has an x-axis covering the interval [0.4, 0.55], meaning the maximal difference between a mean in *any pair of dimensions* is 0.15.", "Scaling back to pixel units, 0.15 * 255 = 38.25, meaning that 38.25 pixels is the maximum difference in means.", "While this is not a difference of zero, we don\u2019t see how you could say this \u201cclearly suggests\u201d that the means are \u201cvery different.\u201d  In the latest draft, this figure---now Fig 5 (a)---has an x-axis that spans from 0-255.", "Hopefully the overlap in the means in now conspicuous.", "6.", "\u201cHowever, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite: V\u00edt \u0160kv\u00e1ra et al. Are generative deep models for novelty detection truly better? at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.\u201d", "Thank you for pointing us to this work.", "We cite it in the revised draft.", "It looks like they test on UCI data sets of dimensionality less than 200, and therefore their results speak to a much different data regime than the one we are studying.", "7.  \u201cA part of the paper's contribution (section 5 conclusion) seem to overlap with others' work.", "The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).\u201d", "While we do also analyze constant images, we believe that our results for multiple data set pairs (FashionMNIST-MNIST, CIFAR10-SVHN, CelebA-SVHN, ImageNet-CIFAR10/CIFAR100/SVHN) and for multiple deep generative models (flow-based models, VAE, PixelCNN) is novel.", "Our conclusions are arrived at through focused experimentation and a novel analytical expression applied to CV-Glow."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_by-cr_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_social_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_social_label", "rebuttal_done_label", "rebuttal_answer_label", "rebuttal_structuring_label", "rebuttal_structuring_label", "rebuttal_answer_label", "rebuttal_answer_label"]}
{"abstract_id": 505, "sentences": ["Thanks for your attention to our work.", "1) For the motivation, Because the traditional algorithms deal with GANs via a Markov chain:", "$(f_0,g_0)\\rightarrow (f_1, g_0)\\rightarrow (f_1,g_1) \\rightarrow \\cdots\\rightarrow (f_{n},g_{n-1})\\rightarrow (f_n,g_n)$. It is like a kind of reinforcement learning--- but the environment (Here it is $f$) is changing. And we want to view it from the angle of game theory. And then we try to minimize the new loss.", "2) For the presentation: we will try to modify it.", "And we apologize that there are some typos about the $f^*$ and $g^*$ in the Eq.(22).", "The $f^*$ and $g^*$ means the dependent variable of duality gap.", "And the $\\Leftrightarrow$ definition of $\\mathcal{F}$ means equivalence.", "It can also be written as $f\\in \\mathcal{F}\\rightarrow 1-f\\in mathcal{F}$. And we will modify other improper presentation.", "3)For the experiment: we will spend some time to train GANs with more iteration and modify it.", "Thanks"], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["rebuttal_social_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_concede-criticism_label", "rebuttal_answer_label", "rebuttal_answer_label", "rebuttal_done_label", "rebuttal_done_label", "rebuttal_social_label"]}
