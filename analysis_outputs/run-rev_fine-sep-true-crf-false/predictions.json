{"abstract_id": 0, "sentences": ["The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions.", "The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to \"directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems.\" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.", "As such the paper is not convincing.", "On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?", "The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.", "The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning."], "labels": ["arg-structuring_summary", "none", "none", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 1, "sentences": ["The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup.", "The proposed model combines few shot meta-learning with the adversarial domain adaptation to demonstrate performance improvements in several experiments.", "Pros:", "1. A new few shot learning with domain shift problem is studied in the paper.", "2. A new model combining prototypical network with GAN and cycle-consistency loss for addressing meta-learning domain shift scenario.", "The experimental improvements on omniglot seem quite substantial.", "Cons:", "1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?", "It seems that both are using meta-learning with domain adaptation technique.", "What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?", "I feel the baseline in domain adaptation area is a bit limited.", "2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?", "3. It seems the domain shift in the paper is less dramatic.", "i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.", "4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.", "Minor:", "1. Where is L_da in Figure 2? In Figure 2, what\u2019s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?", "2. In the caption of figure 2, there should be a space after `\":\"."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_experiment", "none", "arg-request_experiment", "none", "arg-request_experiment", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-request_clarification_label", "arg-request_typo_label"]}
{"abstract_id": 2, "sentences": ["This paper explores self-supervised learning in the low-data regime, comparing results to self-supervised learning on larger datasets.", "BiGAN, RotNet, and DeepCluster serve as the reference self-supervised methods.", "It argues that early layers of a convolutional neural network can be effectively learned from a single source image, with data augmentation.", "A performance gap exists for deeper layers, suggesting that larger datasets are required for self-supervised learning of useful filters in deeper network layers.", "I believe the primary claim of this paper is neither surprising nor novel.", "The long history of successful hand-designed descriptors in computer vision, such as SIFT [Lowe, 1999] and HOG [Dalal and Triggs, 2005], suggest that one can design (with no data at all) features reminiscent of those learned in the first couple layers of a convolutional neural network (local image gradients, followed by characterization of those gradients over larger local windows).", "More importantly, it is already well established that it is possible to learn, from only a few images, filter sets that resemble the early layers of filters learned by CNNs.", "This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.", "For example, see the following paper (over 5600 citations according to Google scholar):", "[1] Bruno A. Olshausen and David J. Field.", "Emergence of simple-cell receptive field properties by learning a sparse code for natural images.", "Nature, 1996.", "Figure 4 of [1] shows results for learning 16x16 filters using \"ten 512x512 images of natural scenes\".", "Compare to the conv1 filters in Figure 2 of the paper under review.", "This 1996 paper clearly established that it is possible to learn such filters from a small number of images.", "There is long history of sparse coding and dictionary learning techniques, including multilayer representations, that follows from the early work of [1].", "The paper should at minimum engage with this extensive history, and, in light of it, explain whether its claims are actually novel."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 3, "sentences": ["Summary:", "This paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another.", "Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation.", "Authors argue that ME bias could help the model to handle new classes and rare events better.", "My comments:", "I very much enjoyed reading this paper.", "I support accepting this paper.", "It highlights one of the missing inductive biases in ML and proposes it as a challenge.", "As the authors also agree, ME bias is missing not just in DNNs.", "It is the issue of MLE.", "It would be good to have some non-NN results too.", "I see this is a challenge for MLE than DNNs.", "1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?", "2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.", "3. Section 4.2 need more clarity. For example, what do you mean by classifying the image as \u201cnew\u201d? Is \u201cnew\u201d a class name? Also, how is P(N|t) computed? Please explain.", "4. Are the authors willing to release the code and data to reproduce the results?", "Minor comments:", "1. Page 3: second para, line 4: \u201cour aim is to study\u201d", "2. Page 5: last line: estimate for -> estimated for", "3. Section 4.2: 3rd line: \u201cthe class for the from\u201d", "=====================================================", "After rebuttal: I have read the authors' response and  I stand by my decision."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_explanation", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 4, "sentences": ["This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.", "Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances.", "This is an interesting work.", "The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning.", "(2) The proposed approach produced effective empirical results.", "The drawbacks  of the work include the following: (1) There is not much technical contribution.", "It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning.", "Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.", "(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.", "Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.", "For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.", "This is a major concern."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-structuring_quote", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 5, "sentences": ["Summary:", "This paper addresses the computational aspects of Viterbi-based encoding for neural networks.", "In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis.", "Now consider a codebook with n convolutional codes, of rate 1/k.", "Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits.", "Then the memory footprint (in terms of messages) is reduced by rate k/n.", "This is the format that will be used to encode the row indices in a matrix, with n columns.", "(The value of each nonzero is stored separately.)", "However, it is clear that not all messages are possible, only those in the \"range space\" of my codes. (This part is previous work Lee 2018.)", "The \"Double Viterbi\" (new contribution) refers to the storage of the nonzero values themselves.", "A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task.", "Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords.", "Pros:", "- I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance.", "- The idea is theoretically sound and interesting.", "Cons:", "- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.", "Compressability is evaluated, but that was already present in the previous work.", "Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.", "- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.", "- Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_typo_label"]}
{"abstract_id": 6, "sentences": ["This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training.", "The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds.", "However, the experimental results are weak in justifying the paper's claims.", "Pros:", "* The problem is interesting and well explained", "* The proposed method is clearly motivated", "* The proposal looks theoretically solid", "Cons:", "* It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides.", "There is no direct comparison of performance.", "* Fig. 3 needs more explanation.", "The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation.", "Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.", "* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?", "* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.", "However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.", "Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).", "It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.", "A typo in page 6, last line: wth -> with"], "labels": ["none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_edit", "arg-request_edit", "arg-request_explanation", "none", "none", "none", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_typo_label"]}
{"abstract_id": 7, "sentences": ["The paper extends previous work on differentiable placticity to include neuro modulation by parameterizing the learning rate of Hebbs update rule.", "In addition, the authors introduce retroactive modulation that basically allows the system to delay incorporation of plasticity updates via so eligibility traces.", "Experiments are performaed on 2 simple toy datasets and a simple language modeling task.", "A newly developed cue-reward association task shows the clear limitations of basic plasticity and how modulation can resolve this.", "Slight improvements can also be seen on a simple maze navigation task as well as on a basic language modeling dataset.", "Overall I like the motivation, provided background information and simplicity of the approach.", "Furthermore, the cue-reward experiment seems to be a well designed show case for neuro-modulation.", "However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.", "Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.", "Therefore, although I would like to see an extended version of this paper at the conference, without further experiments and analysis I see the current version rather as an interesting workshop contribution.", "Strengths:", "- motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step", "- cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation", "- maze navigation shows incremental benefits over non-modulated plasticity", "- thorough experimentation", "- clipping-trick is a neat observation", "Weaknesses:", "- evaluation: only on toy tasks (which includes PTB), no real world tasks", "- very incremental improvements on PTB over a very simple baseline (far from SotA)", "- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures", "- no qualitative analysis on how modulation is actually use by the systems.", "E.g., when is modulation strong and when is it not used", "Comments:", "- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\". Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.", "Furthermore PTB is not a \"challenging\" LM benchmark."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 8, "sentences": ["This paper describes the method for performing self-training where the unlabeled datapoints are iteratively added to the training set only if their predictions by the classifier are confident enough.", "The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection.", "On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.", "The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.", "Also, the writing can be improved by making the writing more concise and formal (examples of informal: \"spoil the network\", \"model is spoiled\", \"problem of increased classes\", \"many recent researches have been conducted\", \"lots of things to consider for training\", \"supervised learning was trained\" etc.).", "The contributions of the method could also be underlined more clearly in the abstract and introduction.", "The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.", "The idea of selective sampling for self-training is promising and the investigated questions are interesting.", "As far as I understand, the main contribution of this paper is the use of separate \"selection network\" to estimate the confidence of predictions by \"classification network\".", "However, as the \"selection network\" uses exactly the same input as \"classification network\", it is hard to imagine how it can learn additional information.", "For example, imagine the case of binary classification.", "If the selection network predicts 0 in come cases, it can be used to improve the result of \"classification network\" by flipping the corresponding label.", "How can you interpret such a thought experiment?", "One could understand the use of \"selection network\" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of \"selection network\" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.", "Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of \"classification network\" is greater than some threshold?", "Finally, could you show a plot of top-1 prediction of \"classification network\" vs score of \"selection network\" and elaborate on that?", "Then, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes.", "Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including \"selection network\" with threshold) is not very principled.", "Ablation study shows that the use of the \"selection network\" strategy does not improve the results without these heuristics.", "It would be interesting to see how these heuristics would do without \"selection network\", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG.", "In the current form of evaluation, it is hard to say if there is any benefit of using the \"selection network\" that is the main novelty of the paper.", "It is very valuable that the experimental results include many recently proposed methods.", "Besides, the settings are described in details that could help for the reproducibility of the results.", "However, I have a few concerns about the results.", "First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).", "Besides, as the base classifier is different for various baselines, it is hard to compare the methods.", "Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).", "How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice?", "Another important parameters is the number of iterations of the algorithm.", "How was it chosen?", "Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes?", "What would happen if you use random class splits or split animal classes (like in a more realistic scenario)?", "To conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.", "Some questions and comments:", "- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?", "- In the training procedure of \"selection network\" of Sections 3.1, do you use the same datapoints to train a \"classification network\" and \"selection network\"? If it is the case, how do you insure that the \"classification network\" does not learn to fit the data perfectly and thus all labels s_i are 1?", "- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?", "- What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?", "- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.", "- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?", "- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?", "- Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?", "- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?"], "labels": ["arg-structuring_summary", "none", "none", "none", "arg-request_edit", "arg-request_edit", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-request_explanation", "arg-request_experiment", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_clarification", "arg-request_clarification", "arg-request_explanation", "arg-request_explanation", "arg-structuring_summary", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_edit", "arg-request_explanation", "arg-request_result", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 9, "sentences": ["* Summary", "This paper addresses machine reading tasks involving tracking the states of entities over text.", "To this end, it proposes constructing a knowledge graph using recurrent updates over the sentences of the text, and using the graph representation to condition a reading comprehension module.", "The paper reports positive evaluations on three different tasks.", "* Review", "This is an interesting paper.", "The key technical component in the proposed approach is the idea that keeping track of entity states requires (soft) coreference between newly read entities and locations and the ones existing in the knowledge graph constructed so far.", "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "This is especially the case in a few places involving coreference:", "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.", "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.", "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.", "Why does the graph update require coreference pooling again?", "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?", "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?", "That the model implicitly learns constraints from data is interesting!", "Bottomline: The paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_summary", "none", "none", "arg-structuring_quote", "arg-structuring_quote", "arg-request_result", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_quote_label", "arg-structuring_quote_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 10, "sentences": ["The paper presents an approach to extract a character from a video and then maneuver that character in the plane, optionally with other backgrounds.", "The character is then redrawn into the background with a neural net, and all of this is done in real time.", "All in all, this paper was well structured and extensively detailed wrt how it engineered this solution (and why).", "If I had a complaint, it would be that I did not learn anything scientifically from the paper.", "There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.", "Those are important as well for the field, and I suspect that this direction could be pushed a lot more.", "For example, it's not close to getting realistic spatial movement relative to the plane nor is the control that impressive wrt limbs.", "However, as a next-contribution, this work deserves to be seen more widely.", "Hence, I rate it as a weak accept."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 11, "sentences": ["Paper summary - This paper extends the differentiable plasticity framework of Miconi et al. (2018) by dynamically modulating the plasticity learning rate.", "This is accomplished via an output unit of the network which defines the plasticity learning rate for the next timestep.", "A variation on this dynamic learning rate related to eligibility traces is also proposed.", "Both dynamic modulation variations strikingly outperform non-plastic and plastic non-modulated recurrent networks on a cue-reward association task with high-dimensional cues.", "The methods marginally outperform plastic non-modulated recurrent networks on a 9x9 water maze task.", "Finally, the authors show that adding dynamic plasticity to a small LSTM without dropout improves performance on Penn Treebank.", "The paper motivates dynamic plasticity by analogy to the hypothesized role of dopamine in reward-driven learning in humans and animals.", "Clarity -  The paper is very clear and well written.", "The introduction provides useful insights, motivates the work convincingly, and provides interesting connections to past work.", "Originality - I don't know of any other work that models the role of dopamine in quite this way, or that applies dynamic plasticity modulation in settings like these.", "Quality - The experiments are well chosen and seem technically sound.", "Significance - The results show that meta-learning by gradient descent to modulate the plasticity learning rate is a promising direction -- a significant contribution in my view.", "Other Comments - The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.", "One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017.", "I gather that Experiment 3 presents small LSTMs without recurrent dropout instead because combining plasticity and dropout proved challenging (or at least the authors haven't tried it yet).", "I think the paper is solid as-is; positive results in this comparison would take it to the next level.", "Questions:", "Why were zero-sequences necessary in Experiment 1?", "This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.", "Perhaps the authors could clarify on what a confounding \"time-locked scheduling strategy\" would look like in this task?", "Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?", "Why is non-plastic rnn left out of Figure 2b?", "Typos", "\"However, in Nature,\" -- no caps", "in appendix: \"(see Figure A.4)\" -- the figure is labeled \"Figure 3\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_experiment", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_clarification", "arg-request_explanation", "arg-request_clarification", "arg-structuring_heading", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 12, "sentences": ["This paper studies non-additive utility aggregation for sets.", "The problem is very interesting.", "Choquet Integral is used to deal with set input.", "The authors propose two architectures.", "The two architectures, though not novel enough, are towards representing \u201cnon-additive utility\u201d.", "However, the experimental comparison is not fair, the description of the model (e.g. how Choquet is integrated into the model and help to learn \u201cintermediate meaningful results\u201d) is not clear, some claims are not true.", "First, the authors claim that they are the first to combine Choquet integral with deep learning.", "However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.", "For example, \u201cFuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing\u201d by Derek T. Anderson et al.", "Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.", "How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?", "These need to be further clarified.", "Third, the comparison to baseline and \u201cDeepSet\u201d is not fair.", "According to the illustration, it seems that you first obtain \u201cfeatures/representations\u201d.", "Then the representations are fed to the four architectures you listed in figure one.", "RNN-based approaches are with better \u201ccomplexity\u201d comparing to your sum baseline and \u201cDeepset\u201d approach.", "So, I have some doubts about the experimental results."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_clarification", "arg-request_clarification", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 13, "sentences": ["[Summary]", "This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner.", "The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention.", "The final prediction uses all the states and question to infer the final answers.", "The authors verify the effectiveness of the proposed method on the performance of different tasks and modules.", "Experiment on VQA shows the proposed model benefits from utilizing different modules.", "The authors also qualitatively show the model's reasoning process and human study on judging answering quality.", "[Strength]", "1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering.", "This is different from most existing work.", "2: By examing different modules, the proposed method is more interpretable compare to canonical methods.", "3: The experiment results are good, especially for the counting problem.", "[Weakness]", "1. The title of the paper is \"visual reasoning by progressive module networks.\" The title may be a little overstated since the major task is focused on visual question answering (VQA).", "2. Annotation is not clear in this paper.", "For example, on page 3, Query transmitter and receiver, \"the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k).", "\" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand.", "On page 4, State update function, what is the meaning of variable \"Epsilon\" in the equation?", "From the supplementary, it seems Epsilon means the environment?", "3. On the object counting task, the query transmitter needs to produce a query for a relationship module.", "The authors mentioned that this is softly calculated by softmax on the importance score.", "Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?", "4. The cider score of image captioning is 109 compared to the baseline 108.", "The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data.", "Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input.", "My assumption is the visual feature already contains the label information for image captioning.", "5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.", "6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?", "7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others.", "Is the number right?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_quote", "none", "arg-request_clarification", "arg-request_clarification", "none", "none", "arg-request_clarification", "none", "none", "none", "arg-request_explanation", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_quote_label", "none_label", "arg-request_clarification_label", "arg-request_clarification_label", "none_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_clarification_label"]}
{"abstract_id": 14, "sentences": ["Summary:", "This paper introduces Amortized Proximal Optimization (APO) that optimizes a proximal objective at each optimization step.", "The optimization hyperparameters are optimized to best minimize the proximal objective.", "The objective is represented using a regularization style parameter lambda and a distance metric D that, depending on its definition, reduces the optimization procedure to Gauss-Newton, General Gauss Newton or Natural Gradient Descent.", "There are two key convergence results which are dependent on the meta-objective being optimized directly which, while not practical, gives some insight into the inner workings of the algorithm.", "The first result indicates strong convergence when using the Euclidean distance as the distance measure D.", "The second result shows strong convergence when D is set as the Bregman divergence.", "The algorithm optimizes the base optimizer on a number of domains and shows state-of-the-art results over a grid search of the hyperparameters on the same optimizer.", "Clarity and Quality: The paper is well written.", "Originality: It appears to be a novel application of meta-learning.", "I wonder why the authors didn\u2019t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.", "Also how does this compare to adaptive hyperparameter training techniques such as population based training?", "Significance:", "Overall it appears to be a novel and interesting contribution.", "I am concerned though why the authors didn\u2019t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.", "Also, your convergence results appear to rely on strong convexity of the loss.", "How is this a reasonable assumption?", "These are my major concerns.", "Question: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_summary", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "none", "none", "none", "arg-request_explanation", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 15, "sentences": ["This paper proposed Compound Density Networks (CDNs), a neural network architecture that parametrises conditional distributions as infinite mixtures, thus generalising the traditional finite mixture density networks (MDNs).", "The authors realise CDNs by treating the weights of each neural network layer probabilistically, and letting them be matrix variate Gaussians (MVGs) with their parameters given as a function of the layer input via a hypernetwork.", "CDNs can then be straightforwardly optimised with SGD for a particular task by using the reparametrization trick.", "The authors further argue that in case that overfitting is present at CDNs, then an extra KL-divergence term can be employed such that the input dependent MVG distribution is close to a simple prior that is input agnostic.", "They then proceed to evaluate the predictive uncertainty that CDNs offer on three tasks: a toy regression problem, out-of-distribution example detection on MNIST/notMNIST and adversarial example detection on MNIST and CIFAR 10.", "The objective of this work is to provide a method for better uncertainty estimates from deep learning models.", "This is an important research area and relevant for ICLR.", "The paper is generally well written with a clear presentation of the proposed model.", "The generalisation from the finite MDN to the continuous CDN seems straightforward, the model is relatively easy to implement and it is evaluated extensively against several modern baselines.", "Nevertheless, I believe that it still has to address some points in order to be better suited for publication:", "- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).", "Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?", "- How many samples did you use from p(theta|x) during training?", "It seems that with a single sample the method becomes an instance of VIB [1], only considering the weights of the network as latent variables rather than the hidden units.", "- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.", "- Have you checked / visualised what type of weight distributions do CDNs capture? It would be interesting to see if e.g. the marginal (across the dataset) weight distribution at each layer has any multimodality as that could hint that the network learns to properly specialise to individual data points.", "- The authors mention that in order to avoid overfitting they add an extra (weighted) KL-divergence term to the log-likelihood of the dataset, that encourages the weight distributions for specific points to be close to simple priors.", "How influential is that extra term to the uncertainty quality that you obtain in the end?", "How does this term affect the learned distributions in case of CDNs?", "Furthermore, the way that CDNs are constructed seems to be more appropriate at capturing input specific uncertainty (i.e. aleatoric) rather than global uncertainty about the data (i.e. epistemic).", "I believe that for the specific uncertainty evaluation tasks this paper considers the latter is more appropriate.", "More discussion on both of these aspects can help in improving this paper.", "-", "As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.", "For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.", "For KFLA a hyper parameter \u201ctau\u201d was tuned; this hyperparameter instead corresponds to the precision of the Gaussian prior on the parameters.", "In this case, KFLA always optimises a \u201ccorrect\u201d Bayesian model for every value of the hyperparameter whereas MNF and noisy K-FAC do not.", "Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.", "[1] Deep Variational Information Bottleneck"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 16, "sentences": ["This paper proposed \"F pooling\" for Frequency Pooling, which is a pooling operation satisfying shift equivalence and anti-aliasing properties.", "The method is very simple: first, transform the input 1D/2D signal into the spectrum domain based on discrete Fourier transform (DFT), then cut the high-frequencies, then transform back to the time domain using the inverse DFT.", "The method can be implemented using FFT and auto differentiation frameworks.", "The method is tested on Resnet/Desnet on CIFAR-100 and subsets of ImageNet, showing better performance than the original models.", "The reviewer votes for rejection as the method has limited novelty.", "Spectrum pooling has been used in the community of computer vision and machine learning.", "Taking a random example (there are others by simple searching), in the ECCV paper \"DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018\" The DFT magnitude pooling is almost the same as the authors' propositions, where the \"Fourier coefficients are cropped by cutting off high-frequency components\".", "The reviewer encourages the authors to make further new developments and have a more comprehensive literature review. But in the current form, the paper has less value to be published in ICLR."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 17, "sentences": ["This paper proposed a framework to incorporate GAN into MAP inference process for general image restoration.", "First, the motivation of the proposed framework is not convincing for me.", "That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.", "However, in real world scenarios, it is actually challenging to obtain exact degradation information.", "Thus we may only apply the proposed model on a few tasks with exactly known F.", "Second, due to the norm based constraints, authors actually need to optimize a highly nonconvex optimization problem.", "Moreover, due to the trace based loss function, the computational cost will also be very high.", "Please notice that standard MAP based methods only need to solve a simple convex optimization model (e.g., TV) and these methods can also be applied for different restoration tasks.", "Actually, we only need to specify particular fidelity terms for different tasks.", "Moreover, very recent works have also successfully incorporate both generative and discriminative network architectures (e.g., [1,2]) into the optimization process.", "Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.", "Finally, the experimental part is also too weak to evaluate the proposed method.", "As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.", "Some works actually also incorporate generative and/or discriminative networks into MAP inference process for these tasks.", "Thus I believe authors must compare their method with these state-of-the-art approaches.", "Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.", "This is because the digitals images in MNIST do not have rich texture and detail structures, thus are not very challenging for standard image restoration methods.", "[1]. Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang: Learning Deep CNN Denoiser Prior for Image Restoration. CVPR 2017: 2808-2817", "[2]. Jiawei Zhang, Jin-shan Pan, Wei-Sheng Lai, Rynson W. H. Lau, Ming-Hsuan Yang: Learning Fully Convolutional Networks for Iterative Non-blind Deconvolution. CVPR 2017: 6969-6977"], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 18, "sentences": ["This paper explores maximally expressive linear layers for jointly exchangeable data and in doing so presents a surprisingly expressive model.", "I have given it a strong accept because the paper takes a very well-studied area (convolutions on graphs) and manages to find a far more expressive model (in terms of numbers of parameters) than what was previously known by carefully exploring the implications of the equivariance assumptions implied by graph data.", "The result is particularly interesting because the same question was asked about exchangeable matrices (instead of *jointly* exchangeable matrices) by Hartford et al. [2018] which lead to a model with 4 bases instead of the 15 bases in this model, so the additional assumption of joint exchangeability (i.e. that any permutations applied to rows of a matrix must also be applied to columns - or equivalently, the indices of the rows and columns of a matrix refer to the same items / nodes) gives far more flexibility but without losing anything with respect to the Hartford et al result (because it can be recovered using a bipartite graph construction - described below).", "So we have a case where an additional assumption is both useful (in that it allows for the definition of a more flexible model) and benign (because it doesn't prevent the layer from being used on the data explored in Hartford et al.).", "I only have a couple of concerns:", "1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from.", "The additional parameters of this paper come from having parameters associated with the diagonal (intuitively: self edges get treated differently to other edges) and having parameters for the transpose of the matrix (intuitively: incoming edges are different to outgoing edges).", "Neither of these assumptions apply in the exchangeable setting (where the matrix may not be square so the diagonal and transpose can't be used).", "Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.", "The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).", "2 - Those more familiar of the graph convolution literature will be more familiar with GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.. Most of these approaches are more restricted version of this work / Hartford et al. so we wouldn't expect them to perform any differently from the Hartford et al.  baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work.", "I would have also liked to see a comparison to these methods in the the classification results.", "3 - Appendix A - the 6 parameters for the symmetric case with zero diagonal reduces to the same 4 parameters from Hartford et al. if we constrained the diagonal to be zero in the output as well as the input.", "This is the case when you map an exchangeable matrix into a jointly exchangeable matrix by representing it as a bipartite graph [0, X; X^T, 0].", "So the two results coincide for the exchangeable case. Might be worth pointing this out."], "labels": ["arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "none", "none", "arg-request_explanation", "none", "none", "arg-request_result", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 19, "sentences": ["The paper proposes a way to pre-train quantized representations for speech.", "The approach proposed is a two-stage process: 1. train a quantized version of wav2vec [my understanding is that wav2vec is the same thing as CPC for Audio except for using a binary cross-entropy loss instead of InfoNCE softmax-cross entropy loss].", "the authors propose to use gumbel softmax / VQ codebook for the vector quantization.", "2. once you have a discrete representation, you could train BERT (as if it were a seq of language tokens).", "this makes a lot of sense especially given that CPC / wav2vec recovers phonemes and quantizing the phonemes will recover a language-like version of the raw audio. And running BERT across those tokens will allow you to capture the dependencies at the phoneme level.", "After pre-training, the authors use the learned representations for speech recognition.", "They compare this to using log-mel filterbanks.", "The results (WER / LER) is lower for the proposed pipeline compared to using dense wav2vec representation for n-gram and character LM.", "It also makes sense that BERT helps for the k-means (vq) setting since the number of codes is large.", "The authors also cleverly adopt/adapt span-BERT which is more suited to this setting.", "I think this paper presents a useful contribution as far as improving speech / phoneme recognition using self-supervised learning goes, and also has useful engineering aspects in terms of combining CPC and BERT. I would like to see this paper accepted."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 20, "sentences": ["This paper presented a relational graph attention networks that could consider both node features and relational information (edge features) to perform node-level and graph-level classifications.", "The basic idea is to combine the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid networks.", "This paper is generally easy to follow and written clearly.", "Several experiments are conducted to demonstrate the performance of the proposed model.", "Although some promising results have been achieved, I think there are several limitations regarding the novelty and significance of the proposed model.", "i) The proposed architecture is mainly adopted from the graph attention networks (Velic\u030ckovic\u0301 et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).", "Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.", "ii) In table 2, I don\u2019t really see any promising results compared to baselines. There are", "little improvements over the baselines or even significantly worse. More importantly,", "compared two schemes of this work, the ones with attentions are \u201calmost\u201d identical with ones without attentions, which implies that the proposed attentions mechanism is not really useful", "in practice. For most of newly proposed graph embedding algorithms, it is hard to convince it is indeed better without some significant improvements (at least 2% absolute accuracy more).", "iii) For MUTAG dataset, the statistical information of this dataset is quite different from what I used to use.", "MUTAG is a standard dataset for testing graph-level classification for both graph kernels and graph networks.", "MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete node labels.", "Each chemical compound is labeled according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella", "Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node classification problem?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 21, "sentences": ["This paper proposes a method for mathematical problem embedding, which firstly decomposes problems into concepts by an abstraction step and then trains a skip-gram model to learn concept embedding.", "A problem can be represented as the average concept (corresponding to those in the problem) embeddings.", "To handle the imbalanced dataset, a negative pre-training method is proposed to decrease false and false positives.", "Experimental results show that the proposed method works much better than baselines in similar problem detection, on an undergraduate probability data set.", "Strong points:", "(1)\tThe idea of decomposing problems into concepts is interesting and also makes sense.", "(2) The training method for imbalanced datasets is impressive.", "Concerns or suggestions:", "1.\tThe main idea of using contents to represent a problem is quite simple and straightforward.", "The contribution of this paper seems more on the training method for imbalanced data sets.", "But there are no comparisons between the proposed training method and previous related works.", "Actually, imbalance data sets are common in machine learning problems and there are many related works.", "The comparisons are also absent in experiments.", "2.\tThe experimental data set is too small, with only 635 problems.", "It is difficult to judge the performance of the proposed model based on so small data set.", "3.\tThe proposed method, which decomposes a problem into multiple concepts, looks general for many problem settings.", "For example, representing a movie or news article by tags or topics.", "In this way, the proposed method can be tested in a broader domain and on larger datasets.", "4.\tFor the final purpose, comparing problem similarity, I am wondering what the result will be if we train a supervised model based problem-problem similarity labels?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 22, "sentences": ["The paper talks about calculating various statistics over data streams.", "This is a very popular topic and is very relevant in big data analysis.", "A lot of work has been done in this general area and on the problems that are discussed in the paper.", "The new idea in the paper is better streaming algorithms under the assumption that there is a \u201cheavy hitters\u201d oracle that returns data items that have a lot of representation in the stream.", "The authors give provably better algorithms for the distinct elements problem, F_p moment problem (p > 2), and some more problems.", "These are important problems in streaming data analysis.", "They improve the space bounds and interestingly in some cases the bounds are better than what is possible without the oracle assumption.", "This also shows the power of such an oracle.", "There are experimental results to demonstrate the efficiency of the algorithms.", "At a high level the work seems good and interesting for a large audience interested in streaming data analysis.", "I have not gone over the proofs in detail (much of which is in the appendix).", "- Even though oracle results are interesting, to make it practical it may make sense to talk about a more realistic, weaker oracle where some of the queries may be incorrect.", "- It may even make sense to minimise the number of oracle calls which can be thought of as a resource and discuss the relationship between number of oracle calls and other resources such as space."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_clarification", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 23, "sentences": ["This paper proposes a distributed backdoor attack strategy, framed differently from the previous two main approches (1) the centralised backdoor approach and (2) the (less discussed in the paper) distributed fault tolerance approach (often named \"Byzantine\").", "The authors show through experiments how their attack is more persistent than centralised backdoor attack.", "The authors also compare two aggregation rules for federated learning schemes, (Fung et al 2018 & Pillutla et al 2019), suggesting that both rules are bypassed by the proposed distributed backdoor attack.", "Strength:", "what I found most interesting in the paper is Section 3.4, presenting an appreciable attempt to \"interpret\" poisoning. Together with Section 4.", "This kind of fine-grained analysis of poisoning is highly needed.", "Weakness:", "in section 3.3, the authors compare against RFA and take what is claimed in Pillulata et al as granted (that RFA detects more nuanced outliers than the wort-case of the Byzantine setting", "(Blanchard et al 2017) ) .", "In fact, there is more to the Byzantine setting than that, see e.g. Draco (Chen et al 2018 SysML), Bulyan (El Mhamdi et al 2018 ICML) and SignSGD (Bernstein et al 2019 ICLR) which have proposed more sophisticated approches to distributed robustness.", "Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.", "post rebuttal: thank your for your detailed reply, I acknowledge your new comparisons with the distributed robustness mechanisms of Krum and Bulyan, too bad time was short to compare with the other measures such as Draco and SignSGD."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 24, "sentences": ["The paper proposes a new method for robustifying a pre-trained model improving its decision boundaries.", "The goal is to defend the model from mistakes in training labels and to be more robust to adversarial examples at test time.", "The main idea is to train a LDA on top of the last-layer, or many layers in its ensemble version, making use of a small set of clean labels after training the main model.", "Additionally, robustness to outliers is achieved by the minimum covariance determinant estimator for the LDA covariance matrix.", "While I find this idea interesting and of potential practical use, I have concerns about novelty and the experimental results and overall I recommend rejection.", "== Method", "At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.", "See for example [A, B].", "In particular, [B] performs experiments on adversarial examples.", "Moreover, in spite of the authors writing that their goal is \u201ccompletely different\u201d from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.", "Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.", "Theorem 1 well supports the proposed method and it is well explained. I did not check the proofs in appendix.", "Regarding the presentation, I found odd having some experimental results (page 5) before the Section on experience even have started.", "== Experiments", "The authors did not comment on the computational overhead of training their LDA.", "But I assume it is very cheap compared to training e.g. the ResNet, correct?", "I also did not find an explanation of which version backward/forward losses [Patrini et al. 17] is used in the experiments: are the noise transition matrices estimated on the data or assumed to be known (for fair comparison, I would do the former).", "I disagree on the importance of the numbers reported on the abstract: DenseNet on Cifar10 with 60% goes from 53.34 to 74.72.", "This is the improvement with the weakest possible baseline, i.e. no method to defend for noise!", "Looking at Table 3, which is on ResNets, I will make this point clear.", "Noise 60% on CIFAR10, DDGC improves 60.05-> 71.38, while (hard) bootstrap and forward do better.", "Even more, it seems that forward does always better than DDGC with noise 60% on every dataset.", "Therefore, I don\u2019t find interesting to report how DDGC improve upon \u201cno baseline\u201d, because known methods do even better.", "Yet, it is interesting --- and I find this to be a contribution of the paper --- that DDGC can be used in combination with prior work to boost performance even further.", "A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).", "An additional column on the table showing that the algorithm can also work in this case would improve the confidence that the proposed method is useful in practice.", "Uniform noise is the least realistic assumption for label noise.", "Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.", "There are now dozens of defence methods that work (partially) for improving robustness.", "I don\u2019t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.", "A proper baseline should have been compared.", "One more unclear but important point: is Table 3 obtained by white-box attacks on the Resnet/Denset but oblivious of the MCD? Is so, I don\u2019t think such an experiment tells the whole story: as the the MCD would arguably also be deployed for classification, the attacker would also target it.", "Additionally, the authors state \u201cwe remark that accessing the parameters of the generative classifiers [\u2026] is not a mild assumption since the information about training data is required to compute them\u201d.", "I don\u2019t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what is the difference here?", "Table 8 rises some concerns.", "I appreciate the idea of testing full white-box adversarial attacks here. But I don\u2019t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.", "[A] Wen, Yandong, et al. \"A discriminative feature learning approach for deep face recognition.\" European Conference on Computer Vision. Springer, Cham, 2016.", "[B] Wan, Weitao, et al. \"Rethinking feature distribution for loss functions in image classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-structuring_quote", "none", "none", "arg-request_experiment", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "arg-request_clarification", "none", "none", "arg-structuring_quote", "none", "none", "none", "none", "arg-request_experiment", "arg-request_result", "none", "none", "none", "none", "arg-request_experiment", "arg-request_explanation", "arg-structuring_quote", "arg-request_explanation", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_quote_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "arg-structuring_quote_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 25, "sentences": ["Summary of the paper:", "The paper has two intertwined goals.", "These goals are to illuminate the generalization/memorization properties of large and deep ConvNets in tandem with trying to develop procedures related to identifying whether an input to a trained ConvNet has actually been used to train the network.", "The latter task is generalized to detecting if a particular dataset has been used to train a ConvNet. These goal are tackled empirically with multiple sets of experiments on largescale datasets such as ImageNet22k and modern deep ConvNets architectures such as VGG and ResNet.", "Paper's positive points", "+ The paper has a very comprehensive set of references in the areas it touches upon.", "+ Some of the experimental results presented are quite interesting. They show that regularization data-augmentation helps prevent a network from explicit memorization and could be used as a way to help make training data more anonymous.", "+ Large scale experiments are reported on modern architectures.", "Paper's negative points", "- The paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. If I'm not mistaken, from this result the authors extrapolate that the capacity of a (deep) neural network is proportional to the number of parameters in the network.", "This is true, but there are a couple of caveats.", "The first is that the coefficient of proportionality must depend very much on the number of layers in the network.", "Increasing the network's depth increases the efficiency of the representation (i.e. fewer total parameters needed to have the same representational power as a shallow network). And as MacKay also says in his book (chapter 44 quoting findings from Radford Neal) that for MLPs what determines the complexity of the typical function (once the network has a large enough width) represented by the MLP is the \"characteristic magnitude of the weights\". So the regularization technique applied is very significant in the controlling the effective capacity of a network.", "This paper experimentally shows that is the case multiple times as it is shown that with increasing degrees of regularization (figure 1, figure 2) it becomes harder and harder to memorize the positive training", "images. It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.", "- There is a slight oxymoron in the premise of the first set of experiments. The network is forced to memorize a set of positive examples relative to the negative set it sees during training. What is memorized I presume depends a lot on the negative set used for training (its diversity, closeness to the positive set and how frequently each negative example is seen during training).", "This issue is not really commented upon in the paper. Is there a training task which would allow one to more explicitly memorize the image (some sort of reconstruction task) as opposed to an in/out classification task?", "- This paper is a slightly difficult read - not because of the language or the presentation of the material but more because there is not one main coherent argument or goal for the paper. This is reflected in the \"Related work\" section where 4 different issues/tasks are referred to.", "Each one of these topics is worthy of a paper in itself, but this paper dips into each one and then swiftly moves onto the next one. For example in section 3 the paper explores if a network can be forced to explicitly memorize a set of images and how the size of this set is affected by the number of parameters in the network and data augmentation.", "High-level conclusions are made: more parameters in the network implies more images can be memorized and data-augmentation makes explicit memorization more difficult. Then it is off to considering pre-trained networks and determining whether by analyzing the statistics of the responses at different layers one can decide if a set of images was used for training or not (or similar tasks). Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.", "- The conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author? If yes, would it better to re-organize the paper and devote more of it to the material presented in section 3 and filling this out with more analysis and experiments to perhaps explore the issue of the capacity of a network in more", "Queries/ points that need some clarification", "- I'm a little unclear when data-augmentation is included in the training phase whether the goal is to be able to also recognise perturbed versions of the input images at test time. In section 3 is a perturbed positive image considered a positive training image? And in the testing phase are only unperturbed versions of the positive images given to the ConvNet as input?", "- Last paragraph page 4: \"when the accuracy gets over 60\\% and again at 90\\%\". Is this training or validation accuracy?", "Typos possible errors spotted along the way:", "* First paragraph page 5: \"more shallow\" --> \"shallower\"", "* Page 7, first paragraph of section 5.: \"is ran\" --> \"is run\"", "* Using \"scenarii\" for the plural of \"scenario\" I would say is pretty non-standard and most people would use \"scenarios\""], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 26, "sentences": ["The authors propose in this paper an approach for learning models with tractable approximate posterior inference.", "The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described.", "From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC).", "Concerning the experimental section:", "- The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution.", "However, I do not understand how are the *discrete* output y is handled.", "Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice?", "- The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem.", "- For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn\u2019t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better.", "I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments.", "The method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method.", "However, the real-world experiments are not necessarily the easiest to read.", "EDIT: the concerns were mostly addressed in the revision."], "labels": ["arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "arg-request_explanation", "arg-request_clarification", "none", "none", "arg-request_clarification", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 27, "sentences": ["In this paper, the authors address representation learning in non-Euclidean spaces.", "The authors are motivated by constant curvature geometries, that can  provide a useful trade-off between Euclidean representations and Riemannian manifolds, i.e. arriving at more suitable representations than possible in the Euclidean space, while not sacrifising closed-form formulae for estimating distances, gradients and so on.", "The authors point out that an extension of the gyrovector space formalization to spaces of constant positive curvature (spherical) is required, and with the corresponding formalization for hyperbolic spaces, one can arrive at a unified formalism that can interpolate smoothly between all geometries of constant curvature.", "The authors propose to do so by replacing  the curvature while flipping the sign in the standard Poincare model.", "This is a strong point reagarding this work, as it seems that no such unification has been attempted in the past (although simply replacing the curvature in the Poincare model seems a bit too straightforward to not have been attempted, it seems to be the case).", "The authors also provide extensive supplementary material (around 20 pages) with detailed derivations and descriptions of experiments.", "This also makes me wonder if this paper is more suitable for a journal - both in terms of the extensive supplementary material (e.g., curvature sampling algorithm and other details can be found only in supplementary), as well as the more rigorous review process that a journal paper goes through.", "In the main paper, only proof of concept experiments are provided (one experiment), that nevertheless show competitive performance under varying settings.", "However, it seems to me that such contributions in the rising field of geometric deep learning, where several challenges are yet to be overcome, can be beneficial for future research.", "Since in the supplementary experiments also, it seems that curvature does have small variance in the results, how would the authors assess the robustness of the curvature sampling method with respect to the results?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_edit", "arg-structuring_summary", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_edit_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 28, "sentences": ["I take issue with the usage of the phrase \"skill discovery\".", "In prior work (e.g. VIC, DIAYN), this meant learning a skill-conditional policy.", "Here, there is only a single (unconditioned) policy, and the different \"skills\" come from modifications of the environment -- the number of skills is tied to the number of environments.", "This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.", "Skill discovery in this context implies being able to have a single agent execute a variety of learned skills, rather than having one agent per environment with each environment designed to elicit a specific skill.", "Rather than \"skill discovery\", I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1}).", "Modifying the objective to incorporate domain knowledge (as done in your DIAYN baseline) yields I(a; s_i | s_{t-1}) and is amenable to maximization by either of the lower bounds considered here.", "Indeed, your DIAYN baseline with skill length set to 1 and the number of skills equal to the number of actions (or same parameterization in the case of continuous actions) should recover this approach.", "I believe this would be a much more appropriate baseline, and I'd be curious to hear the intuition for why I(s_c ; s_i) should be superior.", "Apart from this missing baseline, the experimental results seem convincing.", "However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction).", "Indeed, an appendix would be greatly appreciated, as many experimental details were omitted.", "Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn't done as incorporating domain knowledge is the main contribution of the paper.", "That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned).", "Rebuttal EDIT:", "The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.", "Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non-trivial to specify or learn for harder problems (e.g. pixel observations).", "That said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work.", "The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would've also been nice, the experimental results pass the bar for acceptance in my view."], "labels": ["none", "none", "none", "none", "none", "arg-request_edit", "none", "none", "arg-request_result", "none", "arg-request_clarification", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 29, "sentences": ["This paper presents a world model-based approach in which behaviours are optimised by rollouts (i.e. imagination) in latent space.", "The paper achieves impressive results across a large selection of tasks, both in terms of sample efficiency and final performance.", "I found the paper interesting to read and well written.", "The main contribution (backpropagating analytic gradients through imagined trajectories?) could potentially be highlighted more but otherwise the paper was clear.", "I wonder if the authors ever looked at how much the size of the latent vector determines the performance of the system?", "Is there an optional latent vector size across domains or is that optimal size task dependent?", "Additionally, how much variance is there in the imagined trajectories from a certain starting state? In other words, are the endpoints of most imagined trajectories similar or very different?", "There is actually not too much for me to critique and I would suggest this paper should be accepted.", "Minor comment:", "- On page 2 it says \u201cWe approach this limitation in latenby\u201d, which I assume is a typo?", "### #After rebuttal# # ##", "The authors' response addressed my remaining questions."], "labels": ["arg-structuring_summary", "none", "none", "arg-request_edit", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_typo", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 30, "sentences": ["The paper presents a maximally expressive parameter-sharing scheme for hypergraphs, and in general when modeling the high order interactions between elements of a set.", "This setting is further generalized to multiple sets.", "The paper shows that the number of free parameters in invariant and equivariant layers corresponds to the different partitioning of the index-set of input and output tensors.", "Experimental results suggest that the proposed layer can outperform existing methods in supervised learning with graphs.", "The paper presents a comprehensive generalization of a recently proposed model for interaction across sets, to the setting where some of these sets are identical.", "This is particularly useful and important due to its applications to graphs and hyper-graphs, as demonstrated in experiments.", "Overall, I enjoyed reading the paper. My only concern is the experiments:", "1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al\u201918 and references in there) are missing.", "2) Applying the model of Hartford et al\u201918 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation. (In both cases the equivariance group of data is a strict subgroup of the equivariance of the layer.)", "Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 31, "sentences": ["This paper proposed to use dropout to randomly choose only a subset of neural network as a potential way to perform exploration.", "The dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration.", "The paper shows that with small amount of Gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments.", "And it can significantly outperform vanilla PPO for environments with sparse rewards.", "The paper is clearly written.", "The introduced technique is interesting.", "I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration.", "I feel that it is a straightforward extension/generalization of the parameter space exploration. But the stochastic alignment and policy space constraint seem novel and important.", "The motivation of this paper is mostly about learning with sparse reward.", "I am curious whether the paper has other good side effects.", "For example, will the dropout cause the policy to be more robust?", "Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.", "In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.", "Overall, I like this paper. It is well written.", "The method seems technically sound and achieves good results.", "For this reason, I would recommend accepting this paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_experiment", "none", "none", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 32, "sentences": ["Summary:", "The authors present a video prediction model called SAVP that combines a Variational Auto-Encoder (VAE) model with a Generative Adversarial Network (GAN) to produce more realistic and diverse future samples.", "Deterministic models and certain loss functions such as Mean Squared Error (MSE) will produce blurry results when making uncertain predictions.", "GAN predictions on the other hand usually are more visually appealing but often lack diversity, producing just a few modes.", "The authors propose to combine a VAE model with a GAN objective to combine their strengths: good quality samples (GAN) that cover multiple possible futures (VAE).", "Strengths:", "[+] GANs are notoriously unstable to train, especially for video.", "The authors formulate a VAE-GAN model and successfully implement it.", "Weaknesses:", "[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).", "[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts. For example, the experiment of Figure 5 does not show SAVP being significantly more diverse than GANs for KTH (as compared to VAEs).", "Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.", "While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better. Since a direct application of video prediction is model-based planning, it seems that plausibility might be as important as sample quality.", "This work proposes to combine VAEs and GANs in a single model to get the benefits of both models.", "However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results.", "While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.", "In order to better assess this model and compare it to its individual parts and other VAE models, could the authors:", "1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)?", "2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects \u2013 implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_explanation_label"]}
{"abstract_id": 33, "sentences": ["The paper attempts to clarify the debate on large-batch neural network training, particularly on the relationship between learning rate, batch sizes and test performance.", "The authors claim two contributions towards understanding how the hyper-parameters of SGD affect final training and test performance: (1) SGD exhibits two regimes with different behaviours and (2) large-batch training leads to degradation of test performance even with same step budgets.", "Overall, the authors did a comprehensive study on large-batch training with the support of extensive experiments.", "But I'm concerned with the novelty and contributions of this paper.", "I tend to reject this paper because (1) the first contribution of the paper is not new as it has already been recognized by a few paper that SGD exhibits two different regimes; (2) this paper makes the debate of large-batch training even muddier.", "Main argument:", "The paper does not do a great job in clarify the debate.", "Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.", "For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.", "Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar.", "The only new observation I'm aware of in these two sections is that the training loss and test accuracy are independent of batch size in the noise dominated regime.", "Back to introduction section, the goal of this paper (as claimed in the beginning of second paragraph) is to clarify the debate.", "But does this paper really achieves this goal?", "In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).", "In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.", "I think the authors should instead focus on the discussion of generalization performance and the observation that training loss and test accuracy are independent of batch size in noise dominated regime.", "To my knowledge, this part is novel and interesting.", "In summary, I'm inclined to reject this paper given the current version.", "However, I think the paper is still worth reading if the authors can reorganize the paper and I might increase my score if my concerns get resolved."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 34, "sentences": ["*Summary*", "This paper study the convergence of Hamiltonian gradient descent (HGD) on minmax games.", "The paper show that under some assumption on the cost function of the min max that are (in some sense) weaker than strong convex-concavity.", "More precisely, they use the \u2018bilinearity\u2019 of the objective (due to the interaction between the players) to prove that the squared norm of the vector field of the game follows some Polyak Lojasiewicz condition.", "Thus the proof is concluded by the linear (resp. sublinear) convergence of gradient descent (resp. stochastic GD) under PL assumption.", "*Decision*", "I think that is work is clearly very interesting.", "The fact to prove linear convergence rate without strong-convex-concavity is quite surprising. And this paper brings nice tools to analyse HGD.", "Also the result on Stochastic HGD is very interesting.", "However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).", "One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.", "Regarding the practical limitation of this work:", "- the sufficient bilinearity condition are hard to meet in practice. (even for convex-concave problems)", "- In a non-convex-concave setting, Hamiltonian gradient descent is attracted the any stationary point, even \u201clocal maxima\u201d (or the equivalent in the minmax setting).", "Making this algorithm not very practical. (However, CO is)", "However, I really think that the community is currently lacking of understanding on minmax optimization and that we need better training method in many practical emergent frameworks that are minmax (such as GANs or multi agent learning). That is why, I would vote for a weak accept.", "*Questions*", "- What are the practical implication of your work ? for instance does it say anything on how to tune $\\gamma$ for CO ?", "*Remarks*", "- It is claimed that Theorem 3.4 gives the first linear convergence rate for minmax that does not require strong-convex or linearity.", "Note that, recently [1] seem to propose a result on extragradient in the same vein (i.e. without strong convexity or linearity).", "- (Minor) $\\alpha$ not alway have the same unit: Thm 3.2 it is proportional to a strong convexity and in Lemma 4.7 it is proportional to a strong convexity squared (actually the PL of the squared norm of the gradient).", "For clarity it might be interesting to use the notation $\\alpha^2$ in Lemma 4.7.", "The same way for unit consistency I would use $L_H^2$ instead of $L_H$", "[1] Azizian, Wa\u00efss, et al. \"A Tight and Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games.\" arXiv preprint arXiv:1906.05945 (2019).", "=== After rebuttal = ==", "I've read the authors's response.", "The concern raised by reviewer 3 is very important.", "The descent lemma used by the author is not valid for the stochastic result.", "The authors should address that in their revision.", "I however maintain my grade."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-structuring_heading", "arg-structuring_quote", "none", "arg-request_edit", "arg-request_edit", "arg-request_edit", "none", "arg-structuring_heading", "none", "arg-structuring_quote", "none", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-structuring_quote_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 35, "sentences": ["The paper builds upon Deep Image Prior (DIP) - work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc.", "The paper proposes a new architecture for the DIP method which has much less parameters, but works on par with DIP.", "Another contribution of the paper is theoretical treatment of (a simplified version of) the proposed architecture showing that it can\u2019t fit random noise (and thus maybe better suited for denoising).", "The paper is clearly written, and the proposed architecture has too cool properties: it\u2019s compact enough to be used for image compression; and it doesn\u2019t overfit thus making early stopping notnesesary (which was crucial for the original DIP model).", "I have two main concerns about this paper.", "First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture.", "Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with \u201cThe network is not learned and itself incorporates all assumptions on the data.\u201d).", "My second concern is about the theoretical contribution.", "On the one hand, I enjoyed the angle the authors tackled proving that the network architecture is underparameterized enough to be a good model for denoising.", "On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.", "Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).", "This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.", "Also, the theorem only applies to the iid noise, while most natural noise patterns have structure (e.g. JPEG artifacts, broken pixels, etc) and thus can probably be better approximated with deep models.", "Since the paper manages to use very few parameters (BTW, how many parameters in total do you have? Can you please add this number to the text?), it would be cool to see if second order methods like LBFGS can be applied here.", "Some less important points:", "Fig 4 is very confusing.", "First, it doesn\u2019t label the X axis.", "Second, the caption mentions that early stopping is beneficial for the proposed method, but I can\u2019t see it from the figure.", "Third, I don\u2019t get what is plotted on different subplots.", "The text mentions that (a) is fitting the noisy image, (b) is fitting the noiseless image, and (c) is fitting noise. Is it all done independently with three different models?", "Then why does the figure says test and train loss? And why DIP loss goes up, it should be able to fit anything, right? If not and it\u2019s a single model that gets fitted on the noisy image and tested on the noiseless image, then how can you estimate the level of noise fitting? ||G(C) - eta|| should be high if G(C) ~= x.", "Also, in this quote \u201cIn Fig. 4(a) we plot the Mean Squared Error (MSE) over the number of iterations of the optimizer for fitting the noisy astronaut image x + \u03b7 (i.e., FORMULA ...\u201d the formula doesn\u2019t correspond to the text.", "And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.", "I don\u2019t get the details of the batch normalization used: with respect to which axis the mean and variance are computed?", "The authors claim that the model is not convolutional.", "But first, it\u2019s not obvious why this would be a good thing (or a bad thing for that matter) .", "Second, it\u2019s not exactly correct (as noted in the paper itself): the architecture uses 1x1 convolutions and upsampling, which combined give a weak and underparametrized analog of convolutions.", "> The deep decoder is a deep image model G: R N \u2192 R n, where N is the number of parameters of the model, and n is the output dimension, which is typically much larger than the number of parameters (N << n).", "I think it should be vice versa, N >> n", "The following footnote", "> Specifically, we took a deep decoder G with d = 6 layers and output dimension 512\u00d7512\u00d73, and choose k = 64 and k = 128 for the respective compression ratios.", "Uses unintroduced (at that point) notation and is very confusing.", "It would be nice to have a version of Figure 6 with k = 6, so that one can see all feature maps (in contrast to a subset of them).", "I\u2019m also wondering, is it harder to optimize the proposed architecture compared to DIP?", "The literature on distillation indicates that overparameterization can be beneficial for convergence and final performance."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_clarification", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_clarification", "arg-request_edit", "arg-request_clarification", "none", "arg-request_clarification", "none", "arg-request_edit", "none", "none", "arg-request_typo", "arg-structuring_heading", "none", "none", "arg-request_edit", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_clarification_label", "none_label", "arg-request_clarification_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_clarification_label", "none_label"]}
{"abstract_id": 36, "sentences": ["The paper proposed variational selective autoencoders (VSAE) to learn from partially-observed multimodal data.", "Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.", "See below for detailed comments.", "[Pros]", "1. The main idea of the paper is to propose a generative model that can handle partially-observed multimodal data during training.", "Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.", "Especially in the field of multimodal learning, we often face the issue of imperfect sensors.", "This line of work should be encouraged.", "2. In my opinion, the idea is elegant.", "The way the author handles the missingness is by introducing an auxiliary binary random variable (the mask) for it.", "Nevertheless, its presentation and Figure 1 makes this elegant idea seems over-complicated.", "[Cons]", "1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.", "Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.", "It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?", "One possible explanation is because we want to handle the partially-observable issues from multimodal data, and it would be easier to make the latent factors factorized (see Eq. (6)).", "The author should comment on this.", "2. [Phrasing.] There are too many unconcise or informal phrases in the paper.", "For example, I don't understand what does it mean in \"However, if training data is complete, ..... handle during missing data during test.\" Another example would be the last few paragraphs on page 4; they are very unclear.", "Also, the author should avoid using the word \"simply\" too often (see the last few paragraphs on page 5).", "3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.", "I list some instances here.", "a. In Eq. (3), it surprises me to see the symbol \\epsilon without any explanation.", "b. In Eq. (6), it also surprises me to see no description of \\phi and \\psi.", "The author should also add more explanation here, since Eq. (6)  stands a crucial role in the author's method.", "c. Figure 1 is over-complicated.", "d. What is the metric in Table 1 and 2?", "The author never explains. E.g., link to NRMSE and PFC to the Table.", "e. What are the two modalities in Table 2? The author should explain.", "f. The author completely moved the results of MNIST-SVHN to Supplementary.", "It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.", "g. The author mentioned, in Table , the last two rows serve the upper bound for other methods.", "While some results are even better than the last two rows.", "The author should explain this.", "h. Generally speaking, the paper does require a significant effort to polish Section 3 and 4.", "4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.", "First, I consider the tabular features as multi-feature data and less to be the multimodal data.", "Second, the synthetic image pairs are not multimodal in nature.", "These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.", "The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.", "Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.", "I do expect the paper be a strong submission after a significant effort in presentation and experimental designs.", "Therefore, I vote for weak rejection at this moment."], "labels": ["arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "none", "arg-request_explanation", "none", "none", "arg-request_edit", "none", "arg-structuring_heading", "none", "none", "arg-request_edit", "none", "arg-request_explanation", "none", "arg-request_clarification", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_clarification_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 37, "sentences": ["The paper deals with the problem of recovering an exact solution for both the dictionary and the activation coefficients.", "As other works, the solution is based on a proper initialization of the dictionary.", "The authors suggest using Aurora 2015 as a possible initialization.", "The contribution improves Arora 2015 in that it converges linearly and recovers both the dictionary and the coefficients with no bias.", "The main contribution is the use of a IHT-based strategy to update the coefficients, with a gradient-based update for the dictionary (NOODL algorithm).", "The authors show that, combined with a proper initialization, this has exact recovery guaranties.", "Interestingly, their experiments show that NOODL converges linearly in number of iterations, while Arora gets stuck after some iterations.", "I think the paper is relevant and proposes an interesting contribution.", "The paper is well written and the key elements are in the body.", "However, there is a lot of important material in the Appendix, which I think may be relevant to the readers.", "It would be nice to have some more intuitive explanations at least of Theorem 1.", "Also, it is clear in the experiments the superiority with respect to Arora in terms of iterations (and error), but what about computational time?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_clarification_label"]}
{"abstract_id": 38, "sentences": ["The authors propose three improvements to the DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links --- and show that this allows the model to solve synthetic memory tasks faster and with better precision.", "They also show the model performs better on average on bAbI than the original DNC.", "The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.", "I think the paper is particularly clearly written, and I would vote for it being accepted as it has implications beyond the DNC.", "The fact that masked attention works so much better than the standard cosine-weighted content-based attention is pretty interesting in itself.", "The insights (e.g. Figure 5) are interesting and show the study is not just trying to be a benchmark paper for some top-level results, but actually cares about understanding a problem and fixing it.", "Although most recent memory architectures do not seem to have incorporated the DNC's slightly complex memory de-allocation scheme, any resurgent work in this area would benefit from this study."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 39, "sentences": ["Summary:", "The manuscript proposes a multi-domain adversarial learning (MDL) method called MULANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting.", "The authors define a new discrimination task to discriminate, within each domain, labeled samples from unlabeled ones that most likely belong to extra classes (classes with no labeled or unlabeled samples in the domain).", "They also introduce a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence.", "Strengths:", "-\u00a0The idea of using discriminators for separating the labeled samples from unlabeled ones that most likely belong\u00a0to extra classes is interesting.", "-\u00a0A new generalization bound for MDL is introduced.", "-\u00a0The paper was clear, well written, well-motivated and nicely structured.", "-\u00a0The authors perform numerous empirical experiments on several types of problems on various datasets (Digit, OFFICE,CELL) successfully showing how the MULANN can reduce the nasty effects of the adversarial domain\u00a0discriminator and repulse (a fraction of) unlabeled examples from labeled ones in each domain.", "Weaknesses:", "-\u00a0all the experiments except the last row of Table 2 concern adaptation between two domains.", "Given the paper title, the reviewer would have expected more experiments in a multiple domain context.", "More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.", "Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.", "-\u00a0The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier.", "Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples (e.g. the classifier may output high entropy for the unlabeled samples of the classes with labeled samples) and they may harm the performance of adaptation.", "It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.", "-\u00a0Although the paper introduces the generalization bound for MDL, it does not give new formulation or algorithm to handle MDL (MULANN handles only the class asymmetry when domains involve distinct sets of classes and it has nothing to do with MDL).", "hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.", "-", "Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).", "The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.", "[1]\u00a0Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\"\u00a0Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017.", "[2]", "Bousmalis, Konstantinos, et al. \"Domain separation networks.\"\u00a0Advances in Neural Information Processing Systems. 2016.", "[3]\u00a0Zhao, Han, et al. \"Multiple Source Domain Adaptation with Adversarial Learning.\" Advances in Neural Information Processing Systems. 2018.", "[4]\u00a0Hoffman, Judy, Mehryar Mohri, and Ningshan Zhang. \"Algorithms and Theory for Multiple-Source Adaptation.\"\u00a0\u00a0Advances in Neural Information Processing Systems. 2018."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 40, "sentences": ["The paper proposed a hierarchical framework for problem embedding and intended to apply it to adaptive tutoring.", "The system first used a rule-based method to extract the concepts for problems and then learned the concept embeddings and used them for problem representation.", "In addition, the paper further proposed negative pre-training for training with imbalanced data sets to decrease false negatives and positives.", "The methods are compared with some other word-embedding based methods and showed 100% accuracy in a similarity detection test on a very small dataset.", "In sum, the paper has a very good application but not good enough as a research paper.", "Some of the problems are listed as follows:", "1.\tLack of technical novelty.", "It seems to me just a combination of several mature techniques.", "I do not see much insight into the problem.", "For example, if the rule-based concept extractor can already extract concepts very well, the \u201cproblem retrieval\u201d should be solved by searching with the concepts as queries.", "Why should we use embedding to compare the similarity?", "Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.", "2.\tData size is too small, and the baselines are not state-of-the-art. There are some unsupervised sentence embedding methods other than the word-embedding based models.", "Some clarity issues.", "For example, Page 6. \u201cis pre-trained on a pure set of negative samples\u201d\u2014 what is the objective function? How to train on only negative samples?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "arg-structuring_heading", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label"]}
{"abstract_id": 41, "sentences": ["This is an novel, interesting paper on an important topic: semi-supervised learning.", "Even though the proposed approach seems to have significant potential, the experimental is somewhat disorganized,", "and it also includes some weak claims that should be removed.", "For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).", "In this reviewer's opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.", "In 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?", "The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for \"stratified SSL.\" Without this extra work, your claim is just a conjecture.", "You should also show the performance of regular SSL methods in the setup on Table 4.", "Last but not least, you have repeatedly made the claim combining SST and other SSL may further improve the performance;", "however, you do not provide any evidence for it, so you should avoid making such claims.", "Other comments:", "- on page 2, the two terms classification & selection network appear \"out of the blue;\" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.", "- figures 2 & 3 should be a lot larger in order to be readable", "- 4.1.2 top of page 7: claims such as \"SST could have obtained better performance\" have no place in such a paper; you could instead make a note about the method being \"prohibitively CPU intensive for the time being\"", "- lower on the same page you say: \"SST may get better performance\" - see above"], "labels": ["none", "none", "arg-request_edit", "arg-request_edit", "arg-request_result", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_clarification", "arg-request_edit", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 42, "sentences": ["This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks.", "In general, the whole paper tries to tell a very interesting, and good story.", "The paper is very well organized and written.", "However, I have the following concerns.", "1, the ME problem is quite similar to the concept ontology, e.g. , a \u201cDalmatian,\u201d a \u201cdog\u201d, or a \u201cmammal\u201d.", "So what\u2019s the key difference? Hierarchical learners can avoid this problem.", "2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning.", "The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian.", "It\u2019s probably a bit unfair or misleading to claim neural networks suffering from ME bias.", "3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments.", "Please give more explanations.", "4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues?", "5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN , the ME bias will be solved.", "Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning.", "6, the experimental design of Sec. 4.2 is also a bit unfair.", "It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this  task. Of course, this common NNs can not address it.", "----", "I read the rebuttal. the authors clarified and answered the questions. I would like to raise the score."], "labels": ["arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "none", "none", "none", "arg-request_clarification", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 43, "sentences": ["The paper investigates the possibility of learning a model to predict the training behaviour of deep learning architectures from hyperparameter information and a history of training observations.", "The model can then be used by researchers or a reinforcement learning agent to make better hyperparameter choices.", "The paper first adapts the Transformer model to be suitable to this prediction task by introducing a discretization scheme that prevents the transformer decoder's predictions from collapsing to a single curve.", "Next, the problem is formalized as a partially-observable MDP with a discrete action set, and PPO and SimPLe are introduced.", "The proposed model-based method is compared against a human and a model-free baseline training a Wide ResNet on CIFAR-10.", "The model-based method achieves better validation error than the other baselines that use actual data.", "Next, the method is compared against a human and a model-free baseline training Transformer models on the Penn Treebank dataset.", "While the human achieves the best performance at the end of the run, the proposed method appears to learn more quickly than the others and finishes with performance comparable to the model-free baseline.", "Currently I lean towards accepting this paper for publication, despite a few issues.", "It asks an interesting question: can we learn a model of the training dynamics to avoid actually having to do the training?", "This could potentially prevent a lot of unnecessary computation and also lead to better-performing models.", "It then shows some experimental evidence suggesting that this is possible.", "Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:", "1. In the PTB experiment, it looks like the human only adapts the learning rate and leaves the rest of the hyperparameters alone.", "Why was this policy used as the baseline? It seems extremely basic and unlikely to truly lead to optimal performance.", "2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.", "3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?", "4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?", "5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "none", "none", "arg-request_explanation", "none", "none", "arg-request_result", "none", "arg-request_clarification", "arg-request_result", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 44, "sentences": ["Quality: A simple approach accompanied with a theoretical justification and large number of experimental results.", "The theoretical justification is spread out in the main body and appendices.", "The proof given in the appendix is overly short and not detailed enough.", "The large number of experiment although welcoming needs to be properly discussed and related to the state of the art numbers, including any work that the authors are referring themselves in this submission.", "The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.", "Clarity: The simple approach is clearly described.", "However, the theoretical justification and experimental results are not.", "Originality: The work is moderately original.", "Significance: It is hard to assess given the current submission."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_edit", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 45, "sentences": ["The authors of this paper are proposing a neural network approach for learning diffusion dynamics in networks.", "The authors argue that the main advantage of their framework is that it incorporates the structure of independent cascades into the model which predicts the diffusion process.", "The primary difficulty in reviewing this paper is the poor presentation of the paper.", "There are many typos and mistakes (e.g., the last paragraph of the paper starts with a sentence that does not make any sense), missing references (e.g., there is an empty parenthesis at the end of the second paragraph on the second page) and in at least two cases, there are references to a formula that is not in the manuscript (e.g., reference to formula 15 on line 3 of page 5).", "This issues makes reviewing this paper very difficult.", "In the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, where p(I|D) is the conditional probability that a particular node infected an observed infected node first.", "Plugging p(I|D) in Eq. 12 and using decomposition of p(D ,I) used in Eq. 10, we arrive at a formulation which drops all p(I|D) terms.", "This results in an objective function which only involves infected nodes (and no term associated with the parent node), weighted by likelihood of each node j infecting the node at step i. This should make the training more simplified than what is discussed in Algorithm 2.", "Beyond this simplification, I am not clear if that is actually intended by the authors.", "The experiments demonstrate a superior performance of the proposed model compared to alternative benchmarks.", "The authors explain how they trained their own model but there is no mention on how they trained benchmark models.", "However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.", "Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_quote", "arg-structuring_quote", "none", "arg-request_clarification", "arg-structuring_quote", "arg-request_clarification", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_quote_label", "arg-structuring_quote_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 46, "sentences": ["Though rather dense in its exposition, this paper is an interesting contribution to the area of self-supervised learning  based on discrete representations.", "What would make it stronger imo is to address the issue of how much is gained from a discrete vs. continuous representation.", "The authors take it as a given that discrete is good because it allows us to leverage work in NLP.", "That makes sense -- but at what cost?", "\"Table 4 shows that our first results are promising, even though they are not as good as the state of the art.\" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result", "The Conclusion is very sparse. \"In future work, we are planning to apply other algorithms requiring discrete inputs to audio data\":", "can  you elaborate?"], "labels": ["none", "arg-request_edit", "arg-structuring_quote", "arg-request_clarification", "none", "arg-structuring_quote", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-request_edit_label", "arg-structuring_quote_label", "arg-request_clarification_label", "none_label", "arg-structuring_quote_label", "arg-request_explanation_label"]}
{"abstract_id": 47, "sentences": ["= Summary", "Embeddings of mathematical theorems and rewrite rules are presented.", "An in-depth analysis of the resulting embeddings is presented, showing that a network can learn to \"apply\" embedded rewrite rules to embedded theorems, yielding results that are similar to the embedding of the rewritten theorem.", "[i.e., app'(emb(thm), emb(rule)) is near to emb(app(thm, rule))] This is an interesting property for the application of deep learning to automated theorem proving, though not directly a breakthrough result.", "= Strong/Weak Points", "+ Simply a cute result, showing that proof search can remain in embedding space for a limited time horizon without having to switch back into the theorem prover environment.", "+ Nicely designed experiments testing this (somewhat surprising) property empirically", "- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail", "- Writing sometimes a bit overcomplicated (e.g., Sect. 4.5 could just be a figure of a commuting diagram and two sentences...)", "- Architecture choice unclear: Why are $\\sigma$ and $\\omega$ separate networks.", "This is discussed on p4, but it's unclear to me how keeping $\\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?", "= Recommendation", "Overall, this is a nice, somewhat surprising result.", "The writing and experiments could use some improvement, but I believe that the majority of the ICLR audience would enjoy seeing this result (even though it would have no impact on most people's research)", "= Detailed Comments", "- page 4, Sect. 4.4: Architecture of $\\alpha$ would be nice (more than a linear layer?)", "- page 5, paragraph 3: \"we from some\" -> \"we start from some\"", "- p6par1: \"much cheaper then computing\" -> than", "- p6par6: \"on formulas that with\" -> no that", "- p6par7: \"measure how rate\" -> \"measure the rate\"", "- p8par1: \"approximate embedding $\\alpha(e(\\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.", "However, I don't understand the use of $\\alpha$ here.", "If Fig. 4 is following Fig. 3 in considering $p(c(\\gamma(T), \\pi(P)))$, then Fig. 4 should plot the performance of, e.g., $p(c(e'(c'(\\gamma'(T_{i-1}), \\pi'(P_{i-1}))), \\pi(P_i)))$ (i.e., $p$ applied to approximate embedding of $T_i$ and (\"true\") embedding of $P_i$).", "I believe that's what \"Pred (One Step)\" expresses, but it would maybe be generally helpful to be more precise about the notation in Sect. 6."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_edit", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 48, "sentences": ["The paper proposes 4d convolution and an enhanced inference strategy to improve the feature interaction for video classification.", "State-of-the-art performance is achieved on several datasets.", "However, there are still details and concerns.", "1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work", "2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?", "3. can you provide the training memory, inference speed, and total training time?", "4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.", "5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019."], "labels": ["arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_clarification", "arg-request_result", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 49, "sentences": ["Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds.", "The drawback of a vanilla GAN with a DeepSet classifier is analyzed.", "The rationality that decoupling the point generator with the object generator is also discussed.", "A sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance.", "Compared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.", "Comments:", "1. Authors calculate W_U in a primal form via solving an assignment programming problem.", "Have authors ever tried Sinkhorn iteration?", "To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively.", "It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U.", "2.", "Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function.", "Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 > epsilon2 - epsilon1 (according to the assumption in lemma 1).", "Does it mean that the W_s is not as stable as W_L or W_U during training?", "Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05.", "In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U?", "Authors should analyze the stability of their method in details.", "Essentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability.", "In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants.", "Typos:", "- The end of the 2nd line of lemma 1: P, G should be \\mathbb{P}, \\mathbb{G}", "- The 3rd line of lemma 1: epsilon1 -> epsilon_1", "- Page 14, Eq(14), \\lambda should be s", "- Page 14, Eqs(13, 14), w(\\mathbb{P}, \\mathbb{G}) should appear on the right."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_quote", "arg-request_clarification", "none", "arg-request_explanation", "arg-structuring_heading", "arg-structuring_quote", "arg-structuring_quote", "arg-request_clarification", "arg-structuring_quote", "arg-request_clarification", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_quote_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-structuring_quote_label", "arg-structuring_quote_label", "arg-request_clarification_label", "arg-structuring_quote_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 50, "sentences": ["# Weaknesses", "Applications are a bit unclear.", "It would be nice to see a better case made for spherical convolutions within the experimental section.", "The experiments on SHREC17 show all three spherical methods under-performing other approaches.", "It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.", "Is there a task that this representation significantly outperforms other spherical methods and non-spherical methods?", "Or a specific useful application where spherical methods in general outperform other approaches?", "# Strengths:", "The method is well developed and explained.", "Ability to implement in a straight-forward manner on GPU."], "labels": ["arg-structuring_heading", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 51, "sentences": ["This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems.", "The contributions are i) a publicly available dataset, and ii) an evaluation of two existing model families, recurrent networks and the Transformer.", "I think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results.", "I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset.", "The paper is a reasonable dataset/analysis paper.", "Whether to accept it or not depends on what standard ICLR has towards such papers (ones that do not propose a new model/new theory).", "I think that the dataset generation process is well-thought-out.", "There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion.", "The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.", "I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly.", "Details on how to generate the dataset, however, can be moved into the appendix.", "I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a \"soft\", secondary metric?", "One other thing I want to see is a test set with multiple different difficulty levels.", "The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)?", "Also, is there an option for \"unsolvable\"?", "For example, the answer being a special \"this is impossible\" character for \"factorise x^2 - 5\" (if your training set does not use \\sqrt, of course)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_edit", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 52, "sentences": ["*", "Description", "The work is motivated by the empirical performance of Batch Normalization and in particular the observed better robustness of the choice of the learning rate.", "Authors analyze theoretically the asymptotic convergence rate for objectives involving normalization, not necessarily BN, and show that for scale-invariant groups of parameters (appearing as a result of normalization) the initial learning rate may be set arbitrary while still asymptotic convergence is guaranteed with the same rate as the best known in the general case.", "Offline gradient descent and stochastic gradient descent cases are considered.", "* Strengths", "The work addresses better theoretical understanding of successful heuristics in deep learning, namely batch normalization and other normalizations.", "The technical results obtained are non-trivial and detailed proofs are presented.", "Also I did not verify the proofs the paper appears technically correct and technically clear.", "The result may be interpreted in the following form: if one chooses to use BN or other normalization, the paper gives a recommendation that only the learning rate of scale-variant parameters need to be set, which may have some practical advantages.", "Perhaps more important than the rate of convergence, is the guarantee that the method will not diverge (and will not get stuck in a non-local minimum).", "* Criticism", "This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.", "-- Concerns regarding the clarity of presentation and interpretation of the results.", "The properties of BN used as motivation for the study, are observed non-asymptotically with constant or empirically decreased learning rate schedules for a limited number of iterations.", "In contrast, the studied learning rates are asymptotic and there is a big discrepancy.", "SGD is observed to be significantly faster than batch gradient when far from convergence (experimental evidence), and this is with or without normalization.", "In practice, the training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.", "There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.", "It makes a nice story that the theoretical properties justify the observations, but they may be as well completely unrelated.", "As seen from the formal construction, the theoretical results apply equally well to all normalization methods.", "It occludes the clarity that BN is emphasized amongst them.", "Considering theoretically, what advantages truly follow from the paper for optimizing a given function? Let\u2019s consider the following cases. 1. For optimizing a general smooth function with all parameters forming a single scale-invariant vector.", "In this case, the paper proves that no careful selection of the learning rate is necessary.", "This result is beyond machine learning and unfortunately I cannot evaluate its merit.", "Is it known / not known in optimization?", "2. The case of data-independent normalization (such as weight normalization).", "Without normalization, we have to tune learning rate to achieve the optimal convergence.", "With normalization we still have to tune the learning rate (as scale-variant parameters remain or are reintroduced with each invariance to preserve the degrees of freedom), then we have to wait for the phase two of Lemma 3.2 so that the learning rate of scale-invariant parameters adapts, and from then on the optimal convergence rate can be guaranteed.", "3. The case of Batch Normalization.", "Note that there is no direct correspondence between the loss of BN-normalized network (2) and the loss of the original network because of dependence of the normalization on the batches.", "In other words, there is no setting of parameters of the original network that would make its forward pass equivalent to that of BN network (2) for all batches.", "The theory tells the same as in case 2 above but with an additional price of optimizing a different function.", "These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate.", "-- Difference from Wu et al. 2018", "This works is cited as a source of inspiration in several places in the paper.", "As the submission is a theoretical result with no immediate applicability, it would be very helpful if the authors could detail the technical improvements over this related work.", "Note, ICLR policy says that arxiv preprints earlier than one month before submission are considered a prior art. Could the authors elaborate more on possible practical/theoretical applications?", "* Side Notes (not affecting the review recommendation)", "I believe that the claim that \u201cBN reduces covariate shift\u201d (actively discussed in the intro) was an imprecise statement in the original work.", "Instead, BN should be able to quickly adapt to the covariate shift when it occurs.", "It achieves this by using the parameterization in which the mean and variance statistics of neurons (the quantities whose change is called the covariate shift) depend on variables that are local to the layer (gamma, beta in (1)) rather than on the cumulative effect of all of the preceding layers.", "* Revision", "I took into account the discussion and the newly added experiments and increased the score.", "The experiments verify the proven effect and make the paper more substantial.", "Some additional comments about experiments follow.", "Training loss plots would be more clear in the log scale.", "Comparison to \"SGD BN removed\" is not fair because the initialization is different (application of BN re-initializes weight scales and biases). The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017).", "Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.", "The use of Glorot uniform initializer is somewhat subtle. Since BN is used, Glorot initialization has no effect for a forward pass. However, it affects the gradient norm.", "Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1?"], "labels": ["none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_result", "none", "none", "arg-request_clarification", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_result", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_experiment", "none", "arg-request_experiment", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_clarification_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 53, "sentences": ["This work extends Schlichtkrull et al. (2018) by adding attention in two distinct ways: attention between pairs of nodes per relation, and attention between pairs of nodes averaged over all relations.", "The paper is well written and the equations easy to follow.", "The results are not strong. And, unfortunately, the model contribution currently is too modest.", "Inductive task results: Wu et al. (2018) reports that for Tox21 (Duvenauld et al. 2015) is the best-performing approach.", "We should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)).", "My introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs).", "Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.", "--- After rebuttal ---", "Still not convinced of the value of the work to the community. Will keep my score the same."], "labels": ["none", "none", "none", "none", "arg-request_experiment", "arg-request_edit", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 54, "sentences": ["Summary:", "The authors propose using non-Euclidean spaces for GCNs.", "This is inspired by the recent work into non-Euclidean, and especially hyperbolic, embeddings.", "A few papers have recently tried to go past embeddings into building non-Euclidean models, requiring the lifting of standard operations in Euclidean space to non-Euclidean settings.", "This has been done in particular in hyperbolic space, but some datasets benefit from more complex spaces.", "The authors combine the mixed-curvature product formalism that uses products of Euclidean, hyperbolic, and spherical spaces for embeddings, but use these for GCN operations.", "Doing this requires, in particular, developing a reasonable way to perform these operations in spherical space (since Euclidean is trivial and hyperbolic has been recently worked on).", "The authors do a nice lifting via complex operations, and both the hyperbolic and spherical spaces can devolve into the flat Euclidean space when their curvature goes to 0.", "The authors implement these GCNs, train the curvatures, and demonstrate performance improvements over Euclidean only versions on node classification on benchmark datasets.", "They also give a fairly nice introduction to all of these ideas in an extended appendix.", "Strengths, Weaknesses, Recommendation:", "This paper is reasonably interesting---it joins an effort to produce non-Euclidean models in a tractable way, which is fairly challenging, but could have a good impact.", "On the plus side, it's great that the authors added the nice development for the spherical operations, since that will come in handy for many models.", "The experiments are also good.", "On the downside, everything here is an extension of existing work, and the body of the paper is hard to read (though this may be inevitable, there's a lot of background to go over here).", "Overall I recommend accepting it; I think it's a solid contribution.", "Comments:", "- I don't understand why the authors say that their space \"interpolates smoothly\" just because the limit in the curvature is the same from the left and right side.", "For example, the absolute value function has the same limit from the left and the right at 0, but it's not differentiable there. Is it actually true that if we take the derivatives of the piecewise hyperbolic/spherical distance function that it's differentiable at c=0?", "- There are a couple of recent papers that also consider hyperbolic GCNs, and in fact use  similar ideas for the aggregation and update steps (i.e., same lift to hyperbolic space).", "However, these were recently NeurIPS papers, and the text is not yet out, so I don't think this should affect the authors' independent work (and also the product part is new).", "I do recommend that the authors compare against those results in a future update of this work.", "The papers are \"Hyperbolic Graph Convolutional Neural Networks\" by Chami et al and Hyperbolic Graph Neural Networks by Liu et al.", "- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.", "For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).", "As an example, consider S^2 and the mean of two antipodal points on it---there's many choices for the midpoint.", "You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).", "- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?", "- Are the curvatures the same for each layer for the GCNs?", "This is an interesting point to discuss (some of the NeurIPS papers I mentioned train the curvature for each layer).", "Also, how do you select the number of factors of each type?", "- Minor, but some of these citations can be updated.", "The \"De Sa\" et al 2018 arxiv citation is really Sala et al and is an ICML '18 paper.", "Similarly, Gulcehre et al is a 2019 ICLR paper, and so on. It's always good to get these right.", "- Is there any actual empirical importance from recovering the Euclidean case exactly for 0 curvature?", "The reason I ask is that my experience is that the hyperboloid is typically easier to work with .", "- One useful thing to point out in B.3.3 is that in general, it need not be a diffeomorphism for all of M for any manifold, which leads to non-uniqueness.", "In differential geometry, the \"cut locus\" is the region beyond which there is this non-uniqueness.", "- In the appendix, the statement \"Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees\" is confusing to me. The \"general class\", as far as I know, is actually *all* trees, weighted or unweighted."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-request_edit", "arg-request_explanation", "arg-request_clarification", "arg-request_experiment", "arg-request_clarification", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_explanation", "none", "arg-request_edit", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 55, "sentences": ["The paper devises a pipeline that aims to address catastrophic forgetting in continual learning (CL) by the well-known generative replay (GR) technique.", "The key ingredient of the pipeline is a modern variational auto-encoder (VAE) that is trained with class labels with respect to a mutual information maximization criterion.", "The paper does not follow a smooth story line, where an open research question is presented and a solution to this problem is developed in steps.", "The flowchart in Fig 1 is rather a system design consisting of many components, the functionality of which is not clearly described and existence of which is not justified.", "This complex flowchart does not even describe the complete task.", "It is in the end plugged into a continual learning algorithm which also performs domain transformation.", "All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.", "Hence, I kindly do not think the outcome is truly a research result.", "It is more system engineering than science.", "The next submission of the paper could choose one or few of these pieces as target research problems and develop a thoroughly analyzed novel technical solution for them.", "If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.", "Minor: The abstract could be improved by providing more clear pointers to the presented novelty."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 56, "sentences": ["The main contributions of this work are essentially on the theoretical aspects.", "It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.", "The authors need to describe in detail the algorithmic novelty of their work.", "The definition of \u201crecovering true factor exactly\u201d need to be given.", "The proposed algorithm involves several tuning parameters, when alternating between two updating rules, an IHT-based update for coefficients and a gradient descent-based update for the dictionary.", "Therefore, an appropriate choice of their values need to be given.", "In the algorithm, the authors need to define the HT function in (3) and (4).", "In the experiments, the authors compare the proposed method to only the one proposed by Arora et al. 2015.", "We think that this is not enough, and more extensive experimental results would provide a better paper.", "There are some typos that can be easily found, such as \u201cof the out algorithm\u201d."], "labels": ["arg-structuring_summary", "none", "arg-request_edit", "arg-request_edit", "none", "arg-request_edit", "arg-request_edit", "arg-structuring_quote", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-structuring_quote_label", "none_label", "arg-request_typo_label"]}
{"abstract_id": 57, "sentences": ["The paper shows that Bayesian neural networks, trained with Dropout MC (Gal et al.) struggle to fully capture the posterior distribution of the weights.", "This leads to over-confident predictions which is problematic particularly in an active learning scenario.", "To prevent this behavior, the paper proposes to combine multiple Bayesian neural networks, independently trained with Dropout MC, to an ensemble.", "The proposed method achieves better uncertainty estimates than a single Bayesian neural networks model and improves upon the baseline in an active learning setting for image classification.", "The paper addresses active deep learning which is certainly an interesting research direction since in practice, labeled data is notoriously scarce.", "However, the paper contains only little novelty and does not provide sufficiently new scientific insights.", "It is well known from the literature that combining multiply neural networks to an ensemble leads to better performance and uncertainty estimates.", "For instance, Lakshminarayanan et al.[1] showed that Dropout MC can produce overconfident wrong prediction and, by simply averaging prediction over multiple models, one achieves better performance and confidence scores.", "Also, Huand et al. [2] showed that by taking different snapshots of the same network at different timesteps performance improves.", "It would also be great if the paper could related to other existing work that uses Bayesian neural networks in an active learning setting such as Bayesian optimization [3, 4] or Bandits[5].", "Another weakness of the paper is that the empirical evaluation is not sufficiently rigorous:", "1) Besides an comparison to the work by Lakshminarayanan et. al, I would also like to have seen a comparison to other existing Bayesian neural network approaches such as stochastic gradient Markov-Chain Monte-Carlo methods.", "2) To provide a better understanding of the paper, it would also be interesting to see how sensitive it is with respect to the ensemble size M.", "3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.", "The same holds for the type of data, since the paper only shows results for image classification benchmarks.", "4) Figure 3: Are the results averaged over multiple independent runs? If so, how many runs did you perform and could you also report confidence intervals? Since all methods are close to each other, it is hard to estimate how significant the difference is.", "[1] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundel NIPS 2017", "[2] Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger Snapshot Ensembles: Train 1, get {M} for free} ICLR 2017", "[3] Bayesian Optimization with Robust Bayesian Neural Networks J. Springenberg and A. Klein and S.Falkner and F. Hutter NIPS 2016", "[4] J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams Scalable Bayesian Optimization Using Deep Neural Networks ICML 2015", "[5] Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling Carlos Riquelme, George Tucker, Jasper Snoek ICLR 2018"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 58, "sentences": ["This paper is clearly written and in an interesting domain.", "The question asked is whether or not pretrained mean-field RBMs can help in preventing adversarial attacks.", "However, there are some key issues with the paper that are not clear.", "The first is regarding the structure of the paper.", "The authors combine two ideas, 1: the training of MF RBMs and 2: the the ability to prevent adversarial attacks.", "The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.", "It would make the paper much stronger if the authors perform quantitative + qualitative evaluation on the MF training of RBMs first.", "Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).", "In a related note, using MF for training BMs have been proposed previously and found to not work due to various reasons:", "see paragraph after equation 8 of the Deep BM paper: http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf", "It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.", "It is also unclear how the calculation of relative entropy \"D\" was performed in figure 3.", "Obtaining the normalized marginal density in a BM is very challenging due to the partition function.", "The second part of the paper associate good performance in preventing adversarial attacks with the possibility of denoising by the pretrained BM.", "This is a very good point, however the paper do not compare or contrast with existing methods.", "For example, it is curious to see how denoising Auto encoders would perform.", "In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf", "- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.", "Defending against black box attacks is considerably easier than defending against white-box attacks.", "In summary, the paper is interesting, however, more experiments could be added to concretely demonstrate the advantage of the proposed MF BMs in increasing robustness against adversarial attacks."], "labels": ["none", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_summary", "none", "arg-request_explanation", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation", "none", "arg-structuring_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 59, "sentences": ["Summary:", "This paper proposes a suite of tricks for training large-scale GANs, and obtaining state-of-the-art results for high-resolution images.", "The paper starts from a self-attention GAN baseline (Zhang 2018), and proposes:", "-\tIncreasing batch size (8x) and model size (2x)", "-\tSplitting noise z in multiple chunks, and injecting it in multiple layers of the generator", "- Sampling from truncated normal distribution, where samples with norms that exceed a specific threshold are resampled.", "This seems to be used only at test-time and is used to control variety-fidelity tradeoff.", "The generator is encouraged to be smooth using an orthogonal regularization term.", "In addition, the paper proposes practical recipes for characterizing collapse in GANs.", "In the generator, the exploding of the top 3 singular values of each weight matrix seem to indicate collapse.", "In the discriminator, the sudden increase of the ratio of first/second singular value of weight matrices indicate collapse in GANs.", "Interestingly, the paper suggests that various regularization methods which can improve stability in GAN training, do not necessarily correspond to improvement in performance.", "Strengths:", "-\tProposed techniques are intuitive and very well motivated", "-\tOne of the big pluses of this work is that authors try to \"quantify\" each proposed technique with training speed and/or performance improvement. This is really a good practice.", "-\tDetailed analysis for detecting collapse and improving stability in large-scale GAN", "-\tProbably no need to mention that, but results are quite impressive", "Weaknesses:", "-\tComputational budget required is massive.", "The paper mentions model use from 128-256 TPUs, which severely limits reproducibility of results.", "Comments/Questions:", "-\tCan you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance?", "-\tIt is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs.", "Providing such analysis would be also helpful for the community.", "-\tHow do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?", "Overall recommendation:", "The paper is well written, ideas are well motivated/justified and results are very compelling.", "This is a good paper and I higly recommend acceptance."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_clarification", "arg-request_result", "arg-request_explanation", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 60, "sentences": ["Summary:", "This paper proposes a generative point cloud model based on adversarial learning and definitti\u2019s representation theorem of exchangeable variables.", "The main focus in experiments and the exposition is on 3D point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify).", "The main idea is to represent a point cloud using a global latent variable that captures the overall shape, and a collection of local latent variables that code for the position of a point on the shape.", "The model consists of thee components: (i) an \u201cencoder\u201d that takes a point cloud as input and maps it to a (point estimate of) the global latent variable of the shape represented by the input cloud, a point-net architecture is used here (ii) a \u201cdecoder\u201d that takes the estimated global latent variable, and a local latent variable, and maps it to an \u201coutput\u201d point in the cloud to be produced by the model. (iii) a \u201cdiscriminator\u201d network that aims to distinguish points from a *given* shape, and the points produced by pipe-lining the encoder and decoder. Critically different from conventional GANs, the discriminator is optimized *per shape*, ie each point cloud is considered as a *distribution* over R^3 specific to that shape. (iv) a \u201cshape prior\u201d that, once the encoder-decoder model from above is trained, is used to model the distribution over the global latent variables.", "This model is trained, presumably, in a conventional GAN style using the global latent variable representations inferred across the different training point clouds.", "As compared to prior work by Achiloptas et al (2017), the proposed approach has the advantage to allow for sampling an arbitrary number of points from the target shape, rather than a fixed pre-defined number.", "In addition, the authors propose to minimize a weighted average of a lower bound and upper bound on the Wasserstein distance between the distributions of points corresponding to given shapes. This approach translates to improved quantitative evaluation measures, Experiments are conducted on a simple toy data set, as  a proof of concept, and on data from ModelNet10 and ModelNet40.", "Two performance metrics are introduced to assess the auto-encoding ability of the model: to what extent does the encoder-decoder pipeline result in point clouds similar to the shape from which the input point-cloud is generated.", "Overall I find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work.", "The experimental validation of the proposed approach can also be further improved, see more specific comments below.", "Specific comments:", "- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.", "- The notation in section 3 (before 3.1) is rather sloppy. For example, - please define P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3. - it is not defined in which space theta lives, it is not clear what the authors intend with the notation G_theta(u) \\sim p(theta).", "- what prior distributions p(z) and p(u) are used? What is the choice based on?", "- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier.", "- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?", "- Lack of clarity in the following passage: \u201cIn our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images\u201d", "- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.", "- The following paper merits a discussion in the related work section:", "\u201cTOWARDS A NEURAL STATISTICIAN\u201d, ICLR\u201917,", "https://openreview.net/pdf?id=HJDBUF5le", "- The manuscript contains many typos. For example", "\u201cvedio\u201d op page 4, \u201ccircile\u201d on page 5, \u201ccondct\u201d on page 8, etc.", "Please proof read your paper and fix these.", "The refenence to  Bengio 2018 is incomplete: what do you refer to precisely?", "- There seems to be no mention of the dimension of the \u201clocal\u201d latent variables z_i.", "Please comment on the choice, and its impact on the behavior of the model.", "- The quantitative evaluation in table 1 is interesting and useful.", "It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.", "Quantitative evaluation of generative modeling performance is unfortunately missing from this paper, as it is in much of the GAN literature. Could you please comment on how this can/will be fixed?", "- The toy data set experiments could be dropped  to make room for experiments suggested below.", "- An experimental study of the effect of the mixing parameter \u201cs\u201d would be useful to include. For example, by taking s on a grid from 0 to 1, one could plot the coverage and distance-to-face measures.", "- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?", "- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we \u201cchop off\u201d part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts? This is potentially useful to practitioners which have to deal with incomplete point clouds acquired by range scanners.", "- Analysis of shapes with different genus and dimensions would be interesting.", "Does the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk), despite a simple prior on the local latent variables z?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_clarification", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_typo", "none", "arg-request_clarification", "none", "arg-request_explanation", "none", "none", "none", "arg-request_edit", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_result", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "none_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_explanation_label"]}
{"abstract_id": 61, "sentences": ["In this work the authors propose an extension of mixture density networks to the continuous domain, named compound density networks.", "Specifically the paper builds on top of the idea of the ensemble neural networks (NNs) and introduces a stochastic neural network for handling the mixing components.", "The mixing distribution is also parameterised by a neural network.", "The authors claim that the proposed model can result in better uncertainty estimates and the experiments attempt to demonstrate the benefits of the approach, especially in cases of having to deal with adversarial attacks.", "The paper in general is well written and easy to follow.", "I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.", "Let me elaborate.", "First of all, I don\u2019t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?", "I also find weird the way that the authors arrive to their final objective in Equation (5).", "They start from Equation (4) which is incorrectly denoted as the log-marginal distribution while it is the same conditional distribution introduced in Equation (3) with the extra summation for all the available data points.", "Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.", "However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\\theta) = p(\\theta | g(x_n; \\psi).", "Is there a reason why the authors do not introduce their objective by following the variational framework?", "Furthermore, in the beginning of Section 3.1 the authors present their idea on probabilistic hypernetoworks which \u201cmaps x to a distribution over parameters instead of specific value \\theta.\u201d How is this different from the case that we were considering so far? If we had a point estimate for \\theta we would not require to take an expectation in Equation (3) in the first place.", "My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).", "The Kronecker product between two diagonal matrices results in another diagonal matrix, i.e., diagonal covariance, which implies that the weights within a layer are given by an independent multivariate Gaussian.", "What is the purpose then for introducing the matrix variate Gaussian?", "I would expect that you would like to impose additional structure to the weights.", "I expect the authors to comment on that.", "Regarding the experimental evaluation of the model rather confusing.", "The authors have proposed a model that due to the mixing is better suited for predictions with heteroscedastic noise and can better quantify the aleatoric uncertainty.", "However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system\u2019s noise, i.e. epistemic uncertainty.", "The generative process of the toy data clearly states that there is no heteroscedastic noise to handle.", "The same applies for the notMNIST data which belong to a completely different data set compared to MNIST and thus out of sample prediction cannot benefit from the mixing; i.e., variations have to be explained by system\u2019s noise.", "So overall I have the feeling that the authors have not succeeded to evaluate the model\u2019s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.", "To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing.", "The plot by itself, as I understood, quantifies the model\u2019s uncertainty in in- and out-of sample prediction.", "While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.", "There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.", "Finally, it is unclear how the authors have picked the best \\lambda parameter for their approach? On page 5 they state that they \u201cpick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.\u201d Does this mean that you get to observe the performance in the test in order to select the appropriate value for \\lambda? If this is the case this is completely undesirable and is considered a bad practice."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 62, "sentences": ["The authors propose a notion of conductance to attribute the deep neural network\u2019s prediction to its hidden units.", "The conductance is the flow of attribution via the hidden unit(s) in consideration.", "The paper proposes using conductance to not only evaluate importance of hidden unit to the prediction for a specific input but also over a set of inputs.", "The strongest part of the analysis of conductance is that conductance naturally couples  the path at the base features with that of the hidden layer.", "The authors position their work well within the existing approaches in the community and generalizes the efficient use of measuring hidden activation wrt to specific input or set of inputs.", "The analysis makes efficient use of mean value theorem in the context of  parametrization of the loss function.", "Conductance seems to satisfy the completeness of hidden features.", "Further, it also satisfies the layer-wise conservation principle with the outputs completely redistributed  to the inputs.", "It would be good to see more analysis on the axioms 1 through to 4 for the sake of completeness in the light of partial axiomatization of conductance.", "The authors provide empirical evaluation of conductance over a variety of tasks.", "It would be good to see some more insight in order to relate to interpretability of the importance of neurons, although there has been no claims made on it as its hard to measure importance without interpretability."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 63, "sentences": ["This paper points out that the traditional way of model selection is flawed due to that the validation/test set is often small.", "The authors also attribute the existence of adversarial examples to the small validation/test set, which I agree to some degree.", "Hence, the authors proposed an alternative approach to comparing different classification models by the notion of inter-model discrepancy.", "The main idea is reasonable, but it requires that the models to compare all perform reasonably well.", "Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.", "Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.", "Another question which is not answered in the paper is the number $k$ of images to select for each pair of classifiers. Is this number task-dependent? Is it related to the number of classes?", "What is a general guideline for one to choose this number $k$ given a new application scenario?", "The unlabeled set is not \"unlabeled\" in essence. If my understanding was correct, it cannot contain open-set images which do not belong to any of the classes of interest.", "It is also nontrivial to control that the images contain only one salient object per image.", "Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_explanation", "arg-request_clarification", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 64, "sentences": ["The paper tackles the problem of semi-supervised classification using GAN-based models.", "They proposed a manifold regularization by approximating the Laplacian norm using the stochastic finite difference.", "The motivation is that making the classifier invariant to perturbations along the manifold is more reasonable than random perturbations.", "The idea is to use GAN to learn the manifold.", "The difficulty is that (the gradient of) Laplacian norm is impractical to compute for DNNs.", "They stated that another approximation of the manifold gradient, i.e. adding Gaussian noise \\delta to z directly (||f(z) - f(g(z+\\delta))||_F) has some drawbacks when the magnitude of noise is too large or too small.", "The authors proposed another improved gradient approximation by first computing the normalized manifold gradient \\bar r(z) and then adding a tunable magnitude of \\bar r(z) to g(z), i.e., ||f(z) - f(g(z) +\\epsilon \\bar r(z) )||_F. Since several previous works Kumar et al. (2017) and Qi et al. (2018) also applied the idea of manifold regularization into GAN, the authors pointed out several advantages of their new regularization.", "Pros:", "- The paper is clearly written and easy to follow. It gives some intuitive explanations of why their method works.", "- The idea is simple and easy to implement based on a standard GAN.", "- The authors conduct various experiments to show the interaction of the regularization and the generator.", "Cons:", "- For semi-supervised classification, the paper did not report the best results in other baselines.", "E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels).", "The performance of the proposed method is worse than the previous work but they claimed \"state-of-the-art\" results.", "The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].", "The experimental results are not very convincing because many importance baselines are neglected.", "- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).", "I'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3].", "It would be better to compare with them.", "References:", "[1] Adversarial Dropout for Supervised and Semi-Supervised Learning, AAAI 2018", "[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018", "[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2018"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 65, "sentences": ["Summary:", "The authors present a empirical investigation of methods for scaling GANs to complex datasets, such as ImageNet, for class-conditioned image generation.", "They first build and describe a strong baseline based on recently proposed techniques for GANs and push the performance on large datasets with several modifications presented sequentially, to obtain strong state-of-the-art IS/FID scores, as well as impressive visual results.", "The authors propose a simple truncation trick to control the fidelity/variance which is interesting on its own but cannot always scale with the architecture.", "The authors further propose a orthogonalization-based regularization to mitigate this problem.", "An investigation of training collapse at large scale is also performed; the authors investigate some regularization schemes based on gathered empirical evidence.", "As a result, they explore and discard Spectral Normalization of the generator as a way to prevent collapse and show that a severe tradeoff between stability and quality can be controlled when using zero-centered gradient penalties in the Discriminator.", "In the end, no solution that can ensure quality and stability is found, except having prohibitively large amounts of data (~300M images).", "Models are evaluated on the ImageNet and on this internal, bigger dataset.", "Pros:", "- This investigation gives a significant amount of insights on GAN stability and performance at large scales, which should be useful for anyone working with GANs on complex datasets (and that have access to great computational resources).", "- Even though commonly used evaluations metrics for GANs are still not fully adequate, the authors obtain quantitative performance significantly beyond previous work, which seems indeed correlated with remarkable visual results.", "- The baseline and added modifications are well presented and clearly explained.", "The Appendices also have great value in that regard.", "Cons:", "- Discussions sometimes lack depth or are absent.", "For example, it is unclear to me why some larger models are not amenable to truncation.", "Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts? Were samples from those networks better without using truncation? Why would this be?", "Authors report how wider networks perform best, and how deeper networks degrade performance.", "Again, discussions are lacking, and it doesn\u2019t seem the authors tried to understand why such behaviors were shown.", "Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.", "- In Section 3.1 : \u201cAcross runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.\u201d For me, this is not particularly clear. Is this something the reader should understand from Table 1?", "- I question the choice of sections chosen to be in the main paper/appendices.", "I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.", "However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.", "In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.", "However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.", "With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.", "Suggestions/Comments:", "- Regarding the diversity/fidelity tradeoff using different truncation thresholds, I think constraining the norm of the sampled noise vectors to the exact threshold value (by projecting the samples on the 0-centered hyper-sphere of radius = threshold) could yield even more interesting or more informative Figures, as obtained scores or samples on the edge of that hyper-sphere might provide information on the \u2018guaranteed\u2019 (not proven) quality/fidelity of samples mapped from inside that hyper-sphere.", "- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.", "Similar curves could also be produced with the hyper-sphere projection proposed above to have a slightly clearer idea of the behavior on the limit of that hyper-sphere.", "- In Section 4.2, in the second paragraph, you refer to Appendix F and describe \u201csharp upward jump at collapse\u201d in D\u2019s loss.", "However, it seems the only Figure showing D\u2019s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.", "- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says \u201closses\u201d.", "This investigation of GAN scalability is successful results-wise even though the inability to stabilize training without sacrificing great performance on ImageNet is disappointing.", "The improvement over previous SOTA is definitely significant.", "This work thus shows a modern GAN architecture for complex datasets that could be a strong basis for future work.", "However, I think the paper could and should be improved with some more detailed analysis and discussions of exhibited behaviors in order to further guide and encourage future work.", "It could also be clarified on some aspects, and potentially re-structured a bit to be better align with its probable impact directions.", "I would also be curious to see the proposed techniques applied on simpler datasets.", "Can this be useful for someone having less compute power and working on something similar to CelebA?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 66, "sentences": ["The authors introduce the idea of distributed backdoor attacks in the FL framework, in which the dishonest participants in FL add local triggers to their training data to influence the global model to classify triggered images in a desired way.", "They show empirically that the learned models then are more likely to be successfully forced to misclassified images in which all the local triggers are present at test time, than are models learned using centralized backdoor attacks, where all attackers use the same trigger pattern (one of the same size as the concatenation of the local triggers, to be fair in the comparison).", "They then demonstrate that because the local triggers cause smaller corruptions in the model coefficients, these distributed attacks survive robust FL training algorithms (namely FoolsGold, and a recent robust regression based method) more often than centralized attacks.", "Similar experiments are conducted on the Loan text dataset, using appropriate analogs of local triggers, with similar results.", "The paper contributes a novel model for conducting backdoor attacks in the FL setup, and shows that this model is more successful at attacking when training using robust FL algorithms than the standard centralized backdoor attack model.", "I lean towards accept, as this is a realistic attack model, and as such can further stimulate research into the robustification of FL model aggregation algorithms."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 67, "sentences": ["This paper studies the properties of SGD as a function of batch size and learning rate.", "Authors argue that SGD has two regimes:  a noise dominated regime (small batch size) and curvature dominated regime (large batch size).", "Authors conduct through numerical experiments highlighting how learning rate changes as a function of batch size (initially linear growth and then saturates).", "The critical contribution of this work appears to be the observation that large batch size can be worse than small under same number of steps demonstrating implicit regularization of small batch size.", "The two regime claim of the paper is not really novel.", "These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).", "When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).", "The interesting part in my opinion is the experiments on constant steps.", "Authors verify large batch size reduces test accuracy while improving train.", "I believe these experiments are novel and the results are interesting.", "Besides CIFAR 10, authors test this hypothesis in two other datasets while tuning the learning rate.", "On the other hand, contribution is somewhat incremental given observations made by related literature (Keskar et al and others).", "Some remarks:", "1) In Table 1, batch size 16k has effective LR of 32.", "However in Figure 1c SGD with momentum at batch size 8k uses an effective LR of 4.", "Can you explain this inconsistency i.e. why is there such a huge jump from 4 to 32 (in reality we expect the effective LR to stay constant in the curvature regime).", "I also understand that one is constant epoch and other is constant step.", "However 4 to 32 seems a bit inconsistent.", "2) Does momentum help in constant step budget (with sufficiently large steps so that training loss is small)?", "3) Readability: Consider explaining what is meant by \"warm-up\", \"epoch budget\", \"step budget\" clearly and upfront."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "arg-request_explanation", "none", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_clarification_label"]}
{"abstract_id": 68, "sentences": ["This paper formulates a new inference method called DDGC for noise labels and adversarial attacks.", "Their main idea is to induce a generative classifer on top of hidden feature spaces of the discriminative deep model.", "To improve the robustness, their DDGC model leverages the minimum covariance determinant (MCD) estimator.", "Besides, the author proposes Theorem 1 to justify their MCD-based generative classifer.", "Pros:", "1. The authors find a new angle for learning with noisy labels.", "Motivated by the fact that LDA-like generative classifer assuming the class-wise unimodal distribution might be robust, they introduce a generative classifer on top of hidden feature spaces of the discriminative deep model.", "2. The authors perform numerical experiments to demonstrate the effectiveness of their framework in benchmark datasets. And their experimental result support their previous claims.", "Cons:", "We have two questions in the following.", "1. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [1-3], estimating noise transition matrix [4-6], and explicit and implicit regularization [7-9].", "I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.", "2. Experiment:", "2.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong.", "At the same time, they should compare with VAT [7].", "For adversarial attacks, the author should compare with data type from [10], and list L-FBGS [11] as a basic baseline.", "2.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data.", "Besides, the current paper only verifies on vision datasets.", "The authors are encouraged to conduct 1 NLP dataset.", "References:", "[1] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.", "[2] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.", "[3] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, M. Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NIPS, 2018.", "[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.", "[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.", "[6] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels. In ICLR workshop, 2015.", "[7] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.", "[8] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.", "In NIPS, 2017.", "[9] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.", "[10] C. Nicholas and W. David. Towards evaluating the robustness of neural networks. In IEEE Symposium on SP, 2017.", "[11] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2013."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 69, "sentences": ["This paper studies backdoor attacks under federated learning setting.", "To inject a certain backdoor pattern, existing work generate poisoning samples by blending the same pattern with different input samples.", "Even for federated learning where the adversary can control multiple parties, such as [1], all parties still use the same global backdoor pattern to generate poisoning samples locally.", "On the contrary, in this work, they decompose the global pattern into several small local patterns, and each adversarial party only uses a local pattern to generate poisoning samples.", "In their evaluation, they show that the backdoor attacks generated in this way are more effective, resilient to benign model parameter updates, and also survive better against existing defense algorithms against attacks in federated learning settings.", "I think the topic studied in this paper is very important and meaningful, and I am convinced that by decomposing a global pattern into several smaller local pieces, the model parameter updates computed by each party should be more similar to benign updates and thus can better bypass the defense algorithms.", "Meanwhile, the evaluation is pretty comprehensive and it is good to see that the conducted backdoor attacks are effective.", "However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.", "Thus, I would like to see more possible explanation on it.", "Specifically, I have the following questions for clarification:", "1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?", "2. To evaluate A-S setting, I understand that it may be tricky to enable a fair comparison between the centralized attack and DBA.", "However, one explanation of why DBA is more persistent in this case is because the adversarial parameter updates happen 4x times compared to the centralized attack.", "Therefore, another baseline to check is to conduct centralized attacks with the same number of times as DBA, but each update includes 1/4 number of poisoning samples, so that the total number of poisoning samples included to compute the gradient update still stays the same.", "3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?", "For backdoor attacks, a line of work studies physical triggers, e.g., glasses in [2].", "It is not natural to decompose such kind of patterns into several smaller pieces, unless the performance is significantly boosted.", "4. Can the authors show concrete examples on how the attacks are generated?", "The details are especially unclear on LOAN.", "Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?", "[1]  Bagdasaryan et al., How to backdoor federated learning.", "[2] Chen et al., Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.", "------------", "Post-rebuttal comments", "I appreciate the authors' great effort to address my concerns! I think the evaluation in the current version of the paper is pretty comprehensive and provides a valuable study, and I am happy to raise my score accordingly.", "-------------"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation", "arg-structuring_heading", "arg-request_clarification", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_explanation", "none", "arg-request_clarification", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 70, "sentences": ["This is an interesting paper with a new approach to learn a sparse, positive (and hence interpretable) semantic space that maximizes human similarity judgements, by training to specifically maximize the prediction of human similarity judgements.", "The authors have collected the dataset themselves and have rating of sets of 3 objects from 1854 unique objects.", "They end up with a space (SPoSE) with relatively low dimensionality with respect to usual word embeddings (49 dimension) but perhaps not surprising when considering the small size of the words to embed.", "The authors run a set of experiment to show the usefulness of SPoSE.", "The most interesting one is the prediction of its dimensions by the CSLB features, which reveals a nice clustering in the different SPoSE dimensions.", "Perhaps the results would be a little more convincing if additional common word embeddings were also tested.", "Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.", "A good extension of this work would be to combine a text-derived embedding  or the synsets to interpolate the SPoSE dimensions for missing words in the original set.", "Or perhaps the object similarity ratings could be used in a semi-supervised setting to inform the learning of a co-occurence word embedding.", "This will allow the model to better describe a larger set of words.", "Another possible extension is to test this larger set of words on a non-behavioral NLP task to show possible improvements that the behavioral data and the interpretable space give."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label"]}
{"abstract_id": 71, "sentences": ["The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs.", "The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem.", "The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images.", "There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.", "In summary, the quality of the paper is poor and the originality of the work is low.", "The paper is easily readable."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 72, "sentences": ["This paper proposed to train a network with training curves and corresponding parameters, and use policy search to find optimal parameter to replace hundreds or thousands of training in real case scenario, and it is clearly much faster using the trained network to infer parameters, instead of tuning the network manually.", "The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.", "The cited paper 'Learning an adaptive learning rate schedule' does not appear online."], "labels": ["arg-structuring_summary", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 73, "sentences": ["Summary: The paper focuses on comparing the impact of explicit modularity and structure on systematic generalization by studying neural modular networks and \u201cgeneric\u201d models.", "The paper studies one instantiation of this systematic generalization for the setting of binary \u201cyes\u201d or \u201cno\u201d visual question answering task.", "They introduce a new dataset called in which model has to answer questions that require spatial reasoning about pairs of randomly scattered letters and digits in the image.", "While the models are evaluated on all possible object pairs, they are trained on a smaller subset.", "They observe that NMNs generalize better than other neural models when an appropriate choice of layout and parametrization is made.", "They also show that current end-to-end approaches for inducing model layout or learning model parametrization fail to generalize better than generic models.", "Pros:", "- The conclusions of the paper regarding the generalization ability of neural modular networks is timely given the widespread interest in these class of algorithms.", "- Additionally, they present interesting observations regarding how sensitive NMNs are to the layout of models.", "Experimental evidence (albeit on specific type of question) of this behaviour will be helpful for the community and hopefully motivate them to incorporate regularizers or priors that steer the learning towards better layouts.", "- The authors provide a nice summary of all the models analyzed in Section 3.1 and Section 3.2.", "Cons:", "- While the results on SQOOP dataset are interesting, it would have been very exciting to see results on other synthetic datasets.", "Specifically, there are two datasets which are more complex and uses templated language to generate synthetic datasets similar to this paper:", "- CLEVR environment or a modification of that dataset to reflect the form of systematic the authors are studying in the paper.", "- Abstract Scenes VQA dataset introduced in\u201cYin and Yang: Balancing and Answering Binary Visual Questions\u201d by Zhang and Goyal et al. They provide a balanced dataset in which there are a pairs of scenes for every question, such that the answer to the question is \u201cyes\u201d for one scene, and \u201cno\u201d for the other for the exact same question.", "- Perhaps because the authors study a very specific kind of question, they limit their analysis to only three modules and two structures (tree & chain).", "However, in the most general setting NMN will form a DAG and it would have been interesting to see what form of DAGs generalize better than other.", "- It is not clear to me how the analysis done in this paper will generalize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more.", "Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.", "Other Questions / Remarks:", "- Given that the accuracy drop is very significant moving from NMN-Tree to NMN-Chain, is there an explanation for this drop?", "- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.", "- Small typo in the last line of section 4.3 on page 7. It should say: This is in stark contrast with \u201cNMN-Tree\u201d \u2026 ..", "- Small typo in the \u201cLayout induction\u201d paragraph, line 6 on Page 7:", "\u2026 and for $p_0(tree) = 0.1$ and when we use the Find module"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "arg-structuring_heading", "none", "none", "arg-structuring_summary", "arg-request_experiment", "arg-request_experiment", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 74, "sentences": ["The paper proposes an approach to construct surrogate objectives for the effective application of policy gradient methods to combinatorial optimization without known neighborhood structure.", "The surrogate is constructed with the goal of reducing the need of hyper-parameter tuning and evaluated on a clique finding task.", "The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.", "The proposed sampling distributions assumes independence between the random variables over which the authors optimize \u2014 I find it surprising that this leads to good empirical results are relatively little structure can be captured using this distribution.", "Can the authors elaborate on this?", "However, as also observed by the authors, the sampling distribution can also be replaced by more sophisticated distributions.", "The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 75, "sentences": ["Summary:", "Many prior works have found that the features output by the final layer of neural networks can often be used as informative representations for many tasks despite being trained for one in particular.", "These feature representations, however, are learned transformations of low-level input representations, e.g. RGB values of an image.", "In this paper, they aim to learn useful feature representations without meaningful low-level input representations, e.g. just an instance ID.", "Instead, meaningful representations are learned through gathered triplet comparisons of these IDs, e.g. is instance A more similar to instance B or instance C? Similar existing techniques fall in the realm of learning ordinal embeddings, but this technique demonstrates speed-ups that allow it to scale to large real world datasets.", "The two primary contributions of the paper are given as:", "- a showcase of the power of neural networks as a tool to approximately solve NP-hard optimization problems with discrete inputs", "- a scalable approach for the ordinal embedding problem", "After experimentation on synthetic data, they compare the effectiveness of their proposed method Ordinal Embedding Neural Network (OENN) against the baseline techniques of Local Ordinal Embedding (LOE) and t-distributed Stochastic Triplet Embedding (TSTE).", "The test error given by the systems is comparable, but there are clear speed benefits to the proposed method OENN as the other techniques could not be run for a dataset size of 20k, 50k, or 100k.", "Then, they gathered real-world data using MTurk applied to a subset of ImageNet and applied OENN to learning embeddings of different image instances using only the MTurk triplet information rather than the input RGB input features.", "Decision: Weak Reject", "1. Interesting technique to take advantage of neural networks to efficiently learn ordinal embeddings from a set of relationships without a low-level feature representation, but I believe the experiments could be improved.", "One of the main advantages of this approach is efficiency, which allows it to be used on large real-world datasets.", "The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison). By this I mean, that you may be able to use relationships learned using conventional triplet methods which use input RGB features as ground truth, and test your learned relationships against those.", "However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed.", "The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.", "2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem. This claim can be made secondarily or as motivation for continued exploration along this direction, but I think listing them as two distinct contributions is necessary.", "Additional feedback:", "Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.", "You may be able to show more plots which help display the quality of the embedding space varying with the number of triplets used. For example, an additional plot after Figure 5 (b) which shows a few scatter plots of points (color coded by class) for training with different numbers of collected triplets.", "Also, since it should be fairly easy to distinguish between cars and animals or cars and food, it may be more interesting to focus on the heat-maps from along the block diagonal of Figure 5 (a) and talk about what relationships may have been uncovered within the animal or food subsets.", "Very minor details:", "In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.", "In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 76, "sentences": ["*** Increased to weak accept after discussion of merits of ME bias was improved in the paper ***", "This paper investigates whether neural networks exhibit a \u2018mutual exclusivity (ME) bias\u2019, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects. Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data.", "The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning.", "While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.", "The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example.", "Comments / questions:", "* The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples.", "Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.", "* For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples? * If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically.", "It\u2019s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).", "So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes?", "Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.", "* The authors say that \u201cME can be generalized from applying to 'novel versus familiar\u2019 stimuli to instead handling \u2018rare versus frequent\u2019 stimuli\u201d and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above.", "It\u2019s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.", "* It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2. When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones? From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes? Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not.", "Minor comments /questions not affecting review:", "* Is the acronym ME pronounced like the word \u201cme\u201d or is it spelled out \u201cM-E\u201d? If the latter, then all cases of \u201ca ME bias\u201d should be corrected to \u201can ME bias\u201d.", "* Section 4.2 line 3: \u201csample the class [for the] from a power law distribution\""], "labels": ["none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "none", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "arg-structuring_heading", "arg-request_typo", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 77, "sentences": ["The paper proposes an approach to learn nonlinear causal relationship from time series data that is based on empirical risk minimization regularized by mutual information.", "The mutual information at the minimizer of the objective function  is used as causal measure.", "The paper is well written and the proposed method well motivate and intuitive.", "However I am concerned by the assumption that the lagged variables X_{t-1}^{(j)} follow a diagonal gaussian distribution.", "This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.", "Another key concern concerns scalability.", "The authors mention gene regulatory networks , neuroscience etc as key applications.", "Yet the experiments considered in the paper are limited to very few time series.", "For instance the simulation experiments use  N=30,", "which is much smaller than the number of time series usually involved say in gene regulatory network data .", "The real data experiments use N= 6 or N=2.", "This is way to small.", "The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.", "How do these compare? Does the proposed approach offer  insights on these datasets which are not captured by the comparison methods?"], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 78, "sentences": ["This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack.", "Under a set of conditions, the authors proved convergence of the proposed attack algorithm.", "My main concern about this paper is why this algorithm has a better performance than CW attack?", "I would suggest comparing with CW attack under different sets of hyper-parameters.", "Minor comment:", "The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-request_explanation", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 79, "sentences": ["Paper summary.", "The paper proposes Dreamer, a model-based RL method for high-dimensional inputs such as images.", "The main novelty in Dreamer is to learn a policy function from latent representation-and-transition models in an end-to-end manner.", "Specifically, Dreamer is an actor-critic method that learns an optimal policy by backpropagating re-parameterized gradients through a value function, a latent transition model, and a latent representation model.", "This is unlike existing methods which use model-free or planning methods on simulated trajectories to learn the optimal policy.", "Meanwhile, Dreamer learns the remaining components, namely a value function, a latent transition model, and a latent representation model, based on existing methods (the world models and PlaNet).", "Experiments on a large set of continuous control tasks show that Dreamer outperforms existing model-based and model-free methods.", "Comments.", "Efficiently learning a policy from visual inputs is an important research direction in RL.", "This paper takes a step in this direction by improving existing model-based methods (the world models and PlaNet) using the actor-critic approach.", "I am leaning towards weak accepting the paper.", "I am reluctant to give a higher score due to its incremental contribution.", "Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.", "The main difference between Dreamer and SVG is that Dreamer incorporates a latent representation model.", "From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.", "Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.", "Besides the above comments, I have these additional comments.", "- Effectiveness on very long horizon trajectories:", "Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.", "This is an open issue in model-based RL.", "The paper attempts to solve this issue by backpropagating policy gradients through the transition model, which is known to be more robust against model errors (see e.g., PILCO (Deisenroth et al., 2011)).", "However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).", "This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).", "I think this point should be discussed in the paper.", "That is, the issue still exists, and Dreamer is less effective with very long horizon.", "- Inapplicability to discrete controls:", "One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.", "This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.", "Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.", "This restriction should be noted in the paper.", "- There is no mention about variance of policy gradient estimates.", "Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.", "- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).", "Also, I suggest moving Section 4 to be right after Section 2, since Section 4 presents existing techniques similarly to Section 2, while Section 3 presents the main contribution.", "Update after authors' response.", "I read the response.", "The paper is more clear after authors' clarification.", "Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).", "Nonetheless, I am keen to acceptance. I would increase the rating from 6 to 7, but I will keep the rating of 6 since the rating of 7 is not possible."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "arg-structuring_summary", "none", "none", "arg-request_edit", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_edit", "arg-structuring_summary", "none", "none", "arg-request_edit", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 80, "sentences": ["The authors propose a method for image restoration, where the restored image is the MAP estimate.", "A pretrained GAN is utilized to approximate the prior distribution of the noise-free images.", "Then, the likelihood induces a constraint which is based on the degradation function.", "In particular, the method tries to find the latent point for which the GAN generates the image, which if gets degraded will match the given degraded image.", "Also, an optimization algorithm is presented that solves the proposed constrained optimization problem.", "I find the paper very well written and easy to follow.", "Also, the idea is pretty clean, and the derivations are simple and clear.", "Additionally, the Figures 2,3 are very intuitive and nicely explain the theory.", "However, I think that there are some weaknesses (see comments):", "Comments:", "#1) I do not understand exactly what the \"general method\" means. Does it mean that you propose a method, where you can just change the F, such that to solve a different degradation problem? So you provide the general framework where somebody has to specify only the F?", "#2) Clearly, the efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.", "#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.", "For instance, Eq. 2,3 can be easily combined using the proportional symbol, Eq. 8,9,10,11 show actually the same thing.", "#4) I think that the function F has to be differentiable, and this should be mentioned in the text.", "Also, I believe that some actual (analytic) examples of F should be provided, at least  in the experiments.", "The same holds for the p(Omega).", "This parameter Omega is estimated individually for each degraded image?", "#5) Before Eq. 8 the matrix V is a function of z and should be presented as such in the equations.", "#6) I believe that it would be nice to include a magnified image of Fig. 3, where the gradient steps are shown.", "Also, my understanding is that the optimization goal is to find first a feasible solution, and then find the point that maximizes f. I think that this can be clarified in the text.", "#7) The optimization steps seem to be intuitive, however, there is not any actual proof of converge.", "Of course, the example in the Figure 3 is very nice and intuitive, but it is also rather simple.", "I would suggest, at least, to include some empirical evidences in the experiments that show convergence.", "#8) In the experiments I think that at least one example of F and p(Omega) should be presented.", "Also, what the numbers in Table 4 show? Which is the best value that can be achieved?", "These numbers correspond to several images, or to a unique image?", "#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.", "I believe that a more challenging experiment should be conducted e.g. using celebA dataset.", "Minor comments:", "#1) In the paragraph after Eq. 4 the equality p_r(x)=p_G(x) is very strong assumption.", "I would suggest to use the \\simeq symbol instead.", "#2) After Eq. 6 the \"nonnegative\" should be \"nonzero\".", "#3) Additional density estimation models can be used e.g. VAEs, GMM.", "Especially, I believe that the VAE will provide a way to approximate the prior easier than the GAN.", "#4) In Section 2 paragraph 2, the sentence \"However, they only ... and directly\" is not clear what means.", "In general, I find both the proposed model and optimization algorithm interesting.", "Additionally, the idea is nicely presented in the paper.", "Most of my comments are improvements which can be easily included.", "The two things that make me more skeptical, is the convergence of the proposed algorithm and the experiments.", "The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.", "Also, I think that additional methods to compute the image prior should be included in the experiments."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_clarification", "none", "none", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_clarification", "arg-request_edit", "arg-request_edit", "arg-request_clarification", "none", "none", "arg-request_experiment", "arg-request_edit", "arg-request_explanation", "arg-request_clarification", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_edit", "arg-request_experiment", "none", "arg-request_clarification", "none", "none", "arg-request_edit", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_clarification_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_clarification_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 81, "sentences": ["The paper studies self-supervised learning from very few unlabeled images, down to the extreme case where only a single image is used for training.", "From the few/single image(s) available for training, a data set of the same size as some unmodified reference data set (ImageNet, Cifar-10/100) is generated through heavy data augmentation (cropping, scaling, rotation, contrast changes, adding noise).", "Three popular self-supervised learning algorithms are then trained on this data sets, namely (Bi)GAN, RotNet, and DeepCluster, and the linear probing accuracy on different blocks is compared to that obtained by training the same methods on the reference data sets.", "The linear probing accuracy from the first few conv layers of the network trained on the single/few image data set is found to be comparable to or better than that of the same model trained on the full reference data set.", "I enjoyed the paper; it addresses the interesting setting of an extremely small data set which complements the large number of studies on scaling up self-supervised learning algorithms.", "I think it is not extremely surprising that using the proposed strategy allows to learn low level features as captured by the first few layers, but I think it is worth studying and quantifying.", "The experiments are carefully described and presented, and the paper is well-written.", "Here are a few questions and concerns:", "- How much does the image matter for the single-image data set?", "The selected images A and B are of very high entropy and show a lot of different objects (image A) and animals (image B). How do the results change if e.g. a landscape image or an abstract architecture photo is used?", "- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.", "- [1] found that the network architecture for self-supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved.", "In particular, the linear probing accuracy appears to be often monotonic as a function of the depth of the layer it is computed from.", "This is in contrast to what is observed for AlexNet in Tables 2 and 3, where the conv5 accuracy is lower than the conv4.", "It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.", "- Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?", "Overall, I\u2019m leaning towards accepting the paper, but it would be important to see how well the experiments generalize to i) ResNet and ii) other (lower entropy) input images.", "[1] Kolesnikov, A., Zhai, X. and Beyer, L., 2019. Revisiting self-supervised visual representation learning. arXiv preprint arXiv:1901.09005.", "---", "Update after rebuttal:", "I thank the authors for their detailed response.", "I appreciate the efforts of the authors into investigating the issues raised, the described experiments sound promising.", "Unfortunately, the new results are not presented in the revision.", "I will therefore keep my rating."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_summary", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "arg-request_experiment", "arg-request_explanation", "arg-request_experiment", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 82, "sentences": ["The paper studies the problem of question generation from sparql queries.", "The motivation is to generate more training data for knowledge base question answering systems to be trained on.", "However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them: - Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young, EMNLP 2015: https://arxiv.org/abs/1508.01745 - Globally Coherent Text Generation with Neural Checklist Models Chloe Kiddon Luke Zettlemoyer Yejin Choi: https://aclweb.org/anthology/D16-1032", "Thus the main novelty claim of the paper needs to be hedged appropriately.", "Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.", "Some other points:", "- How is the linearization of the inout done? It  typically matters", "- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.", "- On the human evaluation: showing the gold standard reference to the judges introduces bias to the evaluation which is inappropriate as in language generation tasks there are multiple correct answers. See this paper for discussion in the context of machine translation: http://www.aclweb.org/anthology/P16-2013", "- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.", "Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.", "- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect. E.g. \"what job did jefferson have\" is semntically related to his role in the declaration of independence but rather different. SImilarly, being married to someone is not the same as having a baby with someone.", "While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.", "What were the guidelines used?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_experiment", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 83, "sentences": ["This paper proposes an evaluation method for confidence thresholding defense models, as well as a new approach for generating of adversarial examples by choosing the wrong class with the most confidence when employing targeted attacks.", "Although the idea behind this paper is fairly simple, the paper is very difficult to understand.", "I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.", "Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.", "However, in Figure 2, it is used for evaluating defense schemes.", "Again, this confuses me on what is the main topic of this paper.", "Indeed, why the commonly used attack success ratio or other similar measures cannot be used in the case?", "Intuitively, it should provide similar results to the success-failure curve.", "The paper also lacks experimental results, and the main conclusion from these results seems to be \"MNIST is not suitable for benchmarking of adversarial attacks\".", "If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.", "Meanwhile, the computational cost on large dataset such as ImageNet could be huge, the authors should further develop the method to make sure it works in all situations."], "labels": ["arg-structuring_summary", "none", "none", "arg-request_clarification", "none", "none", "arg-request_explanation", "none", "arg-request_result", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 84, "sentences": ["Overview:", "This paper considers unsupervised (or self-supervised) discrete representation learning of speech using a combination of a recent vector quantized neural network discritization method and future time step prediction.", "Discrete representations are fine-tuned by using these as input to a BERT model; the resulting representations are then used instead of conventional speech features as the input to speech recognition models.", "New state-of-the-art results are achieved on two datasets.", "Strengths:", "The core strength of this paper is in the results that are achieved on standard speech recognition benchmarks.", "The results indicate that, while discritization in itself does not give improvements, coupling this with the BERT-objective results in speech features which are better in downstream speech recognition than standard features. I think the main technical novelty is in combining discritization with future time step prediction (but see the weakness below).", "Weaknesses:", "The main weakness of the paper is that it does not situate itself within existing literature in this area.", "Over the last few years, researchers in the speech community have invested significant effort in learning better speech representations, and this is not discussed.", "See e.g. [1].", "Even more importantly, very recently there has been a number of papers investigating discrete representations of speech; see the review [2].", "Some of these papers specifically use VQ-VAEs [3].", "[4] actually compares VQ-VAE and the Gumbel-Softmax approach.", "These studies should be mentioned.", "This paper is different in that it incorporates future time step prediction.", "But context prediction has also been considered before, also for speech [5, 6, 7].", "This paper can be situated as a new contribution combining these two strands of research.", "In the longer run it would be extremely beneficial to the community if this approach is applied to the standard benchmarks as set out in [2].", "As a minor weakness, some parts of the paper is not described in enough detail and the motivation is weak or not exactly clear (see detailed comments below).", "Overall assessment:", "I think the results as well as the new combination of existing approaches in the paper warrants publication. But it should be amended significantly to situate itself within the existing literature. I therefore award a \"weak accept\".", "Detailed questions and suggestions:", "- Section 1: As motivation for this work, it is stated that \"we aim to make well performing NLP algorithms more widely applicable\".", "As noted above, some NLP-like ideas (such as prediction of future speech segments, stemming from text-based language modelling) have already been considered within the speech community.", "Rather than motivating the work in this way, it might be helpful to focus the contribution as a combination of future time step prediction and discretization (both of which have been considered in previous work, but not in combination).", "- Section 4: Would it be possible to train the vq-wav2vec model jointly with BERT, i.e. as one model? I suspect it would be difficult since, for the masking objective, the discrete units are already required, but maybe there is a scheme where this could work.", "- Section 2.2: Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end-to-end model, e.g. by randomly masking some of the continuous input speech?", "- Section 3.3: What exactly does \"mode collapse\" refer to in this context? Would this be using only one codebook entry, for instance?", "- Section 6: It seems that in all cases to obtain improvements from discritization, BERT is required on top of the vq-wav2vec discrete symbols.", "Is it possible that the output acoustic model is simply better-matched to continuous rather than discrete input (direct vq-wav2vec gives discrete while BERT gives continuous)? Would it make sense to train the wav2vec acoustic model on top of the vqvae codebook entries (e) instead of directly on the symbols?", "Typos, grammar and style:", "- \"gumbel\" -> \"Gumbel\" (throughout; or just be consistent in capitalization)", "- \"which can be mitigated my workarounds\" -> \"which can be mitigated *by* workarounds\"", "- \"work around\" -> \"workaround\"", "Missing references:", "1. Versteegh, M., Anguera, X., Jansen, A. & Dupoux, E. (2016). The Zero Resource Speech Challenge 2015: Proposed Approaches and Results. In SLTU-2016 Procedia Computer Science, 81, (pp 67-72).", "2. https://arxiv.org/abs/1904.11469", "3. https://arxiv.org/abs/1905.11449", "4. https://arxiv.org/abs/1904.07556", "5. https://arxiv.org/abs/1904.03240", "6. https://arxiv.org/abs/1807.03748 (this paper is cited)", "7. https://arxiv.org/abs/1803.08976", "Edit: Based on the feedback from the authors, I changed my rating from a 'weak accept' to an 'accept'."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_quote", "none", "none", "none", "arg-request_edit", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-structuring_heading", "arg-structuring_quote", "none", "arg-request_edit", "arg-request_explanation", "arg-request_explanation", "arg-request_clarification", "arg-structuring_quote", "arg-request_explanation", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-structuring_quote_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 85, "sentences": ["While the invertible model structure itself is essentially the same as Real-NVP, the use of observation variables in the framework with theoretically sound bidirectional training for safe use of the seemingly na\u00efve inclusion of y (i.e., y and z can be independent).", "Its abilities to model the posterior distributions of the inputs are supported by both quantitative and qualitative experiments.", "The demonstration on practical examples is a plus.", "The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.", "This is an interesting paper overall, so I am looking forward for further discussions.", "Pros:", "1.\tExtensive analyses of the possibility of modeling posterior distributions with an INN have been shown.", "Detailed experiment setups are provided in the appendix.", "2.\tThe theoretical guarantee (with some assumptions) of the true posterior might be beneficial in practice for relatively low-dimensional or less complex tasks.", "Comments/Questions:", "1.\tFrom the generative model point of view, could the authors elaborate on the comparison against cGAN (aside from the descriptions in Appendix 2)? It is quoted \u201ccGAN\u2026often lack satisfactory diversity in practice\u201d.", "Also, can cGAN be used estimate the density of X (posterior or not)?", "2. For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)?", "This question comes from my observation that the nature of the losses, especially for L_y vs. L_y,L_x (i.e., SL vs. USL) seem to be different.", "3.\t\u201cwe find it advantageous to pad both the in- and output of the network with equal number of zeros\u201d: Is this to effectively increase the intermediate network dimensions?", "Also, does this imply that for both forward and inverse process those zero-padded entries always come out to be zero?", "It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).", "4.\tIt seems that most of the experiments are done in relatively small dimensional data.", "This is not necessarily a drawback, I am curious if this model could succeed on higher dimensional data (e.g., image), especially with the observation y."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_clarification", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_summary", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 86, "sentences": ["## Summary ##", "The authors apply policy gradients to combinatorial optimization problems.", "They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size.", "They demonstrate performance on a clique-finding problem.", "## Assessment ##", "I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.", "I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.", "They both approximate the reward CDF from K samples and use this to construct a surrogate reward.", "The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.", "## Specific Comments and Questions ##", "1. Cakewalk is *very* closely related to the cross-entropy method.", "The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.", "Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward.", "The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.", "2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.", "Consider $x$ a binary vector and reward equal to the parity $S(x) = \\sum{x_j} % 2$.", "3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).", "Is there any explanation for this?", "4. How were the hyperparameters (learning rate, AdaGrad $\\delta$, Adam $\\beta_1, \\beta_2$) chosen?", "It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.", "I would suggest tuning these values for each method independently.", "5. It would be nice to see experimental results on more than one problem.", "The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.", "6. In Table 3, the figure in bold is not the lowest (best) in the table.", "The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing.", "I would replace these values with N/A or something similar."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_clarification", "none", "arg-request_experiment", "arg-request_experiment", "none", "arg-structuring_quote", "arg-request_explanation", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 87, "sentences": ["The authors consider the few-shot / meta-learning scenario in which the test set of interest is drawn from a different distribution from the training set.", "This scenario is well-motivated by the \"researcher example\" given throughout the paper.", "The authors assume access to a large unlabelled set in test (target) domain, and a large labelled (few-shot) set in the source domain.", "Thus, the paper is concerned with unsupervised version of the meta-learning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).", "The key idea is to learn a mapping from the source domain to the target domain.", "This mapping is learned jointly with the meta-learner, who performs the meta-learning in the target domain, on examples from the labelled domain.", "In practice however, it appears from the experimental section that the domain mapping is learned offline, and then frozen for the meta-learning phase.", "Thus, at test time, given examples from the target domain, the meta-learner can perform few-shot learning.", "Pros:", "- The paper addresses an important scenario which has not been addressed to this point: namely, meta-learning without the assumption that the train and test sets are drawn from the same domain/distribution.", "- The authors propose a novel task and experimental framework for considering their method, and show (somewhat unsurprisingly) that their method outperforms standard meta-learning methods that do not properly account for domain shift.", "- The paper reads well and is easy to follow.", "Cons:", "- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / \"additional improvements\".", "Further, there a number of experimental details that need to be further elaborated upon.", "e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this).", "It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?", "- Some assumptions are not explicitly stated.", "In particular, it is unclear what the assumption on the size of the unlabelled test set is.", "This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.", "- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further?", "Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?", "- The overall method seems to be not very principled, and requires a lot of \"tweaks and tunes\", with additional losses and regularizers, to work.", "Overall, the paper proposes a method combining a number of existing useful works (prototypical networks for meta-learning and image-to-image translation for domain adaptation) to tackle an important problem setting that is not currently addressed in existing meta-learning research.", "Further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches).", "Hopefully, such a benchmark will inspire more researchers to explore this setting, and perhaps propose simpler, more principled approaches to perform this task.", "It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_explanation", "arg-request_edit", "arg-request_clarification", "none", "arg-request_clarification", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_clarification_label", "none_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 88, "sentences": ["This paper studies multi-source domain adaptation problem.", "First this paper proposes a new theory for this domain that extends generalized discrepancy theory to multi-source setting.", "After derive a new generalization bound, this paper also proposes a new method based on the theory.", "Evaluation on real world datasets are proposed to show the efficiency of the proposed method.", "+ The theory in this paper improve bounds for multi-source DA in previous paper.", "The new bound provides new insight and helps the design of algorithm.", "+ This paper proposes elegant method to tackle new terms in the loss function and gives its complexity analysis.", "+ The evaluation results show that the algorithm is efficient.", "Comments:", "- The main contribution of the proposed theory is the alpha term.", "Is \\eta_{\\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \\eta_{\\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?", "- The evaluation of the proposed method is not complete.", "Some baseline DA methods [A, B] and datasets [C, D] are not considered.", "[A] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.", "[B] Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.", "[C] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010.", "[D]H.Venkateswara, J.Eusebio, S.Chakraborty, and S.Panchanathan. Deep hashing network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 89, "sentences": ["The paper considers adaptive regularization, which has been popular in neural network learning.", "Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix.", "When you say that full-matrix computation \"requires taking the inverse square root\", I assume you know that is not really correct?", "As a matter of good implementation, one never takes the inverse of anything.", "Instead, on solves a linear system, via other means.", "Of course, approximate linear system solvers then permit a wide tradeoff space to speed things up.", "There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.", "The latter may be important in practice, but it is orthogonal to the full matrix theory.", "There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.", "Instead, it is a low-rank approximation to the full matrix.", "If there were theory to be had here, then I would guess that the low-rank approximation may work even when full matrix did not, e.g., since the full matrix case would involve too may parameters.", "The discussion of convergence to first order critical points is straightforward.", "Adaptivity ratio is mentioned in the intro but not defined there.", "Why mention it here, if it's not being defined.", "You say that second order methods are outside the scope, but you say that your method is particularly relevant for ill-conditioned problems.", "It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems..", "It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.", "The results shown in Figure 4 are much more interesting than the usual training curves which are shown in the other figures.", "If this method is to be useful, understanding how these spectral properties change during training for different types of networks is essential.", "More papers should present this, and those that do should do it more systematically.", "You say that you \"informally state the main theorem.\"  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-request_clarification", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_quote", "none", "arg-request_explanation", "arg-structuring_quote", "arg-request_clarification", "arg-request_explanation", "none", "arg-request_explanation", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label"]}
{"abstract_id": 90, "sentences": ["The paper addresses a challenging problem of predicting the states of entities over the description of a process.", "The paper is very well written, and easily understandable.", "The authors propose a graph structure for entity states, which is updated at each step using the outputs of a machine comprehension system.", "The approach is novel and well motivated.", "I will suggest a few improvements:", "1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN.", "Also, NPN can probably be modified to output spans of a sentence.", "I will be curious to know how it performs.", "2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper.", "3. What are the results when using the whole training set of Recipes ?"], "labels": ["none", "none", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_experiment", "none", "arg-request_explanation", "arg-request_edit", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_explanation_label"]}
{"abstract_id": 91, "sentences": ["The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks.", "The probabilities are approximated with variational autoencoders.", "During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.", "This paper focuses on the second part, with a different model.", "Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification.", "This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.", "This pre-trained model is then incorporated into the neural net for MNIST classification.", "The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model.", "This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)", "The authors compare to the work of Schott for one type of attack.", "It would be nice to see more detailed experiments as done in Schott.", "Questions:", "1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.", "2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?", "3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.", "4- Could you please add the found J_h's to the appendix.", "This architecture reminds me of the good old MRFs for image denoising.", "Could it be that what we are seeing is the attack being denoised?", "I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.", "Thanks in advance. I will re-adjust the review rating following your reply."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_experiment", "arg-structuring_heading", "arg-request_experiment", "arg-request_explanation", "arg-request_experiment", "arg-request_edit", "none", "arg-request_explanation", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 92, "sentences": ["This paper proposes a new method to compare existing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images.", "The main idea seems similar to adopting active learning for the test set selection.", "One of the main advantage is that it can select a sample set from an arbitrarily large unlabeled images.", "However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods.", "Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.", "The authors invite five volunteer graduate students to annotate the selected example.", "However, for many categories, it\u2019s nor easy for normal people to distinguish.", "So the experiments in this paper is also not convincing."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 93, "sentences": ["This paper aims to estimate time-delayed, nonlinear causal influences from time series under the causal sufficiency assumption.", "It is easy to follow and contains a lot of empirical results.", "Thanks for the results, but I have several questions.", "First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.", "In order to correctly estimate causal relations from data, both cases must be considered.", "Second, the conclusion of Theorem 2 seems to be flawed.", "Let me try to make it clear with the following example.", "Suppose x^1_{t-2} directly causes x^2_{t-1} and that x^2_{t-1} directly causes x^3_{t}, without a direct influence from x^1_{t-2}  to x^3_{t}. Then when minimizing (2), we have the following results step by step:", "1) The noise standard deviation in x^2_{t-1}, denoted by \\eta_2, may be non-zero.", "This is because we minimize a tradeoff of the prediction error (the first term in (2)) and a function of the reciprocal of the noise standard deviation \\eta_2 (the second term in (2)), not only the prediction error.", "2) If \\eta_2 is non-zero, then x^1_{t-2} will be useful for the purpose of predicting x^3_{t}. (Note that if \\eta_2 is zero, then x^1_{t-2} is not useful for predicting x^3_{t).) From the d-separation perspective, this is because x^1_{t-2} and x^3_{t} are not d-separated by x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, although they are d-separated by x^2_{t-1}. Then the causal Markov condition tells use that x^1_{t-2} and x^3_{t} are not independent conditional on x^2_{t-1} + \\eta_2 \\cdot \\epsilon_2, which means that x^1_{t-2} is useful for predicting x^3_{t}.", "3) Given that x^1_{t-2} is useful for predicting x^3_{t}, when (2) is minimized, \\eta_1 will not go to infinity, resulting in a non-zero W_{13), which *mistakenly* tells us that X^{1}_{t-1} directly structurally causes x^{(3)}_t.", "This illustrates that the conclusion of Theorem 2 may be wrong.", "I believe this is because the proof of Theorem 2 is flawed in lines 5-6 on Page 16.", "It does not seem sensible to drop X^{j}_{t-1} + \\eta_X \\cdot \\epsilon_X and attain a smaller value of the cost function at the same time.", "Please carefully check it, especially the argument given in lines 10-13.", "Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).", "Such methods are directly applicable to time-delayed causal relations by further considering the constraint that effects temporally follow the causes.", "Fourth, please make it clear that the proposed method aims to estimate \"causality-in-mean\" because of the formulation in terms of regression.", "For instance, if x^j_{t-1} influences only the variance of x^i_{t}, but not its mean, then the proposed method may not detect such a causal influence, although the constraint-based methods can.", "Any response would be highly appreciated."], "labels": ["arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 94, "sentences": ["This paper is an empirical contribution regarding SGD arguing that it presents two different behaviors which the authors name a noise dominated regimen, and a curvature dominated regime.", "They observe that the behaviors seem to arise in different batch sizes", "The authors derive empirical conclusions and perform experiments in different settings.", "The paper is well-written and the experimental setup seems to be carefully carried out.", "I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label"]}
{"abstract_id": 95, "sentences": ["Summary:", "The manuscript proposes a modification of generators in GANs which improves performance under two popular metrics for multiple architectures, loss, benchmarks, regularizers, and hyperparameter settings.", "Using the conditional batch normalization mechanism, the input noise vector is allowed to modulate layers of the generator.", "As this modulation only depends on the noise vector, this technique does not require additional annotations.", "In addition to the extensive experimentation on different settings showing performance improvements, the authors also present an ablation study, that shows the impact of the method when applied to different layers.", "Strengths:", "- The idea is simple.", "The experimentation is extensive and results are convincing in that they show a clear improvement in performance using the method in a large majority of settings.", "- I also like the ablation study showing the impact of the method applied at different layers.", "Requests for clarification/additional information:", "- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?", "- The ablation study shows that the impact is highest when modulation is applied to the last layer (if only one layer is modulated).", "It seems modulation on layer 4 comes in as a close second.", "I am curious about why that might be.", "- I would like to see some more interpretation on why this method works.", "- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?", "Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not)."], "labels": ["arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "none", "none", "arg-request_explanation", "arg-request_clarification", "arg-request_experiment", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 96, "sentences": ["This paper proposes an approach for automatic robot design based on Neural graph evolution.", "The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice.", "My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).", "The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature.", "What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?", "I would like to see additional experiments to answer this questions.", "In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.", "You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.", "If you can also compare against one or two algorithms of your choice from the recent literature it would also give more value to the comparison.", "Detailed comments:", "- in the abstract you say that \"NGE is the first algorithm that can automatically discover complex robotic graph structures\".", "This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?", "- in the introduction you mention that automatic robot design had limited success.", "This is rather subject, and I would tend to disagree.", "Moreover, the same limitations that apply to other algorithms to make them successful, in my opinion, apply to your proposed algorithm (e.g., difficulty to move from simulated to real-world).", "- The digression at the bottom of the first page about neural architecture search seem out of context and interrupts the flow of the introduction.", "What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.", "- The stated contributions number 3 and 5 are not truly contributions.", "#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.", "#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.", "- Sec 2.2: \"(GNNs) are very effective\" effective at what? what is the metric that you consider?", "- Sec 3 \"(PS), where weights are reused\" can you already go into more details or refer to later sections?", "- First line page 4 you mention AF, without introducing the acronym ever before.", "- Sec 3.1: the statements about MB and MF algorithms are inaccurate.", "Model-based RL algorithms can work in real-time (e.g. http://proceedings.mlr.press/v78/drews17a/drews17a.pdf) and have been shown to have same asymptotic performance of MB controllers for simple robot control (e.g. https://arxiv.org/abs/1805.12114)", "- \"to speed up and trade off between evaluating fitness and evolving new species\" Unclear sentence. speed up what ? why is this a trade-off?", "- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.", "- Sec 4.1: would argue that computational cost is rarely a concern among evolutionary algorithms.", "The cost of evaluating the function is typically more pressing, and as a result it is important to have algorithms that can converge within a small number of iterations/generations.", "- Providing the same computational budget seem rather arbitrary at the moment, and it heavily depends from implementation. How many evaluations do you perform for each method? why not having the same budget of experiments?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "none", "arg-request_edit", "arg-request_experiment", "arg-structuring_heading", "arg-structuring_quote", "arg-request_explanation", "arg-structuring_quote", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "arg-request_clarification", "arg-request_clarification", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-structuring_quote_label", "arg-request_explanation_label", "arg-structuring_quote_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_clarification_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 97, "sentences": ["This paper proposes a self-auxiliary-training method that aims to improve the generalization performance of simple supervised learning.", "The basic idea is to train the classification network to predict fine-level auxiliary labels in addition to the ground-truth coarse label, where the auxiliary labels used in training is generated by a generator network.", "During training, the classification network and the generator network are alternatively updated, and the update of the latter aims to maximize the improvement of the former after using the generated auxiliary label for training.", "The method requires a class hierarchy in advance to define the binary mask applied to the output layer for auxiliary class prediction.", "A KL divergence term is attached to the optimization objective to avoid generating trivial and collapsing auxiliary classes.", "Pros:", "1) The main idea is simple and easy to understand.", "2) It discusses the class collapsing problem in generating pseudo (auxiliary) labels and provides a reasonable solution, i.e., using KL divergence as regularization.", "3) Uses several visualizations to show experimental results.", "Cons:", "1) The problem it aims to solve is neither multi-task learning nor meta-learning: it tries to solve a supervised classification problem defined on principle classes, with the help of simultaneously predicting/generating auxiliary class labels.", "Although the concept of \"task\" is not explicitly defined in this paper, the authors seem to associate each task with a specific class. This is not correct: in meta-learning, each task is a subset of classes drawn from a ground set of classes, and different tasks are independently sampled.", "In addition, the classification models for different tasks are independent, though their training might be related by a meta-learner.", "Hence, the claims in multiple places of this paper and the names for the two networks are misleading.", "2) At the end of Page 4, the authors show that the update of the generator only depends on the improvement of the classifier after using the auxiliary label for training.", "In fact, the optimal auxiliary labels minimizing the objective is the ground truth label for principle classes.", "This results in the class collapsing problem observed by the authors.", "The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness. In other words, the auxiliary labels for a specific principle class are very possible to be multiple noisy copies of the principal label with random perturbations. So it is not convincing to me that the auxiliary labels generated by the generator can be really helpful.", "My conjecture is that the observed improvements are mainly due to the softness of the auxiliary labels, which has been proved by model compression/knowledge distillation and recent \"born-again neural networks\".", "To verify this, the authors might need to compare the results with those methods (which use the generated soft probability of ground truth classes for training), and the \"random-noisy copies of soft principle label\" mentioned above.", "3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).", "A successful idea of self-supervised learning is to use the output feature map of the trained classification network to generate auxiliary training signals, since it provides extra information about the learned distance beyond the ground-truth labels.", "The authors might want to compare to \"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.", "Deep Clustering for Unsupervised Learning of Visual Features. ECCV 2018.\" and \"Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. ICCV 2017.\" Moreover, since the method is not a meta-learning approach for few-shot learning, it is not fair and also not appropriate to compare with Prototypical Network.", "4) Although the paper claims that the ground truth fine labels are not required, it requires a class hierarchy, which in the experiments are provided by the dataset and defined between true coarse and fine classes.", "In practice, such hierarchy might be much harder to achieve than the primary (coarse) labels, and might be as costly to obtain as the true fine-class labels. This weakens the feasibility of the proposed method.", "5) The experiments only test the proposed method on CIFAR100 and CIFAR10, which has at most 100 fine classes.", "It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.", "Minor comments:", "Some important equations in the paper should be numbered."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_result", "none", "none", "arg-request_result", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label"]}
{"abstract_id": 98, "sentences": ["The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations.", "Unfortunately the paper falls short in two main areas:", "- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)", "- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)", "However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 99, "sentences": ["The paper design a low variance gradient for distributions associated with continuous or discrete random variables.", "The gradient is designed in the way to approximate the  property of reparameterization gradient.", "The paper is comprehensive and includes mathematical details.", "I have following comments/questions", "1. What is the \\kappa in \u201cvariable-nabla\u201d stands for? What is the gradient w.r.t. \\kappa?", "2. In Eq(8), does the outer expectation w.r.t . y_{-v} be approximated by one sample? If so, it is using the local expectation method.", "How does that differs from Titsias & Lazaro-Gredilla(2015) both mathematically and experimentally?", "3. Assume y_v is M-way categorical distribution, Eq(8) evaluates f by 2*V*M times which can be computationally expensive.", "What is the computation complexity of GO? How to explain the fast speed shown in the experiments?", "4. A most simple way to reduce the variance of REINFORCE gradient is to take multiple Monte-Carlo samples at the cost of more computation with multiple function f evaluations.", "Assume GO gradient needs to evaluate f N times, how does the performance compared with the REINFORCE gradient with N Monte-Carlo samples?", "5. In the discrete VAE experiment, upon brief checking the results in Grathwohl(2017), it shows validation ELBO for MNIST as (114.32,111.12), OMNIGLOT as (122.11,128.20) from which two cases are better than GO.", "Does the hyper parameter setting favor the GO gradient in the reported experiments?", "Error bar may also be needed for comparison.", "What about the performance of GO gradient in the 2 stochastic layer setting in Grathwohl(2017)?", "6. The paper claims GO has less parameters than REBAR/RELAX. But in Figure 9, GO has more severe overfitting. How to explain this contradicts between the model complexity and overfitting?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_result", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 100, "sentences": ["Update 11/21", "With the additional experiments (testing a new image, testing fine-tuning of hand-crafted features), additions to related work, and clarifications, I am happy to raise my score to accept.", "Overall, I think this paper is a nice sanity check on recent self-supervision methods.", "In the future, I am quite curious about how these mono-image learned features would fare on more complex downstream tasks (e.g., segmentation, keypoint detection) which necessarily rely less on texture.", "Summary", "This paper seeks to understand the role of the *number of training examples* in self-supervised learning with images.", "The usefulness of the learned features is evaluated with linear probes at each layer for either ImageNet or CiFAR image classification.", "Empirically, they find that a single image along with heavy data augmentation suffices for learning the first 2-3 layers of convolutional weights, while later layers improve with more self-supervised training images.", "The result holds for three state-of-the-art self-supervised methods, tested with two single-image training examples.", "In my view, learning without labels is an important problem, and it is interesting what can be learned from a single image and simple data augmentation strategies.", "Comments / Questions", "It seems to me that for completeness, Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features, because this experiment is actually testing what we want - performance of the features when fine-tuned for a downstream task.", "So for example, even if a linear classifier on top of Scattering features does poorly, if downstream fine-tuning results in the same performance as another pre-training method, then Scattering is a perfectly fine approach for initial features.", "Could the authors please either correct this logic or provide the experiments?", "Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.", "I wonder if the learned features require fewer fully supervised images to obtain the same performance on the downstream task?", "Can the authors clarify how the neural style transfer experiment is performed?", "The method from Gatys et al. requires features from different layers of the feature hierarchy, including deeper layers.", "Are all these features taken directly from the self-supervised network or is it fine-tuned in some way?", "While I appreciate the computational burden of testing more images, it does feel that Image A and B are quite cherry-picked in being very visually diverse.", "Because of this, it seems like a precise answer to what makes a good single training image remains unknown.", "I wonder how feasible it is to find a proxy metric that corresponds to the performance on downstream tasks which is expensive to compute.", "It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties.", "I disagree with the claim of practicality in the introduction (page 2, top).", "While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn\u2019t seem likely that *any* image would work for this method.", "Finally, more images are needed to learn the deeper layers for the downstream task anyway.", "The paper is well-written and clear."], "labels": ["arg-structuring_heading", "none", "none", "arg-request_result", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_result", "arg-request_result", "none", "none", "arg-request_explanation", "arg-request_clarification", "none", "arg-request_explanation", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 101, "sentences": ["This paper proposes an extension to the continual learning framework using existing variational continual learning (VCL) as the base method.", "In particular, it proposes to use the weight of evidence (WE) (from Zintgraf et al 2017) for each task.", "Firstly, this WE can be used to visualize the learned model (as used in Zintgraf et. al. 2017).", "The novelty of this paper is:", "1. to use this WE from the current task to generate a silence map (by smoothing the WE) for the next task.", "This is interpreted the learned the learned attention region.", "Such an approach is named Interpretable COntinual Learning (ICL)", "2. The paper proposes a metric for the saliency map naming FSM which is an extension of existing metric SSR.", "The extension is to take pixel count to compute the area instead of using rectangular region area, as well as taking the distance between pixels into account.", "This metric can be used to evaluate the level of catastrophic forgetting.", "Pro:", "In general, the idea is very intuitive and make sense.", "The paper also demonstrates superior performance with the proposed method on continual learning on all classic tasks comparing with VCL and EWC.", "The presentation is very easy to follow.", "It seems like a valid and flexible extension that can be used in other continual learning frameworks.", "Cons:", "The theoretical contribution is very limited.", "The work is rather incremental from current state-of-the-art methods.", "There should be a better discussion of related work on the topic.", "The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.", "A general overview of related work in these directions are needed.", "Other:", "1. The paper should also consider more recently proposed evaluation metrics such as discussed in https://arxiv.org/pdf/1805.09733.pdf", "2. The author should try to avoid using yellow color in plots."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_experiment", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_edit_label"]}
{"abstract_id": 102, "sentences": ["The paper proposes a method to do math reasoning purely using formula embeddings.", "The proposed method employs a graph neural network to embed math formulas to a latent space.", "The formula embeddings are then combined with theorem embeddings (also formulas, computed in the same way as formula embeddings) to predict whether one can do one step of math reasoning using the corresponding theorem, and also to predict the embeddings of the resulting formula.", "Empirically the authors demonstrate that the method can be chained end-to-end to do multiple steps of reasoning purely in the latent space.", "I tend to accept this paper, (but also OK if it gets rejected), for the following reasons: (1) the idea is novel and interesting; (2) the writing of the paper is below conference standard and very hard to read, especially the method and the experiment sections.", "===========================================================================", "Novelty and significance", "I really like the idea of doing math reasoning in latent space.", "The idea is definitely novel and interesting.", "It is related to existing works such as neural logic induction[1] and planning in latent space[2].", "It is amazing that one can do multiple steps of math reasoning after only training the model using data from one single step.", "It would be interesting to see how it can improve existing learning-based theorem provers.", "My question is if we want to integrate the proposed method into theorem provers, after multiple steps of math reasoning, how would us know the goal has been proved? Is it possible that we can train a decoder that maps back from the latent space to the formula space?", "Also can it work with theorems that decompose the current goal into several sub-goals?", "I know these are not the concerns of this paper, but I would be really grateful if you could provide some intuitive answers!", "===========================================================================", "Writing", "The paper is not well-organized and not written in a consistent way. For the method and the experiment sections, I need to jump back and forth several times in order to understand what the authors are trying to say.", "1. Typo: Third paragraph in section 1, \"...which is makes use of ...\".", "2. It's very confusing when the authors introduce \\sigma and \\omega in the beginning of section 4: why would you need two networks predict the same thing?", "3. Mentioning \"merging \\sigma and \\omega, is left for future work\" is confusing before formally introducing \\sigma and \\omega.", "4. Even when the authors formally introduce \\sigma and \\omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.", "5. In fact, I don't know why \\omega needs to output p. It's never mentioned in the experiment section.", "6. The rationale of the two tower design (why not combine two) is not clearly explained.", "7. Typo: Page 5 last paragraph, \"... negative instances for for each ...\".", "8. The itemized part in 5.3, \"...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx\". However, both 3 and 4 are not baselines!", "9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.", "10. Reading the baselines before the experiments is very confusing.", "For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a \"random baseline\".", "11. Baseline 2 is actually referred to as \"usage baseline\" but this name is not introduced in the itemized part.", "[1] Rockt\u00e4schel, Tim, and Sebastian Riedel. \"End-to-end differentiable proving.\" Advances in Neural Information Processing Systems. 2017.", "[2] Srinivas, Aravind, et al. \"Universal planning networks.\" arXiv preprint arXiv:1804.00645 (2018)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-request_typo", "arg-request_explanation", "none", "none", "none", "none", "arg-request_typo", "none", "none", "none", "arg-request_explanation", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_typo_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_typo_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 103, "sentences": ["This paper proposes using spectral normalization (SN) as a regularization for adversarial training, which is based on [Miyato et. al., ICLR 2018], where the original paper used SN for GAN training.", "The paper also uses the results from [Neyshabur et. al., ICLR 2018], where the original paper provided generalization bounds that depends on spectral norm of each layer.", "The paper is well written in general, the experiments are extensive.", "The idea of studying based on the combination of the results from two previous papers is quite natural, since one uses spectral normalization in practice for GAN training, and the other provides generalization bound that depends on spectral norm.", "The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.", "The experimental result itself is quite comprehensive.", "On the other hand, this paper provides specific generalization bounds under three adversarial attack methods, which explains the power of SN under those settings.", "However, it is not clear to me that these are some novel results that can better help adversarial training."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 104, "sentences": ["This paper proposes variational selective autoencoders (VSAE) to learn the joint distribution model of full data (both observed and unobserved modalities) and the mask information from arbitrary partial-observation data.", "To infer latent variables from partial-observation data, they introduce the selective proposal distribution that switches encoders depending on whether each input modality is observed.", "This paper is well written, and the method proposed in this paper is nice.", "In particular, the idea of the selective proposal distribution is interesting and provides an effective solution to deal with the problem of missing modality in conventional multimodal learning.", "The experiment is also well structured and shows higher performance than the existing models.", "However, I have some questions and comments, so I\u2019d like you to answer them.", "Comments:", "- The authors state that x_j is sampled from the \"prior network\" to calculate E_x_j in Equation 10, but I didn\u2019t understand how this network is set up. Could you explain it in detail?", "- The authors claim that adding p(m|z) to the objective function (i.e., generating m from the decoder) allows the latent variable to have mask information.", "However, I don\u2019t know how effective this is in practice.", "Specifically, how performance differs compared to when p (m | z) is not used and the decoder p (x | z, m) is conditioned by the mask included in the training set instead of the generated mask?", "- Why did you not do image inpainting in higher-dimensional experiments like Ivanov et al. (2019), i.e., considering each pixel as a different modality? Of course, I know that Ivanov et al. require the full data as input during training, but I\u2019m interested in whether VSAE can perform inpainting properly even if trained given imperfect images."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-structuring_quote", "none", "arg-request_clarification", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-structuring_quote_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label"]}
{"abstract_id": 105, "sentences": ["* Summary", "The paper proposes an improved method for computing derivatives of the expectation.", "Such problems arises with many probabilistic models with noises or latent variables.", "The paper proposes a new gradient estimator of low variance applicable in certain scenarios, in particular it allows training of generative models in which observations and/or latent variables are discrete.", "The submission clearly improves the state-of-the-art, experimentally demonstrates the method on several problems comparing with the alternative techniques.", "In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.", "The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.", "In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.", "It also contains lots of additional technical details and experiments in the appendix, which I unfortunately did not review.", "*", "Clarity", "In the abstract the paper promises more than it delivers.", "Many problems can be cast as optimizing an expectation-based objective.", "The result does not at all apply to all of them.", "The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.", "Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional \u201ccontinuous variables\u201d (to which the reparameterization trick is applicable).", "This very much limits the utility of the method.", "In particular it is not applicable to learning e.g. sigmoid belief networks [Neal, 92] (with conditional Bernoulli units) and many other problems.", "\u201creparametrizable distributions\u201d", "A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.", "Because of the above many discussions about discrete vs. continuous variables are missleading.", "Section 2.", "The notation of the true distribution as \u201cq\u201d the model as p and the approximate posterior of the model as \u201cq\u201d again is inconsistent.", "I find the background on ELBO and GANs unnecessary occluding the clarity at this point.", "For the purpose of introduction, it might be better to give examples of expectation objectives such as:", "- dropout: q is the distribution of NN outputs given the input image and integrating out latent dropout noises, gamma are parameters of this NN.", "- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.", "- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.", "Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).", "Section 3.", "Contrary to the discussion, there are examples of non-negative distributions to which the reparameterization trick can be applied, including log-Normal and Gamma distributions.", "Method:", "In the case when Rep trick is applicable, is it identical to GO?", "The difference seems to be only in that the mapping tau may be different from Q^-1.", "However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.", "Yet, in Fig.1 some difference is observed between the methods, why is that so?", "Sec 7.1", "\u201cWe adopt the sticking approach hereafter\u201d. Does it mean it is applied with all experiments with GO?", "* Related Work", "The state of the art allows combining differentiable and non-differentiable pieces of computation:", "[Schulman, J., Heess, N., Weber, T., Abbeel, P.: Gradient estimation using stochastic computation graphs.]", "I believe it should be discussed in related work.", "Limitations / where the proposed method brings an improvement should be highlighted.", "* Technical Correctness", "Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.", "Equation (7) (integration by parts) holds only with some additional requires on f.", "Theorem 1 does not take account for the above conditions."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_result", "none", "arg-request_edit", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "none_label", "none_label"]}
{"abstract_id": 106, "sentences": ["This work is clearly the work of a large team.", "the paper clearly defines what is being done.", "I have spent a lot of effort with MCTS. I can not find the corresponding allowance for stochastic jumps in the latent space long horizon learning.", "You have the phrase \"allowing to imagine thousands of trajectories in parallel\". I would like some elaboration on this. I think you have ideas of what is happening in the latent space that I am not following.", "You are heavy on the machinery and math. I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. How does the effort compare to the 1E9 steps of the base line your refer to?", "Your team is highly competent your style is distinct.", "Now may be the time to move you to understanding what structures get learned in latent space, are the in fact compact, diverse?", "Perhaps there is room for memory/memories in the latent space?", "Massive effort, nice results. Now for learning on our part (the humans)."], "labels": ["none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_explanation", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 107, "sentences": ["The paper proposed to use a prior distribution to constraint the network embedding.", "The paper used very restricted Gaussian distributions for the formulation.", "The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec."], "labels": ["arg-structuring_summary", "none", "none"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label"]}
{"abstract_id": 108, "sentences": ["Algorithms for Streaming data using a machine learning oracle is analyzed theoretically and empirically.", "The idea is to build on some recent work (Hsu 19) which used RNNs to predict heavy hitters in streaming data.", "The purpose of this paper is to analyze whether such an oracle can help streaming algorithms to obtain improved bounds.", "I am not very familiar with this line of research so my comments will be more general in this case.", "The idea of improved bounds for streaming algorithms using machine learning oracle seems to be very appealing to me.", "The authors present novel theoretical results supporting this.", "Experiments are performed on real as well as synthetic datasets using Hsu et al.\u2019s method as an oracle.", "Two real-world problems are selected, i.e., distinct packets in a network flow, Number of occurrences of each type of search query, and it is shown that using a oracle improves performance as compared to methods that do not use the oracle.", "Overall, I think the paper seems to be  an interesting direction which has both formal guarantees and experiments validating them in real-world datasets.", "One issue is perhaps, very little in terms of related work.", "I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 109, "sentences": ["This paper proposes two new architectures for processing set-structured data: An RNN with an accumulator on its output, and an RNN with gating followed by an accumulator on its output.", "While sensible, this seems to me to be too minor a contribution to stand alone as a paper.", "Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques."], "labels": ["arg-structuring_summary", "none", "none"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label"]}
{"abstract_id": 110, "sentences": ["The paper proposes a technique to perform reasoning on mathematical formulas in a latent space.", "The model is trained to predict whether a rewrite rule can be applied to a formula given its latent representation.", "When the rewrite is possible, the model also predicts the embedding of the resulting formula.", "Experiments show that the network can be applied multiple steps in a row, while operating only in the embedding space.", "1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \\sigma and \\alpha become unnecessary and we only need to train \\omega.", "Did you try to have a single network?", "This seems a much more natural approach to me, and I'm surprised that you did not start with that.", "From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.", "The role of \\sigma seems very redundant given \\omega.", "2. If you consider \\sigma, why do you also predict the rewrite success with \\omega? Couldn't it be simply a function from S x S -> L ?", "3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.", "It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.", "4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.", "5. To train \\sigma and \\omega, the negative instances are selected randomly.", "You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?", "6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').", "I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).", "This could be a possibility to remove the noise you have when doing multi-step operations (and potentially go way beyond 4 steps).", "Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.", "Overall, the model is a bit complicated (e.g. question 1.), but the results are promising, the paper is well written, and the ability to manipulate formula embeddings is probably going to be useful in the context of theorem proving."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "none", "arg-request_edit", "arg-request_explanation", "none", "arg-request_explanation", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 111, "sentences": ["This paper studies the dynamics-based curiosity intrinsic reward where the agent is rewarded highly in states where the forward dynamic prediction errors are high in an embedding space (either due to complexity of the state or unfamiliarity).", "Overall I like the paper, it's systematic and follows a series of practical considerations and step-by-step experimentations.", "One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.", "While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.", "In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.", "Another area of improvement is the experiments around VAE.", "While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.", "Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).", "An interesting area for future work could be on early stopping techniques for embedding training - it seems that RFs perform well without any training while in some scenarios the IDFs work overall the best.", "So it would be interesting to explore how much training is needed for the embedding model.", "RFs are never trained and IDFs are continuously trained.", "So maybe somewhere in between could be the sweet spot with training for a short while and then fixing the features."], "labels": ["arg-structuring_summary", "none", "none", "none", "arg-request_edit", "arg-request_experiment", "none", "arg-request_edit", "arg-request_experiment", "arg-request_experiment", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 112, "sentences": ["The authors introduce a  novel  on-policy  temporally  consistent  exploration  strategy, named Neural  AdaptiveDropout Policy Exploration (NADPEx), for deep reinforcement learning agents.", "The main idea is to sample from a distribution of plausible subnetworks modeling the temporally consistent exploration.", "For this, the authors use the ideas of the standard dropout for deep networks.", "Using the proposed  dropout transformation that is differentiable, the authors show that the KL regularizers on policy-space play an important role in stabilizing its learning.", "The experimental validation is performed on continuous control learning tasks, showing the benefits of the proposed.", "This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.", "This poses a challenge in evaluating this paper.", "Nevertheless, this paper clearly explores and offers a novel approach for more efficient on-policy exploration which allows for more stable learning compared to traditional approaches.", "Even though the authors answer positively to each of their four questions in the experiments section , it would like that the authors provide more intuition why these improvements occur and also outline the limitations of their approach."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 113, "sentences": ["Summary:", "In the semi-supervised self-training setting, this paper proposes to select a certain subset of unlabelled data for training rather than all unlabelled data, where the ensemble of confidence scores of the trained model in iterations is used to guide the selection.", "Strong points:", "It is a good idea to conduct an ensemble based on the confidence scores of trained models in iterations, although the authors did not mention any theoretical explanation or guarantee behind this.", "Weak points:", "1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.", "Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter.", "Therefore, the technical contribution of this paper is moderate.", "Zhu, Xiaojin. \"Semi-supervised learning literature survey.\" Computer Science, University of Wisconsin-Madison 2.3 (2006): 4.", "2) The writing is poor and hard to follow.", "First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.", "For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?", "From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.", "The descriptions of the datasets used are not clear, e.g., the number of classes for each data.", "Second, many typos and grammar errors need to fix, e.g., \"the proposed SST is suitable for lifelong learning which make use...\", \"the error 21.44% was lower than\" 18.97?", "3) The overall performance of the proposed SST in the experiments is not convincing and not promising.", "First, the labeled data portion is fixed and is relatively high compared to most standard semi-supervised learning settings .", "Second, SST itself is only comparable with or even worse than the state-of-art methods.", "Combining SST with other existing techniques can help.", "However, the additional cost is expensive.", "Further demonstrations are necessary for the proposed SST method."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_experiment", "none", "arg-request_clarification", "arg-request_typo", "none", "none", "none", "arg-request_edit", "none", "arg-request_result"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_clarification_label", "arg-request_typo_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 114, "sentences": ["###Summary###", "This paper tackles the multi-source domain adaptation by aggregate multiple source domains dynamically during the training phase.", "The observation is that in many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.", "Firstly, the paper derives a multiple-source domain adaptation upper-bound from single-to-single domain adaptation generalization bound, based on the theoretical work from Cortes et al (2019).", "The idea is similar to Zhao et al (2019), which introduces a weighted parameter \\alpha to combine the source domains together.", "Secondly, based on the theoretical result, the paper proposes an algorithm to minimize the upper bound of the theoretical result.", "The upper bound can be simplified as the quartic form (Eq. 4) and can be optimized with the Lagrangian form.", "Since no closed-form expression for the optimal v can be derived, the authors propose to use binary search to find it.", "Based on the theoretical results and the algorithm, the paper introduces Domain AggRegation Network (DARN), which contains a base network for feature extraction, h_y to minimize the task loss and h_d to evaluate the discrepancy between each source domain and target domain.", "The loss is aggregation with the parameter \\alpha.", "Finally, the paper conduct experiments on sentimental analysis benchmark, Amazon Review and digit datasets.", "The paper selects MDAN, DANN, MDMN as the baselines.", "On the amazon review dataset, the performance of the proposed DARN model is comparable with the MDMN baseline.", "On the digit dataset, the model can outperform the baselines.", "### Novelty ## #", "The theoretical results in this paper are extended from Cortes et al (2019) and Zhao et al (2018).", "Thus, the theoretical contribution of this paper is limited.", "The algorithm proposed in this paper is interesting.", "However, the motivation of the proposed method is to minimize the upper bound, not the loss itself, i.e. L_T(h, f_T).", "Intuitively, when the upper bound of the loss is minimized, it will be beneficial to minimize the loss itself.", "But it's not guaranteed as the upper bound contains other variables, such as the number of training samples and model complexity.", "If the training samples and model complexity (think about the parameters in the deep models) are significantly large, the upper bound of the loss might be also very large.", "As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.", "The selected baselines are not sufficient.", "The improvement from the baselines is also limited.", "###Clarity## #", "Overall, the paper is well organized and logically clear.", "The images are well-presented and well-explained by the captions and the text.", "The derivation of the algorithm in Sec 3.2 is logically clear and easy to follow.", "###Pros# ##", "1) The paper proposes a new theoretical upper-bound based on the prior works, the upper-bound and its derivation are interesting and heuristic to the domain adaptation research community.", "2) The paper is applicable to many practical scenarios since the data from the real-world application is typically collected from multiple sources.", "3) The paper is overall well-organized and well-written.", "The claims of the paper are verified by the experimental results.", "## #Cons# ##", "1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.", "The idea is intuitive when the upper bound is small.", "However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.", "It's an intuitive idea to weight different source domains in multi-source domain adaptation.", "The paper derives the weight by the Lagrangian form to minimize the upper bound.", "While another trivial trick is to evaluate \\alpha by the domain closeness between each source domain with the target domain.", "2) The experimental results provided in this paper are weak.", "In the abstract and introduction ,  the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.", "But the paper only provides empirical results on sentimental analysis and digit recognition.", "Besides, the results on the sentimental analysis are comparable with the compared baselines.", "It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:", "DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019.", "http://ai.bu.edu/DomainNet/", "Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017.", "http://hemanthdv.org/OfficeHome-Dataset/", "3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).", "Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.", "To improve the rating, the author should explain the following questions:", "1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \\alpha by the closeness of the source domain with the target domain?", "2) .", "In the introduction, the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.", "While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 115, "sentences": ["This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.", "In active learning, there is generally a trade-off between data efficiency and computational cost.", "This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both.", "The improvements over basic ensembling are rather minimal, at the cost of extra computation.", "More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so.", "On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments).", "As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout?", "At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?", "The novelty of this method is minimal.", "The technique basically fills out the fourth entry in a Punnett square.", "The paper is well-written, has good experiments, and has a comprehensive related work section.", "Overall, this paper is good, but is not novel or important enough for acceptance."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 116, "sentences": ["Summary:", "In this paper, the authors propose a framework for continual learning based on explanations for performed classifications of previously learned tasks.", "In this framework, an average saliency map is computed for all images in the test set of a previous task to identify image regions, which are important for that task.", "When learning the next task, this average saliency map is used in an attention mechanism to help learning the new task and to prevent catastrophic forgetting of previously learned tasks.", "Furthermore, the authors propose a new metric for the goodness of a saliency map by taking into account the number of pixels in the map, the average distance between pixels in the map, as well as the prediction probability given only the salient pixels.", "The authors report that their approach achieves the best average classification accuracy for 3 out of 4 benchmark datasets compared to other state-of-the-art approaches.", "Relevance:", "This work is relevant to researchers in the field of continual/life-long learning, since it proposes a framework, which should be possible to integrate into different approaches in this field.", "Significance:", "The proposed work is significant, since it explores a new direction of using learner generated, interpretable explanations of the currently learned task as help for learning new tasks.", "Furthermore, it proposes a new metric for the goodness of saliency maps.", "Soundness:", "In general, the proposed approach of using the average saliency map as attention mask for learning appears to be reasonable.", "However, the following implicit assumptions/limitations of the approach should be made more clear:", "- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)", "- the locations for important features should be comparatively stable (for example, one would expect the average saliency map to become fairly meaningless if important features, such as the face of a dog, can appear anywhere in the image.", "Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)", "Furthermore, the authors appear to imply that increased FSM values for an old task after training on a new task indicate catastrophic forgetting.", "While this is a reasonable assumption, it does not necessarily seem to be the case that a larger, more disconnected saliency map indicates worse classification performance.", "Comparatively small changes in FSM may not affect the classification performance at all, while larger changes may not necessarily lead to worse classifications either.", "For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.", "Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.", "Evaluation:", "The evaluation of the proposed approach on the four used datasets appears to be reasonable and well done.", "However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.", "Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.", "Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.", "Clarity:", "The paper is clearly written and easy to follow.", "One minor issue is that the first sentence of the third paragraph in Section 4 is not a full sentence and therefore difficult to understand.", "Furthermore, on page 6, it is stated that the surrounding square $\\hat{x}_i$ is 15 x 15 pixels, while the size of the square $x_i$ is 10 x 10.", "This appears strange, since it would mean that $x_i$ cannot be in the center of $\\hat{x}_i$."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 117, "sentences": ["This paper proposes a self-supervised reinforcement learning approach, Mutual Information-based State-Control (MISC), which maximizes the mutual information between the context states (i.e. robot states) and the states of interest (i.e. states of an object to manipulate).", "For this, they first split the entire state into two mutually exclusive sets of the context states and the states of interest.", "Then, the neural discriminator is trained to estimate the (lower-bound of) mutual information between the two states.", "The (mutual-information) intrinsic reward is computed by the trained neural discriminator, which is used for policy pre-training.", "Experimental results show that MISC helps to improve the performance of DDPG/SAC and the learned discriminator can be transferred to different environments.", "Detailed comments and questions:", "- In the paper, the states are represented by only object positions (x, y, z). Is this sufficient? (e.g. velocity is unnecessary?)", "- For MISC, the additional assumption is required: the agent should know that which parts of the states are its own controllable state and object's state respectively.", "Is this additional assumption realistic enough and has it been adopted in other previous works? Is there any way to discriminate robot states and object states automatically?", "- Can MISC deal with the problems where the number of objects of interest is more than two? In this case, how can we define mutual information?", "- In Eq. (4), T(x_1:N, y_1:N) is assumed to be decomposable into the sum of T(x_t, y_t) / N. Can this make the lower bound (Eq. (3)) arbitrarily loose since the class of functions becomes very limited?", "- Detailed experimental setups are missing. e.g. network architecture, hyper-parameters (e.g. I_tran^max), and how they were searched.", "- Similarly to the problem of sparse reward, if the robot and the object are far apart and it is difficult to reach the object with random exploration, it would also be difficult to train the mutual information discriminator. How was the discriminator trained? How many time steps were used to train MI discriminator?", "- It seems that the MI discriminator learns to estimate the 'proximity' between the robot and the object.", "Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.", "- For the MISC+DIAYN, what if we train the agent using MISC and DIAYN at the same time, instead of pre-training MISC first and fine-tuning DIAYN later?", "- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.", "- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.", "- It seems that MISC is beneficial when the robot should get closer to the object for the success of the task.", "Then, how about the opposite situation? What if the task requires that the robot should 'avoid' the object of interest? Does MISC still work? Is it helpful for the improvement of sample efficiency?", "- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.", "- In section 4.3, what happens if we transfer the learned discriminator to Pick&Place from Push that has a gripper fixed to be closed, rather than the opposite direction (i.e. from Pick&Place to Push)? Does the MISC-t still well work? Can the learned MI discriminator be transferred to different tasks even when the state space is different?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-request_explanation", "none", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment", "none", "arg-request_clarification", "arg-request_experiment", "none", "none", "none", "arg-request_clarification", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 118, "sentences": ["This paper proposed a few-shot graph classification algorithm based on graph neural networks.", "The learning is based on a large set of base class labeled graphs and a small set of novel class labeled graphs.", "The goal is to learn a classification algorithm over the novel class based on the sample from the base class and novel class.", "The learning process constitutes of the following steps.", "First, the base class is classified into K super classes based on the spectral embedding of the graph (onto distributions over the corresponding graph spectrum) and the k-means algorithm with the Wasserstein metric.", "Second, for each super class, the classification is done through a feature extractor and a classifier.", "In the training of the feature extractor and classifier, the author introduces a super-graph with each node representing a super class.", "Finally, in the fine-tuning stage, the feature extractor is fixed, and the classifier is trained based on the novel class.", "This work seems to be the first attempt to adopt the few-shot learning in graph classification tasks.", "The architecture is novel, and the classification of graph based on spectral embedding together with the Wasserstein metric is novel to me.", "I vote for rejecting this submission for the following concerns.", "1. The classification of base class into super classes seems questionable to me.", "In the meta-learning language, the author attempts to learn a good representation of graphs based on different graph classification tasks generated by a task distribution.", "In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).", "Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.", "2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.", "I would appreciate it if the author could provide more explanation on the introduction of the super-graph in training."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 119, "sentences": ["The study is motivated by the observation that the Q-value matrix in reinforcement learning problems often has a low-rank structure.", "The paper proposes an approach called structured value-based planning or learning, where the Q matrix or the Q function is estimated from incomplete observations based on the prior that it is low-rank.", "The proposed strategy is demonstrated in stochastic control tasks and reinforcement learning applications.", "The paper is clearly written and the experimental results show that the proposed strategy leads to performance gains especially in problems where the Q matrix indeed conforms to a low-rank model.", "A few comments and questions:", "- The assumption that the Q matrix should be low-rank is demonstrated with several experiments.", "Is there any theoretical motivation or guarantee for this assumption as well?", "- The experimental results show that the proposed strategy performs well in problems that are low-rank, while the performance may degrade in problems where the low-rank assumption is not met. Would it be possible to detect the rank of the problem in a dynamical manner (i.e., during the learning), so that the number of incomplete observations of Q can be increased to improve the performance, or the solution strategy (e.g. whether to use the low-rank assumption or not) can be adapted to the nature of the problem?", "- The Q-value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes. Would it be possible to go beyond the low-rank assumption and propose and use a more elaborate type of prior that employs the special structure of MDPs?", "- Please clearly define the notation used in Section 4.2."], "labels": ["none", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_clarification_label"]}
{"abstract_id": 120, "sentences": ["This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes.", "The key components of the approach are :", "- increasing the batch size by a factor 8", "- augmenting the width of the networks by 50%", "These first two elements result in an Inception score (IS) boost from 52 to 93.", "- the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.", "The core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.", "Variations of this threshold lead to variations in FD and IS, as shown in insightful experiments.", "The comments that more data helps (internal dataset experiments) is also informative.", "Very nice to have included negative results and detailed parameter sweeps.", "This is a very nice work with impressive results, a great progress achievement in the field of image generation.", "Very well written.", "Suggestions/questions:", "- it would be nice to also propose unconditioned experiments.", "It would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve.", "- I understand that no data augmentation was used during training?", "- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?", "- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.", "- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.", "- It would be nice to display more Nearest neighbors for the dog image.", "- It would be nice to add a figure of random generations.", "- make the bib uniform: remove unnecessary doi - url - cvpr page numbers"], "labels": ["arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_edit", "arg-request_clarification", "arg-request_explanation", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_result", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 121, "sentences": ["This paper studied learning unsupervised node embeddings by considering the structural properties of networks.", "Experimental results on a few data sets prove the effective of the proposed approaches over existing state-of-the-art approaches for unsupervised node embeddings.", "Strength:", "- important problem and interesting idea", "- the proposed approach seems to be effective according to the experiments", "Weakness:", "- some parts of the paper are quite unclear", "- the complexity of the proposed algorithm seems to be very high", "- the data sets used in the experiments are very small", "Details:", "-In the introduction, \"it is in general impossible to find an embedding in R^d such that ...\", why do we have to make v and v'(and u, and u') far from each other?", "- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part", "- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right \uff1f", "- In Table 2 and 3, how are the degree and block information leveraged into the model?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_clarification", "arg-request_edit", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_clarification_label", "arg-request_edit_label", "arg-request_clarification_label"]}
{"abstract_id": 122, "sentences": ["This paper proposes a method to address the interesting task, i.e. controllable human activity synthesis, by conditioning on the previous frames and the input control signal.", "To synthesis the next frame, a Pose2Pose network is proposed to first transfer the input information into the next frame body structure and object.", "Then, a Pose2Frame network is applied to generate the final result.", "The results on several video sequences look nice with more natural boundaries, object, and backgrounds compared to previous methods.", "Pros:", "1. The proposed Pose2Pose successfully transfer the pose conditioned on the past pose and the input control signal.", "The proposed conditioned residual block, occlusion augmentation and stopping criteria seem to help the Pose2Pose network work well.", "Besides, the object is also considered in this network, which makes the method generalized well to the videos where human holds some rigid object.", "2. The Pose2Frame network is similar to previous works but learns to predict the soft mask to incorporate the complex background and to produce shallow.", "The mask term in Eq. (7) seems to work well for the foreground (body+object) and the shallow regions.", "3. The paper is easy to follow.", "Cons:", "1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.", "Results on more scenes will make the performance more convincing.", "I also wonder if the video data will be released, which could be important for the following comparisons.", "2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.", "Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?", "I assume the authors first train the Pose2Pose network, then use the output to train the Pose2Frame network.", "Otherwise, the artifacts from Pose2Pose will affect the testing performance of the Pose2Frame network.", "3. The mask term seems to work well for the shallow part.", "I wonder how the straightforward regression term plus the smooth term will perform for the mask.", "Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "arg-request_result", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 123, "sentences": ["The authors present an architecture search method where connections are removed with sparse regularization.", "It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet.", "There are a few grammatical/spelling errors that need ironing out.", "e.g. \"In specific\" --> \"Specifically\" in the abstract, \"computational budge\" -> \"budget\" (page 6) etc.", "A few (roughly chronological comments).", "- Pioneering work is not necessarily equivalent to \"using all the GPUs\"", "- There are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work!", "- From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?", "-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.", "- How do you specifically encode the number of surviving connections? Is it entirely dependent on budget?", "- You should add DARTS 1st order to table 1.", "- Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?", "- The ablation study is good, and the results are impressive.", "I propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time.", "However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified. ------------", "UPDATE: Score changed based on author resposne ------------"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_typo", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_clarification", "arg-request_experiment", "arg-request_clarification", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 124, "sentences": ["The paper presents a Neural Network based method for learning ordinal embeddings only from triplet comparisons.", "A nice, easy to read paper, with an original idea.", "Still, there are some issues the authors should address:", "- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?", "- the authors state that they use \"the power of DNNs\" while they are experimenting with a neural network with only 4 layers. While there is no clear line between shallow and deep neural networks, I would argue that a 4 layer NN is rather shallow.", "- the authors fix the number of layers of the used network based on \"our experience\". For the sake of completeness, more experiments in this area would be nice.", "- for Figure 6, there is not a clear conclusion.", "While, it supports that \" that logarithmic growth of the layer width respect to n is enough to obtain desirable performance.\"  I don't see a clear conclusion of how to pick the width of hidden layers, maybe a better representation could be used.", "- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).", "- in section 4.4 when comparing the proposed approach with another methods why not use more complex datasets (like those used in section 4.3)", "- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.", "- in section 4.3 how is the reconstruction built (Figure 3b)?", "A few typos found:", "- In figure 3 (c) \"number |T of input\" should be  \"number |T| of input\"", "- In figure 5 (a) \"cencept\" should be \"concept\"", "- In figure 8 \"Each column corresponds to ...\" should be \"Each row corresponds to ...\".", "- In the last paragraph of A1 \"growth of the layer width respect\" should be \"growth of the layer width with respect\"", "- In the second paragraph of A2 \"hypothesize the that relation\" should be \"hypothesize that the relation\".", "- In section 4.3 last paragraph, first sentence: \"with the maximunm number\" should be \"with the maximum number\""], "labels": ["arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_experiment", "none", "none", "arg-request_clarification", "arg-request_explanation", "none", "arg-request_clarification", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_explanation_label", "none_label", "arg-request_clarification_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 125, "sentences": ["Brief summary:", "This paper presents a deep decoder model which given a target natural image and a random noise tensor learns to decode the noise tensor into the target image by a series of 1x1 convolutions, RELUs, layer wise normalizations and upsampling.", "The parameter of the convolution are fitted to each target image, where the source noise tensor is fixed.", "The method is shown to serve as a good model for natural image for a variety of image processing tasks such as denoising and compression.", "Pros:", "* an interesting model which is quite intriguing in its simplicity.", "* good results and good analysis of the model", "* mostly clear writing and presentation (few typos etc. nothing too serious).", "Cons and comments:", "* The author say explicitly that this is not a convolutional model because of the use of 1x1 convolutions.", "I disagree and I actually think this is important for two reasons.", "First, though these are 1x1 convolutions, because of the up-sampling operation and the layer wise normalizations the influence of each operation goes beyond the 1x1 support.", "Furthermore, and more importantly is the weight sharing scheme induced by this - using convolutions is a very natural choice for natural images (no pun intended) due to the translation invariant statistics of natural images.", "I doubt this would have worked so well hadn't it been modeled this way (not to mention this allows a small number of parameters).", "* The upsampling analysis is interesting but it is only done on synthetic data - will the result hold for natural images as well? should be easy to try and will allow a better understanding of this choice.", "Natural images are only approximately piece-wise smooth after all.", "* The use of the name \"batch-norm\" for the layer wise normalization is both wrong and misleading.", "This is just channel-wise normalization with some extra parameters - no need to call it this way (even if it's implemented with the same function) as there is no \"batch\".", "* I would have loved to see actual analysis of the method's performance as a function of the noise standard deviation.", "Specifically, for a fixed k, how would performance increase or decrease, and vice versa - for a given noise level, how would k affect performance.", "* The actual standard deviation of the noise is not mentioned in any of the experiments (as far as I could tell)", "* What does the decoder produce when taking a trained C on a given image and changing the source noise tensor?", "I think that would shed light on what structures are learned and how they propagated in the image, possibly more than Figure 6 (which should really have something to compare to because it's not very informative out of context)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_result", "none", "none", "arg-request_edit", "arg-request_edit", "arg-request_explanation", "arg-request_clarification", "arg-request_result", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label"]}
{"abstract_id": 126, "sentences": ["This paper presents a method for adapting a model that has been trained to perform one task, so that it can perform a new task (potentially without using any new training data at all\u2014i.e., zero-shot learning).", "In some ways the presented work is a form of meta-learning or *meta-mapping* as the authors refer to it.", "The premise of the paper is very interesting and the overall problem is definitely of high interest and high potential impact.", "I believe that the presentation of the proposed method can be significantly improved.", "The method description was a bit confusing and unclear to me.", "The experimental results presented were all done on small synthetic datasets and it\u2019s hard to evaluate whether the method is practically useful.", "Furthermore, no comparisons were provided to any baselines/alternative methods.", "For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.", "Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively. This makes a comparison with MAML even more desirable.", "Without any comparisons it\u2019s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.", "In summary, I feel the paper tackles an interesting problem with an interesting approach, but the content could be organized much better.", "Also, this work would benefit significantly from a better experimental evaluation.", "For these reasons I lean towards rejecting this paper for now, but would love to see it refined for a future machine learning conference.", "Also, the work by Platanios, et al. on contextual parameter generation is very relevant to this work as it tackles multi-task learning using HyperNetworks.", "It may be worth adding a short discussion/comparison to that work as it also considers zero-shot learning.", "Minor comments:", "- Capitalize: \u201csection\u201d -> \u201cSection\u201d, \u201cappendix\u201d -> \u201cAppendix\u201d, \u201cfig.\u201d -> \u201cFigure\u201d.", "Sometimes these are capitalized, but the use is inconsistent throughout the paper.", "- \u201cHold-out\u201d vs \u201cheld-out\u201d .", "Be consistent and use \u201cheld-out\u201d throughout."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_edit", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "arg-request_experiment", "none", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_typo", "none", "arg-request_typo", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_typo_label", "none_label", "arg-request_typo_label", "arg-request_edit_label"]}
{"abstract_id": 127, "sentences": ["Overall:", "This paper introduces the Scratchpad Encoder, a novel addition to the sequence to sequence (seq2seq) framework and explore its effectiveness in generating natural language questions from a given logical form.", "The proposed model enables the decoder at each time step to modify all the encoder outputs, thus using the encoder as a \u201cscratchpad\u201d memory to keep track of what has been generated so far and to guide future generation.", "Quality and Clarity:", "-- The paper is well-written and easy to read.", "-- Consider using a standard fonts for the equations.", "Originality :", "The idea of question generation: using logical form to generate meaningful questions for argumenting data of QA tasks is really interesting and useful.", "Compared to several baselines with a fixed encoder, the proposed model allows the decoder to attentively write \u201cdecoding information\u201d to the \u201cencoder\u201d output.", "The overall idea and motivation looks very similar to the coverage-enhanced models where the decoder also actively \u201cwrites\u201d a message (\u201ccoverage\u201d) to the encoder's hidden states.", "In the original coverage paper (Tu et.al, 2016), they also proposed a \u201cneural network based coverage model\u201d where they used a general neural network output to encode attention history, although this paper works differently where it directly updates the encoder hidden states with an update vector from the decoder.", "However, the modification is slightly marginal but seems quite effective.", "It is better to explain the major difference and the motivation of updating the hidden states. -------------------", "Comments:", "-- In Equation (13), is there an activation function between W1 and W2?", "-- Based on Table 1, why did not evaluate the proposed model with beam-search?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-request_edit", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 128, "sentences": ["The paper considers the problem of dictionary learning.", "Here the model that we are given samples y, where we know that y = Ax where A is a dictionary matrix, and x is a random sparse vector.", "The goal is typically to recover the dictionary A, from which one can also recover the x under suitable conditions on A. The paper shows that there is an alternating optimization-based algorithm for this problem that under standard assumptions provably converges exactly to the true dictionary and the true coefficients x (up to some negligible bias).", "The main comparison with prior work", "is with [1].", "Both give algorithms of this type for the same problem, with similar assumptions (although there is some difference; see below).", "In [1], the authors give two algorithms: one with a better sample complexity than the algorithm presented here, but which has some systematic, somewhat large, error floor which it cannot exceed, and another which can obtain similar rates of convergence to the exact solution, but which requires polynomial sample complexity (the explicit bound is not stated in the paper).", "The algorithm here seems to build off of the former algorithm; essentially replacing a single hard thresholding step with an IHT-like step.", "This update rule is able to remove the error floor and achieve exact recovery.", "However, this makes the analysis substantially more difficult.", "I am not an expert in this area, but this seems like a nice and non-trivial result.", "The proofs are quite dense and I was unable to verify them carefully.", "Comments:", "- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.", "The authors claim that some amount of noise can be tolerated, but do not quantify how much.", "- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.", "[1] Arora, S. Ge, R., Ma, T. and Moitra, A. Simple, Efficient, and Neural Algorithms for Sparse Coding. COLT 2015."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 129, "sentences": ["Second update:", "We thank the authors for updating their paper.", "The work is now improving, and is on the right track for publication at a future conference.", "There are a few comments on the new results, and suggestions for further improvement:", "* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal. Without careful treatment of this effect, these results are vacuous.", "* There is still no comparison with competing nonparametric tests on the fMRI data.", "* the results linking the COCO and MINE estimators are interesting.", "Some statements don't make sense, however, eg. \"HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.", "\" First, f and g are functions, not kernels.", "Second, testing with HSIC or COCO does not require generalisation to unseen data points: this is why testing is an easier problem than regression. For HSIC testing, I am surprised to read in the footnote that the Gamma approximation report significantly more than 5% errors, especially given the Table 4 results that show the correct level. In any case, I very strongly suggest using a permutation approach to obtain the test threshold for HSIC, which is by far the most robust and reliable method.", "The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.", "Permutation gives a guarantee of the correct level. See the NeurIPS paper for details. Once you have verified the correct false positive rate for the permutation threshold, then you can compute the p-value on the alternative.", "While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e. the Berrett  and Samworth test. In addition, HSIC is a non-adaptive test, but your test is adaptive, so a fairer comparison would be to a modern adaptive test such as \"An Adaptive Test of Independence with Analytic Kernel Embeddings.\" *", "The false positive rate in the sanity check is far below the design level of 0.05.", "This is as I expected, given the use of the Hoeffding bound.", "This should be stated clearly in the main text, and not disclosed in the final sentence of the final page of the appendix.", "====================", "Update: thank you for your rebuttal.", "\"the necessary and sufficient condition of dependency, is more general and is complementary to other techniques that make stronger assumptions about the data. \"", "The alternative tests listed are nonparametric.", "That is to say, unlike Pearson correlation, they do not make specific parametric assumptions on the nature of the dependence.", "Rather they make generic smoothness assumptions (your test also makes such assumptions by the choice of neural network architecture).", "Thus, comparison with the prior work in Statistics and Machine Learning is relevant, since these tests have the same aims and scope as your tests.", "The cited Berrett and Samworth MI test uses a permutation approach to obtaining the test threshold, not an asymptotic approach  (see the results of Section 4 of that paper).", "Several of the other cited tests also use a permutation approach for the test threhsold.", "These tests are therefore relevant prior work.", "\" If things change very little from one second to the next, the signals could be very similar and may not really be, intuitively, independent samples and may bias result of the study.", "However, which independence assumptions to use is not in scope for our paper, because our fMRI study is trying to show that dependency testing works \"", "In the work cited by Chwialkowski et al, failure to account for the dependence between samples results in excessive false positives.", "This is because, for dependent data, the effective sample size is reduced, and the tests must be made more conservative to correct for this effect.", "It is therefore the case that the fMRI results may be false positives.", "Re level:\"This proof could be experimentally verified ...\"  This should be verified. In particular, Hoeffding can be very loose in practice, which is likely to be observed in experiments.", "======", "The authors propose a procedure for improving neural mutual information estimates, via a combination of data augmentation and cross validation. They then use these estimates in hypothesis testing, on low dimensional toy datasets and on high dimensional real-world fMRI data.", "The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved. In hypothesis testing, it is important to verify that the test has the correct level (false positive rate). This is all the more essential when the estimate has required optimisation over parameters.", "It is not clear from the presentation that this has been confirmed.", "There are a number of prior approaches to testing for multivariate statistical dependence in the machine learning and statistics literature (including a 2017 paper which uses mutual information).", "A small selection is given below, although a literature search will reveal many more papers.", "In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.", "In statistics: ---------------", "https://arxiv.org/abs/1711.06642 Nonparametric independence testing via mutual information", "Thomas B. Berrett, Richard J. Samworth 2017 Measuring and testing dependence by correlation of distances", "G\u00e1bor J. Sz\u00e9kely, Maria L. Rizzo, and Nail K. Bakirov Ann. Statist. Volume 35, Number 6 (2007), 2769-2794. Large-scale kernel methods for independence testing", "Qinyi ZhangEmail Sarah Filippi, Arthur Gretton, Dino Sejdinovic Statistics and Computing January 2018, Volume 28, Issue 1, pp 113\u2013130| Cite as", "In machine learning: ---------------------", "Multivariate tests of association based on univariate tests Heller, Ruth and Heller, Yair", "Advances in Neural Information Processing Systems 29 2016", "A Kernel Statistical Test of Independence Gretton, Arthur and Fukumizu, Kenji and Choon H. Teo and Song, Le and Sch\\\"{o}lkopf, Bernhard and Alex J. Smola", "Advances in Neural Information Processing Systems 20 2008 http://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf http://proceedings.mlr.press/v70/jitkrittum17a/jitkrittum17a.pdf", "An Adaptive Test of Independence with Analytic Kernel Embeddings Wittawat Jitkrittum, Zolt\u00e1n Szab\u00f3, Arthur Gretton ; ICML 2017, PMLR 70:1742-1751", "Time dependence ----------------", "It is also the case that if the variables have time dependence, then appropriate corrections must be made for the test threshold, to avoid excessive false positives.", "Does the fMRI data exhibit time dependence? For the case of multivariate statistical dependence testing, such corrections are described e.g. in: https://papers.nips.cc/paper/5452-a-wild-bootstrap-for-degenerate-kernel-tests.pdf A Wild Bootstrap for Degenerate Kernel Tests Kacper Chwialkowski, Dino Sejdinovic, Arthur Gretton NeurIPS 2014"], "labels": ["arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_quote", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_quote", "none", "none", "none", "none", "arg-structuring_quote", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_clarification_label"]}
{"abstract_id": 130, "sentences": ["This paper proposes a Self-Modulation framework for the generator network in GANs, where middle layers are directly modulated as a function of the generator input z.", "Specifically, the method is derived via batch normalization (BN), i.e. the learnable scale and shift parameters in BN are assumed to depend on z, through a small one-hidden layer MLP.", "This idea is something new, although quite straight-forward.", "Extensive experiments with varying losses, architectures, hyperparameter settings are conducted to show self-modulation improves baseline GAN performance.", "The paper is mainly empirical, although the authors compute two diagnostic statistics to show the effect of the self-modulation method.", "It is still not clear why self-modulation stabilizes the generator towards small conditioning values.", "The paper presents two loss functions at the beginning of section 3.1 - the non-saturating loss and the hinge loss.", "It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].", "It seems that the authors are not aware of this difference.", "In addition to report the median scores, standard deviations should be reported.", "===========", "comments after reading response ===========", "I do not see in the updated paper that this typo (in differentiating D in hinge loss and non-saturating loss) is corrected.", "Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings."], "labels": ["none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_experiment", "arg-structuring_heading", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 131, "sentences": ["This paper presents a method for unsupervised representation learning of speech.", "The idea is to first learn discrete representation (vector quantization is done by Gumbel softmax or k-means) from audio samples with contrastive prediction coding type objective, and then perform BERT-style pre-training (borrowed from NLP).", "The BERT features are used as inputs to ASR systems, rather than the usual log-mel features.", "The idea, which combines those of previous work (wav2vec and BERT) synergetically, is intuitive and clearly presented, significant improvements over log-mel and wav2vec were achieved on ASR benchmarks WSJ and TIMIT.", "Based on these merits, I suggest this paper to be accepted.", "On the other hand, I would suggest directions for investigation and improvements as follows.", "1. While I understand that vector quantization makes the use of NLP-style BERT-training possible (as the inputs to NLP models are discrete tokens),  there are potential disadvantages as well.", "One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs) .", "Also, without BERT pre-training, using directly the discrete tokens seems to consistently give worse performance for ASR.", "I think some more motivations or explorations (what kind of information did BERT learn) are needed to understand why that is the case.", "2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.", "A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.", "3. One concern I have with discrete representation is how robust they are wrt different dataset.", "The ASR datasets used in this work are relatively clean (but there does exists domain difference between them).", "It remains to see how the method performs with more acoustically-challenging speech data, and how universally useful the learned features are (as is the case for BERT in NLP).", "4. Another curious question is whether the features would still provide as much improvement when a stronger ASR system than AutoSeg (e.g., Lattice-free MMI) is used.", "Overall, while I think the computational cost of the proposed method is high, rendering it less practical at this point, I believe the approach has potential and the result obtained so far is already significant."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_experiment", "none", "arg-request_edit", "none", "none", "none", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 132, "sentences": ["[Summary]", "The paper presents a video classification framework that employs 4D convolution to capture longer term temporal structure than the popular 3D convolution schemes.", "This is achieved by treating the compositional space of local 3D video snippets as an individual dimension where an individual convolution is applied.", "The 4D convolution is integrated in resnet blocks and implemented via first applying 3D convolution to regular spatio-temporal video volumes and then the compositional space convolution, to leverage existing 3D operators.", "Empirical evaluation on three benchmarks against other baselines suggested the advantage of the proposed method.", "[Decision]", "Overall, the paper addresses an important problem in computer vision (video action recognition) with an interesting.", "I found the motivation and solution are reasonable (despite some questions pending more elaboration), and results also look promising, thus give it a weak accept (conditional on the answers though).", "[Comments]", "At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper \u201cLearning realistic human actions from movies\u201d, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.", "The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied. It seems to me that the proposed framework also falls in this category, with a treatment from deep learning. It is definitely worth some discussion on this path.", "That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.", "Despite the claim that the proposed method can capture long-term video patterns, the static compositional nature seems to work best for activities with well-defined local events and clear temporal boundaries.", "These assumptions hold mostly true for the three datasets used in the experiment, and also are suggested by results in table 2(e), where 3 parts are necessary to achieve optimal results.", "How does the proposed method perform in more complicated tasks such as - action detection or localization (e.g., in benchmarks JHMDB or UCF101-24). - complex video event modeling (e.g., recognizing activities in extended video of TRECVID).", "Will it still be more favorable than other concerning baselines?", "Besides, on the computation side, it would be complexity, an explicit comparison of complexity makes it easier to evaluate the performance when compared to other state-of-the-art methods.", "[Area to improve]", "Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.", "Proof reading", "- The word in the title should be \u201cConvolutional\u201d, right?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment", "none", "none", "arg-request_experiment", "arg-request_explanation", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label"]}
{"abstract_id": 133, "sentences": ["This paper builds upon the assumption that GANs successfully approximate the data manifold, and uses this assumption to regularize semi-supervised learning process.", "The proposed regularization strategy enforces that a discriminator or a given classifier should be invariant to small perturbations on the data manifold z. It is empirically shown that naively enforcing such a constraint by randomly adding noise to z could lead to under-smoothing or over-smoothing in some cases which can harm the final classification performance.", "Consequently, the proposed regularization technique takes a step of tunable size in the direction of the manifold gradient, which has the effect of smoothing along the direction of the gradient while ignoring its norm.", "Extensive experiments have been conducted, showing that the proposed approach outperforms or is comparable with recent state-of-the-art approaches on cifar 10, especially in presence of fewer labelled data points.", "On SVHN however, the proposed approach fails in comparison with (Kumar et al 2017) but performs better than other approaches.", "Furthermore, it has been shown that adding the proposed manifold regularization technique to the training of GAN greatly improves the image quality of generated images (in terms of FID scores and inception scores).", "Also, by combining the proposed regularizer with a classical supervised classifier (via pre-training a GAN and using it for regularization) decreases classification error by 2 to 3%.", "Finally, it has also been shown that after training a GAN using the manifold regularization, the algorithm is able to produce similar images giving a low enough perturbation of the data manifold z.", "Overall, this paper is well written and show significant improvements especially for image generation.", "However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.", "The paper would be improved if the following points are taken into account:", "A comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).", "How do the FID/Inception improvements compare to (Mescheder et al 2018)?", "It would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.", "Although there is a clear improvement in FID scores for Cifar10.", "It would be informative to show the generated images w/ and w/o manifold regularization.", "More analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.", "It should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_explanation", "arg-request_edit", "none", "arg-request_edit", "arg-request_experiment", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_edit_label"]}
{"abstract_id": 134, "sentences": ["This work focuses on learning a good policy for hyperparameters schedulers, for example learning rate or weight decay, using reinforcement learning.", "The main contributions include 1) a discretization on the learning curves such that transformer can be applied to predict the them; 2) an empirical evaluations using the predicted learning curves to train the policy.", "The main novelties are two folds.", "On the methodology side, using predicted learning curves instead of real ones can speed up training significantly.", "On the technical side, the author presented a discretization step to use transformer for learning curve predictions.", "The results are mixed, we see slightly advantage over human baseline on one task but worse in the other.", "Human baseline does not need any training!", "On the writing part, it would be nice to provide more context for both transformer, Proximal Policy Optimization and Simulated Policy Learning to make the paper more self-complete.", "I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:", "* Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).", "* Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.", "Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.", "How does the transformer based method comparing to others?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "none", "none", "none", "arg-request_explanation", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 135, "sentences": ["The paper presents algorithms for solving computational problems in a datastream model augmented with an oracle learned from data.", "The authors show that under this model, there exist algorithms that have significantly better time and space complexity than the current best known algorithms that do not use an oracle.", "The authors support their theoretical analysis with experiments in which the oracle is represented by a deep neural network and demonstrate improvement over classical algorithms that do not use machine learning.", "Overall, this paper seems like a solid contribution to the literature.", "However, in its current state it does seem to be presented and motivated in a way that is appropriate for the audience of ML researchers at ICLR.", "It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.", "Therefore my score for now is a weak reject, but I am very happy to increase the score if the authors address my presentations concerns.", "Major comments:", "* The oracle-augmented datasteam model needs to be contextualized better.", "I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me. For example, how do I even know that the oracle in question exists? What are the particular assumptions under which it exists? What are the requirements on the training data, optimization ability, generalization error, etc. How do we know that we can create in practice ML learning models that are sufficiently accurate to serve as an oracle?", "* The connections to deep learning seem arbitrary in some of the experiments. In one of the experiments, the authors train neural networks over a concatenation of IP address embeddings. Why do we need to use deep learning here?", "What is the benefit of using DL algorithms within the oracle-augmented datastream model?", "Is a simple algorithm enough? What algorithms should we ideally use in practice?", "What if you used simpler online learning algorithms with formal accuracy guarantees?", "Minor comments:", "* I thought there was a bit over-selling in intro.", "The authors say that they match the theoretical lower bounds for several problems.", "However, you are in a different computational model in which you now have access to an oracle. This needs to be made more explicitly, and language could be a bit toned down (e.g. in this model, we can obtain runtime that match or improve over lower bounds...)"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 136, "sentences": ["This paper presents a new method to measure the importance of hidden neurons in deep neural networks.", "The method integrates notions of activation value, input influence to a neuron and neuron influence to the network's output.", "They provide results confirming that this measure is able to identify neurons that are important for specific tasks.", "Quality", "The experiments are well designed to verify their hypothesis, although there could be more to make sure those results are not particular to the few selected problems.", "Nevertheless, the results are consistent across those experiments.", "Clarity", "The text is well written in general, but the structure could be improved.", "The introduction contains too much related work, which should be divided in another section.", "Section 2 contains mostly high level explanations of the work, which should be integrated in the introduction, and thus before the related work section, to improve readability.", "See minor comments for more specific suggestions.", "It is difficult to understand the goal of Section 4.2.", "Section 2 states that section 4.2 proves that a \"path method\" must be used in order to satisfy the axioms, but why such axioms are important is not stressed enough.", "Also, it is not clear why those are called axioms since they are not use to build anything else.", "It seems to me that those are rather \"desirable properties\" than axioms.", "Originality", "A important number of related works are cited and compared with the current work.", "Although the proposed measure is close to what is proposed by Datta et al., this paper makes the distinction clear and benchmarks its results properly against it.", "Significance", "There is an increasing need to interpretability of deep neural networks as they get more and more applied to real-world problems.", "Measures as the one proposed in this paper are a very important building block towards this.", "Conclusion", "For its original importance measure and the proper experiment benchmarks, I believe this paper should be accepted.", "There is however many minor issues that should be fixed for the camera-ready version.", "Although the recommended length is 8 pages, the strict limit is 10, so I would recommended to use a bit of the remaining extra space to conclude the paper properly with a discussion on the results and their consequences, as well as a conclusion to wrap up the paper.", "***", "Minor Comments", "Introduction:", "- The term flow is never defined precisely, we need to infer it based on the definition of conductance and attribution.", "- First paragraph would be more clear with simple word explanation rather than maths.", "Also, second sentence is not a complete sentence", "- Work on image indicators of importance could be compared better with current work.", "Indicators can be seen as a measure of importance.", "- This sentence is not clear: \"[...]; the nature of correlations in the two models may differ\".", "Section 2:", "- Last paragraph of section 2 can be true for any well-performing importance measure.", "The statements should be put in perspective with others.", "Section 3:", "- Section 3 should be introduced by explaining the goal of the section otherwise it breaks the flow of reading.", "- The role of the baseline x' should be better explained when it is presented (first paragraph of section 3).", "- The interchangeable use of the term \"conductance of neuron j\" for equations 2 and 3 is confusing.", "Different terms should be use, even if the context makes it possible to infer which one is being referred to.", "- Remark 1 seems trivial, but the selection of baseline x' seems less trivial.", "Some explanations should be devoted to it.", "- Second paragraph of remark 1 is not clear.", "Why couldn't we take another layer's neuron as the neuron of interest, bounding the conductance measure on one layer as the input and the output of the model? If we make the input to be a neuron y rather than the true input x, we could take another neuron z in a subsequent layer to be the neuron of interest, resulting in conductance measure Cond^z_i(y).", "Section 4:", "- List of importance measure at beginning of Section 4 should probably have citations.", "- Backward reference to section 3 seems to be a mistake, should it be subsection 4.2?", "- Each of the justifications to get around the issue of distinguishing strange model behavior from bad feature importance technique should be explained briefly in paragraph before section 4.1.", "Subsection 4.1:", "- I do not understand the problem explained in fourth paragraph of section 4.1.", "g(f(1 - epsilon)) = 0, why would it be 1- epsilon?", "- Problem explained in fifth Paragraph of section is not clear unless what the influence of the unit is clearly stated. Is it simply dg/df?", "- A short explanation of what is tested in section 6 should be given at last paragraph of section 4.1.", "Although the results are favorable to the conductance metric, it is not clear how they precisely confirm the problem of incorrect signs presented in the caricature examples.", "Subsection 4.2:", "- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only \"path methods\" can satisfy.", "- Footnote 2 on page 5 it difficult to read.", "- Although the proof does not seem to use the axioms as a building block, which is fortunate since it would make it a circular argument otherwise, the text suggests so: \"Given these three axioms, we can show that:\".", "- The importance of section 4.2 should be clarified.", "More emphasis on the importance of the axioms (desirable properties) should be made.", "Section 5:", "- Choices for experiments should be explained. Why choosing layers mixed** rather than others? Why choosing filters?", "- Figures 1-4 are difficult to interpreted on a printed version.", "Since this is qualitative, I suggest to change the saturation of the images to make them easier to interpret.", "The absolute values are not important for a qualitative interpretation", "- Figure 4 could be more interesting if compared with other classes, like other animal faces.", "Anyhow, I understand that those were chosen based on the subset of classes used for the experiments.", "- Space should be added between figures to better divide the captions", "Section 6:", "- The difference between experiments of Figure 5 and 6 should be made more clear.", "Section 7-8:", "- Where are they? No discussion? No conclusion?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_edit", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_edit", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_clarification", "arg-request_clarification", "none", "arg-request_edit", "none", "arg-request_edit", "arg-structuring_heading", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "none", "arg-request_edit", "none", "arg-request_explanation", "none", "arg-request_explanation", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_explanation", "arg-structuring_heading", "none", "arg-request_typo", "arg-request_clarification", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "none", "arg-request_edit", "none", "arg-request_clarification", "arg-request_edit", "arg-structuring_heading", "arg-request_explanation", "none", "arg-request_edit", "none", "arg-request_experiment", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_edit", "arg-structuring_heading", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_clarification_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "arg-request_typo_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_clarification_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_explanation_label"]}
{"abstract_id": 137, "sentences": ["The authors propose a generative model of networks by learning embeddings and pairing the embeddings with a prior distribution over networks.", "The idea is that the prior distribution may explain structure that the embeddings would not have to capture.", "The motivation for doing this is that this structure is typically hard to model for network embeddings.", "The authors propose a clean -if improper- prior on networks and proceed to perform maximum likelihood inference on it.", "The experiments show that the approach works fine for link porediction and can be used for visualization.", "Two points:", "a) Why not try to do this with Variational inference? It should conceptually still work and be fast and potentially more robust.", "b) The prior seems to be picked according to properties of the observed data and expressed in a product of constraints.", "This seems clunky, I would have been more impressed with a prior structure that ties in closer with the embeddings and requires less hand-engineering.", "A key point of interest is the following: very exciting recent work (GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models by You et al ICML2018) has proposed neural generative models of networks with a high degree of fidelity and much less hand-picked features.", "The work here tries to not learn a lot of these structures but impose them.", "Do the authors think that ultimately learning priors with models like GraphRNN might be more promising for certain applications?", "The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.", "A more predictive generative model that makes less hard assumptions on graph data would be interesting.", "Update After rebuttal:", "Given the authors' rebuttal to all reviews, I am upgrading my score to a 6.", "I still feel that more learning (as inGraphRNN) to build a fuller generative model of the graph would be interesting, but the authors make a strong case for the usefulness and practicality of their approach."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 138, "sentences": ["The author's present a dual learning framework that, instead of using a single mapping for each mapping task between two respective domains, the authors learn multiple diverse mappings.", "These diverse mappings are learned before the two main mappings are trained and are kept constant during the training of the two main mappings.", "Though I am not familiar with BLEU scores and though I didn't grasp some of the details in 3.1, the algorithm yielded consistent improvement over the given baselines.", "The author's included many different experiments to show this.", "The idea that multiple mappings will produce better results than a single mapping is reasonable given previous results on ensemble methods.", "For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.", "Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?", "Minor Comments:", "Dual-1 and Dual-5 are introduced without explanation.", "Perhaps I missed it, but I believe Dan Ciresan's paper \"Multi-Column Deep Neural Networks for Image Classification\" should be cited.", "### After reading author feedback", "Thank you for the feedback.", "After reading the updated paper I still believe that 6 is the right score for this paper.", "The method produces better results using ensemble learning.", "While the results seem impressive, the method to obtain them is not very novel; nonetheless, I would not have a problem with it being accepted, but I don't think it would be a loss if it were not accepted."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_clarification", "arg-request_clarification", "arg-structuring_heading", "none", "arg-request_edit", "arg-structuring_heading", "none", "none", "arg-structuring_summary", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 139, "sentences": ["The authors propose a new on-policy exploration strategy by using a policy with a hierarchy of stochasticity.", "The authors use a two-level hierarchical distribution as a policy, where the global variable is used for dropout.", "This work is interesting since the authors use dropout for policy learning and exploration.", "The authors show that parameter noise exploration is a particular case of the proposed policy.", "The main concern is the gap between the problem formulation and the actual optimization problem in Eq 12.", "I am very happy to give a higher rating if the authors address the following points.", "Detailed Comments", "(1) The authors give the derivation for Eq 10.", "However, it is not obvious that how to move from line 3 to line 4 at Eq 15.", "Minor:  Since the action is denoted by \"a\",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of \"\\alpha\" at Eq 10 and 15.", "(2) Due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at Eq 12.", "Does such approximation guarantee the policy improvement?", "Any justification?", "(3) Instead of using the mean policy approximation in Eq 12, the authors should consider existing Monte Carlo techniques to reduce the variance of the gradient estimation.", "For example, [1] could be used to reduce the variance of gradient w.r.t. \\phi.", "Note that the gradient is biased if the mean policy approximation is used.", "(4) Are \\theta and \\phi jointly and simultaneously optimized at Eq 12?", "The authors should clarify this point.", "(5) Due to the mean policy approximation, does the mean policy depend on \\phi?", "The authors should clearly explain how to update \\phi when optimizing Eq 12.", "(6) If the authors jointly and simultaneously optimize \\theta and \\phi, why a regularization term about q_{\\phi}(z)  is missing in Eq 12 while a regularization term about \\pi_{\\theta|z} does appear in Eq 12?", "(7) The authors give the derivations about \\theta such as the gradient and the regularization term about \\theta (see, Eq 18-19).", "However, the derivations about \\phi are missing.", "For example, how to compute the gradient w.r.t. \\phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \\phi.", "Minor, 1/2 is missing in the last line of Eq 19.", "Reference:", "[1] AUEB, Michalis Titsias RC, and Miguel L\u00e1zaro-Gredilla. \"Local expectation gradients for black box variational inference.\" In Advances in neural information processing systems, pp. 2638-2646. 2015."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "arg-request_typo", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-request_experiment", "arg-request_typo", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 140, "sentences": ["This paper proposed an aggregation algorithm (DARN) for the multi-source domain adaptation problem which is highly useful in real-world applications.", "The proposed method is based on the theoretical extension of the single-source domain discrepancy measure proposed by Mansour et al. 2009 to the multi-source setting.", "This paper also showed the effectiveness of the proposed method on some real-world datasets.", "Strengths", "The paper introduces new technical insights to understand their bound, e.g. effective sample size.", "The paper proposed the way to estimate coefficient, optimal \\alpha, with theoretical justification, and I think this is the biggest contribution of this paper and is interesting.", "The proposed method is also able to be used in the regression task since it is based on the disc which can be estimated in the regression task.", "Weakness", "The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.", "A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.", "Experimental results itself are fine but not complete.", "- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.", "- It would be also better to show the coefficient of existing methods that have no theoretical justification.", "- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.", "Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.", "So this work has to be supported with more detailed experimental results to express the potential of this approach fully.", "For this reason, I think it is okay but not good enough at this time.", "*", "***After the authors' response****", "Increase rating.", "[1] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In COLT, 2009.", "[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. In NeurIPS, 2007.", "[3] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine Learning, 2010.", "[4] Seiichi Kuroki, Nontawat Charoenphakdee, Han Bao, Junya Honda, Issei Sato, and Masashi Sugiyama.", "Unsupervised domain adaptation based on source-guided discrepancy. In AAAI, 2019."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 141, "sentences": ["The idea is nice.", "It is well aligned with tools that are needed to understand neural networks.", "However, the experiments feel like they are missing motivation as to why this method is being used.", "The paper does not provide very significant evidence that this method is useful.", "The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.", "More motivation for experimental section is needed.", "If the authors don't discuss a motivation then how will a reader know how to apply the tool?", "It seems there is no conclusion to take away from the experiments in section 5 (convolutions).", "The authors should rethink the structure of the experimental section from the standpoint of convincing someone to use this method.", "In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.", "The paper needs more discussion and experiments to explain how and why to use this approach.", "While the authors say \"attributing a deep network\u2019s prediction to its input is well-studied\" they don't compare directly against these methods.", "There are many typos and grammar errors", "While I think the paper could be much more impactful if the experimental section was greatly reworked; I believe the first 5 pages of the paper are a very good contribution and it should be accepted."], "labels": ["none", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "arg-request_edit", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_typo", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_typo_label", "none_label"]}
{"abstract_id": 142, "sentences": ["Summary:", "This paper introduces a generative model for 3D point clouds.", "Authors aim at theoretically showing the difficulties of using existing generative models to learn distributions of point clouds, and propose a variant that supposedly solves the issues.", "Pros:", "+ The problem of designing generative models for 3D data is important.", "Cons:", "- Paper is often hard to follow, and contains a significant number of typos.", "- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?", "As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.", "- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.", "- It is not clear why authors did not follow the evaluation protocol of [Achlioptas\u201917] or [Wu\u201916] more closely.", "In particular, evaluation for the classification task should be compatible with the proposed model, which would give a much better picture of the learned representations."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 143, "sentences": ["Authors propose a novel combination of RBM feature extractor and CNN classifiers to gain robustness toward adversarial attacks.", "They first train a small mean field boltzmann machine on 4x4 patches of MNIST, then combine 4 of these into a larger 8x8 feature extractor.", "Authors use the RBM 8x8 feature representation as a fixed convolutional layer and train a CNN on top of it.", "The intuition behind the idea is that since RBMs are generative, the RBM layer will act as a denoiser.", "One question which is not addressed is the reason for only one RBM layer.", "In \"Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning\" by Norouzi et al, several RBM layers are trained greedily (same as here, only difference is contrastive loss vs mean field) and they achieve 0.67% error on MNIST.", "Attacking CRBMs is highly relevant and should be included as a baseline.", "The only set of experiments are comparisons on first 500 MNIST test images.", "If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.", "Authors should clarify the justification behind experimenting only on 'first 500 test images'.", "Furthermore, as authors discussed the iterative weight sharing which increases the depth can vanish the gradient toward input.", "Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.", "The iterative architecture is similar to the routing in CapsNet (Hinton 2018) in terms of weight sharing between successive layers.", "Although their network was resilient toward white box attacks they suffered from black box attacks.", "The boundary method on MNIST could be  weaker than a black box attack."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_result", "none", "none", "arg-request_clarification", "arg-structuring_quote", "arg-request_experiment", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 144, "sentences": ["Summary", "The paper proposes to modify the \"Dual Learning\" approach to supervised (and unsupervised) translation problems by making use of additional pretrained mappings for both directions (i.e. primal and dual).", "These pre-trained mappings (\"agents\") generate targets from the primal to the dual domain, which need to be mapped back to the original input.", "It is shown that having >=1 additional agents improves training of the BLEU score in standard MT and unsupervised MT tasks.", "The method is also applied to unsupervised image-to-image \"translation\" tasks.", "Positives and Negatives", "+1 Simple and straightforward method with pretty good results on language translation.", "+2 Does not require additional computation during inference, unlike ensembling.", "-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).", "-2 Diversity of additional \"agents\" not analyzed (more below).", "-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.", "-4 Talking about \"agents\" and \"Multi-Agent\" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just \"mapping\" or \"network\"?", "-1: Potential Issues with the Maths.", "The maths is not clear, in particular the gradient derivation in equation (8).", "Let's just consider the distortion objective on x (of course it also applies to y without loss of generality).", "At the very least we need another \"partial\" sign in front of the \"\\delta\" function in the numerator.", "But again, it's not super clear how the paper estimates this derivative.", "Intuitively the objective wants f_0 to generate samples which, when mapped back to the X domain, have high log-probability under G, but its samples cannot be differentiated in the case of discrete data.", "So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.", "In the case of continuous data x, is the reparameterization trick used?", "This should at the very least be explained more clearly.", "Note that the importance sampling does not affect this issue.", "-2: Diversity of Agents.", "As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).", "The paper proposes to use different random seeds and iterate over the dataset in a different order for distinct pretrained f_i.", "The paper should quantify that this leads to diverse \"agents\".", "I suppose the proof is in the pudding; as we have argued, multiple agents can only improve performance if they are distinct, and Figure 1 shows some improvement as the number of agents are increase (no error bars though).", "The biggest jump seems to come from N=1 -> N=2 (although N=4 -> N=5 does see a jump as well).", "Presumably if you get a more diverse pool of agents, that should improve things.", "Have you considered training different agents on different subsets of the data, or trying different learning algorithms/architectures to learn them?", "More experiments on the diversity would help make the paper more convincing."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_clarification", "none", "none", "arg-structuring_heading", "none", "arg-structuring_quote", "arg-request_edit", "none", "arg-structuring_quote", "none", "arg-request_explanation", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label"]}
{"abstract_id": 145, "sentences": ["This paper develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights.", "There are a number of interesting predictions made in this paper on the basis of this analysis.", "The main technical results of the paper are Theorems 5-8 which compute the statistics of the covariance of the activations and the gradients.", "Comments:", "1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?", "2. In a similar vein, there a number of highly technical results in the paper and it would be great if the authors provide an intuitive explanation of their theorems.", "3. Can the statistics of activations be controlled using activation functions or operations which break the symmetry?", "For instance, are BSB1 fixed points good for training neural networks?", "4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.", "For instance, when the authors observe that the structure of the fixed point is such that activations are of identical norm equally spread apart in terms of angle, this is quite far from practice.", "It would be good to mention this in the introduction or the conclusions."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-request_explanation", "arg-request_clarification", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 146, "sentences": ["The paper proposes to use the triplet loss as a convex relaxation of the ordinal embedding problem.", "The loss is solved using feed-forward neural network with the input to the network being the ids of the items encoded in binary codes.", "The benefit of using a deep network is to exploit its optimization capability and the parallelism on GPUs.", "The experiments presented in the paper include a set of simulation experiments and a real-world task.", "I am giving a score of 3.", "This work is an interesting application of deep learning, but it gives little insight as to why deep networks are able to solve the problem and how to solve ordinal embedding itself.", "To elaborate, the problem is known to be NP-hard in the worst case, while the data sets used in the paper seem to have certain nice properties.", "It would be interesting to see how deep networks do for the hard cases.", "It would also be interesting to see if additional assumptions, such as the existence of clusters or separation between clusters, make ordinal embedding simpler and thus tractable.", "Another approach is to assume the solution to have low surrogate loss (4), and any convex solver with sufficiently large number of points is able to find such a solution.", "Then the question becomes how deep networks solve the particular convex optimization problem.", "Thinking along these directions would bring more insight and impact to both the ordinal embedding problem and optimization in deep networks.", "one quick question:", "equations (3) and (4)", "--> isn't this the same as using the hinge loss to bound the zero-one loss?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-request_result", "none", "none", "arg-request_result", "arg-structuring_heading", "arg-structuring_heading", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_clarification_label"]}
{"abstract_id": 147, "sentences": ["This paper seeks to understand the AlphaGo Zero (AGZ) algorithm and extend the algorithm to regular sequential decision-making problems.", "Specifically, the paper answers three questions regarding AGZ: (i) What is the optimal policy that AGZ is trying to learn?", "(ii) Why is cross-entropy the right objective?", "(iii) How does AGZ extend to generic sequential decision-making problems?", "This paper shows that AGZ\u2019s optimal policy is a Nash equilibrium, the KL divergence bounds distance to optimal reward, and the two-player zero-sum game could be applied to sequential decision making by introducing the concept of robust MDP.", "Overall the paper is well written.", "However, there are several concerns about this paper.", "In fact, the key results obtained in this paper is that minimizing the KL-divergence between the parametric policy and the optimal policy (Nash equilibrium) (using SGD) will converge to the optimal policy.", "It is based on a bound (2), which states that when the KL-divergence between a policy and the optimal policy goes to zero then the return for the policy will approach that of the optimal policy.", "This bound is not so surprising because as long as certain regularity condition holds, the policies being close should lead to the returns being close.", "Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ\u2019s core learning algorithm.", "As mentioned earlier, the actual conclusion in Section 4.2 is that minimizing the KL-divergence between the parametric policy and the optimal policy by SGD will converge to the optimal policy, which is straightforward and is not what AlphaGo Zero does.", "This is because there is an important gap: the MCTS policy is not the same as the optimal policy.", "The effect of the imperfection in the target policy is not taken into account in the paper.", "A more interesting question to study is how this gap affect the iterative algorithm, and whether/how the error in the imperfect target policy accumulates/diminishes so that iteratively minimizing KL-divergence with imperfect \\pi* (by MCTS) could still lead to optimal policy (Nash equilibrium).", "Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.", "It is more or less like a reformulation of the AGZ setting in the MDP problem. And it is commonly known that two-player zero-sum game is closely related to minimax robust control. Therefore, it cannot be called as \u201cgeneralizing AlphaGo Zero\u201d as stated in the title of the paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 148, "sentences": ["The paper focuses on the generalization performance of RNNs and its variant in a theoretical perspective.", "Compared to the previous result (Zhang et al., 2018) for RNNs, this paper refines the generalization bounds for vanilla RNNs in all cases and fills the blank for RNN variants like MGU and LSTM.", "Specifically, in the work of Zhang et al. (2018), the complexity term quadratically depends on the layer (or say, current sequence length, denoted by t in original paper), making it less instructive.", "This paper improves it to at most linear dependence and can achieve at logarithmic dependence in some cases, which should be accredited.", "The key step in the proof is Lemma 2.", "In Lemma 2, the spectral norms of weight matrices and the number of weight parameters are decoupled.", "With Lemma 2, it is natural to construct a epsilon-net for RNNs and then upper bound the empirical Rademacher complexity by Dudley\u2019s entropy integral, since such methodology is not so novel.", "Bartlett, et al. (2017) developed this technique to analyze the generalization bound for neural networks in a margin-based multiclass classification.", "However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.", "I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 149, "sentences": ["Review for MANIFOLD REGULARIZATION WITH GANS FOR SEMISUPERVISED LEARNING", "Summary:", "The paper proposed to incorporate a manifold regularization penalty to the GAN to adapt to semi-supervised learning.", "They approximate this penalty empirically by calculating stochastic finite differences of the generator\u2019s latent variables.", "The paper does a good job of motivating the additional regularization penalty and their approximation to it with a series of experiments and intuitive explanations.", "The experiment results are very through and overall promising.", "The paper is presented in a clear manner with only minor issues.", "Novelty/Significance:", "The authors\u2019 add a manifold regularization penalty to GAN discriminator\u2019s loss function.", "While this is a simple and seemingly obvious approach, it had to be done by someone.", "Thus while I don\u2019t think their algorithm is super novel, it is significant and thus novel enough.", "Additionally, the authors\u2019 use of gradients of the generator as an approximation for the manifold penalty is a clever.", "Questions/Clarity:", "It would be helpful to note in the description of Table 3 what is better (higher/lower).", "Also Table 3 seems to have standard deviations missing in Supervised DCGANs and Improved GAN for 4000 labels. And is there an explanation on why there isn\u2019t an improvement in the FID score of SVHN for 1000 labels?", "What is the first line of Table 4? Is it supposed to be combined with the second? If not, then it is missing results. And is the Pi model missing results or can it not be run on too few labels? If it can\u2019t be run, it would be helpful to state this.", "On page 11, \u201cin Figure A2\u201d the first word needs to be capitalized.", "In Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there?", "What are the differences of the 6 pictures in Figure A7? Iterations?"], "labels": ["arg-structuring_heading", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_explanation", "arg-request_clarification", "arg-request_typo", "arg-request_clarification", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_typo_label", "arg-request_clarification_label", "arg-request_clarification_label"]}
{"abstract_id": 150, "sentences": ["The main contributions of the submission are:", "1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search).", "2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer.", "This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned.", "The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K.", "The weights of this weighted average and K are \"hyper-parameters\" of the metric itself.", "They use K=100 and suggest 3 possible choices of weights.", "The paper appears to treat 2. as the main contribution.", "However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters).", "The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset.", "Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.", "Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).", "A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.", "Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work.", "Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.", "They also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion.", "I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.", "Other comments/notes:", "* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials.", "I think it would be worth discussing this more.", "* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)", "* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 151, "sentences": ["This paper shows that Hamiltonian gradient descent (HGD), which is gradient descent on the norm of the squared norm of the vector field, achieves linear convergence for a broader range of problems than bilinear and convex-strongly concave formulations.", "In particular, the authors show the result for convex-concave problems satisfying a \u201csufficiently bilinear\u201d condition that is related to the PL conditions.", "Finally, the authors argue that consensus optimization (CO) can be viewed as a perturbation of HGD when the parameter choice is big enough.", "From this viewpoint they derive convergence rates for CO on the broader set of problems.", "This provides some further theoretical justification of the success of CO on large scale GAN problems.", "The paper is presented in a clear manner, with the objectives and analysis techniques delineated in the main paper.", "This was helpful to get a sense of the main points before going through the appendix.", "The objective of the paper is to extend the problem settings for which there is last iterate min-max convergence rates, which now exist for bilinear, strongly convex-strongly concave, and convex-strongly concave problems.", "The authors achieve this by analyzing HGD and giving convergence rates for when a \u201csufficiently bilinear condition is satisfied\u201d.", "The primary idea behind the proof techniques is to show that the objective (Hamiltonian) satisfies the PL condition.", "I found this to be an interesting approach.", "As a result, the main question in evaluating this paper is on the significance of the result and the generality of the \u201csufficiently bilinear\u201d condition.", "I tend to lean toward the result carrying some significance since it does extend the class of problems for which the convergence rates exists.", "However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.", "I do acknowledge that the authors did a reasonable job of trying to clear this up in section 3.2 and section G of the appendix.", "It did still leave me wanting more with respect to the practical significance though.", "Finally, I found the connection to CO valuable.", "In particular, since this paper does not show large-scale experiments, the connection serves to provide some more theoretical evidence for they CO performs well in practice.", "Post Author Response: Thanks for the response. I agree with your perspective and think this paper should be accepted."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 152, "sentences": ["PROS:", "* Original idea of using separate \"discriminator\" paths for unknown classes", "* Thorough theoretical explanation *", "A variety of experiments", "* Very well-written, and clear paper", "CONS:", "* The biggest problem for me was the unconvincing results. MNIST-to-MNIST-M has better baselines  (PixelDA performed better on this task for example), Office is not suitable for domain adaptation experiments anymore unless one wants to be in a few-datasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for NN-based domain adaptation); the results on CELL were not convincing, I don't know the dataset but it seems that baseline NN does better than DA most of the times.", "* Comparison with other methods did not take into account a variety of hyperparameters.", "Although I do understand the problem of evaluation in unsupervised DA, this should have at least been done in the semi-supervised case, and some analysis/discussion should be included for the unsupervised one.", "What if the proposed method performs that much better than baselines but they hyperparameters are not set correctly?"], "labels": ["arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_edit", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label"]}
{"abstract_id": 153, "sentences": ["This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem.", "The problem is important to understand in the theoretical machine learning community.", "The paper is written well overall, clearly explaining the results obtained.", "I would like to raise several important points:", "1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.", "The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison.", "2. Vacuous bounds in the regime \\beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \\beta >1.", "A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.", "They can indeed be subsumed by generalization bounds based on VC theory.", "The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.", "3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]", "4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the \"model complexity\" introduced upto numerical constants.", "It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.", "[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86.1 (1998): 63-79.", "[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" Advances in Neural Information Processing Systems. 1996.", "[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" Advances in Neural Information Processing Systems. 2017."], "labels": ["arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 154, "sentences": ["This paper proposes a new application of embedding techniques for mathematical problem retrieval in adaptive tutoring.", "The proposed method performs much better than baseline sentence embedding methods.", "Another contribution is on using negative pre-training to deal with an imbalanced training dataset.", "To me this paper is just not good enough - the method essentially i) use \"a professor and two teaching assistants\" to build a \"rule-based concept extractor\" for problems, then ii) map problems into this \"concept space\" and simply treat them as words. There are several problems with this approach.", "First, doing so does not touch the core of the proposed application.", "For tutoring applications, the most important thing is to select a problem that can help students improve; even if you can indeed select a problem that is the most similar to another problem, is it the best one to show a student? There are no evaluations on real students in the paper.", "Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.", "Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.", "I am not sure how this will generalize to a larger number of problem spanning many different domains.", "I also had a hard time going through the paper - there aren't many details.", "Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the \"rule-based concept extractor\", which is the key technical innovation."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 155, "sentences": ["This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations.", "These transformations are called canonicalizations in the paper.", "The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor.", "The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation.", "It also proposes a loss function and sampling scheme for training the model.", "The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation.", "The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set.", "I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model.", "A couple things I thought were missing in the paper:", "Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?", "For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?", "Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).", "There might be other constructions that are more efficient and less restrictive.", "I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?", "Minor comments:", "* \"data  tripets\" on page 2", "* Figure 5 should appear after Figure 4."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-request_explanation", "arg-structuring_heading", "arg-request_typo", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_edit_label"]}
{"abstract_id": 156, "sentences": ["i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors.", "I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted.", "The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible.", "This is clearly an intractable problem.", "Thus all attacks make some kind of approximation, including this paper.", "I am still a bit confused about the difference between \"zero-confidence attacks\" and those that don't fall into that category such as PGD.", "Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.", "The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced.", "The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case.", "What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized.", "This was not done.", "This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.", "Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.", "Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack.", "So clarifying the above question will help to judge the paper's novelty."], "labels": ["none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_clarification_label"]}
{"abstract_id": 157, "sentences": ["This paper proposes to relax the assumption of disentangled representation and encourage the model to learn linearly manipulable representations.", "The paper assumes that the latent canonicalizers are predefined for each task and that it is possible to obtain the ground-truth image of different latent canonicalizations.", "I find these assumptions too strong for the task of learning disentangled representation.", "Firstly, most prior works such as beta-vae, info-gan do not assume that the factors / canonicalizers are known beforehand.", "In fact, this is a very difficult part of learning disentangled representation.", "Secondly, if it is possible to obtain the ground-truth image of different latent canonicalizations, you can simply train a network to predict the canonicalizations by simple supervised learning.", "Hence, these overly simplified and unrealistic assumptions make the task too trivial.", "The proposed method is very simple and frames the problem basically as a supervised learning problem.", "Although experiments show that learning such representations are beneficial for low-shot setting of SVHN, it is not clear whether such improvement generalizes to more realistic datasets such as ImageNet.", "If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].", "[1] Noroozi, Mehdi, and Paolo Favaro. \"Unsupervised learning of visual representations by solving jigsaw puzzles.\" European Conference on Computer Vision. Springer, Cham, 2016.", "[2] Zhang, Richard, Phillip Isola, and Alexei A. Efros. \"Colorful image colorization.\" European conference on computer vision. Springer, Cham, 2016.", "[3] Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. \"Unsupervised representation learning by predicting image rotations.\" arXiv preprint arXiv:1803.07728 (2018)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 158, "sentences": ["1. I had hard time to understand latent canonicalization.", "Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?", "More explanation of canonicalization is needed.", "Perhaps an example in linear algebra is needed.", "2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?", "3. How can the proposed method be generalized to non-image data?", "The experiments were only done on simple image datasets.", "I am wondering this method can be applied to other complex datasets whose latent factors are unknown.", "4. I do not understand this: \"to fit well the method overfitting rate\" in Section 3.3.", "Minors:", "(1) than -> that", "(2) Eq. (3): is there a superscription \"(j)\" on z_canon in decoder?"], "labels": ["none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 159, "sentences": ["Summary:", "The paper, considers methods for solving smooth unconstrained min-max optimization problems.", "In particular, the authors prove that the Hamiltonian Gradient Descent (HGD) algorithm converges with linear convergence rate to the min-max solution.", "One of the main contributions of this work is that the proposed analysis is focusing on last iterate convergence guarantees for the HGD.", "This result, as the authors claim can be particularly useful in the future for analyzing more general settings (nonconvex-nonconcave min-max problems).", "In addition, two preliminary convergence theorems were provided for two extensions of HGD: (i) a stochastic variant of HGD and (ii)  Consensus Optimization Algorithm (CO) (by establishing connections of CO and HGD).", "Main Comments:", "The paper is well written and the main contributions are clear.", "I believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the combination of different assumptions used in the theory.", "1) I think definition 2.5 of Higher order Lipschitz is very strong assumption to have. What exactly means? Essentially the authors upper bounded any difficult term appear in the theorems. Is it possible to avoid having something so strong? Please elaborate.", "2) In assumption 3.1 is not clear what $L_H$ is.", "This quantity never mentioned before.", "Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).", "3) What is the main difference on the combination of assumptions on Theorems 3.2, 3.2 and 3.4. Which one is stronger. Is there a reason for the existence of Theorem 3.3?", "4) All the results heavily depend on the PL condition.", "I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.", "In particular, one can propose several combinations of assumptions in order for the function H to satisfy the PL condition.", "Can we avoid having the PL condition?", "The authors need to elaborate more on this.", "5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.", "Minor Suggestions:", "In first paragraph of page 5 where the authors divide the existing literature into the three particular cases, I am suggesting to add the refereed papers inside each one of this cases (which papers assumed function g bilinear , which papers strongly convex-concave etc.)", "I understand that the main contribution of the work is the theoretical analysis of the proposed method but would like to see some numerical evaluation in the main paper.", "There are some preliminary results in the appendix but it will be useful for the reader if there are are some plots showing the benefit of the method in comparison with existing methods that guarantee convergence (which method is faster?).", "In the current experiments there is a comparison only with CO algorithm and SGDA.", "In general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on smooth games and their connections to machine learning applications.", "I suggest weak accept but I am open to reconsider in case that my above concerns are answered.", "**********after rebuttal ********", "I would like to thank the authors for their reply and for the further clarification.", "I will keep my score the same but I highly encourage the authors to add some clarification related to my last comment on the globally bounded gradient.", "In their response they mentioned that the analysis only requires that  H is smooth and that $\\|\\xi(x^{(0)})\\|$ is sufficient bound.", "This needs to be clear in the paper (add clear arguments and related references).", "In addition, in their response they highlight the non-increasing nature of function H over the course of the algorithm which is important for their argument.", "Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.", "In SGD,  function H does not necessarily decrease over the course of the algorithm.", "The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_clarification", "none", "none", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_result", "arg-request_result", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-structuring_quote", "arg-request_edit", "arg-structuring_quote", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-structuring_quote_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 160, "sentences": ["This paper proposed a general method for image restoration based on GAN.", "In particular, the latent variable z is optimized based on the MAP framework.", "And the results are obtained by G(z).", "This method looks reasonable to achieve good results.", "However, the idea is very related to Yeh et al.\u2019s work which has already published but not mentioned at all.", "Yeh, Raymond A., et al. \"Image Restoration with Deep Generative Models.\" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.", "Both the proposed method and Yeh et al.\u2019s method optimize the latent variable z of the generator using MAP, although the loss functions are slightly different.", "In addition, the applications are very similar: image inpainting, denoising, super-resolution etc. Yeh et al.\u2019s method should be the right baseline instead of the nearest neighbor algorithm.", "In addition, the results seem very weak.", "There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.", "The paper claims that only the nearest neighbor algorithm can handle different degradations.", "This is not true.", "For example, total variation regularization can do all these tasks.", "Some other comments: what are the parameters of the degradation in the applications? For example, in image inpainting, does the proposed method learn the mask as well? So it is blind inpainting?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_result", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 161, "sentences": ["The authors propose to use the combination of model ensemble and MC dropout in Bayesian deep active learning.", "They empirically show that there exists the mode collapse problem due to the MC dropout which can be regarded as a variational approximation.", "The authors introduce an ensemble of MC-Dropout models with different initialization to remedy this mode collapse problem.", "The paper is clearly written and easy to follow.", "It is interesting to empirically show that the mode collapse problem of MC-Dropout is important in active learning.", "The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.", "Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.", "Therefore, it is a little misleading to still call it Bayesian active learning.", "Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.", "The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.", "So it seems not a reasonable solution for the mode collapse problem of MC-Dropout.", "It is not clear to me why we need to add MC-Dropout to the ensemble.", "What is the benefit of DEBAL over an ensemble method if both of them do not have Bayesian theoretic support?", "In terms of the empirical results, the better performance of DEBAL compared to a single MC-Dropout model is not supervising as Beluch et al. (2018) already demonstrated that an ensemble is better than a single MC-Dropout.", "While the improvement of DEBAL compared to an ensemble is marginal but is reasonable.", "The labels of figures are hard to read."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 162, "sentences": ["This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks.", "The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN.", "Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks).", "Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community.", "The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement.", "This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.", "There are some general discussions on how to adapt gyrovector space theory into spherical spaces.", "This is interesting but no formal results are presented.", "I vote for rejection for four major weaknesses explained as follows.", "(1) The experimental results cannot show the usefulness of the proposed GCN.", "The results on real datasets are similar to the regular GCN.", "As the authors themselves remarked, \"it can be seen that our models sometimes outperform the two Euclidean GCN\".", "The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.", "(2) The method is not well motivated.", "The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle.", "This is too general and not enough as a motivation.", "After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.", "(3) A large body of graph neural network literature is omitted.", "The authors start from a very high-level description of machine learning in constant curvature spaces.", "Such high-level introductions require more comprehensive literatures to support.", "In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN.", "This is misleading.", "For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.", "The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.", "This is a thriving area that requires a careful literature review.", "(4) The writing quality is not satisfactory.", "Here are a few examples: The ICLR citation style needs to use sometimes \\citep.", "The authors instead used \\cite everywhere, making the paper hard to read.", "The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).", "The introduction can start at a lower level (such as flat/hyperbolic neural networks).", "Section 3.4, as the main technical innovation, can be extended and includes some demonstrations."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_quote", "none", "none", "arg-structuring_quote", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_edit", "arg-request_edit", "none", "none", "none", "arg-request_edit", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_quote_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 163, "sentences": ["This paper presents a gradient estimator for expectation-based objectives, which is called Go-gradient.", "This estimator is unbiased, has low variance and, in contrast to other previous approaches, applies to either continuous and discrete random variables.", "They also extend this estimator to problems where the gradient should be \"backpropagated\" through a nested combination of random variables and a (non-linear) functions.", "Authors present an extensive experimental evaluation of the estimator on different challenging machine learning problems.", "The paper addresses a relevant problem which appears in many machine learning settings, as it is the problem of estimating the gradient of an expectation-based objective.", "In general, the paper is well written and easy to follow. And the experimental evaluation is extensive and compares with relevant state-of-the-art methods.", "The main problem with this paper is that it is difficult to identify its main and novel contributions.", "1. In the case of continuous random variables, Go-gradient is equal to Implicit Rep gradients (Figurnov et al. 2018) and pathwise gradients (Jankowiack & Obermeyer,2018).", "Furthermore, for the Gaussian case, Implicit Rep gradients (and Go-gradient too) are equal to the standard reparametrization trick estimator (Kingma & Welling, 2014).", "This should be made crystal-clear in the paper.", "What happens is that the authors arrive at this solution using a different approach.", "In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.", "Moreover, I don't think some of the presented experiments are necessary.", "Simply because for continuous variables similar experiments have been reported before (Figurnov et al. 2018, Jankowiack & Obermeyer,2018).", "2. It seems that the main novel contribution of the paper is to extend the ideas of (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) to discrete variables. And this is a relevant contribution.", "And the experimental evaluations of this part are convincing and compare favourably with other state-of-the-art methods.", "3. Authors should be much more clear about which is their original contribution to the problems stated in Section 4 and Section 5. As authors acknowledge in Section 6 .", "<<Stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015), focusing mainly on re-parameterizable Gaussian random variables and deep latent Gaussian models, exploits the product rule for an integral to derive gradient backpropagation through several continuous random variables. >>", "This is exactly what authors do in these sections.", "Again it seems that the real contribution of this paper here is to extend this stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) ideas to discrete variables.", "Although this extension seems to be easily derived using the contributions made at point 2.", "Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_edit", "none", "none", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 164, "sentences": ["This paper tackles the question generation problem from a logical form and proposes an addition called Scratchpad Encoder to the standard seq2seq framework.", "The new model has been tested on the WebQuestionsSP and the WikiSQL datasets, with both automatic and human evaluation, compared to the baselines with copy and coverage mechanisms.", "Major points:", "Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.", "I don\u2019t recommend to accept this paper, at least in the current format.", "The paper states two major contributions (the last paragraph of Introduction), one is the new model Scratchpad Encoder, and the other is \u201cpossible to generate a large high quality (SPARQL query, local form) dataset\u201d.", "For the second contribution, there isn\u2019t any evaluation/justification about the quality of the generated questions and how useful this dataset would be in any KB-QA applications.", "I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.", "For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn\u2019t explain well the intuition of this \u201cwrite\u201d operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?", "In general I find Section 3 pretty difficult to follow.", "What does \u201ckeeping notes\u201d mean? It seems that the goal of this model is to keep updating the encoder hidden vectors (h_0, .., h_T) instead of fixing them at the decoder stage.", "I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.", "\\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.", "Minor points:", "- tau Yih et al, 2016 --> Yih et al, 2016", "- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.", "- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation", "none", "arg-request_result", "arg-structuring_heading", "arg-request_typo", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_typo_label", "none_label", "none_label"]}
{"abstract_id": 165, "sentences": ["In this paper, the authors propose a method for dimensionality reduction of image data.", "They provide a structured and deterministic function G that maps a set of parameters C to an image X = G(C).", "The number of parameters C is smaller than the number of free parameters in the image X, so this results in a predictive model that can be used for compression, denoising, inpainting, superresolution and other inverse problems.", "The structure of G is as follows: starting with a small fixed, multichannel white noise image, linearly mix the channels, truncate the negative values to zero and upsample.", "This process is repeated multiple times and finally the output is squashed through a sigmoid function for the output to remain in the 0..1 range.", "This approach makes sense and the model is indeed more principled than the one taken by Ulyanov et al. In fact, the DIP of Ulyanov et al. can hardly be considered \"a model\" (or a prior, for that matter), and instead should be considered \"an algorithm\", since it relies on the early stopping of a specific optimization algorithm.", "This means that we are not interested in the minimum of the cost function associated to the model, which contradicts the very concept of \"cost function\".", "If only global optimizers were available, DIP wouldn't work, showing its value is in the interplay of the \"cost\" function and a specific optimization algorithm.", "None of these problems exist with the presented approach.", "The exposition is clear and the presented inverse problems as well as demonstrated performance are sufficient.", "One thing that I missed while reading the paper is more comment on negative results.", "Did the authors tried any version of their model with convolutions or pooling and found it not to perform as well? Measuring the number of parameters when including pooling or convolutions can become tricky, was that part of the reason?", "Minor:", "\"Regularizing by stopping early for regularization,\"", "In this paper \"large compression ratios\" means little compression, which I found confusing."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_experiment", "arg-structuring_heading", "arg-request_typo", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_clarification_label"]}
{"abstract_id": 166, "sentences": ["[Summary]:", "This paper tackles the problem of automatic robot design.", "The most popular approach to doing this has been evolutionary methods which work by evolving morphology of agents in a feed-forward manner using a propagation and mutation rules.", "This is a non-differentiable process and relies on maintaining a large pool of candidates out of which best ones are chosen with the highest fitness.", "In robot design for a given task using rewards, training each robot design using RL with rewards is an expensive process and not scalable.", "This paper uses graph network to train each morphology using RL.", "Thereby, allowing the controller to share parameters and reuse information across generations.", "This expedites the score function evaluation improving the time complexity of the evolutionary process.", "[Strengths]:", "This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms.", "Paper is quite easy to follow.", "[Weaknesses and Clarifications]:", "=> Robot design area has been explored extensively in classical work of Sims (1994) etc. using ES.", "Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.", "=> Environment: The experimental section of the paper can be further improved.", "The approach is evaluated only in three cases: fish, walker, cheetah. Can it be applied to more complex morphologies? Humanoid etc. maybe?", "=> Baselines: The comparison provided in the paper is weak.", "At first, it compares to random graph search and ES.", "But there are better baselines possible.", "One such example would be to have a network for each body part and share parameters across each body part.", "This network takes some identifying information (ID, shape etc.) about body part as input.", "As more body parts are added, more such network modules can be added.", "How would the given graph network compare to this?", "This baseline can be thought of a shared parameter graph with no message passing.", "=> The results shown in Figure-4 (Section-4.2) seems unclear to me.", "As far as I understand, the model starts with hand-engineered design and then finetuned using evolutionary process.", "However, the original performance of the hand-engineered design is surprisingly bad (see first data point in any plot in Figure-4).", "Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?", "[Recommendation]:", "I request the authors to address the comments raised above.", "Overall, this is a reasonable paper but experimental section needs much more attention."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "arg-structuring_quote", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "arg-structuring_summary", "none", "arg-request_explanation", "arg-structuring_heading", "arg-structuring_quote", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-structuring_quote_label", "none_label"]}
{"abstract_id": 167, "sentences": ["Summary", "This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.", "The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results.", "Pros.", "-This paper is well-motivated.", "Studying label propagation in the meta-learning setting is interesting and novel.", "Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low.", "-The empirical results show improvement over the baselines, which are expected.", "Cons.", "-Some technical details  are missing.", "In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing.", "Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?", "-Does episode training help label propagation? How about the results of label propagation without the episode training?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 168, "sentences": ["Summary:", "This paper is built on the top of DNC model.", "Authors observe a list of issues with the DNC model: issues with deallocation scheme, issues with the blurring of forward and backward addressing, and issues in content-based addressing.", "Authors propose changes in the network architecture to solve all these three issues.", "With toy experiments, authors demonstrate the usefulness of the proposed modifications to DNC.", "The improvements are also seen in more realistic bAbI tasks.", "Major Comments:", "The paper is well written and easy to follow.", "The proposed improvements seem to result in very clear improvements.", "The proposed improvements also improve the convergence of the model.", "I do not have any major concerns about the paper.", "I think that contributions of the paper are good enough to accept the paper.", "I also appreciate that the authors have submitted the code to reproduce the results.", "I am curious to know if authors observe similar convergence gains in bAbI tasks as well. Can you please provide the mean learning curve for bAbI task for DNC vs proposed modifications?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 169, "sentences": ["The paper first provides a generalization bounds for adversarial training, showing that the error bound depends on Lipschitz constant.", "This motivates the use of spectral regularization (similar to Miyato et al 2018) in adversarial training.", "Using spectral regularization to improve robustness is not new, but it's interesting to combine spectral regularization and adversarial training.", "Experimental results show significant improvement over vanilla adversarial training.", "The paper is nicely written and the experimental results are quite strong and comprehensive. I really like the paper but I have two questions about the results:", "1. The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper.", "In PGM L_inf adversarial training/attack (column 3 of Figure 5), the prediction accuracy is roughly 50% under 0.1 infinity norm perturbation.", "However, previous papers (e.g., \"Obfuscated Gradients Give a False Sense of Security\") reported 55% accuracy under 0.031 infinity norm perturbation.", "I wonder why the numbers are so different.", "Maybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031.", "Another factor might be the model structure. If Alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (Madry et al and Athalye et al) to make the conclusion more clear.", "2. What's the training time of the proposed method compared with vanilla adversarial training?", "3. The idea of using SN to improve robustness has been introduced in the following paper:", "\"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\"", "(but this paper did not combine it with adv training)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_clarification", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 170, "sentences": ["This paper presents  a controllable model from a video of a person performing a certain", "activity. It generates novel image sequences of that person, according", "to user-defined control signals, typically marking the displacement of the moving", "body. The generated video can have an arbitrary background, and effectively", "capture both the dynamics and appearance of the person.", "It has two networks, Pose2Pose, and Pose2Frame.", "The overall pipeline makes sense; and the paper is well written.", "The main problems come from the experiments, which I would ask for more things.", "It has two components, i.e., Pose2Pose and Pose2Frame.", "So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.", "How about combining only Pose2Pose/ Pose2Frame  with pix2pixHD? Whether the performance can get improved?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label"]}
{"abstract_id": 171, "sentences": ["The paper introduces a generative model for video prediction.", "The originality stems from a new training criterion which combines a VAE and a GAN criteria.", "At training time, the GAN and the VAE are trained simultaneously with a shared generator; at test time, prediction conditioned on initial frames is performed by sampling from a latent distribution and generating the next frames via an enhanced conv LST .", "Evaluations are performed on two movement video datasets classically used for benchmarking  this task - several quantitative evaluation criteria are considered.", "The paper clearly states the objective and provides a nice general description of the method.", "The proposed model extends previous work by adding an adversarial loss to a VAE video prediction model.", "The evaluation compares different variants of this model to two recent VAE baselines.", "A special emphasis is put on the quantitative evaluation: several criteria are introduced for characterizing different properties of the models with a focus on diversity. w.r.t. the baselines, the model behaves well for the \u201crealistic\u201d and \u201cdiversity\u201d measures. The results are more mitigated for measures of accuracy.", "As for the qualitative evaluation, the model corrects the blurring effect of the reference SV2P baseline, and produces quite realistic predictions on these datasets.", "The difference with the other reference model (SVG) is less clear.", "While the general description of the model is clear, details are lacking.", "It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.", "This would also help to explain the difference of performance/ behavior  w.r.t. these models (Fig. 5).", "It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.", "Similarly, you did not indicate what the deterministic version of your model is.", "The generator model with its warping component makes a strong hypothesis on the nature of the videos: it seems especially well suited for translations or for other simple geometric transformations characteristics of the benchmarking videos .", "Could you comment on the importance of this component? Did you test the model on other types of videos where this hypothesis is less relevant?", "It seems that the baseline SVG makes use of simpler ConLSTM for example.", "The description of the generator in the appendix is difficult to follow. I missed the point in the following sentence: \u201cFor each one-step prediction, the network has the freedom to choose to copy pixels from the previous frame, used transformed versions of the previous frame, or to synthesize pixels from scratch\u201d .", "Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.", "Overall, the paper proposes an extension of VAE based video prediction models and produces an extensive evaluation.", "While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "arg-structuring_summary", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 172, "sentences": ["* Strengths:", "- The paper gives theoretical insight into why Batch Normalization is useful in making neural network training more robust and is therefore an important contribution to the literature.", "- While the actual arguments are somewhat technical as is expected from such a paper, the motivation and general strategy is very easy to follow and insightful.", "* Weaknesses:", "- The bounds do not immediately apply in the batch normalization setting as used by neural network practitioners, however there are practical ways to link the two settings as pointed out in section 2.4", "- As the authors point out, the idea of using a batch-normalization like strategy to set an adaptive learning rate has already been explored in the WNGrad paper.", "However it is valuable to have a similar analysis closer to the batch normalization setting used by most practitioners.", "- Currently there is no experimental evaluation of the claims, which would be valuable given that the setting doesn't immediately apply in the normal batch normalization setting.", "I would like to see evidence that the main benefit from batch normalization indeed comes from picking a good adaptive learning rate.", "Overall I recommend publishing the paper as it is a well-written and insightful discussion of batch normalization. Be aware that I read the paper and wrote this review on short notice, so I didn't have time to go through all the arguments in detail."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-structuring_quote", "arg-request_experiment", "arg-request_experiment", "arg-request_result", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 173, "sentences": ["I read the other reviewers' comments as well as the rebuttal.", "I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper.", "Therefore, I do not feel confident in championing this paper.", "PS: I am downgrading my confidence in my evaluation.", "---", "Paper 93 proposes an empirical evaluation of the memorization properties of convnets.", "More specifically, it evaluates three aspects:", "-", "First it evaluates whether convnets can learn to distinguish images from two different sets by training a binary classifier.", "The conclusion is that, indeed, deep convnets can learn to make such a decision. As could be guessed from intuition, the larger the capacity of the network and the smaller the size of the sets, the higher the accuracy.", "-", "Second, it evaluates whether we can detect that a group of samples of a dataset was used to train a model.", "For this purpose, it is proposed to compute the distribution of maximal activation scores of the output softmax layer and to make use of the Kolmogorov-Smirov distance between the cumulative distributions.", "It is shown experimentally that one can detect (even partial) leakage with such a technique.", "-", "Third, it evaluates whether we can detect that a single images was used to train a convnet.", "Two simple techniques are proposed.", "The first one considers that a sample is part of the training set if it correctly classified.", "The second one considers that a sample is part of the training set if its loss is below a threshold.", "It is shown experimentally that one can make such a decision with moderate accuracy.", "On the positive side:", "-\tThis is a topic that should be of broad interest to the ICLR community.", "-\tThe paper is generally well-written.", "-\tThe experiments are reported on large-scale datasets on high-capacity networks which is more realistic than small-scale settings.", "On the negative side:", "-", "It is unclear whether the data augmentation techniques is applied only at training time or also at test time.", "In other words: at test time, do you present the original images only or transformed images too?", "-\tIn section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?", "-\tSection 5 is somewhat less clear than the previous sections.", "The authors should more clearly define what the private, public and evaluation sets are, right from the beginning.", "The purpose of the public set is explained only in section 5.2.", "-\tThe experimental results of section 5.2 are somewhat disappointing.", "Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.", "Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).", "This seems to be too low to be of practical use.", "This might be because the Bayes and MAT attacks are too simplistic.", "Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?"], "labels": ["none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_edit", "none", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 174, "sentences": ["The authors argue that not knowing the distribution of rewards observed in the policy gradient algorithm hinders learning (and the tuning process).", "They propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution, which has a fixed and known U[-1, 1] distribution.", "They test their methods on a toy task that consists in finding inclusion maximal cliques (which tests for local optimality) against REINFORCE (including their variants: centering the rewards with a mean baseline or normalizing them), the cross-entropy method and Exp3.", "I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.", "The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.", "The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.", "I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning.", "Additionally, I find the motivation for caring about local optimality unconvincing.", "I take exception that people care more about local optimality than the actual objective.", "From a practical point of view, local optimality is a mean (that can be achieved via heuristic algorithms) to an end (the objective itself).", "This also holds for k-means, which is usually run multiple times with different starting conditions.", "Some comments:", "- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.", "e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)", "- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).", "- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)", "- I would also encourage the authors to come up with a more descriptive name for the approach."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 175, "sentences": ["This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD.", "Experiments on few shot learning show that meta dropout achieves better performance.", "Overally, I think this paper is well motivated and experiments on few shot learning are impressive.", "I have only two major concerns.", "1. Sec 3.2. According to my understanding, Meta dropout introduces a learnable prior for latent $z$, but the training objective does not require posterior inference and thus no variational inference is needed.", "I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\\theta,\\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework.", "2.", "Experiments on adversarial robustness can be further improved.", "(1) the settings and the analysis of adversarial robustness experiment can be discussed in details.", "For example, how to build ''adversarial learning baseline'' in meta learning settings and why the result implies the perturbation directions for generalization and robustness relates to each other; (2) how other regularization methods (e.g., Mixup, VIB and Information dropout) perform on adversarial robustness? Does Meta dropout performs better than them?", "(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.", "I suggest trying some other STOA attack methods (e.g., iterative methods).", "Some typos:", "Page 3, Regularization methods, 3rd line, ````wwwdiscuss", "Page 7, 2nd line from the bottom, FSGM->FGSM"], "labels": ["arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "none", "arg-request_experiment", "arg-request_edit", "arg-request_explanation", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 176, "sentences": ["This paper presents a new way to represent a dense matrix in a compact format.", "First, the method prunes a dense matrix based on the Viterbi-based pruning.", "Then, the pruned matrix is quantized with alternating multi-bit quantization.", "Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm.", "It spots the problem of each existing approach and solve the problems by combining each method.", "The combination is new and the result is encouraging.", "I find this paper is interesting and I like the strong results.", "It is an interesting combination of methods.", "However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation.", "1. The method should be compared with other combinations of components.", "At least, it should be compared with \"Multi-bit quantization only (Xu et al., 2018)\" and \"Multi-bit-quantization + Viterbi-based binary code encoding\".", "2. The experiments with \"Don't Care\" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits.", "3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods.", "In Section 3.3. , it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed.", "I feel that this is a kind of things which support the proposed method if it is properly assessed.", "4. When you say \"slow\" form something and propose a method to address it, I'd like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow \"sequential sparse matrix decoding process\".", "Minor comments:", "* It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance.", "It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "none", "none", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_clarification_label"]}
{"abstract_id": 177, "sentences": ["In this paper, the authors presented a large experimental study of curiosity-driven reinforcement learning on various tasks.", "In the experimental studies, the authors also compared several feature space embedding methods, including identical mapping (pixels), random embedding, variational autoencoders and inverse dynamics features.", "The authors found that in many of the tasks, learning based on intrinsic rewards could generate good performance on extrinsic rewards, when the intrinsic rewards and extrinsic rewards are correlated.", "The authors also found that random features embedding, somewhat surprisingly, performs well in the tasks.", "Overall, the paper is well written with clarity.", "Experimental setup is easy to understand.", "The authors provided code, which could help other researchers reproduce their result.", "Weaknesses:", "1) as an experimental study, it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards.", "The author is correct that in many tasks, well-behaved extrinsic rewards are hard to find.", "But for problems with well-defined extrinsic rewards, such a comparison could help readers understand the relative performance of curiosity-based learning and/or how much headroom there exists to improve the current methods.", "2) it is surprising that random features perform so well in the experiments.", "The authors did provide literature in classification that had similar findings, but it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "none", "arg-request_experiment", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 178, "sentences": ["This paper proposed a new pooling method (Frequency pooling) which is strict shift equivalent and anti-aliasing in theory.", "The authors first derived the theory of F-pooling to be optimal anti-aliasing down sampling and is shift-equivalent in sec 2, and then demonstrated the experimental results of 1D signals and image classification tasks.", "The experimental results are actually less impressive than what are claimed in contribution and conclusion.", "The authors stated that \"F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs\"; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%).", "Questions:", "1. For the experiment of 1D signal on sine wave, the AA-pooling and F-pooling give the same result?", "2. Compared to AA-pooling, it seems that F-pooling has a better theoretical guarantee (i.e. the optimal anti-aliasing down sampling operation given U).", "But other than this, the empirical performance seem not showing particular advantage over AA-pooling.", "Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?", "3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.", "- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-request_clarification", "none", "none", "arg-request_clarification", "arg-request_clarification", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_explanation_label"]}
{"abstract_id": 179, "sentences": ["In this paper the authors introduce a new technique for softmax inference.", "In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert.", "Then, given the expert, output a particular category.", "The first level of sparsity comes from the first expert.", "The second level of sparsity comes from every expert only outputting a limited set of output categories.", "The paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. \"search right\" -> \"search for the right\", \"predict next word\" -> \"predict the next word\", ...) In section 3, can you be more specific about the gains in training versus inference time?", "I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.", "You motivate some of the work by the fact that the experts have overlapping outputs.", "Maybe in section 3.7 you can address how often that occurs as well?", "Nits:", "- it wasn't clear how the sparsity percentage on page 3 was defined?", "- can you motivate why you are not using perplexity in section 3.2?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "arg-request_explanation", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 180, "sentences": ["In this paper, the authors focus on alleviating the catastrophic forgetting problem in continual learning.", "The authors propose a discriminative variational autoencoder (DiVA) to solve this problem under the generative replay framework.", "DiVA modifies the objective function of VAE by introducing an additional term that maximizes the mutual information between the latent variables and the class labels.", "The authors do not thoroughly explain the motivation of this paper.", "The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.", "It is also not clear to me why these problems are important.", "The idea that introduces labels in VAE is not novel.", "For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.", "I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.", "It is also not clear to me how domain translation is relevant to continual learning.", "In terms of modeling, since the input into the prior network has finite possible discrete values, we do not need a fully connected network to generate $\\hat{\\mu}_c$ and $\\hat{\\sigma}_c$. Instead, we can directly optimize $\\hat{\\mu}_c$ and $\\hat{\\sigma}_c$ for each $c$ as parameters.", "The paper provides some good experimental results.", "But the problem settings are not clear to me.", "I do not understand how the model is trained to solve multiple tasks.", "Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?", "It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.", "In summary, since DiVA gives a good experimental performance, the proposed method might be promising.", "However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.", "References", "[1]Narayanaswamy, Siddharth, T. Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr. \"Learning disentangled representations with semi-supervised deep generative models.\" In Advances in Neural Information Processing Systems, pp. 5925-5935. 2017."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_clarification", "none", "none", "arg-request_explanation", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 181, "sentences": ["This paper investigates the effect of the batch normalization in DNN learning.", "The mean field theory in statistical mechanics was employed to analyze the progress of variance matrices between layers.", "As the results, the batch normalization itself is found to be the cause of gradient explosion.", "Moreover, the authors pointed out that near-linear activation function can improve such gradient explosion.", "Some numerical studies were reported to confirm theoretical findings.", "The detailed analysis of the training of DNN with the batch normalization is quite interesting.", "There are some minor comments below.", "- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?", "- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.", "- The randomized weight is not very practical. Though it may be the standard approach of mean field, some comments would be helpful to the readers."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 182, "sentences": ["The paper studies the Bayesian inferences with the generative adversarial network (GAN).", "In the first half of the paper, the general framework of the Bayes estimation is introduced.", "Then, The authors proposed how to incorporate GAN to the Bayesian inference.", "Some computational methods for calculating the mean of the statistic under the posterior distribution are described.", "Then, numerical experiments using MNIST and Celeb-A datasets are presented.", "Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.", "In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.", "Hence, the effectiveness and advantage of the proposed methods are not clear.", "- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.", "- How does the estimation accuracy of GAN relate to the estimation accuracy of the proposed method? Showing a quantitative description would be nice."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label"]}
{"abstract_id": 183, "sentences": ["This paper researches the pooling operation, which is an important component in convolutional neural networks (CNN) for image classification.", "Taking the perspective from signal processing, this paper proposes a pooling operation called frequency pooling (F-pooling).", "The key motivation is to make the pooling operation shift-equivalent and anti-aliasing.", "This paper gives an improved definition on shift-equivalent functions and shows that the proposed F-pooling is optimal in the sense of reconstructing the orignal signal.", "The F-pooling is then implemented with matrix multiplications and tested with recent convolutional neural networks for image classifiation on CIFAR-100 and a subset of ImageNet dataset.", "It is interesting to take the perspective from signal processing to give pooling operation in CNN a formal treatment.", "As indicated in the recent literature, enforcing shift-invariance does help to improve the performance of a CNN on classification accuracy and the robustness with respect to image shift.", "At the same time, this work can be further enhanced at the following aspects:", "1. This work can make it clearer in principle how anti-aliasing contributes to improving the classification performance and robustness.", "This will help to make this paper more self-contained.", "2. When showing the optimality of F-pooling in Section 2.3, the criterion is to reconstruct the original signal x. Considering that the ultimate goal is classification, the information to be maximally preserved through each operation through the layers shall be the information that relates to the class label y. In light of this, some justification and explanation shall be provided for using this criterion for optimality.", "3. The experimental study is weak.", "Experiments could be conducted on more benchmark datasets with more CNN architectures to convincingly show the effectiveness of the proposed F-pooling.", "Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.", "For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.", "In Table 3, the F-pooling consistently shows inferior classification performance, although obtaining slightly higher consistency.", "This makes the advantage of F-pooling over the existing AA-pooling unclear."], "labels": ["none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "none", "arg-request_explanation", "none", "arg-request_experiment", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 184, "sentences": ["The authors propose to meta-learn, using MAML, the mean of an elementwise, input-dependent, multiplicative noise to improve generalization in few-shot learning.", "The motivation is that meta-learning the noise allows to learn how to best perturb examples in order to improve generlization.", "This claim is supported by ample experimental evidence and comparisons against many baselines, as well as additional ablation studies w.r.t design choices of the algorithm itself.", "The paper is well written and easy to read.", "Consequently, I think this is a nice paper and should be accepted.", "Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?", "Edit:", "Thank you for your response.", "I will leave my score as is.", "I would strongly encourage the authors to incorporate the baseline \"(1)\" as proposed by R3 in a future version of the paper as I agree with them that this is a relevant baseline."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_clarification", "arg-structuring_heading", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_clarification_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 185, "sentences": ["This paper presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models.", "It consists of math problems in various categories such as algebra, arithmetic, calculus, etc.", "The dataset is designed carefully so that it is very unlikely there will be any duplicate between train/test split and the difficulty can be controlled.", "Several models including LSTM, LSTM + Attention, Transformer are evaluated on the proposed dataset.", "The result showed some interesting insights about the evaluated models.", "The evaluation of mathematical reasoning ability is an interesting perspective.", "However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.", "The paper is relatively well-written, although the description of the neural models can be improved.", "The generation process of the dataset is well thought out.", "The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited.", "One suggestion is that it might be useful to also release the structured (parsed) form besides the freeform inputs and outputs, for analysis and for evaluating structured neural network models like the graph networks.", "My main concerns are about the evaluation and comparison of standard neural models.", "The use of \u201cblank inputs (referred to as \u201cthinking steps\u201d)\u201d in \u201cSimple LSTM\u201d and \u201cAttentional LSTM\" doesn\u2019t seem to be a standard approach.", "In the attentional LSTM, the use of \u201cparse LSTM\u201d is also not a standard approach in seq2seq models and doesn\u2019t seem to work well in the experiment (similar result to \u201cSimple LSTM\").", "I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.", "With some improvements in the evaluation and comparison, I believe this paper will be more complete and much stronger.", "typo:", "page 3: \u201cfreefrom inputs and outputs\u201d -> \u201cfreeform inputs and outputs\u201d"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_clarification", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label"]}
{"abstract_id": 186, "sentences": ["I read the paper and understand it, for the most part.", "The idea is to interpret some regularization technics as a from of noisy bottleneck, where the mutual information b tween learned parameters and the data is limited through the injection of noise.", "While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.", "I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)", "Pro: nicely written, clear interpretation of regularization as a noise injection technics, explicit link with information theoery and Shanon capacity.", "Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation"], "labels": ["none", "arg-structuring_summary", "none", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 187, "sentences": ["This paper provides a new dynamic perspective on deep neural network.", "Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers.", "Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system.", "Local performance around the fixed point is explored.", "Extensions are provided to include the batch normalization.", "I believe this paper may stimulate some interesting ideas for other researchers.", "Two technical questions:", "1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point.", "How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?", "This somewhat conflicts the commonsense of \"the deeper the better?\"", "2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-request_explanation", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 188, "sentences": ["= Summary", "A method to predict likely type of program variables in TypeScript is presented.", "It consists of a translation of a program's type constraints and defined objects into a (hyper)graph, and a specialised neural message passing architecture to learn from the generated graphs.", "Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks.", "= Strong/Weak Points", "+ The graph representation of the problem is novel, and draws both on core ideas from Hindley-Milner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity)", "+ The neural message passing architecture is adapted to the problem, handling features not present in the standard GNN literature (hyperedges, ...)", "+ Experiments compare with relevant baselines and consider interesting ablations, studying the effect of the GNN extensions in detail.", "- The hyperparameter selection regime (and the experiments used to find them) is not described", "= Recommendation", "This is an application-driven paper with nice practical results.", "The fact that standard neural architectures are extended and adapted to the task, and the way domain knowledge is used to design the graph representation makes this interesting even to people outside the task-specific audience, and hence I strongly recommend acceptance.", "= Minor Comments", "- page 2: \"network's type to be class\" -> \"to be a class\"", "- Evaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (D\u00e9j\u00e0Vu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_clarification_label"]}
{"abstract_id": 189, "sentences": ["The problem that the paper tackles is very important and the approach to tackle it id appealing.", "The idea of regarding the history as a tree looks very promising.", "However, it\u2019s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.", "Using neural network if an interesting choice for capturing the influence probability and its timing.", "The authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks?", "The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works.", "This could have made the paper much stronger.", "It was nice that the paper iterated and reviewed the possible inference and learning ways.", "There is one more way.", "Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.", "The paper can benefit from a proofreading.", "There are a few typos throughout the paper such as: Reference is missing in section 2.1 Page 2 paragraph 1: \u201can neural attention mechanism\u201d [1] Back to the Past: Source Identification in Diffusion Networks from Partially Observed Cascades, AISTATS 2015"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_clarification", "arg-request_experiment", "none", "none", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 190, "sentences": ["-- This paper seeks to combine several ideas together to propose an approach for image classification based continual learning tasks.", "In this effort, the paper combines previously published approaches from generative modeling with VAEs, mutual information regularization and domain adaptation.", "I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.", "--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don\u2019t believe 1 and 2 are equal.", "--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.", "The following is the concern:", "--In the second line of Equation 5, the KL term appears to be measuring a distance between distributions on two different variables; z|c and c|z.", "If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.", "--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.", "It is also not clear how the loss function proposed differs from that of the CDVAE, etc.", "If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.", "Additional feedback for authors (not part of the main decision reasoning):", "- What is dt in Algorithm 1 description?", "Figure 1:", "-typo \u201cimplmented\u201d", "-What\u2019s the 3d plot supposed to represent?", "Doesn't the classification loss have a dependency on the input condition?", "--What does a \"heavy classifier\" imply concretely?", "--\u201cRedundant weights\u201d seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).", "--The notation for the proposed parameters theta, theta\u2019, phi, phi\u2019 are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.", "In later sections they use theta and theta\u2019 for encoder/decoder resp.", "-- \u201cWhen the encoder and decoder networks are sufficiently complex, it is enough to implement each the prior and classification network as one fully-connected layer\u201d", "\u2192 what do the authors mean \u201c when \u2026 networks are sufficiently complex\u201d or do they actually mean when the \u201cwhen the problem is simple enough\u201d?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_clarification", "arg-structuring_heading", "arg-request_typo", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "none", "none", "none", "arg-structuring_quote", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_clarification_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-request_clarification_label"]}
{"abstract_id": 191, "sentences": ["The paper proposes doubly sparse, which is a sparse mixture of sparse experts and learns a two-level class hierarchy, for efficient softmax inference.", "[+] It reduces computational cost compared to full softmax.", "[+] Ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed", "[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.", "Besides, in evaluation, the paper only compares Doubly Sparse with full softmax.", "Why not compare with Sparsely-Gated MoE?", "Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 192, "sentences": ["This paper discusses the optimization of robot structures, combined with their controllers.", "The authors propose a scheme based on a graph representation of the robot structure, and a graph-neural-network as controllers.", "The experiments show that the proposed scheme is able to produce walking and swimming robots in simulation.", "The results in this paper are impressive, and the paper seems free of technical errors.", "The main criticism I have is that I found the paper harder to read.", "In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.", "This makes the contribution of this paper in terms of the method hard to judge. Please include further description of the ES cost function and algorithm in the main body of the paper.", "The second point is that the proposed approach seems to modify a few things from the ES baseline.", "The efficacy of the separate modifications should be tested.", "Therefore I would like to see experiments with the ES cost function, but with inclusion of the pruning step, and experiments with the AF-function but without the pruning step."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label"]}
{"abstract_id": 193, "sentences": ["The authors propose PARCUS (\"Pattern Representations on Continuous Spaces\"), a model which computes a soft-matching probability for all words in an input sequence with so-called prototypes in order to predict a label for the input.", "Furthermore, for training, PARCUS makes use of rationales.", "Those are indicators of input importance, and help to boost the loss for relevant tokens.", "The main motivation to use PARCUS is that it works better in a low-resource setting than recent state-of-the-art models for the high-resource case.", "This is due to it having relatively few parameters and to it having a strong inductive bias.", "However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.", "Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW.", "Another selling point of PARCUS is that it's interpretable. While neural networks can also be analyzed in different ways, I agree with the authors that this is nice to have.", "Overall, the paper seems solid.", "==========", "Update: After reading the other reviews and the responses by the authors, I lowered my score from 6 to 3."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 194, "sentences": ["This paper introduced a latent space model for reinforcement learning in vision-based control tasks.", "It first learns a latent dynamics model, in which the transition model and the reward model can be learned on the latent state representations.", "Using the learned latent state representations, it used an actor-critic model to learn a reactive policy to optimize the agent's behaviors in long-horizon continuous control tasks.", "The method is applied to vision-based continuous control in 20 tasks in the Deepmind control suite.", "Pros:", "1. The method used a latent dynamics model, which avoids reconstruction of the future images during inference.", "2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.", "3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.", "Cons:", "1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.", "In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.", "However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).", "2. Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.", "3. The world model is fixed while learning the action and value models, meaning that reinforcement learning of the actor-critic model cannot be used to improve the latent state model.", "It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.", "Typos:", "Reward prediction along --> Reward prediction alone", "this limitation in latenby?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_summary", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 195, "sentences": ["This paper analyzes the AlphaGo Zero algorithm by showing that the optimal policy corresponds to a Nash equilibrium.", "The authors then show that the equilibrium corresponds to a KL-minimization.", "Finally, the show on a classical scheduling task.", "On the positive side, the paper is well written and structured.", "The results presented are very interesting, specially showing that stochastic approximation of a KL-divergence minimization.", "The case-study is also interesting, although does not improve current state-of-the-art.", "On the negative side, I think the relevance and novelty of the results should be explained better.", "For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.", "The MDP formalization is rather straightforward.", "Also, MCTS has been used extensively to find Nash equilibria in both perfect and imperfect games, e.g., \"Online monte carlo counterfactual regret minimization for search in imperfect information games\".", "Maybe the authors can elaborate more on the significance/relevance of this contribution.", "Besides, the power of AlphaGo Zero resides in the combination of the MCTS together with the compact representation learning of the value functions.", "The presented analysis seems to neglect the error term corresponding to the value function.", "There are other minor details:", "- Eq(2) . notation: \\forall s is missing", "- Theorem 2 should be Theorem 1", "- \"there are constraints per which state can transition\"", "- \"P1 is agent\" -> \"P1 is the agent\"", "- \"Pinker\" -> \"Pinsker\"", "- C_R in Eq(5) is not introduced."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit", "none", "none", "none", "arg-request_explanation", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_edit", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_edit_label"]}
{"abstract_id": 196, "sentences": ["This paper presents 4D convolutional neural networks for video-level representations.", "To learn long-range evolution of spatio-temporal representation of videos, the authors proposed V4D convolution layer.", "Benchmark on several video classification dataset shows improvement.", "1. In section 3.1, the authors selected a snippet from each section, but this was not rigorously defined.", "Same for action units.", "It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.", "2. In section 3.2, the authors argued that 3D kernel suffers from trade-off between receptive field and cost of computation.", "At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.", "3D convolution is already expensive and not scalable, but 4D operation sounds even more expensive and more prohibitive.", "3. In the paper, the authors argued that clip-level feature learning is limited as it is hard to learn long-range spatio-temporal dependency.", "It makes sense, and I expect the proposed model may benefit from its design for long-range spatio-temporal feature learning.", "However, what I see in the experiments is on ~300 frames for Mini-Kinetics and 36-72 frames for Something-Something dataset.", "Assuming that a second is represented with 15-30 frames, this corresponds to 10-20 sec and 1-4 sec, respectively.", "I'd say these short videos are still clips.", "The paper presents an interesting idea, but there are some issues that need to be addressed before published on ICLR."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 197, "sentences": ["This paper proposes an algorithm for auxiliary learning.", "Given a target prediction task to be learned on training data, the auxiliary learning utilizes external training data to improve learning.", "The authors focus on a setup where both target and external training data come from the same distribution but differ in class labels, where each class in the target data is a set of finer-grained classes in the auxiliary data.", "The authors propose a heuristic for learning from both data sets through minimization of a joint loss function.", "The experimental results show that the proposed methods works well on this particular setup on CIFAR data set.", "Strengths:", "+ a new auxiliary learning algorithm", "+ positive results on CIFAR data set", "Weaknesses:", "- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space", "- there is no attempt to provide a theoretical insight into the performance of the algorithm", "- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance", "- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario", "- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as \"(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set\"??"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 198, "sentences": ["==============Final Evaluation================", "I have gone through the other reviews as well as the author response.", "Firstly, I would like to thank the authors for providing detailed responses to my questions.", "In general, I agree with R2 that the paper generally has some potentially interesting ideas and results but the manner in which the current draft is organized and presented makes it hard to grasp them and there is a lack of coherent message about what the paper is about.", "Moreover, from my understanding the analysis in David McKay\u2019s book (Chapter 41) concerns a single neuron (and the number of parameters for a single neuron) .", "As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper).", "Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review).", "In general, I feel this section could use some tighter formalism and justifications.", "I also remain unconvinced by the response to my issue with the claim \u201cOur experiments show that our networks can remember a large number of images and distinguish them from unseen images\u201d, where the negative images are also seen by the memorization model, so they are not unseen.", "The authors address this by saying 3M of the 15 M negatives have been seen.", "That does not seem like a small enough percentage to claim that these are \u201cunseen\u201d images.", "In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.", "==================", "Summary", "The paper trains classification models to classify a labeling of a subset of images (assigned with label 1) from the rest of the images (assigned with a label 0).", "Firstly, the paper shows that deep learning models are able to learn such classifiers and get low training loss.", "It then proposes to use this model to ``attack\u2019\u2019 task-specific models to perform membership inference, i.e. figuring out if an image provided in a set was used in training or not.", "Strengths", "+ The paper thoroughly covers related work and provides context.", "+ Results on confidence as a signature of a dataset are interesting.", "Weaknesses", "[Motivation] 1.", "In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization. Thus, without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct.", "Also, the claim in Fig. 1 that the transition from \u2018\u2019high capacity\u2019\u2019 to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)", "[Capacity]", "2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization. This is something which would need to be explained/ substantiated separately. (*)", "3. Scenario discussed in Sec. 4 seems somewhat impractical. Given a set of m images, it is not clear that a classifier that is trained to detect between train and validation is sufficient, as one might also need to figure out if it is neither train nor val, which is a very practical scenario.", "4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image \u2018m\u2019 corresponds to is useful or practical, as this seems to be a property of the set \u2018m\u2019 rather than the property of the trained classification model (f_\\theta) .", "Please clarify.", "On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing. (*) 6.", "It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model. While the baseline approaches seem to make use of the model confidence, I cannot see how the proposed approach (which uses a classifier) makes use of the original model.", "It is also not clear why Table. 3 does not report the Bayes baseline results. Also, does this section use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?", "7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images\u2019\u2019 -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen\u2019\u2019 images which it labels as the negative class, thus the negative class is also seen by the memorization model. (*)", "Minor Points", "1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization\u2019\u2019.", "In addition, the paper would also need to show that such a model does not generalize to a validation set of images. This is probably obvious given the results from Zhang et.al. but should be included as a sanity check.", "2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.", "References:", "[A]: Blier, L\u00e9onard, and Yann Ollivier. 2018. ``The Description Length of Deep Learning Models.\u2019\u2019 arXiv [cs.LG]. arXiv.", "http://arxiv.org/abs/1802.07044 .", "Preliminary Evaluation", "There are numerous issues with the writing and clarity of the paper, while it seems like some of the observations around the confidence of classifiers are interesting, in general the connection between those set of results and the ``memorization\u2019\u2019 capabilities of the classifier trained to remember train vs val images is not clear in general.", "Important points for the rebuttal are marked with (*)."], "labels": ["arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_clarification", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 199, "sentences": ["This paper presents a method for single image 3D reconstruction.", "It is inspired by implicit shape models, like presented in Park et al. and Mescheder et al., that given a latent code project 3D positions to signed distance, or occupancy values, respectively.", "However, instead of a latent vector, the proposed method directly outputs the network parameters of a second (mapping) network that displaces 3D points from a given canonical object, i.e., a unit sphere.", "As the second network maps 3D points to 3D points it is composable, which can be used to interpolate between different shapes.", "Evaluations are conducted on the standard ShapeNet dataset and the yields results close to the state-of-the-art, but using significantly less parameters.", "Overall, I am in favour of accepting this paper given some clarifications and improving the evaluations.", "The core contribution of the paper is to estimate the network parameters conditioned on the input (i.e., the RGB image).", "As noted in the related work section this is not a completely new idea (cf. Schmidhuber, Ha et al.).", "There are a few more references that had similar ideas and might be worth adding: Brabandere et al. \"Dynamic Filter Networks\", Klein et al. \"A dynamic convolutional layer for short range weather prediction\", Riegler et al. \"Conditioned regression models for non-blind single image super-resolution\", and maybe newer works along the line of Su et al. \"Pixel-Adaptive Convolutional Neural Networks\".", "The input 3D points are sampled from a unit sphere.", "Does this imply any topological constraints?", "Is this the most suitable shape to sample from? How do you draw samples from the sphere (Similarly, how are the points sampled for the training objects)?", "What happens if you instead densely sample from a 3D box (similar to the implicit shape models)?", "On page 4 the mapping network is described as a function that maps c-dimensional points to 3D points.", "What is c? Isn't it always 3, or how else is it possible to composite the mapping network?", "Regarding the main evaluation: The paper follows the \"standard\" protocol on ShapeNet.", "Recently, Tatarchenko et al. showed in \"What Do Single-view 3D Reconstruction Networks Learn?\" shortcomings of this evaluation scheme and proposed alternatives.", "It would be great if this paper could follow those recommendations to get better insights in the results.", "Further, I could not find what k was set to in the evaluation of Tab. 1.", "It did also not match any numbers in Tab. 4 of the appendix.", "Tab. 4 shows to some extend the influence of k, but I would like to see a more extensive evaluation.", "How does performance change for larger k, and what happens if k is larger at testing then on at training, etc.?", "Things to improve the paper that did not impact the score:", "- The tables will look a lot nicer if booktab is used in LaTeX"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "arg-structuring_summary", "arg-request_clarification", "arg-request_explanation", "arg-request_explanation", "arg-structuring_quote", "arg-request_explanation", "none", "none", "arg-request_edit", "arg-request_clarification", "none", "arg-request_experiment", "arg-request_explanation", "arg-structuring_heading", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_edit_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_edit_label", "arg-structuring_summary_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_clarification_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_edit_label"]}
{"abstract_id": 200, "sentences": ["This paper proposes an approach for mitigating issues associated with high-frequency/amplitude control signals that may be obtained when one applies reinforcement learning algorithms to continuous control tasks.", "The approach taken by the paper is to solve a constrained optimization problem, where the constraint imposes a (potentially state-dependent) lower bound on the reward.", "This is done by using a Lagrangian relaxation that learns the parameters of a control policy that satisfies the desired constraints (and also learns the Lagrange multipliers).", "The presented approach is demonstrated on a cart-pole swing-up task as well as a quadruped locomotion task.", "Strengths:", "+ The paper is generally clear and readable.", "+ The simulation results for the Minitaur quadruped robot are performed using a realistic model of the robot.", "Major concern:", "- My biggest concern is that the technical contributions of the paper are not clear at all.", "The motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimum-time optimal control problems, or control schemes such as sliding mode control).", "The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)).", "The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.", "Overall, the paper does not make a compelling case for the novelty of the problem or approach.", "Other concerns:", "- For the cart-pole task, the paper states that the reward is modified \"to exclude any cost objective\".", "Results are then presented for this modified reward showing that it results in high-frequency control signals (and that the proposed constrained approach avoids this).", "I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.", "- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control) is very broad and should be watered down. This is the case only when one considers a poorly-designed cost function that doesn't take into account realistic factors such as actuator limits.", "- In the last paragraph of Section 3.3, the paper proposes making the lower-bound on the reward state-dependent.", "However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).", "Typos:", "- Pg. 5, Section 3.4: \"...this is would achieve...\"", "- Pg. 6: ...thedse value of 90...\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 201, "sentences": ["This paper proposes a justification to one observation on VAE: \"restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization\".", "The explanation given in this work is based on Gaussian mean-field approximation.", "I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example", "- the sentence under eq. (2)", "- the sentence \"Bacause the identity of the datapoint can never be learned by ...\" What is the identity of a dat point?", "It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods.", "Somehow, those connections are not clear to me.", "Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.", "As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z).", "However, based on the definition of \\theta and \\tilde{\\theta} given in the first sentence of section 2.3, the relation between \\theta, \\tilde{\\theta} and D should be: D <- \\theta -> \\tilde{\\theta} (if it is a generative model) or D -> \\theta -> \\tilde{\\theta} (if a discriminative model).", "Either case, I don't think we can have the inequality in eq. (5)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_clarification", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 202, "sentences": ["I raised my rating. After the rebuttal.", "- the authors address most of my concerns.", "- it's better to show time v.s. testing accuracy as well.", "the per-epoch time for each method is different.", "- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.", "-------------------------------------------------------------", "This paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters.", "The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal.", "1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.", "- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters.", "After all, newton method and natural gradients method are not used in experiments.", "- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.", "2. No need to write so much decorated bounds in section 3.", "The convergence analysis is on Z, not on parameters x and hyper-parameters theta.", "So, bounds here can not be used to explain empirical observations in Section 5.", "3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?", "4. Authors have done a good comparison in the context of deep nets.", "However, - could the authors compare with changing step-size?", "In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates.", "Is it better to decay learning rates for toy data sets?", "It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.", "- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., \"For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01\", \"while for RMSprop-APO, the best lambda was 0.0001 \" .", "What are reasons for these?", "- In Section 5.2, it is said lambda is tuned by grid-search.", "Tuning a good lambda v.s. tuning a good step-size, which one costs more?"], "labels": ["none", "none", "arg-request_result", "arg-request_result", "none", "arg-structuring_heading", "arg-structuring_summary", "none", "arg-request_edit", "none", "none", "arg-request_explanation", "arg-request_edit", "none", "none", "arg-request_clarification", "none", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 203, "sentences": ["Summary:", "This paper proposes to impute multimodal data when certain modalities are present.", "The authors present a variational selective autoencoder model that learns only from partially-observed data.", "VSAE is capable of learning the joint distribution of observed and unobserved modalities as well as the imputation mask, resulting in a model that is suitable for various down-stream tasks including data generation and imputation .", "The authors evaluate on both synthetic high-dimensional and challenging low-dimensional multimodal datasets and show improvement over the state-of-the-art data imputation models.", "Strengths:", "- This is an interesting paper that is well written and motivated.", "- The authors show good results on several multimodal datasets, improving upon several recent works in learning from missing multimodal data.", "Weaknesses:", "- How multimodal are the datasets provided by UCI? It seems like they consist of different tabular datasets with numerical or categorical variables, but it was not clear what the modalities are (each variable is a modality?) and how correlated the modalities are. If they are not correlated at all and share no joint information I'm not sure how these experiments can represent multimodal data.", "- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.", "They should consider larger-scale datasets including image and text-based like VQA/VCR, or video-based like the datasets in (Tsai et al., ICLR 2019).", "- In terms of prediction performance, the authors should also compare to [1] and [2] which either predict the other modalities completely during training or use tensor-based methods to learn from noisy or missing time-series data.", "- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?", "[1] Pham et al. Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities, AAAI 2019", "[2] Liang et al. Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization, ACL 2019", "### Post rebuttal # ##", "Thank you for your detailed answers to my questions."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_experiment", "arg-request_edit", "none", "none", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 204, "sentences": ["Overview:", "This paper proposes modifications to the original Differentiable Neural Computer architecture in three ways.", "First by introducing a masked content-based addressing which dynamically induces a key-value separation .", "Second, by modifying the de-allocation system by also multiplying the memory contents by a retention vector before an update .", "Finally, the authors propose a modification in the link distribution, through renormalization.", "They provide some theoretical motivation and empirical evidence that it helps avoiding memory aliasing.", "The authors test their approach in the some algorithm task from the DNC paper (Copy, Associative Recall and Key-Value Retrieval), and also in the bAbi dataset.", "Strengths: Overall I think the paper is well-written, and proposes simple adaptions to the DNC architecture which are theoretically grounded and could be effective for improving general performance.", "Although the experimental results seem promising when comparing the modified architecture to the original DNC, in my opinion there are a few fundamental problems in the empirical session (see weakness discussion bellow).", "Weaknesses: Not all model modifications are studied in all the algorithmic tasks.", "For example, in the associative recall and key-value retrieval only DNC and DNC + masking are studied.", "For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.", "Moreover, the sparse DNC (Rae el at., 2016) is already a much better performant in this task. (mean error DNC: 16.7 \\pm 7.6, DNC-MD (this paper) 9.5 \\pm 1.6, sparse DNC 6.4 \\pm 2.5).", "Although the authors mention in the conclusion that it's future work to merge their proposed changes into the sparse DNC, it is hard to know how relevant the improvements are, knowing that there are much better baselines for this task.", "It would also be good if besides the mean error rates, they reported best runs chosen by performance on the validation task, and number of the tasks solve (with < 5% error) as it is standard in this dataset.", "Smaller Notes.", "1) In the abstract, I find the message for motivating the masking from the sentence  \"content based look-up results... which is not present in the key and need to be retrieved.\"  hard to understand by itself. When I first read the abstract, I couldn't understand what the authors wanted to communicate with it. Later in 3.1 it became clear. 2) page 3, beta in that equation is not defined", "3) First paragraph in page 5 uses definition of acronyms DNC-MS and DNC-MDS before they are defined.", "4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?", "5)In session 3.1-3.3, for completeness. I think it would be helpful to explicitly compare the equations from the original DNC paper with the new proposed ones.", "--------------", "Post rebuttal update: I think the authors have addressed my main concern points and I am updating my score accordingly."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_quote", "arg-request_experiment", "none", "none", "arg-request_result", "arg-structuring_heading", "none", "none", "arg-request_clarification", "arg-request_experiment", "arg-structuring_heading", "arg-structuring_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 205, "sentences": ["Summary:", "This paper describes a contextual encoding scheme for reconstruction of 3D pointclouds from 2D images.", "An encoder outputs the parameters of a hierarchy of reconstruction networks that can be applied in succession to map random samples on a unit sphere to the surface of the reconstructed shape.", "Strengths:", "The author's model was quite novel in my opinion.", "Deep 2D->3D is becoming a crowded space and there are many other models that encode image inputs, and many others that perform recursive or composition-based decoding.", "However, the particular link here was interesting, and I appreciate the small number of parameters resulting in solid reconstruction performance.", "While most related work was covered well, I believe the authors could have a more up-to-date list of recent work that reconstructs triangle-mesh representations from images [A-C] (especially since several of these methods has an architecture that involves encoding and subsequent compositional refinement).", "Some of the reconstructions shown in this paper are quite impressive, and the quantitative results show outperforming 2 recent methods.", "I did appreciate also the novel path-based evaluation of shape accuracy in the Appendix, although it would have been helpful to see more discussion of this in the main paper.", "Areas for improvement:", "I found that the core technical description was quite brief and would have benefited from simply more detail and space.", "You have argued that your method is sensible to try (cog. sci motivations), and shown that one instance works, but what can we expect in a more mathematical or general sense? Can any sizes of encoder and mapping network fit together? How does the number of mapping layers effect performance? Won't we eventually expect vanishing/exploding gradients with particular activation and can one address this in some way?", "I note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including IOU, F1 score and CD and typically repeating these at a variety of resolutions or on additional datasets or category splits etc.", "Decision:", "Weak reject because the idea is quite interesting, but I believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.", "Additional citations suggested:", "[A] Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. Wang, Zhang, Li, Fu, Liu and Jiang. ECCV 2018.", "[B] MeshCNN: A Network with an Edge. Hanocka, Hertz, Fish, Giryes, Fleishman and Cohen-Or. SIGGRAPH 2019.", "[C] GEOMetrics: Exploiting Structure for Graph-Encoded Objects. Smith, Fujimoto, Romero and Meger. ICML 2019."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-request_edit", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_experiment", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 206, "sentences": ["The paper proposes a recurrent knowledge graph (bipartite graph between entities and location nodes) construction & updating mechanism for entity state tracking datasets such as (two) ProPara tasks and Recipes.", "The model goes through the following three steps: 1) it reads a sentence at each time step t and identifies the location of each entity via machine reading comprehension model such as DrQA (entities are predefined).", "2) Co-reference module adjusts relationship scores (soft adjacency matrix) among nodes, including possibly new nodes introduced by the MRC model.", "3) to propagate the relational information across all the nodes, the model performs L layers of LSTM for each entity that attend on other nodes via attention (where the weights come from the adjacency matrix).", "The model repeats the three steps for each sentence.", "The model is trained by directly supervising for the correct span by the MRC model at each time step, which is possible because the data provides strong supervision for each sentence (not just the answer at the end).", "The model achieves the state of the art in the two tasks of ProPara and Recipes dataset.", "Strengths: The paper provides an elegant solution for tracking relationship between entities as time (sentence) progresses.", "I also agree with the authors that this line of work (dynamic KG construction and modification) is an important area of research.", "While the model shares a similar spirit to EntNet, I think the model has enough distinctions / contributions, especially given that it outperforms EntNet by a large margin.", "The model also obtains non-trivial improvement over previous SOTA models.", "Weaknesses: Paper could have been written better. I had hard time understanding it.", "The notations are overall confusing and not explained well.", "Also there are a few unclear parts which I discuss in questions below.", "Questions:", "1. Are e_{i,t} and lambda_{i,t} vectors? Scalars? Abstract node notations? It is not clear in the model section.", "Also, it took me a long time to figure out that \u2018i\u2019 is used to index each entity (it is mentioned later).", "2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context.", "What happens if there are multiple mentions in the text? Which one does it look at?", "3. For the LSTM in the graph update, why does it have only one input? Shouldn\u2019t it have two inputs, one for previous hidden state and the other for input?", "4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k.", "This is great, but could you also report the number when the full dataset is used?", "5. What does it mean that in training time the model \u201cupdates\u201d the location node representation with the encoding of correct span. Do you mean you use the encoding instead?", "6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score?", "Is it the threshold that maximizes F1?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_clarification", "none", "arg-structuring_quote", "arg-request_explanation", "arg-request_clarification", "arg-structuring_quote", "arg-request_edit", "arg-request_explanation", "arg-request_explanation", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label", "arg-structuring_quote_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 207, "sentences": ["The paper paper proposes a mutual information maximization objective for discovering unsupervised robotic manipulation skills.", "The paper assumes that the state space can be divided into two parts - the state of the robot (\u201ccontext states\u201d) which is controllable via actions and the state of an object (\u201cstates of interest\u201d) which must be manipulated by the robot.", "Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully.", "I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an \u201cobject\u201d or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.", "My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.", "The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.", "It doesn\u2019t seem like a surprising discovery that maximizing the mutual information between the robot state and object state will lead to skills that actually make the robot move the object.", "Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.", "Can the authors elaborate on why this choice should intuitively be better than the proposed method alone?", "The paper does not talk about how these skills can be used as primitive actions by a higher level controller (in a hierarchical RL setup), which would help in demonstrating the usefulness of these skills - e.g.: are these skills sequentially composable such that they can solve a complex task?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 208, "sentences": ["The paper proposes to learn task-level modules progressively to perform the task of VQA.", "Such task-level modules include object/attribute prediction, image captioning, relationship detection, object counting, and finally VQA model.", "The benefit of using modules for reasoning allows one to visualize the reasoning process more easily to understand the model better.", "The results are mainly shown on VQA 2.0 set, with a good amount of analysis.", "- I think overall this is a good paper, with clear organization, detailed description of the approach, solid analysis of the approach and cool visualization.", "I especially appreciate that analysis is done taking into consideration of extra computation cost of the large model; the extra data used for visual relationship detection.", "I do not have major comments about the paper itself, although I did not check the technical details super carefully.", "- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.", "- I am in general curious to see if it will be beneficial to fine-tune the modules themselves can further improve performance. It maybe hard to do it entirely end-to-end, but maybe it is fine to fine-tune just a few top layers (like what Jiang et al did)?", "- One great benefit of having a module-based model is feed in the *ground truth* output for some of the modules.", "For example, what benefit we can get if we have perfect object detection? Where can we get if we have perfect relationships?", "This can help us not only better understand the models, but also the dataset (VQA) and the task in general."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 209, "sentences": ["This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints.", "The tunability of the optimizer is a weighted sum of best performance at a given budget.", "The authors found that in a setting with low budget for hyperparameter tuning, tuning only Adam optimizer\u2019s learning rate is likely to be a very good choice; it doesn\u2019t guarantee the best possible performance, but it is evidently the easiest to find well-performing hyperparameter configurations.", "Comments:", "The paper is easy to follow.", "The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons: In section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of \u201csharpness\u201d of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums. However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions.", "Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails. In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets.", "The definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets?", "The authors seems interchangeably using \u201cruns\u201d and \u201citerations\u201d, which makes the concept more confusable.", "The authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO.", "My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly. For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL. My major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge.", "A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost.", "My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true. Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same. This could significantly increase the tunability of SGDM.", "Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM. The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD.", "The author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained.", "Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?", "[1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451", "[2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489", "[3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350", "[4] Rethinking the Hyperparameters for Fine-tuning", "https://openreview.net/forum?id=B1g8VkHFPH"], "labels": ["arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-request_explanation", "none", "none", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 210, "sentences": ["The authors introduce strategies for pre-training graph neural networks.", "Pre-training is done at the node level as well as at the graph level.", "They evaluate their approaches on two domains, biology and chemistry on a number of downstream tasks.", "They find that not all pre-training strategies work well and can in fact lead to negative transfer.", "However, they find that pre-training in general helps over non pre-training.", "Overall, this paper was well written with useful illustrations and clear motivations.", "The authors evaluate their models over a number of datasets.", "Experimental construction and analysis also seems sound.", "I would have liked to see a bit more analysis as to why some pre-training strategies work over others.", "However, the authors mention that this is in their planned future work.", "Also, in figure 4, the authors mention that their pre-trained models tend to converge faster.", "However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_result"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 211, "sentences": ["This paper proposes a new task/dataset for language-based abductive reasoning in narrative texts.", "Pros:", "-\tThe proposed task is interesting and well motivated.", "The paper contributes a dataset (20,000 commonsense narratives and 200,000 explanatory hypotheses).", "The construction of the dataset was performed carefully (e.g., avoiding annotation artifacts).", "-\tThe paper established many reasonable baselines.", "-\tThe paper conducted detailed analysis, which invites more research on this task: despite the strong performance of many existing systems on NLI/RTE, there are larger gaps between the performance of these models and human performance on the proposed task.", "The experiments well support the conclusions made in the paper.", "-\tThe paper is well structured and easy to follow. It is well written.", "Cons/comments:", "-\tWhile this is a new and interesting task, the contribution (as discussed above in \u201cpros\u201d above) is somewhat limited.", "I also suggest the paper discusses e-SNLI a bit more.", "-\tThe paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.", "-\tShould the title of the paper specify the paper is about \u201clanguage-based\u201d abductive reasoning.", "- A minor one: \u201cTable 7 reports results on the \u03b1NLI task.\u201d Should it be \u201cTable 2\u201d?"], "labels": ["arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "none", "arg-request_edit", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "arg-request_typo_label"]}
{"abstract_id": 212, "sentences": ["The paper proposes a neural-network-based estimation of mutual information, following the earlier line of work in [A].", "The main focus has been to develop an estimator that can reliably work with small dataset sizes.", "They first reduce the sample complexity of estimating mutual information by decoupling the network learning problem and the estimation problem by creating a training and validation set and then using the validation set for estimating mutual information.", "Of course, there is still the problem of learning the network with smaller sized data.", "For this, they propose the strategy of creating multiple tasks from the same dataset, where the dataset is run through transformations that do not affect mutual information.", "I am inclined to accept (weak) the paper for the following reasons:", "1. The paper uses some nice ideas to reduce the variance of the MI estimates and to be able to work with smaller dataset sizes.", "Both splitting data into training and validation and then using task augmentation to make learning robust are pretty nice ideas.", "2. The results on the synthetic datasets show that the resulting estimator does have low variance and the estimates are less than or equal to the true MI value, which is consistent with the fact that it is a lower bound estimation.", "3. The results on fMRI dataset were interesting and showed that the method gives improvements over baselines were the estimates were made on a smaller sized dataset.", "Some improvement suggestions:", "1. I don't see why MINE cannot be applied to the fMRI dataset and results be reported.", "I know that the variance in estimation is large, but it would still be useful to look at the performance of MINE in comparison to DEMINE.", "2. There are several errors in the writing - hyperparamters in Abstract, repetition of the word \"Section\" in fMRI experiment section etc. - which needs to be fixed.", "[A] Mutual Information Neural Estimation, ICML 2018"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "none", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 213, "sentences": ["Summary:", "The role of auxiliary tasks is to improve the generalization performance of the principal task of interest.", "So far, hand-crafted auxiliary tasks are generated, tailored for a problem of interest.", "The current work addresses a meta-learning approach to automatically generate auxiliary tasks suited to the principal task, without human knowledge.", "The key components of the method are: (1) meta-generator; (2) multi-task evaluator.", "These two models are trained using the gradient-based meta-learning technique (for instance, MAML).", "The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.", "Strengths:", "- To my best knowledge, the idea of applying the meta-learning to the automatic generation of auxiliary tasks is novel.", "- The paper is well written and easy to read.", "- The method nicely blends a few components such as self-supervised learning, meta-learning, auxiliary tasks into a single model to tackle the meta auxiliary learning.", "Weakness:", "- The performance gain is not substantial in experiments.", "I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks. You can refer to the state-of-the-arts performance on CIFAR.", "- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 214, "sentences": ["The paper is well written and easy to follow. The topic is apt.", "I don\u2019t have any comments except the following ones.", "Lemma 2.4, Point 1: The proof is confusing.", "Consider the one variable vector case.", "Assuming that there is only one variable w, then \\nabla L(w) is not perpendicular to w in general.", "The Rayleigh quotient example L(w)  = w\u2019*A*w/ (w\u2019*w) for a symmetric matrix A, then \\nabla L(w) = (2/w\u2019*w)(Aw - L(w)*w), which is not perpendicular to w.", "Even if we constrain ||w ||_2 = 1 , then also  \\nabla L(w)  is not perpendicular to w.", "Am I missing something?", "What is G_t in Theorem 2.5. It should be defined in the theorem itself.", "There is another symbol G_g which is a constant."], "labels": ["none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_clarification", "arg-request_edit", "arg-structuring_quote"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 215, "sentences": ["The present paper proposes a fast approximation to the softmax computation when the number of classes is very large.", "This is typically a bottleneck in deep learning architectures.", "The approximation is a sparse two-layer mixture of experts.", "The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.", "See a list of typos below.", "An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.", "Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.", "Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.", "How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?", "The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?", "The column \"FLOPS\" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.", "Also, a \"1x\" label seems to be missing in for the full softmax, so that the reference is clearly specified.", "All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.", "A brief list of typos:", "\"Sparse Mixture of Sparse of Sparse Experts\"", "\"if we only search right answer\"", "\"it might also like appear\"", "\"which is to design to choose the right\"", "sparsly", "\"will only consists partial\"", "\"with \u03b3 is a lasso threshold\"", "\"an arbitrarily distance function\"", "\"each 10 sub classes are belonged to one\"", "\"is also needed to tune to achieve\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 216, "sentences": ["The paper proposes two additional steps to improve the compression of weights in deep neural networks.", "The first is to quantize the weights after pruning, and the second is to further encode the quantized weights.", "There are several weaknesses in this paper.", "The first one is clarity.", "The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.", "The paper can be made more mathematically precise.", "The input and output types of each block in Figure 1. should be clearly stated.", "For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.", "Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.", "The figures are almost useless, because the captions contain very little information.", "For example, the authors should at least say that the \"D\" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.", "Many more can be said in all the figures.", "The second weakness is experimental design.", "There are two conflicting qualities that need to be optimized--performance and compression rate.", "When optimizing the compression rate, it is important not to look at the test set error.", "If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.", "The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%.", "Optimizing compression rates should be done on the training set with a separate development set.", "The test set should not used before the best compression scheme is selected.", "Both the results on the development set and on the test set should be reported for the validity of the experiments.", "I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.", "Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 217, "sentences": ["In this paper, the authors proposed a multi-domain adversarial learning approach, MULANN, to improve the classification accuracy on three datasets-DIGITS, OFFICE and CELL-in the semi-supervised DA setting.", "It\u2019s contributions include: i) using the H-divergence to bound both the risk across all domains and the worst-domain risk (imbalance on a specific domain); ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state-of-the-art on two standard image benchmarks, and a novel bioimage dataset, CELL.", "In addition, this paper has a clear logic to explain and prove the problem to be solved, and has ample experimental evidence.", "Above on, this paper did a meaningful work.", "But there are some errors of expression, so it should be checked."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 218, "sentences": ["This work presents Backpropamine, a neuromodulated plastic LSTM training regime.", "It extends previous research on differentiable Hebbian plasticity by introducing a neuromodulatory term to help gate information into the Hebbian synapse.", "The neuromodulatory term is placed under network control, allowing it to be time varying (and hence to be sensitive to the input, for example).", "Another variant proposes updating the Hebbian synapse with modulated exponential average of the Hebbian product.", "This average is linked to the notion of an eligibility trace, and ties into some recent biological work that shows the role of dopamine in retroactively modulating synaptic plasticity.", "Overall the work is nicely motivated and clearly presented.", "There are some interesting ties to biological work -- in particular, to retroactive plasticity phenomena.", "There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.", "The authors test their model on three tasks: cue-award association, maze learning, and Penn Treebank (PTB).", "In the cue-award association task the retroactive and simple modulation networks perform well, while the non-modulated and non-plastics fail.", "For the maze navigation task the modulated networks perform better than the non-modulated networks, though the effect is less pronounced.", "Finally, on PTB the authors report improvements over baseline LSTMs.", "One of the main claims of this paper is that neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task\u201d, and that therefore \u201cdifferentiable neuromodulation of plasticity offers a powerful new framework for training neural networks\u201d.", "This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.", "The authors cite such models in the appendix (Melor et al), but claim that \u201cmuch larger models\u201d are needed, potentially with other mechanisms, such as dropout.", "Though this may be true, these models still undermine the claim that \u201cneuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task\u201d.", "This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.", "Also, I am left wondering what are considered the parameters of the models -- are only the neuromodulatory terms considered as the additional trainable parameters compared to baseline LSTMs? How are the Hebbian synapses themselves considered in this calculation?", "If the Hebbian synapses are not considered, then the authors need a control with matched memory-capacities to account for the extra capacity afforded by the Hebbian synapses.", "Given the ties between Hebbian synapses and attention (see Ba et al), an important control here could be an LSTM with Bahdanau (2014) style attention.", "Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.", "Overall, the ideas presented in the paper are intriguing, and further research down this line is encouraged.", "However, in its current state the work lacks sufficiently strong baselines to support the paper\u2019s claims; thus, the merits of this approach cannot yet be properly assessed."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_quote", "none", "arg-structuring_quote", "none", "none", "arg-request_explanation", "arg-request_edit", "arg-request_experiment", "arg-request_edit", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_quote_label", "none_label", "arg-structuring_quote_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_edit_label", "none_label", "none_label"]}
{"abstract_id": 219, "sentences": ["This paper studies \"Noisy Information Bottlenecks\".", "The overall idea is that, if the mutual information between learned parameters and the data is limited, then this prevents overfitting.", "It proposes to create a \"bottleneck\" to limit the mutual information.", "Specifically, the bottleneck is created by having the data depend on a noisy version of the parameters, rather than the true parameters and invoking the information processing inequality.", "The paper gives an example of Gaussian mean field inference.", "Ultimately, the analysis boils down to looking at a signal-to-noise ratio of the algorithm, which looks very much like regularization.", "I think this is a very interesting direction, but the present paper is somewhat unclear.", "In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have \"training algorithms that are exactly equivalent.\" I think this example needs to be clarified.", "Many of the parameters here are also unclear and not properly defined/introduced.", "What is the relationship between $\\theta$ and $\\tilde\\theta$ exactly?", "In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?", "The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.", "This paper is giving an information-theoretic perspective on existing variational inference methods.", "Such a perspective is interesting, but needs to be further developed and explained.", "Specifically, how can mutual information in this context be formally linked to generalization/overfitting?", "Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.", "As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.", "Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_clarification", "none", "arg-request_clarification", "arg-request_explanation", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_explanation", "none", "arg-request_explanation", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label"]}
{"abstract_id": 220, "sentences": ["This paper proposed another variant of Langevin dynamics, called \u201cStein self-repulsive dynamics,\u201d which simultaneously decreases the auto-correlation of Langevin dynamics and eliminates the need for running parallel chains in SVGD.", "They combined Langevin dynamics with Stein variational gradient descent and theoretically justified that the proposed method successfully converges to the stationary distribution with only a single chain, unlike SVGD.", "The proposed method decreases the auto-correlation of Langevin dynamics, so the proposed method increases the sample efficiency.", "The paper is well-written.", "The idea of the proposed method is natural, which is incorporating the functionality of SVGD to reduce the auto-correlation of Langevin dynamics.", "The idea is intuitive and justified by their theoretical analysis.", "The authors also well- placed their work in the literature, as described in Section 3.", "The intuitive explanation of the proposed method is given in Section 3.", "I have one technical question as follows. If the authors reply appropriately, I will raise the score to accept.", "In Theorem 4.3 , the result holds for any k and M. The authors claim that if we take a limit of M -> \u221e with fixed k, the practical dynamics converges to the discrete-time mean-field limit, in Section 4.", "However, to state the result of Theorem 4.3, k should be bigger than M c_\\eta from the dentition of \\tilde{\\rho}_k^M, as shown under the equation (4).", "How do we take a limit of M -> \u221e ? Does k also go \u221e?", "Minor comments:", "- The definition of g should depend on only \\theta_k^I and \\hat{\\delta}_k^M, not \\theta_k^k.", "- The equation (1) should hold for any \\theta\u2019, not \\theta.", "- The equation (1) should contain \\rho, not p."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 221, "sentences": ["This paper uses constrained Markov decision processes to solve a multi-objective problem that aims to find the correct trade-off between cost and return in continuous control.", "The main technique is Lagrangian relaxation and experiments are focus on cart-pole and locomotion task.", "Comments:", "1) How to solve the constrained problem (8) is unclear. It is prefer to provide detailed description or pseudocode for this step.", "2) In equation (8), lambda is a trade-off between cost and return.", "Optimization on lambda reduces burdensome hyperparameter selection, but a new hyperparameter beta is introduced.", "How do we choose a proper beta, and will the algorithm be sensitive to beta?", "3) The paper only conducts comparison experiments with fixed-alpha baselines. The topic is similar to safe reinforcement learning. Including the comparison with safe reinforcement learning algorithms is more convincing."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 222, "sentences": ["The paper proposed a novel image classifier comparison approach that went beyond one fixed testing set for all.", "Instead, for a pair of classifiers to be compared, it advocated to sample their \"most disagreed\" test set from a large corpus of unlabeled images.", "The level of disagreement was measured by a semantic-aware distance derived from WordNet ontology.", "Because of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload.", "The proposed MAD competition distinguishes classifiers by finding their respective counterexamples.", "It is therefore an \"error spotting\" mechanism, rather than a drop-in replacement of standard test accuracy.", "I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful.", "If that is true, I would suggest the authors to make this hidden assumption clearer in the paper", "The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment.", "The idea has a cross-disciplinary nature and is fairly interesting to me.", "I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works.", "One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them.", "However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_edit", "none", "none", "none", "arg-structuring_quote", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-structuring_quote_label", "none_label"]}
{"abstract_id": 223, "sentences": ["The paper examines an architectural feature in GAN generators -- self-modulation -- and presents empirical evidence supporting the claim that it helps improve modeling performance.", "The self-modulation mechanism itself is implemented via FiLM layers applied to all convolutional blocks in the generator and whose scaling and shifting parameters are predicted as a function of the noise vector z.", "Performance is measured in terms of Fr\u00e9chet Inception Distance (FID) for models trained with and without self-modulation on a fairly comprehensive range of model architectures (DCGAN-based, ResNet-based), discriminator regularization techniques (gradient penalty, spectral normalization), and datasets (CIFAR10, CelebA-HQ, LSUN-Bedroom, ImageNet).", "The takeaway is that self-modulation is an architectural feature that helps improve modeling performance by a significant margin in most settings.", "An ablation study is also performed on the location where self-modulation is applied, showing that it is beneficial across all locations but has more impact towards the later layers of the generator.", "I am overall positive about the paper: the proposed idea is simple, but is well-explained and backed by rigorous evaluation.", "Here are the questions I would like the authors to discuss further:", "- The proposed approach is a fairly specific form of self-modulation.", "In general, I think of self-modulation as a way for the network to interact with itself, which can be a local interaction, like for squeeze-and-excitation blocks.", "In the case of this paper, the self-interaction allows the noise vector z to interact with various intermediate features across the generation process, which for me appears to be different than allowing intermediate features to interact with themselves.", "This form of noise injection at various levels of the generator is also close in spirit to what BigGAN employs, except that in the case of BigGAN different parts of the noise vector are used to influence different parts of the generator.", "Can you clarify how you view the relationship between the approaches mentioned above?", "- It\u2019s interesting to me that the ResNet architecture performs better with self-modulation in all settings, considering that one possible explanation for why self-modulation is helpful is that it allows the \u201cinformation\u201d contained in the noise vector to better propagate to and influence different parts of the generator.", "ResNets also have this ability to \u201cpropagate\u201d the noise signal more easily, but it appears that having a self-modulation mechanism on top of that is still beneficial.", "I\u2019m curious to hear the authors\u2019 thoughts in this.", "- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?"], "labels": ["none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_clarification", "none", "none", "arg-request_clarification", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_experiment_label"]}
{"abstract_id": 224, "sentences": ["This paper provide a method to produce adversarial attack using a Frank-Wolfe inspired method.", "I have some concerns about the motivation of this method:", "- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.", "- Consequently why did not you compare simple projected gradient method ?", "(BIM) is not equivalent to the projected gradient method since the direction chosen is the sign of the gradient and not the gradient itself (the first iteration is actually equivalent because we start at the center of the box but after both methods are no longer equivalent).", "- There is no motivations for the use of $\\lambda >1$ neither practical or theoretical since the results are only proven for $\\lambda =1$ whereas the experiments are done with \\lambda = 5,20 or 30.", "- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?", "Depending on the answer to these questions I'm planning to move up or down my grade.", "In the experiment there is no details on how you set the hyperparameters of CW and EAD.", "They use a penalized formulation instead of a constrained one.", "Consequently the regularization hyperparameters have to be set differently.", "The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.", "Comment:", "- in the whole paper there is $y$ which is not defined.", "I guess it is the $y_{tar}$ fixed in the problem formulation Sec 3.2.", "In don't see why there is a need to work on any $y$. If it is true", ",  case assumption 4.5 do not make any sense since $y = y_{tar}$ (we just need to note $\\|\\nabla f(O,y_{tar})\\| = C_g$) and some notation could be simplified setting for instance $f(x,y_{tar})  = f(x)$.", "- In Theorem 4.7 an expectation on g(x_a) is missing", "Minor comments:", "- Sec 3.1 theta_i -> x_i", "- Sec 3.3 the argmin is a set, then it is LMO $\\in$ argmin.", "===== After rebuttal ======", "The authors answered some of my questions but I still think it is a borderline submission."], "labels": ["none", "arg-structuring_heading", "arg-request_explanation", "arg-request_experiment", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_edit", "none", "arg-structuring_heading", "arg-request_edit", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 225, "sentences": ["This paper studies the characteristics of representations and their roles in neural network expressiveness.", "The results  are overall not very impressive.", "1. There are many characteristics of representations such as scaling, permutation, covariance, correlation, sparsity, dead units, rank.", "The papers discusses some (not surprising) theoretical properties relating to scaling, permutation, covariance, correlation, while making less efforts on the more interesting characteristics  sparsity, dead units, rank, mutual information.", "Only some heuristic results are obtained for them without rigorous theory.", "It would be better if these heuristic arguments can be formed as theorems as well.", "2. Probably the most interesting experimental finding of this paper is that the mutual information between z and output is constant, while the one between z and input strongly depends on the regularizers.", "That is, the dependence between z and y does not vary with regularizers but the one between z and x does.", "Is this a coincidence or a general phenomenon? Is there a theoretical explanation?", "3."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "arg-request_result", "arg-structuring_summary", "none", "arg-request_explanation", "arg-structuring_heading"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_clarification_label", "arg-structuring_heading_label"]}
{"abstract_id": 226, "sentences": ["This paper proposes to extend VAE-GAN from the static image generation setting to the video generation setting.", "It\u2019s a well-written, simple paper that capitalizes on the trade-off between model realism and diversity, and the fact that VAEs and GANs (at least empirically) tend to lie on different sides of this spectrum.", "The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.", "However, the effort to implement it successfully is commendable and will, I think, serve as a good reference for future work on video prediction.", "There are also several interesting design choices that I think are worth of further exposition.", "Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?", "Please provide a response to these questions.", "If the authors have any ablation studies to back up their design choices, that would also be much appreciated, and will make this a more valuable paper for readers.", "I think Figure 5 is the most interesting figure in the paper.", "I would imagine that playing with the hyperparameters would allow one to traverse the trade-off between realism and diversity.", "I think having such a curve will help sell the paper as giving the practitioner the freedom to select their own preferred trade-off.", "I don\u2019t understand the claim that \u201cGANs prioritize matching joint distributions of pixels over per-pixel reconstruction\u201d and its implication that VAEs do not prioritize joint distribution matching.", "VAEs prioritize matching joint distributions of pixels and latent space: min KL(q(z, x) || p(z, x)) and is a variational approximation of the problem min KL(q(x) || p(x)), where q(x) is the data distribution.", "The explanation provided by the authors is thus not sufficiently precise and I recommend the retraction of this claim.", "Pros:", "+ Well-written", "+ Natural extension of VAE-GANs to video prediction setting", "+ Establishes a good baseline for future video prediction work", "Cons:", "- Limited novelty", "- Limited analysis of model/architecture design choices"], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-request_clarification", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 227, "sentences": ["- Summary", "This paper proposes a neural architecture search method based on a direct sparse optimization, where the proposed method provides a novel model pruning view to the neural architecture search problem.", "Specifically, the proposed method introduces scaling factors to connections between operations, and impose sparse regularizations to prune useless connections in the network.", "The proposed method is evaluated on CIFAR-10 and ImageNet dataset.", "- Pros", "- The proposed method shows competitive or better performance than existing neural architecture search methods.", "- The experiments are conducted thoroughly in the CIFAR-10 and ImageNet.", "The selection of the datasets is appropriate.", "Also, the selection of the methods to be compared is appropriate.", "- The effect of each proposed technique is appropriately evaluated.", "- Cons", "- The search space of the proposed method, such as the number of operations in the convolution block, is limited.", "- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.", "- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.", "Overall, if we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 228, "sentences": ["The paper investigates the Frank-Wolfe (FW) algorithm for constructing adversarial examples both in a white-box and black-box setting.", "The authors provide both a theoretical analysis (convergence to a stationary point) and experiments for an InceptionV3 network on ImageNet.", "The main claim is that the proposed algorithm can construct adversarial examples faster than various baselines (PGD, I-FGSM, CW, etc.), and from fewer queries in a black-box setting.", "The FW algorithm is a classical method in optimization, but (to the best of my knowledge) has not yet been evaluated yet for constructing adversarial examples.", "Hence it is a natural question to understand whether FW performs significantly better than current algorithms in this context.", "Indeed, the authors find that FW is 6x - 20x faster for constructing white-box adversarial examples than a range of relevant baseline, which is a significant speed-up.", "However, there are several points about the experiments that are unclear to me:", "- It is well known that the running times of optimization algorithms are highly dependent on various hyperparameters such as the step size.", "But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.", "Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.", "- Other algorithms in the comparison achieve a better distortion (smaller perturbation).", "Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.", "Instead of reporting a single time-vs-distortion data point, the authors could show the full trade-off curve.", "- The authors only provide running times, not the number of iterations.", "In principle all the algorithms should have a similar bottleneck in each iteration (computing a gradient for the input image), but it would be good to verify this with an iteration count vs success rate (or distortion) plot.", "This would also allow the authors to compare their theoretical iteration bound with experimental data.", "In addition to these three main points, the authors could strengthen their results by providing experiments on another dataset (e.g., CIFAR-10) or model architecture (e.g., a ResNet), and by averaging over a larger number of test data points (currently 200).", "Overall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.", "Additional comments:", "The introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:", "* The abstract claims that the poor time complexity of adversarial attacks limits their practical usefulness.", "However, the running time of attacks is typically measured in seconds and should not be the limiting element in real-world attacks on deep learning systems.", "I am not aware of a setting where the running time of an attack is the main computational bottleneck (outside adversarial training).", "* The introduction distinguishes between \"gradient-based methods\" and \"optimization-based methods\".", "This distinction is potentially confusing to a reader since the gradient-based methods can be seen as optimization algorithms, and the optimization-based methods rely on gradients.", "* The introduction claims that black-box attacks need to estimate gradients coordinate-wise.", "However, this is not the case already in some of the prior work that uses random directions for estimating gradients (e.g., the cited paper by Ilyas et al.)", "I encourage the authors to clarify these points in an updated version of their paper."], "labels": ["none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_clarification_label"]}
{"abstract_id": 229, "sentences": ["The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.", "There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly.", "However, the paper does cover a setup that I am not aware that was studied before.", "The paper is written clearly, and the experiments seem solid.", "Comments:", "-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly.", "-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)", "-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization"], "labels": ["arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 230, "sentences": ["Following the suggested rubric:", "1. Briefly establish your personal expertise in the field of the paper.", "2. Concisely summarize the contributions of the paper.", "3. Evaluate the quality and composition of the work.", "4. Place the work in context of prior work, and evaluate this work's novelty.", "5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.", "6. Provide a summary judgment if the work is significant and of interest to the community.", "1.  I work at the intersection of machine learning and biological vision", "and have worked on modeling word representations.", "2. This paper develops a new representation system for object representations from training on data collected from odd-one-out human judgements of images.", "The vector representation for objects is designed to be sparse and low dimensional (and ends up being about 49D) .", "Similarity is measured by dot products in the space and probabilities of which pair of items will be paired are modeled as the exponential of the similarity.", "3,5", "The resulting embedding\tdoes a good job\tof predicting human similarity judgements and seems to cover similar features to those named by humans.", "They also explain typicality judgements and cluster semantic categories well.", "The creation of the upper limit based on noise between and within subjects was a nice addition.", "4. Some relevant related work is discussed and this seems like a novel and interesting contribution.", "The authors might also want to compare to similar work that looked at similarities among triplets (Similarity Comparisons for Interactive Fine-Grained Categorization http://ttic.uchicago.edu/~smaji/papers/similarity-cvpr14.pdf; Conditional Similarity Networks https://arxiv.org/abs/1603.07810 ).", "6. While this paper is not especially surprising or ground breaking, the number and quality of the comparisons make it a worthwhile contribution and the resulting embeddings are worth further exploration and could be very useful for future research."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 231, "sentences": ["This manuscript studies mutual-information estimation, in particular variational lower bounds, and focuses on reducing their sample complexity.", "The first contribution is based on adapting the MINE energy-based MI estimator family to out-of-sample testing.", "MINE involves fitting a very flexible parametric form of the distribution, such as a neural network, to the data to derive a mutual information lower bound.", "The present work separates the data fitting from the mutual information evaluation to decrease sample complexity, the argument being that the function class is no longer a limiting factor to sample complexity of the mutual information estimation.", "The second contribution uses meta learning to decrease the sample complexity required to fit the neural network, creating a family of tasks derived from the data with data transformation that do not modify the mutual information.", "The approaches are demonstrated on synthetic data as well as fMRI data, to detect significant inter-subject dependencies in time-series of neural responses.", "There are some very interesting and strong contributions of this manuscript.", "However, I worry that one of the central theoretical arguments does not seem correct to me.", "Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.", "Th 1 gives the number of validation samples required to bound error between the mutual information estimate at finite samples and asymptotically for a function T parametrized by \\tilda{theta}. This control is of a very different nature from the control established by MIME which controls the error to the actual best possible variational bound.", "In other terms, the control of th 1 does not control the estimation error of T. This is the reason why it is independent from the function class.", "The total error in estimating the mutual information must take this error in account, and not only the validation error.", "Hence the theoretical sample complexities contributed are not comparable to those of MIME.", "The meta-learning estimator seems to involve a significant implementation complexity, for instance heuristic switchs between estimation approaches.", "The danger is that could hard to make reliable in a wide set of applications.", "It would be more convincing to see more experiments.", "Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.", "On the other hand, the MIME-f-ES tends to have a reasonably good failure mode.", "I would have been interested in \"false detection\" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.", "This is particularly important when the application is to test for independence, as in the fMRI experiments.", "For hyper-parameter search (using hyperopt), the manuscript should make it explicit what metric is optimized.", "Is it the data fit of the neural networks? With what specific measure?", "With regards to the fMRI experiments, good baselines are missing: DEMINE is compared to Pearson correlation.", "Additionally, CNNs are not a particularly good architecture for fMRI, as fMRI is not locally translation invariant (see Ayd\u00f6re ICML 2019 for instance).", "Finally, it seems that the goal here is to test for independence.", "In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.", "Minor comments:", "Alg 1 seems a fairly generic neural-network fitting algorithm.", "In its current state, I am not sure that it adds a lot to the manuscript.", "There are many acronyms that are never defined: MINE, TCPC"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_experiment", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 232, "sentences": ["The paper proposes an approach to adapt hyperparameters online.", "When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.", "A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.", "Thus, it is hard to say whether the results are applicable in practice.", "B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.", "C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.", "This hyperparameter itself benefits from (requires?) some scheduling.", "It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.", "Online tuning of  hyperparameters is an important functionality and I hope your paper will make it more straightforward to use it in practice.", "* Minor notes:", "You mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it. I guess you mean w.r.t. the original RMSprop."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 233, "sentences": ["The authors seek to make it practical to use the full-matrix version of Adagrad\u2019s adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-\u00bd) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix).", "This is a really nice trick.", "I\u2019m glad to see that the authors considered adding momentum (to adapt ADAM to this setting), and their experiments show a convincing benefit in terms of performance *per iteration*. Interestingly, they also show that the models found by their method also don\u2019t generalize poorly, which is noteworthy and slightly surprising.", "However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.", "In Appendix B.1, they report mixed results in terms of wall-clock time, and I strongly feel that these results should be in the main body of the paper.", "One would *expect* the proposed approach to work better than diagonal preconditioning on a per-iteration basis (at least in terms of training loss).", "A reader\u2019s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.", "Finally, the proposed approach seems to sort of straddle the line between traditional convex optimization algorithms, and the fast stochastic algorithms favored in machine learning.", "In particular, I think that the proposed algorithm has a more-than-superficial resemblance to stochastic LBFGS: the main difference is that LBFGS approximates the inverse Hessian, instead of (GG^T)^(-\u00bd).", "It would be interesting to see how these two algorithms stack up.", "Overall, I think that this is an elegant idea and I\u2019m convinced that it\u2019s a good algorithm, at least on a per-iteration basis.", "However, it trades-off computational cost for progress-per-iteration, so I think that an explicit analysis of this trade-off (beyond what\u2019s in Appendix B.1) must be in the main body of the paper."], "labels": ["arg-structuring_summary", "none", "none", "none", "arg-request_edit", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 234, "sentences": ["Summary: the paper purposes a dataset of abductive language inference and generation.", "The dataset is generated by human, while the testing set is adversarially selected using BERT.", "The paper experiments the popular deep learning models on the dataset and observe shortcoming of deep learning on this task.", "Comments: overall, the problem on abductive inference and abductive generation in language in very interesting and important.", "This dataset seems valuable. And the paper is simple and well-written.", "Concerns: I find the claim on deep networks kind of irresponsible.", "1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.", "2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.", "To compare the author should use the average score of human.", "3. The ground truth is selected by human.", "On a high level, the main difficulty of abduction is to search in the exponentially large space of hypothesis.", "Formulating the abduction task as a (binary) classification problem is less interesting.", "The generative task is a better option.", "Decision: despite the seeming unfair comparison, this task is novel. I vote for weak accept."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 235, "sentences": ["The paper proposes a novel training method for variational autoencoders that allows using partially-observed data with multiple modalities.", "A modality can be a whole block of features (e.g., a MNIST image) or just a single scalar feature.", "The probabilistic model contains a latent vector per modality.", "The key idea is to use two types of encoder networks: a unimodal encoder for every modality which is used when the modality is observed, and a shared multimodal encoder that is provided all the observed modalities and produces the latent vectors for the unobserved modalities.", "The whole latent vector is passed through a decoder that predicts the mask of observed modalities, and another decoder that predicts the actual values of all modalities.", "The \u201cground truth\u201d values for the unobserved modalities are provided by sampling from the corresponding latent variables from the prior distribution once at some point of training.", "While I like the premise of the paper, I feel that it needs more work.", "My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic \u201cground truth\u201d for these modalities, which in turn means that the model would produce underconfident predictions for them.", "The samples from MNIST in Figure 3 are indeed very blurry, supporting this.", "Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.", "I believe that the paper should currently be rejected, but I encourage the authors to revise the paper.", "Pros:", "* Generative modelling of partially observed data is a very important topic that would benefit from fresh ideas and new approaches", "* I really like the idea of explicitly modelling the mask/missingness vector. I agree with the authors that this should help a lot with non completely random missingness.", "Cons:", "* The text is quite hard to read.", "There are many typos (see below).", "The text is over the 8 page limit, but I don\u2019t think this is justified.", "For example, the paragraph around Eqn. (11) just says that the decoder takes in a concatenated latent vector.", "The MNIST+SVHN dataset setup is described in detail, yet there is no summary of the experimental results, which are presented in the appendix.", "* The approach taken to train on partially-observed data is described in three sentences after the Eqn. (10).", "The non-observed dimensions are imputed by reconstructions from the prior from a partially trained model.", "I think that this is the crux of the paper that should be significantly expanded and experimentally validated.", "It is possible that due to this design choice the method would not produce sharper reconstructions than the original samples from the prior.", "Figures 3, 5 and 6 indeed show very blurry samples from the model.", "Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.", "* The paper states multiple times that VAEAC [Ivanov et al., 2019] cannot handle partially missing data, but I don\u2019t think this is true, since their missing features imputation experiment uses the setup of 50% truly missing features.", "The trick they use is adding \u201csynthetic\u201d missing features in addition to the real ones and only train on those.", "See Section 4.3.3 of that paper for more details.", "* The paper states that \u201cit can model the joint distribution of the data and the mask together and avoid limiting assumptions such as MCAR\u201d.", "However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.", "* The baselines in the experiments could be improved.", "First of all, the setup for the AE and VAE is not specified.", "Secondly, it would be good to include a GAN-based baseline such as GAIN, as well as some more classic feature imputation method, e.g. MICE or MissForest.", "* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.", "Questions to the authors:", "1. Could you comment on the differences in your setup in Section 4.1 compared to the VAEAC paper? I\u2019ve noticed that the results you report for this method significantly differ from the original paper, e.g. for VAEAC on Phishing dataset you report PFC of 0.24, whereas the original paper reports 0.394; for Mushroom it\u2019s 0.403 vs. 0.244. I\u2019ve compared the experimental details yet couldn\u2019t find any differences, for example the missing rate is 0.5 in both papers.", "2. How do you explain that all methods have NRMSE > 1 on the Glass dataset (Table 1), meaning that they all most likely perform worse than a constant baseline?", "Typos and minor comments:", "* Contributions (1) and (2) should be merged together.", "* Page 2: to literature -> to the literature", "* Page 2: \u201cThis algorithm needs complete data during training cannot learn from partially-observed data only.\u201d", "* Equations (1, 2): z and \\phi are not consistently boldfaced", "* Equations (4, 5): you can save some space by only specifying the factorization (left column) and merging the two equations on one row", "* Page 4, bottom: use Bernoulli distribution -> use factorized/independent Bernoulli distribution", "* Page 5, bottom: the word \u201csimply\u201d is used twice", "* Page 9: learn to useful -> learn useful", "* Page 9: term is included -> term included", "* Page 9: variable follows Bernoulli -> variable following Bernoulli", "* Page 9: conditions on -> conditioning on"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_quote", "none", "arg-structuring_quote", "arg-structuring_summary", "arg-request_experiment", "none", "arg-structuring_quote", "none", "none", "none", "none", "arg-structuring_quote", "none", "arg-request_experiment", "none", "arg-request_experiment", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_clarification", "arg-structuring_heading", "arg-request_edit", "arg-request_typo", "arg-request_edit", "none", "arg-request_edit", "arg-request_edit", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_quote_label", "none_label", "arg-structuring_quote_label", "arg-structuring_summary_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_quote_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 236, "sentences": ["In the manuscript entitled \"Neural Causal Discovery with Learnable Input Noise\" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).", "The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state (\"learnable noise risk\") with a flexible functional approximation (neural network).", "Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.", "The simulation and real data experiments are interesting and seem well applied.", "A concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn't entirely illustrate its relationship to either.", "In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.", "Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.", "In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.", "Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 237, "sentences": ["The paper proposes using the Frank-Wolfe algorithm for fast adversarial attacks.", "They prove upper bounds on the Frank-Wolfe gap and show experimentally that they can attack successfully much faster than other algorithms.", "In general I find the paper novel (to the best of my somewhat limited knowledge), interesting and well written.", "However I find the white-box experiments lacking as almost every method has 100% success rate. Fixing this would significantly improve the paper.", "Main remarks:", "- Need more motivation for faster white-box attack.", "One good motivation for example is adversarial training, e.g. Kurakin et al 2017 \u2018ADVERSARIAL MACHINE LEARNING AT SCALE\u2019 that would benefit greatly from faster attacks", "- White-box attack experiments don\u2019t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.", "Need to compare in more challenging settings where the success rate is meaningful, e.g. smaller epsilon or a more robust NN using some defence.", "Also stating the 100% success rate in the abstract is a bit misleading for the this reason.", "-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other attack seems odd.", "-The average distortion metric (that\u2019s unfavourable to your method anyway) doesn\u2019t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.", "- Regarding lambda>1, you write that \u201cwe argue this modification makes our algorithm more general, and gives rise to better attack results\u201d.", "I did not see any theoretical or empirical support for this in the paper.", "Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.", "Some intuitive explanation on why this should help and/or empirical comparison would be a great addition.", "- The authors claim that this is the first zeroth-order non-convex FW convergence rate, I am not familiar enough with the field to evaluate this claim and its significance.", "- Alg. 1 for T>1 is very similar to I-FGM, but also \u2018pulls\u2019 x_t towards x_orig.", "It would be very useful to write the update more explicitly and compare and contrast this 2 very similar updates. This gives nice insight into why this should intuitively work better.", "- I am not sure what the authors mean by \u201cthe Frank-Wolfe gap is affine invariant\u201d. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?", "- I am not sure what you mean in 5.4 \u201cwe omit all grid search/ binary search steps \u2026\u201d", "Minor remarks:", "- In remark 4.8 in the end option I and II are inverted by mistake", "- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.", "- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length."], "labels": ["none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "arg-request_experiment", "arg-request_explanation", "arg-request_clarification", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_clarification_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 238, "sentences": ["This paper makes concerted efforts to examine the existing beliefs about the significance of various statistical characteristics of hidden layer activations (or representations) in a DNN.", "In the past, many works have argued for encouraging the certain statistical behavior of these representations (e.g., sparsity, low correlation etc) in order to have better classification accuracy.", "However, this paper tries to argue that such efforts are not very useful as these statistical characteristics don't provide any systematic explanation for the performance of DNNs across different settings.", "First, the paper argues that given a DNN, it's possible to construct either an identical output network or a comparable network that can have very different behavior for some of the statistical characteristics.", "This casts doubt on the usefulness of these characteristics in explaining the performance of the network.", "The paper conducts experiments with different regularizers associated with some of the standard statistical characteristics using the MNIST, CIFAR-10, and CIFAR-100 datasets.", "The paper claims that for each dataset the best performing network cannot be attributed to any single regularizer.", "For the same set of regularizers and the MNIST dataset, the paper then explores the mutual information between the inputs and the hidden layer activations.", "The paper observes that the best performing regularizer is the one which minimizes this mutual information.", "Therefore, it is plausible that the mutual information regularization can consistently explain the performance of an NN.", "The paper addresses an interesting problem and makes some good contributions.", "However, the reviewer feels that the brief treatment of mutual information regularizer leaves something to be desired.", "Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.", "In these tables, how do the authors decide which hidden layer representations should be explored for their statistical characteristics?", "The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_explanation", "arg-request_clarification", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 239, "sentences": ["The authors consider the setting of a RL agent that exclusively receives intrinsic reward during training that is intended to model curiosity; technically, \u2018curiosity\u2019 is quantified by the ability of the agent to predict its own forward dynamics [Pathak, et al., ICML17].", "This study primarily centers around an initially somewhat surprising result that non-trivial policies can be learned for many \u2019simpler\u2019 video games (e.g., Atari, Super Mario, Pong) using just curiosity as reward.", "While this is primarily an empirical study, one aspect considered was the observation representation (raw pixels, random features, VAE, and inverse dynamics features [Pathak, et al., ICML17]).", "In examining reward curves (generally extrinsic during testing), \u2018curiosity-based\u2019 reward generally works with the representation effectiveness varying across different testbeds.", "They also conduct more in-depth experiments on specific testbeds to study the dynamics (e.g., Super Mario, Juggling, Ant Robot, Multi-agent Pong) \u2014 perhaps most interestingly showing representation-based transfer of different embeddings across levels in Super Mario.", "Finally, they consider the Unity maze testbed, combining intrinsic rewards with the end-state goal reward to generate a more dense reward space.", "From a high level perspective, this is an interesting result that ostensibly will lead to a fair amount of discussion within the RL community (and already has based on earlier versions of this work).", "However, it isn\u2019t entirely clear if the primary contribution is showing that \u2018curiosity reward\u2019 is a potentially promising approach or if game environments aren\u2019t particularly good testbeds for practical RL algorithms \u2014 given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with \u2018simulator artifact\u2019 based explanations). And honestly, I think the paper reads as if leaning toward the same conclusion.", "Regardless, given the prevalence of these types of testbed environments, either is a useful discussion to have. Maybe the end result could minimally be a new baseline that can help quantify the \u2018difficulty\u2019 of a particular environment.", "From the perspective of a purely technical contribution, there are fewer exciting results.", "The basic method is taken from [Parthak, et al., ICML17] (modulo some empirical choices such as using PPO).", "The comparison of different observation representations doesn\u2019t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).", "The testbeds all existed previously and this is mostly the effort of pulling then together.", "Even the \u2018focused experiments\u2019 can be explained with the intuitive narrative that in the state/action space, there is always more uncertainty the farther one goes from the starting point and this is more of a result of massive computation being applied primarily to problems that are designed to provide some level of novelly (the Roboschool examples are a bit more interesting, but also less conclusive).", "Finally, Figure 5 is interesting in showing that \u2018curiosity + extrinsic\u2019 improves over extrinsic rewards \u2014 although this isn\u2019t particularly surprising for maze navigation that has such sparse rewards and can be viewed as something like \u2018active exploration\u2019.", "With respect to this specific setting, the authors may want to consider [Mirowski, et al., Learning to Navigate in Complex Environments, ICLR17] with respect to auxiliary loss + RL extrinsic rewards to improve performance (in this case, also in maze environments).", "In just considering the empirical results, they clearly entail a fair amount of effort and just a dump of the code and experiments on the community will likely lead to new findings (even if they are that game simulators are weaker testbeds than previously thought).", "It is easy to ask for additional experiments (i.e., other mechanisms of uncertainty such as the count-based discussed in related work, other settings in 2.2) \u2014 but the quality seems high enough that I basically trust the settings and findings.", "Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like \u2018curiosity honeypots\u2019 is interesting).", "Thus, it reads like one interesting finding around curiosity-driven RL working in games plus a bunch of preliminary findings trying to grasp at some explanations and potential future directions.", "Evaluating the paper along the requested dimensions:", "= Quality: The paper is well-written with a large set of experiments, making the case that exclusively using curiosity-based reward is very promising for the widely-used game RL testbeds.", "Modulo a few pointers, the work is well-contextualized and makes reasonable assumptions in conducting its experiments.", "The submitted code and videos result in a high-quality presentation and trustworthiness of the results. (7/10)", "= Clarity: The paper is very clearly written. (7/10)", "= Originality: The algorithmic approach is a combination of [Parthak, et al., ICML17] and [Schulman, et al. 2017] (with some experiments using [Kingma & Welling, 2013]).", "All of the testbeds have been used previously.", "Other than completely relying on curiously-based reward exclusively, there is little here.", "In considering combining with extrinsic rewards, I would also consider [Mirowski, et al., ICLR17], which is actually more involved in this regard. (4/10)", "= Significance: Primarily, this \u2018finishes\u2019 [Parthak, et al., ICML17] to its logical conclusion for game-based environments and should spur interesting conversations and further research.", "In terms of actual technical contributions, I believe much less significant. (5/10)", "=== Pros ===", "+ demonstrates that curiosity-based reward works in simpler game environments", "+ (implicitly) calls into question the value of these testbed environments", "+ well written, with a large set of experiments and some interesting observations/discussions", "=== Cons ===", "- little methodological innovation or analytical explanations", "- offers minimal (but some) evidence that curiosity-based reward works in more realistic settings", "- doesn\u2019t answer the one question regarding observation representation that it set out to evaluate", "- the more interesting problem, RL + auxiliary loss isn\u2019t evaluated in detail", "- presumably, the sample complexity is ridiculous", "Overall, I am ambivalent.", "I think that more casual ML/RL researchers will find these results controversial and surprising while more experienced researchers will see curiosity-driven learning to be explainable primarily by the intuition of the \u201cThe fact that the curiosity reward is often sufficient\u201d paragraph of page 6, demanding more complex environments before accepting that this form of curiosity is particularly useful.", "The ostensible goal of learning more about observation representations is mostly preliminary \u2014 and this direction holds promise of for a stronger set of findings.", "Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.", "However, as I said previously, this is probably a discussion worth having given the popularity and visibility of game-based testbeds \u2014 so, coupled with the overall quality of the paper, I lean toward a weak accept."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_edit", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_experiment", "none", "none", "none", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 240, "sentences": ["The paper explores how well different visual reasoning models can learn systematic generalization on a simple binary task.", "They create a simple synthetic dataset, involving asking if particular types of objects are in a spatial relation to others.", "To test generalization, they lower the ratio of observed  combinations of objects in the training data.", "The authors show the result that tree structured neural module networks generalize very well, but other strong visual reasoning approaches do not.", "They also explore whether appropriate structures can be learned.", "I think this is a very interesting area to explore, and the paper is clearly written and presented.", "As the authors admit, the main result is not especially surprising.", "I think everyone agrees that we can design models that show particular kinds of generalization by carefully building inductive bias into the architecture, and that it's easy to make these work on the right toy data.", "However, on less restricted data, more general architectures seem to show better generalization (even if it is not systematic).", "What I really want this paper to explore is when and why this happens.", "Even on synthetic data, when do or don't we see generalization (systematic or otherwise) from NMNs/MAC/FiLM?", "MAC in particular seems to have an inductive bias that might make some forms of systematic generalization possible.", "It might be the case that their version of NMN can only really do well on this specific task, which would be less interesting.", "All the models show very high training accuracy, even if they do not show systematic generalization.", "That suggests that from the point of view of training, there are many equally good solutions, which suggests a number of interesting questions. If you did large numbers of training runs, would the models occasionally find the right solution? Could you somehow test for if a given trained model will show systematic generalization?", "Is there any way to help the models find the \"right\" (or better) solutions - e.g. adding regularization, or changing the model size?", "Overall, I do think the paper has makes a contribution in experimentally showing a setting where tree-structured NMNs can show better systematic generalization than other visual reasoning approaches.", "However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-request_explanation", "none", "none", "arg-structuring_quote", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 241, "sentences": ["The paper proposes pre-training strategies (PT) for graph neural networks (GNN) from both node and graph levels.", "Two new large-scale pre-training datasets are created and extensive experiments are conducted to demonstrate the benefits of PT upon different GNN architectures.", "I am relative positive for this work.", "Detail review of different aspects and questions are as follows.", "Novelty: As far as I know, this work is among the earliest works to think about GNN pre-training.", "The most similar paper at the same period is [Z Hu, arXiv:1905.13728] .", "I read both papers and found they have similar idea about PT although they have different designs.", "This paper leverages graph structure (e.g., context neighbors) and supervised labels/attributes (e.g., node attributes, graph labels) for PT.", "These strategies are not surprising for me and the novelty is incremental.", "Experiment: The experiments are overall good.", "The authors created two new large scale pre-training graph datasets.", "Experimental results of different GNN architectures w/o different PT for different tasks are provided.", "Comparing to non-pretraining GNN, the improvements are significant for most cases.", "Writing: The writing is good and easy to follow.", "Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].", "Comparing to the other work, what are strengths of this work? In addition, have the authors compared the performances of their work and [Z Hu, arXiv:1905.13728] using the same data?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label"]}
{"abstract_id": 242, "sentences": ["Summary:", "This paper proposes Direct Sparse Optimization (DSO)-NAS, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost.", "The main idea is to treat all architectures as a Directed Acyclic Graph (DAG), where each architecture is realized by a subgraph.", "All architectures in the search space thus share their weights, like ENAS (Pham et al 2018) and DARTS (Liu et al 2018a).", "The DAG\u2019s edges can be pruned via a sparsity regularization term.", "The optimization objective of DSO-NAS is thus:", "Accuracy + L2-regularization(W) + L1-sparsity(\\lambda),", "where W is the shared weights and \\lambda specifies which edges in the DAG are used.", "There are 3 phases of optimization:", "1. All edges are activated and the shared weights W are trained using normal SGD.", "Note that this step does not involve \\lambda.", "2. \\lambda is trained using Accelerated Proximal Gradient (APG, Huang and Wang 2018).", "3. The best architecture is selected and retrained from scratch.", "This procedure works for all architectures and objectives.", "However, DSO-NAS further proposes to incorporate the computation expense of architectures into step (2) above, leading to their found architectures having fewer parameters and a smaller FLOP counts.", "Their experiments confirm all the hypotheses (DSO-NAS can find architectures, having small FLOP counts, having good performances on CIFAR-10 and ImageNet).", "Strengths:", "1. Regularization by sparsity is a neat idea.", "2. The authors claim to be the first NAS algorithm to perform direct search on ImageNet.", "Honestly, I cannot confirm this claim (not sure if I have seen all NAS papers out there), but if it is the case, then it is impressive.", "3. Incorporating architecture costs into the search objective is nice.", "However, this contribution seems to be orthogonal to the sparsity regularization, which, I suppose, is the main point of the paper.", "Weaknesses:", "1. Some experimental details are missing. I\u2019m going to list them here:", "- Was the auxiliary tower used during the training of the shared weights W?", "- Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?", "- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?", "- In Section 3.3, it is written that \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d.", "This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.", "2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.", "On ImageNet, your performance is similar to theirs.", "I think this will be a good comparison.", "3. The paper has some grammatical errors. I obviously missed many, but here are the one I found:", "- Section 3.3: \u201cDifferent from pruning, which the search space is usually quite limited\u201d. \u201cwhich\u201d should be \u201cwhose\u201d?", "- Section 4.4.1: \u201cDSO-NAS can also search architecture [...]\u201d  -> \u201cDSO-NAS can also search for architectures [...]\u201d", "References.", "[1] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/pdf/1608.03983.pdf", "[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_quote", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-structuring_quote", "arg-request_experiment", "arg-request_edit", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_experiment_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 243, "sentences": ["Summary: This paper is about models for solving basic math problems.", "The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type.", "The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems.", "The results are then analyzed and insights are derived explaining where neural models seemingly cope well with math tasks, and where they fall down.", "Strengths: I am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models.", "There are challenging desiderata involved in building the training+tests sets, and the authors have an interesting and involved methodology to accomplish these.", "The paper is very clearly written. I'm not aware of a comparable work, so the novelty here seems good.", "Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).", "It would have been useful to compare the general models here with some specific math problem-focused ones as well.", "Some details weren't clear to me.", "More in the comments below.", "Verdict: I thought this was generally an interesting paper that has some very nice benefits, but also has some weaknesses that could be resolved. I view it as borderline, but I'm willing to change my mind based on the discussion.", "Comments:", "- One area that could stand to be improved is prior work.", "I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems.", "Since this is the core contribution, this should also be the main comparison.", "For example, EMLNP 2017 paper \"Deep Neural Solver for Math Word Problems\" mentions a size 60K problem dataset.", "A more extensive discussion will help convince the readers that the proposed dataset is indeed the largest and most diverse.", "- The authors note that previous datasets are often specific to one type of problem (i.e., single variable equation solving). Why not then combine multiple types of extant problem sets?", "- The authors divide dataset construction into crowdsourcing and synthetic.", "This seems incomplete to me: there are tens of thousands (probably more) of exercises and problems available in workbooks for elementary, middle, and high school students.", "These are solved, and only require very limited validation.", "They are also categorized by difficulty and area.", "Presumably the cost here would be to physically scan some of these workbooks, but this seems like a very limited investment. Why not build datasets based on workbooks, problem solving books, etc?", "- How do are the difficulty levels synthetically determined?", "- When generating the questions, the authors \"first sample the answer\". What's the distribution you use on the answer? This seems like it dramatically affects the resulting questions, so I'm curious how it's selected.", "- The general methodology of generating questions and ensuring that no question is too rare or too frequent and the test set is sufficiently different---these are important questions and I commend the authors for providing a strong methodology.", "- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).", "This is certainly a scientific decision, i.e., the authors are determining which models to use in order to determine the possible insights they will derive.", "But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful.", "In fact, assuming that such methods outperform general-purpose models, we could investigate why and where this is the case (in fact the proposed dataset is very useful for this).", "On the other hand, if these specialized approaches largely fail to outperform general-purpose models, we would have the opposite insights---that these models' benefits are dataset-specific and thus limited.", "- Really would be good to do real-world tests in a more extensive way.", "A 40-question exam for 16 year olds is probably far too challenging for the current state of general recurrent models. Can you add some additional grades here, and more questions?", "- For the number of thinkings steps, how does it scale up as you increase it from 0 to 16? Is there a clear relationships here?", "- The 1+1+...+1 example is pretty intriguing, and could be a nice \"default\" question!", "- Minor typo: in the abstract: \"test spits\" should be \"test splits\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_heading", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "arg-request_explanation", "arg-structuring_quote", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_edit", "none", "none", "arg-request_clarification", "none", "none", "arg-request_experiment", "arg-request_edit", "arg-request_explanation", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_quote_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_typo_label"]}
{"abstract_id": 244, "sentences": ["# Summary", "This paper proposes a new kind of spherical convolution for use in spherical CNNs, and evaluates it on rigid and non-rigid 3D shape recognition and retrieval problems.", "Previous work has either used general anisotropic convolution or azimuthally isotropic convolution.", "The former produces feature maps on SO(3), which is deemed undesirable because processing 3-dimensional feature maps is costly.", "The latter produces feature maps on the sphere, but requires that filters be circularly symmetric / azimuthally isotropic, which limits modeling capacity.", "This paper proposes an anisotropic spherical convolution that produces 2D spherical feature maps.", "The paper also introduces an efficient way of processing geodesic / icosahedral spherical grids, avoiding complicated spectral algorithms.", "# Strengths", "The paper has several strong points.", "It is well written, clearly structured, and the mathematics is clear and precise while avoiding unnecessary complexity.", "Much of the relevant related work is discussed, and this is done in a balanced way.", "Although it is not directly measured, it does seem highly likely that the alt-az convolution is more computationally efficient than SO(3) convolution, and more expressive than isotropic S2 convolution.", "The most important contribution in my opinion is the efficient data structure presented in section 4, which allows the spherical convolution to be computed efficiently on GPUs for a grid that is much more homogeneous than the lat/lon grids used in previous works (which have very high resolution near the poles, and low resolution at the equator).", "The idea of carving up the icosahedral grid in just the right way, so that the spherical convolution can be computed as a planar convolution with funny boundary conditions, is very clever, elegant, and practical.", "# Weaknesses", "There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.", "To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.", "This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.", "For instance we can multiply Rz(phi) Ry(nu) by the element Rz(omega)Ry(0) = Rz(omega), which gives the element Rz(phi) Ry(nu) Rz(omega).", "As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)", ".", "So the closure axiom of a group is violated.", "This matters, because the notion of equivariance really only makes sense for a group.", "If a layer l satisfies l R = R l  (for R a alt-az rotation), then it automatically satisfies l RR' = RR' l, which means l is equivariant to the whole group generated by the set of alt-az rotations.", "As we saw before, this is the whole rotation group.", "This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.", "Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.", "This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary.", "The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.", "I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.", "Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.", "The south pole can be represented by any pair of coordinates of the form phi in [0, 2pi], nu = +/- pi.", "But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.", "This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.", "The set of points on the sphere can only be viewed as the quotient SO(3)/S(2).", "The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.", "I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks.", "I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.", "# Other comments", "The experiments show that the method is quite effective.", "For instance, the SHREC17 results are on par with Cohen et al. and Esteves et al., presumably at a significantly reduced computational cost.", "That they do not substantially outperform these and other methods", "is likely due to the input representation, which is lossy, leading to a maximal performance shared by all three methods.", "An application to omnidirectional vision might more clearly show the strength of the method, but this would be a lot of work so I do not expect the authors to do that for this paper.", "It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2).", "Right now, the numbers reported in Cohen et al. and Esteves et al. are copied over, but there are probably many differences between the precise setup and architectures used in these papers.", "It would be interesting to see what happens if one uses the same architecture on a number of problems, changing only the convolution in each case.", "Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1.", "I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned.", "Some more explanation / discussion would be good.", "It would be nice to explain the spherical parameterization in more detail. Is this operation itself rotation equivariant?", "Typos & minor issues", "- Abstract: \"to extract non-trivial features\".", "The word non-trivial really doesn't add anything here.", "Similarly \"offers multi-level feature extraction capabilities\" is almost meaningless since all DL methods can be said to do so.", "- Below eq. 5, D_R^{-1} should equal D_R(-omega, -nu, -phi).", "The order is reversed when inverting.", "- \"Different notations of convolutions\" -> notions", "- \"For spherical functions there is no consistent and well defined convolution operators.\" As discussed above, the issue is quite a bit more subtle. There are exactly two well-defined convolution operators, but they have some characteristics deemed undesirable by the authors.", "- \"rationally symmetric\" -> rotationally", "- \"exact hierarchical spherical patterns\" -> extract", "- It seems quite likely that the unpacking of the icosahedral/hexagonal grid as done in this paper has been studied before in other fields.", "References would be in order. Similarly, hexagonal convolution has a history in DL and outside.", "- Bottom of page 7, capitalize \"for\".", "- \"principle curvatures\" -> principal.", "- \"deferent augmentation modes\" -> different", "- \"inspite\" -> in spite", "- \"reprort\" -> report", "- \"utlize\" -> utilize", "- \"computer the convolution\" -> compute", "# Conclusion", "Although the alt-az convolution lacks the mathematical elegance of the general anisotropic and azimuthally isotropic spherical convolutions, it still seems like a practically useful operation for some kinds of data, particularly when implemented using the homogeneous icosahedral/hexagonal grid and fast algorithm presented in this paper.", "Hence, I would wholeheartedly recommend acceptance of this paper if the authors correct the factual errors (e.g. the claim of SO(3)-equivariance) and provide a clear discussion of the issues.", "For now I will give an intermediate rating to the paper.", "[1] Kondor, Trivedi, \"On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_typo", "arg-request_edit", "arg-request_typo", "arg-request_typo", "arg-request_edit", "arg-request_edit", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-structuring_heading", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 245, "sentences": ["The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.", "In fact, in machine learning and NLP there is the OOV class which sometimes people in computer vision also use.", "Some of the claims in the paper can be further substantiated or explored.", "For example in abstract there is a simple claim that is presented too strong: We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.", "This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.", "Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.", "In general it is very unlikely that you will be able to choose every variation of out-distribution cases.", "Much easier if you just try to solve the problem using a set of n Sigmoids (n total number of classes) and consider each output a probability distribution.", "However, the studies in this paper are still valuable and I strongly recommend continuing on the same direction."], "labels": ["none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 246, "sentences": ["adaptive versions of sgd are commonly used in machine learning.", "adagrad, adadelta are both popular adaptive variations of sgd.", "These algorithms can be seen as preconditioned versions of gradient descent where the preconditioner applied is a matrix of second-order moments of the gradients.", "However, because this matrix turns out to be a pxp matrix where p is the number of parameters in the model, maintaining and performing linear algebra with this pxp matrix is computationally intensive.", "In this paper, the authors show how to maintain and update this pxp matrix by storing only smaller matrices of size pxr and rxr, and performing 1. an SVD of a small matrix of size rxr", "2. matrix-vector multiplication between a pxr matrix and rx1 vector.", "Given that rxr is a small constant sized matrix and that matrix-vector multiplication can be efficiently computed on GPUs, this matrix adapted SGD can be made scalable.", "The authors also discuss how to adapt the proposed algorithm with Adam style updates that incorporate momentum.", "Experiments are shown on various architectures (CNN, RNN) and comparisons are made against SGD, ADAM.", "General comments: THe appendix has some good discussion and it would be great if some of that discussion was moved to the main paper.", "Pros:  Shows how to make full matrix preconditioning efficient, via the use of clever linear algebra, and GPU computations.", "Shows improvements on LSTM tasks, and is comparable with SGD, matching accuracy with time.", "Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.", "This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.", "It might be possible that if one performs few steps of GGT optimizer in the initial stages and then switches to SGD/ADAM in the later stages, then some of the computational concerns that arise are eliminated.", "Have the authors tried out such techniques?"], "labels": ["none", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 247, "sentences": ["This paper proposes to combine unsupervised adversarial domain adaptation with prototypical networks and finds that the proposed model performs well on few-shot learning task with domain shift, much better than other few-shot learning baselines that do not consider.", "Specifically it tests on Omniglot with natural image background and cliparts to real images.", "It is true that current meta-learning approaches do not address the problem of domain shift, and as a result, the testing domain has to be the same with the training domain.", "However, this paper rather than proposing solution address the meta-learning problem, albeit the title \u201cmeta domain adaptation\u201d, only brings few-shot learning to domain adaptation.", "Here\u2019s why:", "In order for a meta-learning model to be called \u201cmeta domain adaptation,\u201d the type of adaptation cannot be seen during training, and the goal is to test on adaptation that the model has not seen before.", "Indeed, each task in meta domain adaptation should be seen as a pair of source task and target task.", "The problem with the current model is that during training, it is trained to target at one specific type of test domain--the generator network G aims to generated images that align with the unsupervised  test domain X_test.", "Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.", "In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.", "Therefore, the paper cannot be qualified for ``meta domain adaptation\u2019\u2019 and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.", "For the rest of my review, I will treat the paper as \u201cfew-shot learning with domain adaptation\u201d for more appropriate analysis.", "For the experiments, there seems to have a great win of the proposed algorithm against the baselines.", "However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.", "Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.", "Then use the same network to extract the features and then using the nearest neighbor to retrieve the classes.", "Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.", "Another concern is that the evaluation of domain adaptation does not have much varieties.", "Only two domains shifts are evaluated in the paper, specifically Omniglot + BSD500 and Office-Home.", "BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.", "Other domain transfer settings such as synthetic rendered vs. real (e.g. visDA challenge) could have been considered.", "In conclusion, the paper presents a interesting combination of ProtoNet + Adversarial DA + Cycle consistency.", "However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.", "Therefore, I recommend reject.", "---", "Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being \"meta domain adaptation\".", "===", "After rebuttal:", "I would like to thank the authors for the response and updating the draft.", "They have addressed 1) the title issue and 2) adding domain adaptation baselines.", "Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines.", "However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.", "The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-structuring_quote", "none", "none", "arg-request_explanation", "none", "arg-request_experiment", "none", "arg-structuring_quote", "arg-request_experiment", "arg-request_experiment", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 248, "sentences": ["The paper addresses the problem of producing sensible (high) uncertainties on out of distribution (OOD) data along with accurate predictions on in-distribution data.", "The authors consider a model wherein the weights of the network (\\theta) are drawn from a matrix normal distribution whose parameters are in-turn a (non-linear; parameterized by a another network) function of the covariates (x).", "Instead of inferring a posterior over theta that then induces the predictive uncertainties, uncertainties here arise from a regularizer that penalizes the distribution over theta from deviating too far from a standard Normal.", "Experiments present results on toy data, MNIST/not MNIST as well as on adversarial perturbations of MNIST and CIFAR 10 datasets.", "The paper is clearly written and addresses an important problem.", "The paper presents both an alternate model as well as an alternate objective function.", "While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.", "It isn\u2019t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:", "1. The proposed model? Is using a conditional weight prior p(\\theta | x) (Eq 3) instead of p(\\theta) (as in BNNs)  necessary for the inflated uncertainties on OOD data?", "2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\\theta) =  MN(0, I, I) in the KL sense.", "Depending on \\lambda, the objective either closely approximates the marginal likelihood or not.", "It is unclear how important this particular objective is to the results.", "-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \\theta and induce predictive uncertainties.  Were they explored and found to be not effective?", "It would be nice to see how a \u201cgold standard\u201d HMC based inference does on at least the small toy problem of Sec 5.1?", "- There is also a closely related variant of Eq 3 which we can arrive at by switching the log and the expectation in the first term of Eq 5 and applying Jensen\u2019s inequality \u2014> E_p(\\theta| x)[ln p(y | x, \\theta)] - KL (p(\\theta | x) || p(\\theta)).", "This would correspond to maximizing a valid lower bound to the marginal likelihood of a BNN model p(y | x, \\theta) p(\\theta), while interpreting p(\\theta | x) as an amortized variational approximation.", "This variant has the advantage that it provides a valid lower bound on the marginal likelihood, and exploits the well understood variational inference machinery.", "This also immediately suggests, that the variational approximation , p (\\theta | x)  should probably depend on both x and y rather than only on x and the flexibility of the hyper networks g would govern how well the true posterior over weights \\theta can be approximated.", "Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.", "3.  Or simply to a well tuned \\lambda, chosen on a per dataset basis? From the text it appears that \\lambda is manually selected to trade off accuracy against uncertainty on OOD data.", "In the real world, one would not have access to OOD data during training, how is one to pick \\lambda in such cases?", "Detailed comments about experiments:", "a) The uncertainties produced by CDN in Figure 2 seems strange.", "Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?", "b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.", "This forces the VI solution to tend to the MLE, sacrificing uncertainty in the variational distribution.", "It would be good to include comparisons against VI with \\lambda = 1.", "==========", "There are potentially interesting ideas in this paper.", "However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-request_explanation", "arg-request_experiment", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-request_explanation", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 249, "sentences": ["This work is focused on learning 3D object representations (decoders) that can be computed more efficiently than existing methods.", "The computational inefficiency of these methods is that you learn a (big) fixed decoder for all objects (all z latents), and then need to apply it individually on either each point cloud point you want to produce, or each voxel in the output (this problem exists for both the class of methods that deform a uniform distribution R^3 -> R^3 a la FoldingNet, or directly predict the 3D function R^3 -> R e.g. DeepSDF).", "The authors propose that the encoder directly predict the weights and biases of a decoder network that, since it is specific to the particular object being reconstructed, can be much smaller and thus much cheaper to compute.", "The authors then note the fact that their method lacks a continuous latent space that allows for interpolation, as provided by existing (VAE-like) methods.", "They propose to solve this by learning an MLP that produces the output by recurrent application, and then composing subapplications of different networks as a type of interpolation.", "-------------------", "I like this work, it addresses a real problem in a number of models for 3D representation learning (similar models are also used for e.g. cryo-EM reconstruction).", "While the fast weights approach is not totally original, its application to this problem is novel and very well-suited to it.", "I was a bit surprised by just how much the decoder network could be shrunk by using fast weights.", "The paper is also quite well written.", "I especially like how Section 2 synthesizes existing work into model categories which make it easier to think about their relationships.", "I also think the explanation in Sec. 3.2, while kind of obvious, is a nice way think about decoder vs. fast weights.", "I like that the authors are straightforward about the deficiency of the method (i.e. that you can't interpolate in latent space).", "Their proposed solution of functional composition is exceedingly clever but in my opinion too impractical to really be useful.", "It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?", "The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples.", "The function composition doesn't capture that.", "I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).", "But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc.", "I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands reasonably on its own without that .", "Nits:", "- In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?", "- For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)?", "I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_clarification", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 250, "sentences": ["This paper proposes new pre-training strategies for GNN with both a node-level and a graph-level pretraining.", "For the node-level pretraining, the goal is to map nodes with similar surrounding structures to nearby context (similarly to word2vec).", "The main problem is that directly predicting the context is intractable because of combinatorial explosion.", "The main idea is then to use an additional GNN to encode the context and to learn simultaneously the main GNN and the context GNN via negative sampling.", "Another method used is attribute masking where some masked node and edge attributes need to be predicted by the GNN.", "For graph-level pretraining, some general graph properties need to be predicted by the graph.", "Experiments are conducted on datasets in the chemistry domain and the biology domain showing the benefit of the pre-training.", "The paper addresses an important and timely problem.", "It is a pity that the code is not provided.", "In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.", "In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 251, "sentences": ["This paper introduces  a novel DPS(Deep Probabilistic Subsampling) framework for the task-adaptive  subsampling case, which attempts to resolve the issue of end-to-end optimization of an optimal subset of signal with jointly learning a sub-Nyquist sampling scheme and a predictive model for downstream tasks.", "The parameterization is used to simplify the subsampling distribution and ensure an expressive yet tractable distribution.", "The new approach contribution is applied to  both reconstruction and classification tasks and demonstrated with a suite of experiments in a toy dataset, MINIST, and COFAR10.", "Overall, the paper requires significant improvement.", "1. The approach is not well justified either by theory or practice.", "There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); Pl\u00c2\u0161otz & Roth (2018) ).", "2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem)", "The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.", "3. The paper is not nicely written or rather easy to follow.", "The model is not well motivated and the optimization algorithm is also not well described.", "4. A theoretical analysis of the convergence of the optimization algorithm could be needed.", "5. The paper is imprecise and unpolished and the presentation needs improvement.", "**There are so many missing details or questions to answer**", "1. What is the Gumbel-max trick?", "2. How to tune the parameters discussed in training details in the experiments?", "3. Why to use experience replay for the linear experiments?", "4. Are there evaluations on the utility of proposed compared to existing approaches?", "5. Does the proposed approach work in real-world problems?", "6. Was there any concrete theoretical guarantee to ensure the convergence of the algorithm.", "[Post Review after discussion]\u0010: The uploaded version has significantly improved over the first submission. It is now acceptable."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_clarification", "arg-request_explanation", "arg-request_explanation", "arg-request_result", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 252, "sentences": ["The authors propose a network quantization approach with adaptive per layer bit-width.", "The approach is based on a network architecture search (NAS) method.", "The authors aim to solve the NAS problem through SGD.", "Therefore, they propose to first reprametrize the the discrete random variable determining if an edge is computed or not to make it differentiable and then use Gumbel Softmax function as a way to effectively control the variance of the obtained unbiased estimator.", "This variance can indeed make the convergence of the procedure hard.", "The procedure is then adapted to the problem of network quantization with different band-widths.", "The proposed approach is interesting.", "The differerentiable NAS procedure is particularly important and can have an important impact.", "The idea of having an adaptive per layer precision is also well motivated, and shows competitive (if not better) results empirically.", "Some additional experiments can make the paper stronger:", "* Compare the result of the procedure to an exhaustive search in a setting where the latter is feasible (shallow architecture on an easy task with few possible bit widths)", "* Compare the procedure to other state of the art NAS procedures (DARTS and ENAS) with the same search space adapted to the quantization problem, to empirically show that the proposed procedure is a compromise between these two methods as claimed by the authors."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 253, "sentences": ["This paper proposed a general framework, DeepTwist, for model compression.", "The so-called weight distortion procedure is added into the training every several epochs.", "Three applications are shown to demonstrate the usage of the proposed approach.", "Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.", "See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.", "Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient.", "Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function.", "Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework.", "Then proximal function can be applied directly after Distortion Step to project the solutions.", "In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.", "Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.", "PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works."], "labels": ["arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 254, "sentences": ["Summary:", "The authors propose quantize the weights of a neural network by enabling a fractional number of bits per weight.", "They use a network of differentiable XOR gates that maps encrypted weights to higher-dimensional decrypted weights to decode the parameters on-the-fly and learn both the encrypted weights and the scaling factors involved in the XOR networks by gradient descent.", "Strengths of the paper:", "- The method allows for a fractional number of bits per weights and relies of well-known differentiable approximations of the sign function.", "Indeed, virtually any number of bits/weights can be attained by varying the ratio N_in/N_out.", "- The papers displays good results on ImageNet for a ResNet-18.", "Weaknesses of the paper:", "- Some arguments that are presented could deserve a bit more precision.", "For instance, quantizing to a fractional number of bits per weights per layer is in itself interesting.", "However, if we were to quantize different layers of the same network with distinct integer  ratio of bits per weights (say 1 bit per weight for some particular layers and 2 bits per weight for the other layers), the average ratio would also be fractional (see for instance \"Hardware-aware Automated Quantization with Mixed Precision\", Wang et al., where the authors find the right (integer) number of bits/weights per layer using RL).", "Similarly, using vector quantization does allow for on-chip low memory: we do not need to re-instantiate the compressed layer but we can compute the forward in the compressed domain (by splitting the activations into similar block sizes and computing dot products).", "- More extensive and thorough experiments could improve the impact of the paper.", "For instance, authors could compress the widely used (and more challenging) ResNet-50 architecture, or try other tasks such as image detection (Mask R-CNN).", "The table is missing results from: \"Hardware Automated Quantization\", Wang et al ; \"Trained Ternary Quantization\", Zhu et al ; \"Deep Compression\",  Han et al; \"Ternary weight networks\", Li et al (not an extensive list).", "- Similarly, providing some code and numbers for inference time would greatly strengthen the paper and the possible usage of this method by the community.", "Indeed, I wonder what the overhead of decrypting the weights on-the-fly is (although it only involves XOR operations and products)", "- Small typos: for instance, two points at the very end of section 5.", "Justification fo rating:", "The proposed method is well presented and illustrated.", "However, I think the paper would need either (1) more thorough experimental results (see comments above, points 2 and 3 of weaknesses) or (2) more justifications for its existence (see comments above, point 1 of weaknesses)."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_edit", "none", "none", "arg-request_edit", "none", "arg-request_experiment", "arg-request_result", "arg-request_clarification", "arg-request_explanation", "arg-request_typo", "arg-structuring_heading", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 255, "sentences": ["The paper presents a novel hierarchical clustering method over an embedding space.", "In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learnt.", "The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods.", "The paper address a relevant problem, which is of great interest for extracting knowledge from data.", "In general, the quality of the paper is high.", "The presented approach is based on a sound formalization of hierarchical clustering and deep generative models.", "The paper is easy to follow in spite of the technical difficulty.", "The experimental evaluation is really extensive.", "It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view.", "The only issue with this paper is its degree of novelty, which is narrow.", "The proposed method adapt a previously presented hierarchical clustering method in the \"standard space\" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.", "The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label"]}
{"abstract_id": 256, "sentences": ["The paper presents a novel scheme of distributing PPO reinforcement learning algorithm for hundreds of GPUs.", "Proposed technique was validated for pointgoal visual navigation task on recently introduced Habitat challenge and sim.", "Besides the technical contribution, paper shows that when have enough computational power of billions simulation runs, it is possible to learn nearly perfect visual navigation (given RGBD + GPS inputs) via reinforcement learning.", "Authors also study the task itself and show that it is yet not possible to achieve a good results without dense (each step) GPS signal, while the \"Blind\" agent, which has only GPS+compass error achieves quite high results given the billion-scale training time.", "This suggests that PointGoal navigation with dense GPS signal is might be a poor choice to benchmark RL algorithms and we should proceed to harder tasks.", "Overall I like the paper a lot and think that it should be accepted.", "***", "I haven`t changed my mind after the rebuttal: the paper is good and should be accepted."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 257, "sentences": ["In this work the authors introduce a new method for neural architecture search (NAS) and use it in the context of network compression.", "Specifically, the NAS method is used to select the precision quantization of the weights at each layer of the neural network.", "Briefly, this is done by first defining a super network, which is a DAG where for each pair of nodes, the output node is the linear combination of the outputs of all possible operations (i.e., layers with different precision quantizations).", "Following [1], the weights of the linear combination are regarded as the probabilities of having certain operations (i.e., precision quantization), which allows for learning a probability distribution over the considered operations.", "Differently from [1], however, the authors bridge the soft sampling in [1] (where all operations are considered together but weighted accordingly to the corresponding probabilities) to a hard sampling (where a single operation is considered with the corresponding probability) through an annealing procedure based on the Gumbel Softmax technique.", "Through the proposed NAS algorithm, one can learn a probability distribution on the operations by minimizing a loss that accounts for both accuracy and model size.", "The final output of this search phase is a set of sampled architectures (containing a single operation at each connection between nodes), which are then retrained from scratch.", "In applications to CIFAR-10 and ImageNet, the authors achieve (and sometime surpass) state-of-the-art performance in model compression.", "The two contributions of this work are", "1)\tA new approach to weight quantization using principles of NAS that is novel and promising;", "2)\tNew insights/technical improvements in the broader field of NAS.", "While the utility of the method in the more general context of NAS has not been shown, this work will likely be of interest to the NAS community.", "I only have one major concern.", "The architectures are sampled from the learnt probability distribution every certain number of epochs while training the supernet.", "Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?", "This reasoning leads me to a second question.", "In the CIFAR-10 experiments, the authors sample 5 architecture every 10 epochs, which means 45 architectures (90 epochs were considered).", "This is a lot of architectures, which makes me wonder: how would a \u201ccost-aware\u201d random sampling perform with the same number of sampled architectures?", "Also, I have some more questions/minor concerns:", "1)\tThe authors say that the expectation of the loss function is not directly differentiable with respect to the architecture parameters because of the discrete random variable.", "For this reason, they introduce a Gumbel Softmax technique, which makes the mask soft, and thus the loss becomes differentiable with respect to the architecture parameters.", "However, subsequently in the manuscript, they write that Eq 6 provides an unbiased estimate for the gradients.", "Do they here refer to the gradients with respect to the weights ONLY?", "Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.", "2)\tCan the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.", "3)\tThe authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that \u201cthe weights are sufficiently trained\u201d. Can the authors discuss the choice on the number of warmup epochs?", "I gave this paper a 5, but I am overall supportive. Happy to change my score if the authors can address my major concern.", "[1] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018 Jun 24.", "-----------------------------------------------------------", "Post-Rebuttal", "---------------------------------------------------------", "The authors have fully addressed my concerns. I changed the rating to a 7."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_explanation", "arg-structuring_heading", "none", "arg-request_experiment", "arg-structuring_heading", "arg-structuring_quote", "none", "none", "arg-request_clarification", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_summary_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 258, "sentences": ["This paper makes an interesting theoretical contribution; namely, that SGD with momentum (and with a slight modification to the step-size rule) is guaranteed to quickly converge to a second-order stationary point, implying it quickly escapes saddle points.", "SGD with momentum is widely used in the practice of deep learning, but a theoretical analysis has remained largely elusive.", "This paper sheds light theoretical properties justifying its use for deep learning.", "Although the paper makes assumptions (e.g., twice differentiable, with smooth Hessian) that are not valid for the most widely-used deep learning models, the theoretical contributions of this paper should nonetheless be of interest to researchers in optimization for machine learning.", "I recommend it be accepted.", "The experiments reported in the paper, including those used to validate the required properties, are for small toy problems.", "This is reasonable given that the main contribution of the paper is theoretical.", "However, I would have given a higher rating if some further exploration of the validity of these properties was carried out for problems closer to those of interest to the broader ICLR community.", "Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.", "This may also help to understand some of the limitations of this analysis.", "One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label"]}
{"abstract_id": 259, "sentences": ["In this paper, the authors introduce a new convolution-like operation, called a Harmonic Convolution, which operates on the STFT of an audio signal.", "This Harmonic convolution are like a weighted combination of dilated convolutions with different dilation factors/anchors .", "The authors show that for noisy audio signals, randomly initialized/untrained U-Nets with harmonic convolutions can yield cleaner recovered audio signals than U-Nets with plain convolutions or dilated convolutions.", "The authors beat a variety of audio denoising tasks on a variety of metrics for speech and music signals.", "The authors also show that harmonic convolutions in U-Nets are better than plain and dilated convolutions in U-Nets for a particular sound separation task.", "I recommend a weak accept for this paper because a new architecture for audio priors was presented, with reasonable empirical data supporting that this architectural choice an improvement over other more immediate alternatives.", "It is important to extend the work on deep nets for imaging to other domains, such as audio.", "My recommendation is not stronger because of the following concerns.", "I think the paper could be strengthened by", "(a) a comparison to other methods (outside the current framework) for sound separation", "(b) a significant clarification of Figure 4.", "The authors claim that this data shows that Harmonic Convolutions produce a \"cleaner signal faster\" than other methods.", "When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.", "Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).", "Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.", "(c) The authors should present what they mean by a dilated convolution using the notation of the paper.", "(d) In Figure 2, it is unclear to me how the 1/f^2 law is observed in (a) but not in (c) or (e)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_experiment", "arg-request_clarification", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 260, "sentences": ["This paper proposed a bio-inspired sparse coding algorithm where iterations for dictionary updates take into account the past updates.", "It is argued that time takes a crucial rule in learning.", "The paper is quite well written and contains an extensive literature review demonstrating a good understanding of previous literature in both ML/DL and biological vision.", "The idea of using a \"non-linear gain normalization\" to adjust atom selection in sparse coding is interesting and as far as I know novel, while providing interesting empirical results: The system learns in an unsupervised way faster.", "Misc:", "- Using < > for latex brakets is not ideal. I would recommend: $\\langle\\,,\\rangle$", "- \"derivable\" I guess you mean \"differentiable\"", "- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the reference to Pedregosa et al. for sklearn is missing."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_typo", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "none_label"]}
{"abstract_id": 261, "sentences": ["This paper presents an analysis of the inverse invariance of ReLU networks.", "It makes the observation that one can describe the pre-image of an image point z = F(x) using linear algebra arguments.", "They provide necessary conditions for the pre-image to be a singleton or a finite volume polytope.", "They also provide upper-bounds on the singular values of a train network and measure those in standard CNNs.", "The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.", "The proofs seem correct and rely mostly on elementary linear algebra argument; this simplicity makes the analysis quite interesting.", "The argument about a different kind of adversarial examples is also very interesting; instead of looking for small perturbation that affect the mapping in drastic ways, find large perturbations that in invariant directions of the network.", "However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.", "I have several questions for the authors:", "- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?", "- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.", "Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.", "- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?", "In conclusion, this paper does an interesting and original analysis which can help us understand better the polytopes composing the input space.", "The experiments are not very convincing or illustrative of the theoretical results in my opinion.", "It is not clear how those observations can affect practical algorithms and this is something I hope the author can address."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "none", "arg-request_edit", "arg-request_explanation", "none", "none", "arg-request_result"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 262, "sentences": ["This paper deal with learning abstract MDPs for planning in tasks that require long-horizon due to sparse rewards.", "This is an extremely important and timely topic in the RL community.", "The paper is generally clear and well written.", "The proposed algorithm seems reasonable and it is conceptually simple to understand.", "In the current experimental results presented it also seems to outperform the alternative baselines.", "Nonetheless, the paper has few flaws that significantly impact the stated contributions and reduced my rating.", "1) a stated contribution are theoretical guarantees about the performance of the algorithm.", "this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying.", "Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?) .", "Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic.", "Overall, if you want to claim theoretical guarantees you will have to significantly improve the manuscript.", "2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature.", "Listing related work is no the same as describing similarities and differences compared to previous methods.", "For example, a paper that obviously comes to mind is \"FeUdal Networks for Hierarchical Reinforcement Learning\".", "What are the differences to your approach?", "Also, please place the related work earlier on in the paper.", "Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.", "3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.", "This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines. feudal RL should be one, Roderick et al 2017 should be another one (especially considering your discussion in Sec 8)", "Additional feedback:", "- The paper is currently oriented towards discrete states. What can you say about continuous spaces?", "- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?", "- Using only 4 seeds seems too little to provide accurate standard deviations.", "Please run at least 10 experiments.", "- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.", "Otherwise, this choice is incomprehensible."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_edit", "none", "none", "none", "arg-request_explanation", "arg-request_edit", "none", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 263, "sentences": ["The paper presents an auto completion for UI layout design.", "The authors formulate the problem as partial tree completion, and investigate a range of variations of layout decoders based on Transformer.", "The paper proposes two models: Pointer and Recursive Transformer.", "The paper designs three sets of metrics to measure the quality of layout prediction based on the literature and the domain specifics of user interface interaction.", "The writing quality is readable.", "The presentation is nice.", "The task of auto completion for UI layout design is relatively new.", "The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.", "[1] Jenatton, Rodolphe, et al. \"Bayesian optimization with tree-structured dependencies.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.", "NB: the reviewer has low confidence in evaluating this paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 264, "sentences": ["The paper proposes to make a clear connection between the InfoNCE learning objective (which is a lower bound of the mutual information) and multiple language models like BERT and XLN.", "Then based on the observation that classical LM can be seen as instances of InfoNCE, they propose a new (InfoWord) model relying on the same principles, but taking inspiration from other models also based on InfoNCE.", "Mainly, the proposed model  differs both in the nature of the a and b variables used in InfoNCE, and also on the fact that it uses negative sampling instead of softmax.", "Experiments are made on two tasks and compared to a classical BERT model, and on the BERT-NCE model that is a BERT variant proposed by the authors which is somehow in-between BERT and InfoWord.", "They show that their approach works quite well.", "I have a very mitigated opinion on the paper.", "I) First, I really like the idea of trying to unify different models under the same learning principles, and then show that these models can be seen as specific instances of generic principles.", "But the way it is presented and explained lacks of clarity: for instance in Section 2, some notations are not well defined (e.g what is f?) .", "Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.", "It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.", "So, my suggestion would be to improve the writing of this section to make the message stronger and relevant for a larger audience.", "II) The Infoword model can be seen as a simple instance of word masking based models, and as an extension of deep infomax for sequences (it would be certainly nice to describe a little bit what Deep InfoMax is to facilitate the reading).", "Here again, the article moves from technical details (e.g \"hidden state of the first token (assumed to be a special start of sentence symbol \") without providing formal definitions.", "Having a first loss function after paragraph 4 could help to understand the principle of this model (before restricting the model to n-grams) .", "Moreover, the equation J_DIM seems to be wrong since it contains g_\\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\\psi.", "J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).", "At last ,  after unifying multiple models under one common learning objective, the authors propose to mix two different losses which is strange (the effect of the second term is slightly studied in the experimental section) without allowing us to understand why it is important to have this second loss function and why the first one is not sufficient enough.", "At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).", "Concerning the experimental section, experiments are convincing and show that the model is able to achieve a performance which is close to classical models.", "In my opinion, tis section has to be interpreted as  a proof that the proposed unified vision is a good way to easily define new and efficient models.", "To summarize, the unification under the InfoNCE principle is interesting,", "but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_clarification", "none", "none", "arg-request_clarification", "arg-request_clarification", "none", "arg-request_edit", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 265, "sentences": ["- The authors proposed a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity.", "To address intrinsic protein structural heterogeneity, they explicitly model the imaging operation to disentangle the orientation of the molecule by formulating decoder as a function of Cartesian coordinates.", "- The problem and the approach are well motivated.", "- This reviewer has the following comments:", "1) VAE is known to generate blurred images.", "Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.", "What's your opinion?", "2) What's the relationship between reconstructed performance, heterogeneity of the sample and dimensions of latent space?", "3) It would be interesting to show any relationship, reconstruction error with respect to the number of discrete multiclass.", "4) How is the proposed method generalizable?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_explanation_label"]}
{"abstract_id": 266, "sentences": ["- Summary", "This paper proposes a multi-objective evolutionary algorithm for the neural architecture search.", "Specifically, this paper employs a Lamarckian inheritance mechanism based on network morphism operations for speeding up the architecture search.", "The proposed method is evaluated on CIFAR-10 and ImageNet (64*64) datasets and compared with recent neural architecture search methods.", "In this paper, the proposed method aims at solving the multi-objective problem: validation error rate as a first objective and the number of parameters in a network as a second objective.", "- Pros", "- The proposed method does not require to be initialized with well-performing architectures.", "- This paper proposes the approximate network morphisms to reduce the capacity of a network (e.g., removing a layer), which is reasonable property to control the size of a network for multi-objective problems.", "- Cons", "- Judging from Table 1, the proposed method does not seem to provide a large contribution. For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.", "- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.", "- In the case of the search space II, how many GPU days does the proposed method require?", "- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 267, "sentences": ["This paper explores the relation among the generalization error of neural networks and the model and data scales empirically.", "The topic is interesting, while I was expecting to learn more from the paper, instead of some well-known conclusions.", "If the paper could provide some guidance for model and data selection, that would be an interesting paper for the ICLR audience.", "For instance, how deep should a model be for a classification or regression task?", "What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn?", "What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?", "What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?", "How about the gain of the task performance?"], "labels": ["arg-structuring_summary", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 268, "sentences": ["The authors study the learning dynamics of deep neural networks, which is of fundamental importance but lacks understanding.", "The authors study several dynamics like activation independence, gradient starvation, which gives new insights.", "However, the assumption is too strong.", "There are two main results in the paper:", "1) Through learning, the neurons activates of one class.", "2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape.", "However, there are two strong assumptions: 1. the two data are perfectly separable by linear classifier.", "2.  H2 assumes \"at the beginning of training data points from different classes do not activate the same neurons\".", "This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.", "It sounds to me this assumption implicitly suggests that the algorithm is already ALMOST CONVERGENT.", "If this assumption cannot be weakened, I don't think the paper can be accepted."], "labels": ["none", "none", "none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_quote_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 269, "sentences": ["The underlying motivation for the paper is really interesting and cuts straight to the heart of Deep Learning and strives to unravel the key understanding that we are still to a large extent missing.", "When it comes to clarity and organization I find the paper a bit \"messy\" in that it is a collection of quite a few findings on the very specific topic of binary classification with quite strong assumptions.", "Especially given the very specific nature of the topic I miss a strong and clear path through the paper.", "Unfortunately the paper leaves me with the distinct feeling that there are still a lot of work needed to be able to tell the story about the problem under study.", "Having said that the paper does contain several individual findings.", "Having said that I find the ideas leading up to what the authors refers to as \"gradient starvation\" to be really interesting and that would be a great clear idea to focus on.", "A few concrete questions/comments:", "Can you explain somewhere exactly what you mean when you say \"learning dynamics of deep learning\"? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.", "Given the very specific nature of the topic treated in the paper I find the title of the paper largely misleading.", "The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an \"On\" in the beginning of the title.", "In Corollary 3.3. you characterize the convergence speed in a nice way, but I am missing the link to the behaviors observed empirically in e.g. Fig. 2. What am I missing?", "The final sentence in Section 2 is highly speculative and I find this hard to believe without solid backing.", "The sentence reads \"... and helps develop intuitions about behaviors observed in more general settings.\" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.", "Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless."], "labels": ["none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_clarification", "none", "none", "arg-request_explanation", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 270, "sentences": ["The paper proposes learning Restricted Boltzmann Machines for solving small computational tasks (e.g., 1-bit addition) and composing those RBMs to form a more complex computational module (e.g., 16-bit addition).", "The claim is that such an approach can be more data efficient than learning a single network to directly learn the more complex module.", "Results are shown for addition and factoring tasks.", "- The paper is somewhat easy to follow and the figures are helpful. But the overall organization and flow of ideas can be improved significantly.", "- The term \"combinatorial optimization\" is used in a confusing way -- addition would not usually be called a combinatorial optimization problem.", "- It would be good to understand what benefit does the stochasticity of RBMs provide.", "How do deterministic neural networks perform on the addition and factoring tasks?", "The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across as arbitrary.", "- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.", "After all, the former approach gets a lot more knowledge about the target function built into it.", "It's good that the paper empirically confirms the intuition, but doesn't feel like a significant contribution on its own.", "- The paper would be stronger if it includes more complex tasks, e.g., TSP, and show that the same ideas can be applied to improve the learning a solver for such tasks.", "The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_clarification", "arg-request_explanation", "none", "none", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 271, "sentences": ["This work explores the extent to which the natural image manifold is captured by generative adversarial networks (GANs) by performing walks in the latent space of pretrained models.", "To perform these walks, a transformation vector is learned by minimizing the distance between transformed images and the corresponding images generated from transformed latent vectors.", "It is found that when traversing the latent space of the GAN along the direction of the transformation vector, that the corresponding generated images initially exhibit the desired transform (such as zooming or changing X position), but soon reach a limit where further changes in the latent vector do not result in changes to the image.", "It is observed that this behaviour is likely due to bias in the dataset which the GAN is trained on, and that by exploring the limits of the generator, biases which exist in the original dataset can be revealed.", "In order to increase the extents to which images can be transformed, it is shown that GANs can be trained with an augmented dataset and using a loss function that encourages transformations to lie along linear paths.", "Overall, I would tend towards accepting this paper.", "Improving the amount of control that we have over generative models is desirable for image synthesis, and this paper does a great job of demonstrating the extent to which these models can be manipulated in terms of mimicking basic transforms.", "Figures are very clean and informative, and experimental results are extensive.", "I don't have much else to say about this paper, as I did not find anything in it that concerned me, and the paper answered all of my questions."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 272, "sentences": ["This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards.", "In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards.", "To address this problem, the authors propose an abstract MDP algorithm.", "The algorithm consists of three parts: manager, worker, and discoverer.", "The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states.", "Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation.", "The main strong point of this paper is the experiment section.", "The proposed algorithm outperforms all previous state of the art algorithms for Montezuma\u2019s revenge, Pitfall!, and Private eye over a factor of 2.", "It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.", "In some RL tasks, it is not allowed to access the RAM state.", "================================", "I've read all other reviewers' comments and the response from authors, and decreased the score.", "Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm.", "I agree to other reviewers.", "The algorithm assumptions are strong."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 273, "sentences": ["This work proposes a functional form for the relationship between <dataset size, model size> and generalization error, and performs an empirical study to validate it.", "First, it states 5 criteria that such a functional form must take, and proposes one such functional form containing 6 free coefficients that satisfy all these criteria.", "It then performs a rigorous empirical study consisting of 6 image datasets and 3 text datasets, each with 2 distinct architectures defined at several model scales, and trained with different dataset sizes.", "This process produces 42-49 data points for each <dataset, architecture> pair, and the 6 coefficients of the proposed functional form are fit to those data points, with < 2% mean deviation in accuracy.", "It then studies how this functional form performs at extrapolation, and finds that it still performs pretty well, with ~4.5% mean deviation in accuracy, but with additional caveats.", "Decision: Accept.", "This paper states 5 necessary criteria for any functional form for generalization error predictor that jointly considers dataset size and model size, then empirically verifies it with multiple datasets and architectures.", "These criteria are well justified, and can be used by others to narrow down the search for functions that approximate the generalization error of NNs without access to the true data distribution, which is a significant contribution.", "The empirical study is carefully done (e.g., taking care to subsample the dataset in a way that preserves the class distribution).", "I also liked that the paper is candid about its own limitations.", "A weakness that one might perceive is that the coefficients of the proposed functional form still needs to be fit to 40-ish trained NNs for every dataset and training hyperparameters, but I do not think this should be held against this work, because a generalization error predictor (let alone its functional form) that works for multiple datasets and architecture without training is difficult, and the paper does include several proposals for how this can still be used in practice.", "(Caveat: the use of the envelope function described in equation 5 (page 6) is not something I am familiar with, but seems reasonable.)", "Issues to address:", "- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.", "Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.", "The extrapolation section did provide evidence that there probably isn't /that/ much overfitting, but cross validation would directly address this concern.", "- In addition, the paper provides the standard deviation for the mean deviations over 100 fits of the function as the measure of its uncertainty, but I suspect that the optimizer converging to different coefficients at different runs isn't the main source of uncertainty.", "A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.", "Taking the standard deviation over the deviations measured on different folds of the data would be better measure of uncertainty.", "Minor issues:", "- Page 8: \"differntiable methods for NAS.\" differentiable is misspelled."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 274, "sentences": ["This paper proposed DSGAN which learns to generate unseen data from seen data distribution p_d and its somehow \u201cbroad\u201d version p_{\\hat d} (E.g., p_d convolved with Gaussian).", "The \u201cunseen data\u201d is the one that appears in p_{\\hat d} but not in p_d.", "DSGAN is trained to generate such data.", "In particular, it uses samples from p_d as fake data and samples from p_{\\hat d} as the real one.", "Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.", "The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).", "It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.", "I would also expect more ablation studies about how to pick p_{\\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.", "In terms of writing, the paper is a bit confusing in terms of motivations and notations.", "Overall, the method looks incremental and experimental results are mixed on small datasets so I vote for rejection.", "Note that I am not an expert on GAN/VAE so I put low confidence here."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 275, "sentences": ["Summary:", "The authors propose a SfM model which integrates geometric consistency with a learned pose and depth network.", "An initial estimate of depth and pose are used to construct pose and depth cost volumes, which are then fed into a pose regression and depth refinement network, to produce a new set of cost volumes, and so on.", "In this manner, the pose and depth estimation are improved iteratively.", "Strengths:", "The proposed model is well motivated and shows strong performance and generalization ability on several datasets.", "There are convincing experiments to show the importance of the P-CV network.", "Weaknesses:", "The authors claim that the LM optimization in BA-Net is memory inefficient and may lead to non-optimal solutions.", "It\u2019s not clear to me that the proposed method can guarantee optimality any better.", "It\u2019s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.", "Other comments:", "It would be very interesting to see the test time behavior of the network when it is run with more iterations than it is trained with (say 10 or 20), especially since the depth error does not seem to have stopped decreasing at only 4 iterations.", "It\u2019s not made entirely clear whether the training backpropagates through the update/construction of the pose and depth cost volumes.", "In equation 5, \u201cx\u201d should be \u201ci\u201d."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_clarification", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "none_label", "arg-request_typo_label"]}
{"abstract_id": 276, "sentences": ["## Summary", "The authors propose a method for encoding time features using a sine function with learned phase and frequency.", "They apply this method to several synthetic and real-world datasets.", "Temporal and positional encoding is important to many applications, including NLP, sound understanding and time series modeling, so the topic is certainly of interest.", "However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).", "In addition, the authors compare to a baseline that seems to consist of passing time as a float.", "This seems like a very weak baseline---there are any number of other reasonable ways to encode this.", "Due to the incremental nature of the improvement and the weak baseline, I don't think this paper should be accepted to ICLR.", "## Specific Comments", "1. In Section 2, I find the sentence \"We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information\" really unclear. Could you rephrase it?", "2. Often, positional encodings are used to encode ordering for a model architecture that is not inherently sequential.", "This is the case for the positional encodings in the transformer model.", "Did you try these encodings with non-recurrent architectures?", "3. In Section 5.2, did you mean 'fixing t2v(\\tau)[n] = sin(2\\pi n \\tau / 16)'? i.e. I think it's missing a 'tau'", "4. In Section 5.2 \"Fixed frequencies and phase shifts\" you compare Time2Vec to a fixed set of frequencies.", "Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.", "The authors compare these methods only on Event-MNIST and only for 16 frequencies.", "I would like to see this comparison expanded.", "5. Could you clarify exactly how time is encoded for LSTM + T? Are you, in fact, just passing a float value? How is this encoded for each data set? For example, the \"times\" for Event-MNIST is always [0, 783] while the SOF data has timestamps. What is the encoding scheme for each?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "none", "none", "arg-request_experiment", "arg-request_typo", "none", "none", "none", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 277, "sentences": ["The paper presents a an interesting novel approach to train neural networks with so called peer regularization which aims to provide robustness to adversarial attacks.", "The idea is to add a graph neural network to a spatial CNN.", "A graph is defined over similar training samples which are found using a Monte Carlo approximation.", "The regularization using graphs reminds me of recent work at ICML on semi-supervised learning (Kamnitsas et al. (2018) Semi-supervised learning via compact latent space clustering) which is using a graph to approximate cluster density which acts as a regularizer for training on labelled data.", "The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.", "Memory and computation limitations are mentioned, but not sufficently discussed.", "It would be good to add further details on practical limitations.", "Experiments are limited to benchmark data using MNIST, CIFAR-10, CIFAR-100.", "Comprehensive evaluation has been carried out with insightful experiments and good comparison to state-of-the-art.", "Both white- and black-box adversarial attacks are explored with promising results for the proposed approach.", "However, it is difficult to draw conclusions for real-world problems of larger scale.", "The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.", "It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done.", "Is there any hope this could be applied to problems like 3D imaging data or videos?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit", "arg-structuring_quote", "none", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 278, "sentences": ["This paper attempts to mitigate catastrophic problem in continual learning.", "Different from the previous works where episodic memory is used, this work adopts the generative replay strategy and improve the work in (Serra et al., 2018) by extending the output neurons of generative network when facing the significant domain shift between tasks.", "Here are my detailed comments:", "Catastrophic problem is the most severe problem in continual learning since when learning more and more new tasks, the classifier will forget what they learned before, which will be no longer an effective continual learning model.", "Considering that episodic memory will cost too much space, this work adopts the generative replay strategy where old representative data are generated by a generative model.", "Thus, at every time step, the model will receive data from every task so that its performance on old tasks will retain.", "However, if the differences between tasks are significant, the generator cannot reserve vacant neurons for new tasks or in other words, the generator will forget the old information from old tasks when overwritten by information from new tasks.", "Therefore, this work tries to tackle this problem by extending the output neurons of the generator to keep vacant neurons to retain receive new information.", "As far as I am concerned, this is the main contribution of this work.", "Nevertheless, I think there are some deficiencies in this work.", "First, this paper is not easy to follow.", "The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.", "For example, in Section 4.1, I am not sure the equation (3), (4), (5), (6) are the contributions of this paper or not since a large number of citations appear.", "Second, the authors mention that to avoid storing previous data, they adopt generative replay and continuously enlarge the generator to tackle the significant domain shift between tasks.", "However, in this way, when more and more tasks come, the generator will become larger and larger.", "The storing problem still exists.", "Generative replay also brings the time complexity problem since it is time consuming to generate previous data.", "Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.", "Third, the datasets used in this paper are rather limited.", "Three datasets cannot make the experiments convincing.", "In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.", "I hope the author could explain this phenomenon.", "Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.", "Fourth, there are some grammar mistakes and typos.", "For example, there are two \"the\" in the end of the third paragraph in Related Work.", "In the last paragraph in Related Work, \"provide\" should be \"provides\".", "In page 8, the double quotation marks of \"short-term\" are not correct.", "Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.", "The proposed method is also heuristic and lacks promising guarantee."], "labels": ["arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_quote", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_quote", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_edit", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 279, "sentences": ["The paper explores and experiments on extrapolating attributes of images produced by GANs by manipulating their representations in latent space.", "Attribute manipulation is done by predicting latent space walks (linear or non-linear) and is learned in a self-supervised way by using augmented outputs of a pretrained GAN as target images.", "Authors experimentally show dependence of range of possible attribute manipulations on the diversity of the dataset in terms of that attribute as well as propose techniques to improve it.", "Suggested concepts are explained in a clear way with extensive experiments confirming the findings.", "Techniques proposed for improving \"steerability\" of GANs are backed up by both qualitative and quantitative analysis, although missing experiments on more sophisticated datasets than MNIST.", "Overall,  I recommend to accept this paper.", "Several questions I would like the authors to address to make some details more clear and the paper more complete:", "1. Why the color distribution of generated images is evaluated on a sampled subset of pixels, not full images? (\"Quantifying steerability\" section.)", "2. On Figure 6, which classes are outlying on transformation limitation / data variability plots (bottom-right corner) and how it may be explained?", "3. While StyleGAN can not preserve geometry of objects for shift in location-based attributes, when walks are learned in the W space, have you experimented on manipulating those attributes with z space? What are the results?", "Other minor flaws include", "1. Pictures in Fig. 2 are mixed up between G(z) and G(z + \\alpha w)", "2. In Fig. 2 edit(G(z, \\alpha)) -> edit(G(z), \\alpha))", "3. In eq. (2) f^n(z) -> G(f^n(z))", "4. In eq. (6) +\\alpha^* -> -\\alpha^*"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "arg-request_edit", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 280, "sentences": ["This paper first gives a concise yet precise summary of maximizing one of variational lower bounds of mutual information, InfoNCE, then it provides an alternative view to explain case by case why word embedding Skip-gram, BERT, XLNet work in practice can be viewed by InfoNCE framework, thus we have a good understand for these methods.", "Moreover it introduces a self-learning method  that maximizes the mutual information between a global sentence representation and n-grams in the sentence based on deep InfoMax framework instead.", "Experiments show that it is better then BERT and BERT-NCE. It's known that InfoNCE increases bias but reduce variance, the same is true for deep InfoMax. Do you observe this in your experiments? If so, please provide.", "The paper is well-written and easy to follow.", "The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.", "In equations 1 and 2, should a, b be written in capital? Since they represent random variables."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 281, "sentences": ["The paper has two distinct parts.", "In the first part (section 2) it studies the volume of preimage of a ReLU network\u2019s activation at a certain layer as being singular, finite, or infinite.", "This part is an extension of the work in the study of (Carlsson et al. 2017).", "The second part (section 3) builds on the piecewise linearity of a ReLU network\u2019s forward function.", "As a result, each point in the input space is in a polytope where the model acts linearly.", "In that respect, it studies the stability of the linearized model at a point in the input space.", "The study involves looking at the singular values of the linear mapping.", "The findings of the paper are non-trivial and the implications potentially interesting. However, I have some concerns about the study.", "There is a key concern about the feasibility of the numerical analysis for the first part.", "That is, a layer-by-layer study can have a computational problem where the preimage is finite at each layer but can become infinite by the mapping of the preceding layers.", "In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.", "As for the second part, the authors mention the increase in the dimensionality of the latent space in the current deep networks.", "However, this observation views convolutional networks as MLPs.", "However, there is more structure in a convolutional layer\u2019s mapping function.", "The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.", "All in all, while there are some concerns and the contributions are not entirely novel, the reviewer believes the findings of the paper is generally non-trivial and shed more light on the inner workings of the ReLU networks and is thus a valuable contribution to the field."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 282, "sentences": ["This paper proposes a novel transfer learning mechanism through credit assignment, in which an offline supervised reward prediction model is learned from previously-generated trajectories, and is used to reshape the reward of the target task.", "The paper introduces an interesting new direction in transfer learning for reinforcement learning, that is robust to the differences in the environtment dynamics.", "I have the following questions/concerns.", "1. The authors insist that their fous is on transfer and not competing on credit assignment. If accurate credit assignment leads to better transfer, shouldn't achieving the best credit assignment model (thus competing in credit assignment) lead to better transfer results?", "2. What effect does the window size for transforming states to observations have on the performance of SECRET?", "3. On a high-level, how does SECRET compare to transfer through relational deep reinforcement learning: https://arxiv.org/abs/1806.01830? Relational models use self-attention mechanisms to extract and exploit relations between entities in the scenes for better generalization and transfer.", "Although SECRET intentionally avoids using relations, I think a discussion around relational models for RL is warranted.", "I'm curious what happens if SECRET is allowed to exploit relations in the environment.", "4. What happens if the reward model uses very few trajectories and is not able to predict good rewards? Does transfer through credit assignment become detrimental? In other words, in a real-world scenario, how I do know when to start using SECRET, or when am I better off learning from environment rewards alone? Especially given that SECRET requires 40000 trajectories in the source domain.", "5. Are the samples generated in the target domain for collecting attention weights included in the number of episodes when evaluating SECRET? For example, in Figure 4. I believe the number of episodes required to collect those target samples should be added to the number of episodes when using SECRET since the agent must interact with the environment in the target domain.", "6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-request_clarification", "arg-request_explanation", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 283, "sentences": ["The authors study properties of the learning behavior of non-linear (ReLu) neural networks.", "In particular, their main focus is on binary classification for the linear-separable case, when optimization is done using gradient descent minimizing either binary entropy or hinge loss.", "There are 3 main results in the paper:", "1) During learning, each neuron only activates on data points of one class: hence (due to ReLu), each neuron only updates its weights when seeing data points from that class. The authors refer to this property as \"Independent modes of learning\", suggesting that the learning of parameters of the network is decoupled between the two classes. 2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape: slow improvement at the beginning, followed by a period of fast improvement, followed by another plateau.", "3) Most frequent features, if discriminative, can prevent learning of other, less frequent, features.", "Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating \"at the beginning of training data points from different classes do not activate the same neurons\". Even for a shallow net, the authors are essentially assuming that the first layer of weights W is such that each row w is already a hyperplane separating the two classes after initialization (wx > 0 for all x belonging to one class and wx' < 0 for x' in the other class).", "In other words, at initialization, the first layer is already correctly classifying all data points.", "This is of course an extremely stringent assumption that doesn't hold in practice (eg, the probability of such an initialization shrinks to zero exponentially in the number of dimensions and in the number of neurons).", "Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.", "Pros:", "- Authors consider a non-linear (ReLu) neural network, as opposed to the analysis of Save et al which only considers linear nets.", "- The fundamentally different behavior between Hinge and binary entropy loss is interesting, and worth analyzing further.", "- Sigmoidal shape of classification error as a function of number of iterations is inline with what is seen in practice.", "However, I believe the assumptions needed to show this point force the analysis to only characterize learning close to convergence.", "Minor Cons (apart from major concern above):", "- Theorem 3.2: \"[...] converges at a speed proportional to [...]\". Isn't \\bar{u}_t logarithmic (non-linear) in t?", "- Theorem 3.2: Even if strong, I don't mind the assumption on a dataset merely consisting of two (weighted) data points. I would suggest to simulate this case without putting any condition on the initialization of the weights (ie, without assumptions H1-H2), and compare the empirical shape of the classification error with the one you obtain analytically in Figure 2 Right.", "- Theorem 3.2 Interpretation: unfinished sentence \"We can characterize the convergence speeds more quantitatively with the\"", "- Theorem 4.1: Can you give an intuition or lower/upper bounds for u(t) for the Hinge case, to make evident its difference from the binary entropy case (where u(t) ~ log(t))", "- Gradient starvation, Kaggle experiment: I'm not too convinced about the novelty/usefulness of this result. In the end, even a decision tree stump would stop growing after learning the dark/light feature as a discriminator.", "What I'm trying to say is that \"gradient starvation\" is a more general problem that really doesn't have to do with gradient descent.", "Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_experiment", "arg-request_typo", "arg-request_explanation", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 284, "sentences": ["The paper proposes to combine several smaller, pretrained RBMs into a larger model as a way to solve combinatorial optimization problems.", "Results are presented on RBMs trained to implement binary addition, multiplication, and factorization, where the proposed approach is compared with the baseline of training a full model from scratch.", "I found the paper confusing at times.", "It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader. For instance, there\u2019s a brief exposition of the connection between Boltzmann machines and combinatorial optimization problems: the latter is mapped onto the former by expressing constraints as a fixed set of Boltzmann machine weights and biases, and low-energy states (i.e. more optimal solutions) are found by sampling from the model, which involves no training. What\u2019s less clear to me is what kinds of combinatorial optimization problems can be mapped onto the RBM *training* problem.", "The paper states that the problem of training \"large modules\" is \"equivalent to solving the optimization problem\", but does not explain how.", "Similarly, the paper mentions that the \"general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem\", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.", "A concrete example is provided in the Experiments section: the authors propose to implement invertible (reversible?) boolean logic circuits by combining smaller pre-trained RBMs which implement certain logical operations into larger circuits.", "I have two issues with the chosen example: 1) the connection with combinatorial optimization is not clear to me, and 2) it\u2019s not very well explained.", "As far as I understand, these reversible boolean logic operations are expressed as sampling a subset of the RBM\u2019s inputs conditioned on another subset of its inputs.", "An example is presented in Figure 3 but is not expanded upon in the main text.", "I\u2019d like the authors to validate my understanding:", "An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder\u2019s inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin].", "After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling.", "The alternative to this, which is examined in the paper, is to train individual XOR, AND, and OR gates in the same way and compose them into a complete binary adder circuit as prescribed by Section 3.", "I think the paper has the potential to be a lot more transparent to the reader in explaining these concepts, which would avoid them spending quite a bit of time inferring meaning from figures.", "I\u2019m also confused by the presentation of the results. For instance, I don\u2019t know what \"log\", \"FA1\", \"FA2\", etc. refer to in Figure 6.", "Also, Figure 6 is referenced in the text in the context of binary multiplication (\"[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6\"), but presents results for addition and factorization only.", "The way I see it, implementing reversible boolean logic circuits using RBMs is an artificial problem, and the key idea of the paper -- which I find interesting -- is that in some cases it appears to be possible to combine RBMs trained for sub-problems into larger RBMs without needing to fine-tune the model.", "I think there are interesting large-scale applications of this, such as building an autoregressive RBM for image generation by training a smaller RBM on a more restricted inpainting task.", "The connection to combinatorial optimization, however, is much less clear to me."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 285, "sentences": ["This paper proves a theoretical limitation of narrow-and-deep neural networks.", "It shows that, for any function that can be approximated by such networks, its level set (or decision boundary for binary classification) must be unbounded.", "The conclusion means that if some problem's decision boundary is a closed set, then it cannot be represented by such narrow networks.", "The intuition is relatively simple.", "Under the assumptions of the paper, the neural network can always be approximated by a one-to-one mapping followed by a linear projection.", "The image of the one-to-one mapping is homeomorphic to R^n, so that it must be an open topological ball.", "The intersection of this open ball with a linear hyperplane must include the boundary of the ball, thus it extends to infinity in the original input space.", "The critical assumptions here, which guarantees the one-to-one property of the network, are: 1) the network is narrow, and 2) the activation function can be approximated by a one-to-one function.", "The authors claim that 2) captures a large family of activation functions.", "However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.", "As a concrete example, the simple function f(x1,x2) = x_1^2 + x_2^2 has bounded level sets, but it can be represented by a narrow 2-layer neural network with the quadratic activation.", "Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.", "It is also not clear how this theoretical result can shed insight on the empirical study of neural networks."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 286, "sentences": ["Please consider this rubric when writing your review:", "1. Briefly establish your personal expertise in the field of the paper.", "2. Concisely summarize the contributions of the paper.", "3. Evaluate the quality and composition of the work.", "4. Place the work in context of prior work, and evaluate this work's novelty.", "5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.", "6. Provide a summary judgment if the work is significant and of interest to the community.", "1. I am a researcher working at the intersection of machine learning, computational neuroscience and biological vision. I have experience with neural network models and visual neurophysiology.", "2. This paper develops and tests an adaptive homeostatic algorithm for unsupervised visual feature learning (for example for learning models of early visual processing/V1).", "3.The work spends a lot of pages describing the general problem of unsupervised feature learning and the history of the base algorithms.", "The literature review is quite extensive.", "The new content appears to be in section 2.2 (Histogram Equalization Homeostasis - HEH), where a simple idea to keep all units with balanced activity over the set of natural images.", "The authors also develop a computationally cheaper version they call HAP (Homeostasis on Activation Probability) The authors show that their F function is optimized quicker with the HEH and HAP algorithms.", "I would like to see how these curves vary with the number of neurons (e.g. can you add X% more neurons and get similar convergence speed -- and if so which is more computationally costly)?", "4. Many groups have developed various homeostatic algorithms for unsupervised learning, though I have not seen this exact one before.", "5.  The experiments reveal the resulting receptive fields and show the decrease in the F function (error function).", "The resulting receptive fields do not seem that different to me between the different methods.", "I am also not that convinced that the faster convergence as a function of learning step is that important especially as the learning steps may be more computationally expensive for this method.", "6. I am not sure how interesting this work will be for the ICLR audience, as it is not clear how important the faster convergence and more even utilization of neurons is (and how it would compare computationally with just having more neurons)."], "labels": ["none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_experiment", "none", "arg-structuring_quote", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 287, "sentences": ["This paper develops a multi-arm bandit-based algorithm to dynamically adapt the exploration policy for reinforcement learning.", "The arms of the bandit are parameters of the policy such as exploration noise, per-action biases etc.", "A proxy fitness metric is defined that measures the return of the trajectories upon perturbations of the policy z; the bandit then samples perturbations z that are better than the average fitness of the past few perturbations.", "I think this paper is just below the acceptance threshold.", "My reservations and comments are as follows.", "1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments. For instance, the authors use Rainbow as  the base algorithm upon which they add on the exploration. Rainbow itself is an extremely complicated algorithm, how can one be certain that the improvements in performance are caused by the improved exploration and not a combination of the bandit\u2019s actions with the specifics of Rainbow?", "2. I don\u2019t understand Figure 4. The score defined in Appendix is the average over games for which seed performs better.", "Why is the random seed being used to compare the performance of different arms? Do you instead mean that s and s\u2019 are two values of the arm in Figure 4? If not, how should one interpret Figure 4, no fixed arm is always good because the performance varies across the seeds. The curated bandit does not seem to be doing any better than a fixed arm.", "I have a few more questions that I would like the authors to address in their rebuttal or the paper.", "1. The proxy f(z) does not bear any resemblance to LP(z). Why discuss the LP(z) then. The way f(z) is defined, it is just the value function averaged over perturbations  of the policy. If one were to consider z as an additional action space that is available to the agent during exploration, f(z) is the value function itself. The exploration policy is chosen not to maximize the E_z [f(z)] directly but to maximize the lower bound in Markov\u2019s inequality (P(f(z) >= t) <= E_z [f(z)]/t) in Section 4.", "2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?", "3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters. In this aspect, the auto-tuner for exploration is a plug-and-play procedure in other RL algorithms.", "4. From Figure 6 and Figure 8-11, it looks like the bandit is more or less on par with fixed exploration policies. What is the benefit of the added complexity?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "arg-request_clarification", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 288, "sentences": ["The paper proposes using the nested CRP as a clustering model rather than a topic model.", "The clustering is on the latent vector input into a neural network for generating the observation.", "A variational approach is derived.", "The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.", "A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.", "From the generative model it seems every data point has its own Dirichlet vector on levels.", "For topic models this makes sense since that vector is then drawn from multiple times (once per word) from a Discrete, so there's a distribution to actually learn.", "My understanding is that this isn't being done here."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 289, "sentences": ["Summary", "This paper provides an interesting application of GAN which can generate the outlier distribution of training data which forces generator to learn the distribution of the low probability density area of given data.", "To show the effectiveness of the method, the author intuitively shows how it works on 2-D points data as well as the reconstructed Mnist dataset.", "Additionally, this approach reaches a comparable performance on semi-supervised learning and novelty detection task.", "Paper Strengths", "1. The idea of this paper is novel, and the implementation of this method is easily interacted with any GAN model.", "Also, due to its concise structure compared to the existing method, it saves more computational memory and is time efficiency.", "Paper Weaknesses", "1. Experimental settings are clear, however, what makes me confused is that the construction for p_{\\bar{d}} is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images.", "2. The model seems to be sensitive to the hyper-parameter \\alpha, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 290, "sentences": ["After reading the authors' response, I'm revising my score upwards from 5 to 6.", "The authors propose a defense against adversarial examples, that is inspired by \"non local means filtering\".", "The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be \"filtered away\" by using features from other images.", "While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.", "Some examples of verifying this are:", "1. How does varying the number of nearest neighbors change the network behavior?", "2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?", "3. Does just simple filtering of the feature map, say, by local averaging, perform equally well?", "4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?", "Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training.", "It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training."], "labels": ["none", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_clarification", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 291, "sentences": ["The paper introduces a new approach to combine small RBMs that are pretrained in order to obtain a large RBM with good performance.", "This will bypass the need of training large RBMs and suggests to break them into smaller ones.", "The paper then provides experimental evidence by applying the method on \"invertible boolean logic\".", "MCMC is used to find the the solution to large RBM and compare it against the combined solutions of smaller RBMs.", "The paper motivates the problem well, however, it is not well-written and at times it is hard to follow.", "The details of the approach is not entirely clear and no theoritcal results are provided to support the approach. For instance, in the introduced approach, only an example of combination is provided in Figure 1.", "It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.", "From the experimental perspective, the experimental evidence on \"invertible boolean logic\" does not seem to be very convincing for validating the approach.", "Additionally, the details of the settings of the experiments are not fully discussed. For example, what are the atomic/smaller problems and associated RBMs? what is the larger problem and how is the corresponding RBM obtained?", "Overall, the paper seems to be a report consisting of a few interesting observations rather than introducing a solid and novel contribution with theoretical guarantees.", "Remark:", "The term \"Combinatorial optimization\", which is used in the title and throughout the body of paper, sounds a bit confusing to the reviwer. This term is typically used in other contexts.", "Typos: *", "* Page 2 -- Paragraph 2: \"Therefore, methods than can exploit...\" *", "* Page 3 -- 2nd line of math: Super-scripts are missing for some entries of the matrices W^A and W^{A+B} *", "* Page 5 -- Last paragraph: \"...merged logical units is more likly to get get stuck in a ...\" *", "* Page 5 -- Last paragraph: \"...and combining their distributions using the mulistart heuristic...\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_edit_label"]}
{"abstract_id": 292, "sentences": ["A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude.", "They used different model compression techniques in this framework to show the effectiveness of the proposed method.", "This paper proposes a framework intending to use fewer hardware resources without compromising the model accuracy.", "However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.", "Therefore, it is not clear how the proposed framework is helping the model compression techniques."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 293, "sentences": ["This paper proposes LEMONADE, a random search procedure for neural network architectures (specifically neural networks, not general hyperparameter optimization) that handles multiple objectives.", "Notably, this method is significantly more efficient more efficient than previous works on neural architecture search.", "The emphasis in this paper is very strange. It devotes a lot of space to things that are not important, while glossing over the details of its own core contribution. For example, Section 3 spends nearly a full page building up to a definition of an epsilon-approximate network morphism, but this definition is never used. I don't feel like my understanding of the paper would have suffered if all Section 3 had been replaced by its final paragraph.", "Meanwhile the actual method used in the paper is hidden in Appendices A.1.1-A.2.", "Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper.", "That said, those complaints are just about presentation and not about the method, which seems quite good once you take the time to dig it out of the appendix.", "I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?", "Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?", "It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.", "How could scaling be handled?"], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_clarification", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 294, "sentences": ["This paper proposed the DSGAN model to generate unseen data.", "The intuition based on standard GAN is straightforward and makes sense.", "The paper is well written, especially the case studies illustrate the idea clearly.", "The designing of p_{\\bar{d}} also presented the limitation of this method.", "Two main discussed applications, semi-supervised learning and novelty detection are important in machine learning.", "In general, this is an interesting paper.", "However, my concern is about the experiments.", "As a generative model for unseen data, I would like to see the generated results, which is more convincing.", "Only the 1/7 examples of MNIST dataset are provided in case studies.", "I am wondering for more complicated images, how is the performance?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 295, "sentences": ["The idea of image classification based on patch-level deep feature in the BoF model has been studied extensively.", "Just list few of them:", "Wei et al. HCP: A Flexible CNN Framework for Multi-label Image Classification, IEEE TPAMI 2016", "Tang et al. Deep Patch Learning for Weakly Supervised Object Classification and Discovery, Pattern Recognition 2017", "Tang et al. Deep FisherNet for Object Classification, IEEE TNNLS", "Arandjelovi\u0107 et al. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition, CVPR 2016", "The above papers are not cited in this paper.", "There are some unique points.", "This work does not use RoIPooling layer and has results on ImageNet.", "But, the previous works use RoIPooling layer to save computations and works on scene understanding images, such as PASCAL.", "Besides, the paper uses the smallest patch among all the patch-based deep networks.", "It is interesting.", "I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 296, "sentences": ["This in an interesting paper as it tries to alleviate the burden of hyper-parameters tuning for exploration strategies Deep Reinforcement learning.", "The paper proposes an adaptive behaviour in order to shape the data generation process for effective learning.", "The paper considers a behaviour policy that is parametrized by a set of variables z called modulations: for example the Boltzmann softmax temperature, the probability epsilon for epsilon-greedy, per-action biases, ..", "The author frame the modulations search into a non-stationary multi-armed bandit problem and proposes to adapt the modulations according to a proxy to the learning progress.", "The author provides thorough experimental results.", "Comments:", "- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.", "- The proposed proxy is simply the empirical episodic return.", "It is not well explained in the paper how this proxy correlates with the Learning progress criteria.", "- The proxy seems to encourage selecting modulations that lead to generate most rewarding trajectories.", "How this proxy incentives the agent to explore poorly-understood regions? In other terms, how this proxy help to tradeoff between exploration and exploitation ?", "-  The modulation adaptation problem is framed into non-stationary multi-armed bandit problem but the authors present a heuristic to solve it instead of using provably efficient bandit algorithm such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017) cited in the paper.", "- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me. They estimate a certain probability at time step t by empirical frequency based on data from previous time steps. But as the parameters change during the learning, the f_t\u2019(z) at time t\u2019 < t is not distributed as f_t(z). This introduces a biases in the estimate.", "- I appreciate the thorough empirical results and ablation studies in the main paper and the appendix. They are really interesting.", "- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.", "Is it a baseline with the best hyperprameters in hindsight?", "-  From the plots of learning curves in appendix, the proposed methods doesn\u2019t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_experiment_label"]}
{"abstract_id": 297, "sentences": ["The paper is well-written with a few figures to illustrate the ideas and components of the proposed method.", "However, one of the main components in the proposed method is based on Tulsiani et al. CVPR'18.", "The remaining components of the proposed method are not very new.", "Hence, I am not very sure whether the novelty of the paper is significant.", "Nevertheless, the performance of the proposed method is fairly good outperforming all baseline methods.", "I also have a few questions:", "1. How did you get the instance boxes, union boxes, and binary masks in testing?", "2. What are the training and inference time?"], "labels": ["none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 298, "sentences": ["The paper presents a novel idea of generating discrete data such as graphs that is conditional on input data to control the graph structure that is being generated.", "Given an input graph, the proposed method infers a target graph by learning their underlying translation mapping by using new graph convolution and deconvolution layers to learn the global and local translation mapping.", "The idea of learning generic shared common and latent implicit patterns across different graph structure is brilliant.", "Their method learns a distribution over graphs conditioned on the input graph whilst allowing the network to learn latent and implicit properties.", "The authors claim that their method is applicable for large graphs.", "However, it seems the experiments do not seem to support this.", "It is not clear how the noise is introduced in the graphs.", "I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.", "It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.", "Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?", "Towards this, how does the computational complexity scale wrt to the connectedness?", "A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?", "What is the L1 norm applied on?", "I did not completely follow the arguments towards directed graph deconvolution operators.", "There is lack of clarity and the explanation seems lacking in parts in this particular section; especially since this is the key contribution of this work", "Typo:. The \u201cInf\u201d in Tabel 1"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_experiment", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_typo_label"]}
{"abstract_id": 299, "sentences": ["The paper presents a variational inference approach for locally linear dynamical models.", "In particular,  the latent dynamics are drawn from a Gaussian approximation of the parent variational distribution, enabled by Laplace approximations with fixed point updates, while the parameters are optimized the resulting stochastic ELBO.", "Experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.", "Quality: The experiments appear to be well designed and support the main claims of the paper.", "Clarity: The clarity is below average.", "In Section 2 the main method is introduced.", "However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.", "It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way.", "I also struggled a little to understand what is the difference between forward interpolate and filtering.", "Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted.", "In the tasks considered, the proposed method demonstrates convincing advantages over its competitors.", "Significance: The method shall be applicable to a wide variety of sequential data with nonlinear dynamics.", "Overall, this appears to be a board-line paper with weak novelty.", "On the positive side, the experimental validation seems well done.", "The clarity of this paper needs to be strengthened.", "Minor comments:", "- abstract: uncover nonlinear observation? -> maybe change \"observation\" to \"latent dynamics\"?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_quote", "none", "arg-request_edit", "arg-request_clarification", "none", "none", "none", "none", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label"]}
{"abstract_id": 300, "sentences": ["This paper proposes an autocompletion model for UI layout based on adaptations of Transformers for tree structures and evaluates the models based on a few metrics on a public UI dataset.", "I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:", "1) There is no clear rationale on why we need a new model based on Transformers for this task.", "What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?", "Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.", "2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design. UI layout is about visual and functional representation of an application so if one is seeking to evaluate different models, they need to relate to those."], "labels": ["arg-structuring_summary", "none", "none", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 301, "sentences": ["The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques.", "Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process.", "Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors).", "Pros:", "- The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization.", "Cons:", "- The idea is a simple extension of existing work.", "- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 302, "sentences": ["Summary:", "The paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper.", "It also provides an analysis on the mode collapse and lack of stability of classical GANs.", "The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties.", "Positive points:", "The paper is interesting to read and well illustrated.", "An experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.", "Points to improve:", "If I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting.", "Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.", "WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018 to name only a few.", "The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction.", "The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments", "The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.", "The imagenet experiment lacks details."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 303, "sentences": ["This paper propose to study the generalization properties of GANs through interpolation.", "They first propose to learn a linear (and non-linear) interpolation in the latent space for a specific type of image transformation for example zoom, translation, rotation, luminance, etc... They show that linear interpolation in GANs can produce really realistic images along the path and enable to control and transform generated images to some extent.", "They then propose to measure to what extent the generated images can be transformed without \"breaking\".", "Finally they show that the quality of the interpolation can be improved by learning the interpolation and generator jointly.", "I'm in favour of accepting this paper.", "The paper is well written and organized.", "The experiments and observations are very interesting and really illustrate the generalization capacity of GANs.", "Main argument:", "- I think those observations are very valuable to the community and are a good way to get insight into the capabilities of GANs.", "This also give interesting informations about the different bias present and learnt in the dataset.", "This could also lead to very nice applications.", "- The interpolation with StyleGAN and BigGAN seem to give qualitatively very different results.", "It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.", "- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.", "Minor comments:", "- In appendix A.2 the authors explain how the range of $\\alpha$ is set for the different experiments.", "However it's not clear how is this range used in practice ? Do you sample uniformly $\\alpha$ in this range to train the linear interpolation ?", "Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?", "- There is a typo in equation 6", "- In figure 6: What does the right figure represent ? especially what are the different colours ?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "none", "arg-request_clarification", "arg-request_clarification", "arg-request_typo", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_typo_label", "arg-request_explanation_label"]}
{"abstract_id": 304, "sentences": ["The paper tackles Structure from Motion, one of the canonical problems in computer vision, and proposes an approach that brings together geometry and physics on one hand and deep networks on the other hand.", "Camera unprojection and warping (of depth maps and features) are used to build a cost volume onto hypothetical planes perpendicular to the camera axis.", "Similarly, various camera poses are sampled around an initial guess.", "A deep network regresses form the cost volume to a camera pose and a depth map.", "The method can be applied iteratively, using the outputs of the current stage as the initial guess of the next one.", "Training is supervised, and the the results are evaluated on multiple datasets.", "I am inclined to recommend accepting the paper for publication, because it addresses a canonical problem, outperforms the state of the art on multiple datasets and brings together geometry / physics and deep learning, which is IMO very a promising and underexplored direction.", "I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.", "1. In Sec. 3 the Authors write \"We then sample the solution space for depth and pose respectively around their initialization\".", "However in Sec 3.2 they write \"we uniformly sample a set of L virtual planes {dl} Ll=1 in the inverse-depth space\".", "In what way are the planes \"around their initialization\"? If the initial depth map spans over multiple orders of magnitude, will the planes be uniformly sampled between the minimum and maximum disparity of the initial map?", "If yes, it seems that the initial depth map is not really needed, just its minimum and maximum value is needed, but then how come the method can be applied iteratively with respect to depth?", "2. The Authors mention that depth maps are warped onto the virtual planes using differentiable bilinear interpolation.", "Is there a mechanism to protect from interpolating across discontinuities? If no, were bleeding edge artifacts observed?", "3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.", "Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.", "For example, \"our network learns a cost volume of size L \u00d7 W \u00d7 H using several 3D convolutional layers with kernel size 3 \u00d7 3 \u00d7 3\"", "- more details about this network are needed, as well as the others in the paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_clarification", "arg-request_clarification", "none", "arg-request_explanation", "none", "arg-request_explanation", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_quote_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_clarification_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 305, "sentences": ["This paper discusses the addition of a regularizer to a standard sparse coding/dictionary learning algorithm to encourage the atoms to be used with uniform frequency.", "I do not think this work should be accepted to the conference for the following reasons:", "1: The authors show no benefit of this scheme except perhaps faster convergence.", "If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.", "SPAMS (http://spams-devel.gforge.inria.fr/) can train a model on image patches as the authors do here in a few tens of seconds on a modern computer.", "On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.", "In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.", "It is not even clear that the final compression of the baselines would not be better.", "Even if they did show these convincingly, it is not obvious to me that it is valuable; the authors need to *show* that uniform usage is desirable.", "2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.", "The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset .", "Even without the \"train to convergence\" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_experiment", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 306, "sentences": ["The paper gives a big picture view on training objectives used to obtain static and contextualized word embeddings.", "This is very handy since classical static word embeddings, such as SGNS and GloVe, have been studied theoretically in a number of works (e.g., Levy and Goldberg, 2014; Arora et al., 2016; Hashimoto et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019; Assylbekov and Takhanov, 2019), but not much has been done for the modern contextualized embedding models such ELMo and BERT - I personally know only the work of Wang and Cho (2019), and please correct me if I am wrong.", "\"There is nothing as practical as a good theory\", and the authors confirm this statement: their theory suggests them to modify the training objective of the masked language modeling in a certain way and this modification proves to benefit the embeddings in general when evaluated on standard tasks.", "I don't have any major issues to raise.", "A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 307, "sentences": ["Review", "This paper discusses invariances in ReLU networks.", "The discussion is anchored around the observation that while the spectral norm of neural networks layers (their product bounds the Lipschitz constant) has been investigated as a measure of robustness of nets, the smallest singular values are also of interest as these indicate directions of invariance.", "The paper consists mostly of a theoretical analysis with little empirical support, focusing on a property of matrices called omnidirectIonality.", "The definition given seems weird \u2014 an A \\in R^{m \\times n} is omnidirectional if there exists a unique x \\in R^n such that Ax \\leq 0.", "If there is a *unique* x then that x must be 0.", "Else if there were a nonzero x for which Ax \\leq 0, then A(cx) also \\leq 0 for any positive scalar 0 and thus x is not unique .", "Moreover if x must be equal to 0 Ax \\leq 0 and at that point Ax = 0, then that means there exists no x for which Ax < 0, so why not just say this outright?", "Perhaps a cleaner definition would just be \u201cA is full rank and there does not exist any X such that Ax < 0?", "Also perhaps better to use the curly sign for vector inequality.", "Overall the paper, while interesting is unacceptably messy.", "The first two pages have no paragraph breaks!", "!! This means either that the author are separating paragraphs with \\\\ \\noindent or that they have modified the style file to remove paragraph breaks to save space.", "Either choice is unreadable and unacceptable.", "The paper is also littered with typos and vague statements (many enumerated below under *small issues*).", "In this case, they add up to make a big issue.", "The notation at the top of page 4 \u2014 see (1) and (2) \u2014 comes out of nowhere and requires explanation.", "|_{y>0} x + b |_{y>0}  <\u2014 what is the purpose of the subscripts here? Why is this notation never introduced?", "Ultimately this paper focuses on the question on whether the pre-image of a ReLU layer can be concluded (based on the post-image) to be a singleton,  a compact polytope, or if it has infinite volume.", "The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn\u2019t this be more clearly introduced and notated?).", "Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn\u2019t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I\u2019ll be happy to revisit it and re-evaluate my score.", "Small issues", "The following is a *very* incomplete list of small bugs found in the paper:", "\u201cFrom a high-level perspective both of these approaches\u201d --> missing comma after \u201cperspective\u201d", "\"as well as the gradient correspond to the highest possible responds for a given perturbation\" --> incomprehensible \"corresponding?\" \"possible responds?\" do you mean \"response\", and if so what is the precise technical meaning here?", "\"analyzing the lowest possible response\" what does \"response' mean here?", "\"We provide upper bounds on the smallest singular value\" -- the singular value of what? This hasn't been stated yet.", "\"reverse view on adversarial examples\" --- what this means isn't clear from the preceding text.", "\"we aim to theoretically derive means to uncover mechanisms of rectifier networks without assumptions on the weights\" -- what does \"mechanisms\" mean here?", "Notation section -- need a sentence here at the beginning, can't just have a section heading followed by bullets.", "\"realated\""], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "arg-request_edit", "arg-request_edit", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-request_edit", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_edit", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_edit_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 308, "sentences": ["This paper proposes a hybrid technique for rendering \u201ccontrol-variate\u201d and class-conditional image in two steps, first by generating an approximate rendering of the image (\u201cY\u201d) conditional on the control variate and then filling in the details with a conditional GAN dependent on a latent noise variable Z (although I note that the caption of Figure 2 which identifies \u201cZ\u201d as the identity makes this rather confusing).", "To ensure that Z is used to explain aspects of the model that are separate from the controlled variation, Z is combined in the refinement model at later steps (since otherwise the posterior over Z and Y conditional on X could induce entanglement between the variables).", "In the \u201csupervised\u201d setting where the control variates are observed, Y can be learned as a simple regression problem independent of the other parts of the model, and this two-stage refinement process is demonstrated (using inception scores) to generate convincing samples, including when C consists of up to 10 control variates.", "In the unsupervised setting, a beta-VAE is used to learn a disentangled representation of X as a proxy for C, but then the data is regenerated using a two step process.", "Readability suggestion: the paper starts with a very nice motivating example, but when the setup is provided, i.e., that (x,c) pairs are the input to the learner, the intended content of c is not immediately clear- control variates could assume anything from general context information to privileged information.", "A similarly informative example would be great!", "Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the \u201climit\u201d of perfect learning. Is this correct?", "Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.", "In fact, the separate training seems to make this unlikely."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "arg-request_edit", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 309, "sentences": ["This paper suggests a novel and compact neural network architecture which uses the information within bag-of-words features.", "The proposed algorithm only uses the patch information independently and performs majority voting using independently classified patches.", "The proposed method provides the state-of-the-art prediction accuracy unexpectedly, and several additional experiments show the state-of-the-art neural networks mainly learn without association between information in different patches.", "The proposed algorithm is simple and does not provide completely new idea, but this paper has a clear contribution connecting the previous main idea of feature extraction, bag-of-words, and the prevailing blackbox algorithm, CNN.", "The results in the paper are worth to be shared in the community and need further investigated.", "The presented experiments look fair and reasonable to show the importance of the independent patch information (without association between them), and the presented experimental results show some state-of-the-art methods also perform with independent patch information.", "Comparison with attention models is necessary to compare the important patches obtained from conventional networks."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 310, "sentences": ["~The authors build a new method to recapitulate the 3D structure of a biomolecule from cryo-EM images that allows for flexibility in the reconstructed volume.~", "I thought this paper is very well written and tackles a difficult project.", "There is a previous work that these authors should cite:", "Ullrich, K., Berg, R.V.D., Brubaker, M., Fleet, D. and Welling, M., 2019. Differentiable probabilistic models of scientific imaging with the Fourier slice theorem. arXiv preprint arXiv:1906.07582.", "How does your method compare to this paper?", "In Ullrich et al., they report \u201cTime until convergence, MSE [10^-3/voxel], and Resolution [Angstrom]).", "I think these statistics would be useful to report in your work, as they are more familiar with folks in the cryoEM field.", "In Equation 3, how does one calculate Z, the normalization constant?", "For the decoder, how large of the 3D space are you generating? What are the units? Are you using voxels to represent atomic density? What is the voxel size? Is it the same as on Page 11?", "I think more description of the neural network architecture would be useful (more than what is reported on page 12)."], "labels": ["arg-structuring_summary", "none", "arg-request_edit", "arg-request_result", "arg-request_explanation", "none", "arg-request_edit", "arg-request_clarification", "arg-request_explanation", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label"]}
{"abstract_id": 311, "sentences": ["The proposed method tackles class-incremental continual learning, where new categories are incrementally exposed to the network but a classifier across all categories must be learned.", "The proposed method seems to be essentially a combination of generative replay (e.g. Deep Generative Replay) with AC-GAN as the model and attention (HAT), along with a growing mechanism to support saturating capacity.", "Quantitative results are shown on MNIST and SVHN while some analysis is provided on CIFAR.", "Pros", "+ The method combines the existing works in a way that makes sense, specifically AC-GAN to support a single generator network with attention-based methods to prevent forgetting in the generator.", "+ The method results in good performance, although see caveats below.", "+ Analysis of the evolution of mask values over time is interesting.", "Cons", "- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.", "The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of \"growing capacity\" is not made clear at all especially in the beginning of the paper.", "Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.", "The authors should on the claimed contributions.", "Is it a combination of DGR and HAT with some capacity expansion?", "- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.", "Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?", "- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.", "It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.", "- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.", "As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.", "It also seems strange to say that storing instances \"violates the strictly incremental setup\" while generative models do not.", "Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.", "Otherwise you are just defining the problem in a way that excludes other simple approaches which work.", "- There are several methodological issues: Why are CIFAR results not shown in a table as is done for the other dataset? How many times were the experiments run and what were the variances? How many parameters are used (since capacity can increase?) It is for example not clear that the comparison to joint training is fair, when stating: \"Interestingly, DGM outperforms joint training on the MNIST dataset using the same architecture.", "This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations compared to what it would learn given all the data.\" Doesn't DGM grow the capacity, and therefore this isn't that surprising?", "This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.", "Some other minor issues in the writing includes:", "1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).", "The initial narrative mixes prior works' contributions and this paper's contributions; the contributions of the paper itself should be made clear,", "2) Using the word \"task\" in describing \"joint training\" of the generative, discriminative, and classification networks is very confusing (since \"task\" is used for the continual learning description too,", "3) There is no legend for CIFAR; what do the colors represent?", "4) There are several typos/grammar issues e.g. \"believed to occurs\", \"important parameters sections\", \"capacity that if efficiently allocated\", etc.).", "In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.", "More rigorous experiments and analysis is needed to make this a good ICLR paper."], "labels": ["none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_edit", "arg-request_explanation", "none", "arg-request_explanation", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-request_edit", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-request_clarification", "arg-request_typo", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 312, "sentences": ["The primary innovation of this paper seems focused towards increasing the generalization of GANs, while also maintaining convergence and preventing mode collapse.", "The authors first discuss common pitfalls concerning the generalization capability of discriminators, providing analytical underpinnings for their later experimental results.", "Specifically, they address the problem of gradient explosion in discriminators.", "The authors then suggest that a zero-centered gradient penalty (0-GP) can be helpful in addressing this issue.", "0-GPs are regularly used in GANs, but the authors point out that the purpose is usually to  provide convergence, not to increase generalizability.", "Non-zero centered penalties can give a convergence guarantee but, the authors, assert, can allow overfitting.", "A 0-GP can give the same guarantees but without allowing overfitting to occur.", "The authors then verify these assertions through experimentation on synthetic data, as well as MNIST and ImageNet.", "My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?", "Some portion? It is not clear from reading. This would be a serious impediment to reproducibility.", "All in all, however, the authors provide a convincing  combination of analysis and experimentation.", "I believe this paper should be accepted into ICLR.", "Note: there is an error on page 9, in Figure 3. The paragraph explanation should list that the authors' 0-GP is figure 3(e). They list (d) twice."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-request_experiment", "none", "none", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 313, "sentences": ["The authors introduce cryoDRGN, a VAE neural network architecture to reconstruct 3D protein structure from 2D cryo-EM images.", "The paper offers for a good read and diagrams are informative.", "Below are comments for improvement and clarification.", "> Consider explaining cryoSPARC in detail given that is the state-of-the-art technique and to which all the cryoDGRN results are compared.", "> In Figure 4 and the related experiment ,  how are a) the cryoSPARK volumes related to cryoDRGN volumes, b) what do the clusters mean in cryoSPARK and how do they compare with the corresponding outputs of cryoDRGN", "> What would runtime comparisons be for cryoSPARK and cryoDGRN, for an unsupervised heteregeneous reconstruction?"], "labels": ["arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 314, "sentences": ["This work considers a version of importance sampling of states from the replay buffer.", "Each trajectory is assigned a rank, inversely proportional to its probability according to a GMM.", "The trajectories with lower rank are preferred at sampling.", "Main issues:", "1. Estimating rank from a density estimator", "- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.", "- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)", "2. Generalization issues", "- the method is not applicable to episodes of different length", "- the approach assumes existence of a state to goal function f(s)->g", "- although the paper does not expose this point (it is discussed the HER paper)", "3. Scaling issues", "- length of the vector grows linearly with the episode length", "- length of the vector grows linearly with the size of the goal vector", "For long episodes or episodes with large goal vectors it is quite possible that there will not be enough data to fit the GMM model or one would need to collect many samples prior.", "4. Minor issues", "- 3.3 \"It is known that PER can become very expensive in computational time\" - please supply a reference", "- 3.3 \"After each update of the model, the agent needs to update the priorities of the transitions in the replay buffer with the new TD-errors\" - However the method only renews priorities of randomly selected transitions (why would there be a large overhead?).", "Here is from the PER paper \"Our final implementation for rank-based prioritization produced an additional 2%-4% increase in running time and negligible additional memory usage\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-request_clarification", "arg-structuring_heading", "none", "arg-structuring_quote", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_clarification_label", "arg-request_edit_label"]}
{"abstract_id": 315, "sentences": ["The paper addresses the problem of providing saliency-based visual explanations of deep models tasked at image classification.", "More specifically, instead of generating visualizations directly highlighting the image pixels that support the the decision of an image belonging to class A, it generates \"contrastive\" visualizations indicating the pixels that should be added or suppressed in order to support the decision of a image belonging to class A and not to class B.", "The method formulates the generation of these contrastive explanations through a generative adversarial network (GAN), where the discriminator D is the image classification model to be explained and the generator G is a generative model trained to produce images from the dataset used to train D.", "Experiments on the MNIST and fashion-MNIST datasets compares the performance of the proposed method w.r.t. some methods from the literature.", "Overall the manuscript is well written and its content is relatively easy to follow.", "The idea of generating contrastive explanations through a GAN-based formulation is well motivated and seems novel to me.", "My main concern with the manuscript are the following:", "i) The proposed method seems to be specifically designed for the generation of contrastive explanations, i.e. why the model predicted class A and not class B.", "While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?", "ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.", "See Kim et al., NIPS'16, Dhurandhar et al., arXiv:1802.07623.", "Moreover, regarding the presented results on the MNIST dataset (Sec 4.1) where some of the generated explanations highlight gaps to point differences between digit classes.", "The work from Samek et al., TNNLS'17 and  Oramas et al., arXiv:1712.06302 seem to display similar properties in their explanations without the need of explicit constractive pair-wise training/testing.", "The manuscript would benefit from positioning the proposed method w.r.t. these works.", "iii) Very related to the first point, in the evaluation section (Sec.4.1) the proposed method is compared against other methods in the literature.", "Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.", "iv) Finally, the reported results are mostly qualitative.", "I find the set of provided qualitative examples quite reduced.", "In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.", "In addition, I recommend complementing the presented qualitative comparisons with quantitative evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage (Oramas et al. arXiv:1712.06302)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "none", "arg-structuring_quote", "arg-structuring_quote", "none", "arg-request_edit", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 316, "sentences": ["The authors proposed a new embedding for time - Time2Vec.", "Unlike previous research that is either proposing a new architecture or proposing expensive handcrafted features, this work proposes a model-agnostic learnable time embedding.", "I would like to recommend an accept based on the following reasons:", "* Modeling time is crucial for quite a few machine learning tasks.", "With the two most desired properties, learnable and model-agnostic, this time embedding will be very useful in various applications.", "* The authors are good at story-telling and this makes the paper very readable and approachable.", "This increases the chance of the contribution made in this paper to be applied in real-world applications.", "* This work did clear and detailed analysis on both the empirical results and the probing experiments."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 317, "sentences": ["This paper proposed a fractional quantization method for deep net weights.", "It adds XOR gates to produce quantized weight bits compared with existing quantization method.", "It used tanh functions instead of a straight-through estimator for backpropagation during training.", "With both designs, the proposed method outperformed the existing methods and offered sub bit quantization options.", "The use of XOR gates to improve quantization seems novel.", "The sub bit quantization achieved by this method should be interesting to the industrial.", "It significantly improved the quantization rate with slightly quality degradation.", "With 1 bit quantization, it outperformed the state-of-the-art.", "The results seem thorough and convincing."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 318, "sentences": ["This paper proposes a method for learning disentangled representations.", "The approach is used on both supervised (where the factors to be disentangled are known) and unsupervised settings.", "The authors demonstrate the efficacy of their approach in both settings on several datasets with both quantitative and qualitative results.", "This task is an important one.", "However, I found that the contribution of this paper is fairly small.", "The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.", "The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).", "Perhaps the authors could give examples of situations where this would naturally arise.", "In practice, it seems difficult to obtain these data for all required variables to be disentangled.", "The unsupervised results are more interesting but not very much explored (a single set of sampled faces).", "I was also curious as to why the learned Y's are blurry.", "This sort of two-stage generation is also potentially interesting, I was wondering if the authors had ideas to generalize this idea.", "I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.", "Detailed comments:", "- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?", "- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?", "- Comparing to CGAN seems reasonable but given the task at hand, it seems like other methods could have been tried (although I do realize that no one may have done this before for deep generative models).", "Other comments:", "- In Figure 3, it would be good to label the upper trapezoid.", "- Some paragraphs are very long and the manuscript may benefit from segmenting them into multiple paragraphs."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_clarification", "arg-request_experiment", "none", "arg-structuring_heading", "arg-request_explanation", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 319, "sentences": ["This paper introduces a particular learnable vector representation of time which is applicable across problems without the use of a hand-crafted time representation.", "Their representation makes use of a feed-forward layer with sine activations which operates on time data.", "As it is a vector representation, it combines well with other deep neural network methods.", "They motivate their problem well, explaining why time data is important to a variety of problems and situate their solution as an orthogonal approach to many current solutions in the literature.", "They make reference to fourier analysis as motivation for their representation.", "Finally, they provide experimental results to support their claims using fabricated and real-world time series datasets, as well as ablation studies to support their design decisions.", "While I think this work has the potential to be a significant contribution, I rate this a weak reject because the theoretical motivation and analysis of the experimental results are lacking the depth of evidence I would expect for an ICLR paper.", "If you provide a deeper discussion of the provable claims about the power of your model via Fourier analysis and provide a table of test accuracy/recall@K with/without your representation for more than one other state of the art algorithm for these datasets, I would be convinced to strong accept.", "Specific comments:", "* p.3 third paragraph: you repeat yourself in math notation a few times here.", "Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.", "I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j * p.3", "A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.", "* p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.", "There are plenty of simple real-world datasets available which show multi-scale periodic phenomena (activity or location data, weather data, travel data, etc.).", "In fact, segmentation and recognition of wearable device activity would be a great application for this method. * p.4", "third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).", "You show that this works for a rescaling from 2pi/7 to 2pi/14, but it would be nice if there was experimental confirmation of this property with frequency > 1.", "* p.6 Showing accuracy/recall across training epochs is not sufficient evidence to show that this is a useful representation.", "There should be some kind of comparison with test set results from other state-of-the-art work on these datasets. If adding your representation to the SOTA model improved test set performance (or at least sped up training without hurting test set performance), then that would be better evidence. If LSTM+T is the SOTA, say so and restate the author's test performance compared to yours .", "If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.", "* p.8 I think sine functions make optimization harder because they make the gradient function periodic with respect to the weights, creating infinitely many local extrema.", "Historically this may have been an issue, but deep neural networks have so many local minima it might not matter.", "Still, it would be good to show that trained performance doesn't depend on the initialization values more than a standard LSTM+T model.", "* You have an interesting corner case where your neural network parameters are interpretable: you can interpret the omega values from your model as frequencies and investigate their values to see which kinds of periodicity your model uses. You do something like this on p.7, but it would be neat to see a histogram like the one you have for EventMNIST for one of the real-world datasets to see if it learns the domain-relevant time knowledge you claim that it should learn."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none", "arg-request_result", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label"]}
{"abstract_id": 320, "sentences": ["This paper studies the problem of how to design generative networks for auditory signals in order to capture natural signal priors.", "Compared to state-of-art methods in images [Lempitsky et al., 2018], this problem is not so easy on audio signals.", "Existing work [Michelashvili &Wolf] trains generative networks to model signal-to-noise ratio rather than the signal itself.", "This paper proposes a new convolutional operator called Harmonic Convolution to improve these generative networks to model both signals or signal-to-noise ratio.", "Applications on audio restoration and source separation are given.", "The paper starts to show that an existing generative network Wave-U-Net does not capture audio signal priors.", "The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?", "The Harmonic Convolution is similar to deformable convolutions, but specifically designed to capture audio harmonics.", "It is further combined with the idea of anchors and mixing to capture fractional frequencies.", "The explanation of this section is slightly unclear.", "There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.", "Is Harmonic Convolution applicable to complex STFT coefficients as well?", "It seems to be yes based on Section 4.2.", "If so it would be better to define the operator in a more general notation.", "Numerical experiments show that the Harmonic Convolution improves over existing regular and dilated convolutions in various settings.", "Section 4.2 aims to fit the complex STFT coefficients of corrupted signals.", "However, the setting is less clear to me for both the unsupervised speech/music restoration and supervised source separation problems.", "In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?", "It seems to me x_0 = ratio mask in Section 4.4.", "What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?", "These details can be written in supplementary material if more space is needed.", "After all, the numerical results seem to me encouraging."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_explanation", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_edit", "arg-request_clarification", "none", "arg-request_edit", "none", "none", "none", "arg-request_explanation", "none", "arg-request_explanation", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 321, "sentences": ["This paper presents a new approach in network quantization.", "The key insights of this paper is quantizing different layers with different bit-widths, instead of using fixed 32-bit width for all layer weights and activation in previous works.", "At the same time, this paper adopted the idea form both DARTS and ENAS with parameter sharing, and introduces a new differentiable neural architecture search framework.", "As the authors proposed, this DNAS framework is able to search efficiently and effective through a large search space.", "As demonstrated in the Experiment section of the paper, it achieves better validation accuracy than ResNet with much smaller model size and lower computational cost.", "1. An improved gradient method in updating the network architecture and parameters compared to DARTS and ENAS.", "It applies the Gumbel softmax to refine the sub-graph structure without training the entire super-net through the whole process.", "The work is able to obtain the same level of validation accuracy on Cifar-10 as ResNet while reduce the model parameters by a large margin.", "2. The work is in the middle ground of two previous works: ENAS by Pham et al. (2018) and DARTS by Liu et al. (2018).", "However, there is no comparison with ENAS and DARTS in experiments.", "ENAS samples child networks from the super net to be trained independently while DARTS trains the entire super net together without decoupling child networks from the super net.", "By using Gumbel Softmax with an annealing temperature, The proposed DNAS pipeline behaves more like DARTS at the beginning of the search and behaves more like ENAS at the end."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 322, "sentences": ["Summary: This paper introduces the task of using deep learning for auto-completion in UI design.", "The basic idea is that given a partially completed tree (representing the design state of the UI), the goal is to predict or \"autocomplete\" the final tree.", "The authors propose a transformer-based solution to the task, considering three variants: a vanilla approach where the tree is flattened to a sequence, a pointer-network style approach, and a recursive transformer.", "Preliminary experiments indicate that the recursive model performs best and that the task is reasonable difficulty.", "Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.", "In particular, the authors spend a bulk of the paper describing the three different baselines they implement.", "However, despite the fact that most of the paper is dedicated to the explanation of these baselines.", "There is not sufficient detail to reproduce the models based on the paper alone.", "Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.", "Further technical background and detail would drastically improve the paper.", "Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.", "In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.", "In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.", "For example, one question is how often a single partial tree has multiple possible completions in the data.", "A major issue---mainly due to the lack of technical details and the lack of promise to provide code/data (unless I missed this)---is that the paper does not appear to be reproducible. Given the intent to have this be a new benchmark, ensuring reproducibility seems critical.", "Reasons to accept:", "- Interesting new application of GNNs", "Reasons to reject:", "- Incremental modeling contribution", "- Lack of sufficient technical detail on models and dataset", "- Does not appear to be reproducible"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_experiment", "none", "arg-request_clarification", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 323, "sentences": ["The paper proposes a novel method for sampling examples for experience replay.", "It addresses the problem of having inbalanced data (in the experience buffer during training).", "The authors trained a density model and replay the trajectories that has a low density under the model.", "Novelty:", "The approach is related to prioritized experience replay, PER is computational expensive because of the TD error update, in comparison, CDR only updates trajectory density once per trajectory.", "Clarity:", "The paper seems to lack clarity on certain design/ architecture/ model decisions.", "For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.", "Also, I had to go through a large chunk of the paper before coming across the exact setup.", "I think the paper could benefit from having this in the earlier sections.", "Other comments about the paper:", "-  I do like the idea of the paper.", "It also seems that curiosity in this context seems to be very related to surprise? There are neuroscience evidence indicating that humans turns to remember (putting more weights) on events that are more surprising.", "- The entire trajectory needs to be stored, so the memory wold grow with episode length.", "I could see this being an issue when episode length is too long."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_clarification", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 324, "sentences": ["The paper proposes an approach to provide contrastive visual explanations for deep neural networks -- why the network assigned more confidence to some class A as opposed to some other class B. As opposed to the applicability of previous approaches to this problem -- the approach is designed to directly answer the contrastive explanations question rather adapting other visual saliency techniques for the same.", "Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.", "- Apart from some flaws in the claims made in the paper, the paper is easy to follow and understand.", "- Assuming the availability of a latent model over the images of the input distribution, the proposed approach is directly applicable and faster.", "- The authors clearly highlight the problems associated with existing explanation modalities and approaches; ranging from ones applicable to only specific deep architectures to ones using backpropagation based heuristics.", "- The proposed approach to generate contrastive explanations is simple and is structured along the lines of methods utilizing probe images to explain decisions -- except for the added advantage that the provided explanations are instance-agnostic due to the assumption of a latent model over the input distribution.", "Comments:", "- One of the problems highlighted in the paper regarding existing explanation modalities is the use of another black-box to explain the decisions of an existing deep network (also somewhat of a black-box) which the authors claim their model does not suffer from.", "The proposed approach provides explanations by operating in the latent space of a learned generative model of the input distribution.", "The learned generator in itself is somewhat of a black-box itself -- there has been prior work indicating how much of the input distribution are GANs able to capture.", "As such, conditioning on a generative model to propose such contrastive explanations is to some extent using another black-box (generator) to explain the decisions of an existing one.", "Thus, the above claim made in the paper does not seem well-founded.", "Furthermore, in experiments, the paper does not provide any quantitatively convincing results to suggest the generator in use is a good one.", "- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.", "In this light, experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger (as the proposed approach relies explicitly on how good the generative model is).", "- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class \u20188\u2019). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.", "- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.", "Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.", "In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.", "In Section 4.1 , the use of Gradcam and Lime to generate counterfactual explanations is not very clear and makes it slightly hard to follow.", "Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.", "Experimental Issues:", "- Experimental results are provided only on MNIST and Fashion-MNIST.", "Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.", "Additional experiments on at least ImageNet would have made the paper stronger.", "Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.", "Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a \u2018cat\u2019 to be classified as a \u2018dog\u2019 while there is an instance of the class - \u2018dog\u2019 present in the image itself.", "Also, section 7 in Gradcam (https://arxiv.org/pdf/1610.02391.pdf) provides a procedure to generate counter-factual explanations using Gradcam.", "Is there a particular reason the authors did not choose to adopt the above technique as a baseline?", "- Experimental results provided in the paper are only qualitative -- as such, I do not find the comparisons (and improvements) over the existing approaches convincing enough.", "Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.", "The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am not inclined towards increasing my rating and will stick to my original rating for the paper."], "labels": ["arg-structuring_summary", "arg-request_edit", "none", "none", "none", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_quote", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_experiment", "arg-request_typo", "arg-request_edit", "arg-request_edit", "none", "arg-request_edit", "arg-structuring_heading", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_explanation", "none", "arg-request_typo", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 325, "sentences": ["Summary:", "The paper proposes the use of a hierarchical model for a generative modeling task.", "They propose a framework of introducing an intermediate latent variable to enforce the independence of the control and noise variable.", "The paper report extensive experimental results to validate the proposed hierarchical model.", "The authors also provide the anonymized code to observe the exact implementation in TensorFlow to visualize the latent variable traversals.", "Comments:", "The paper proposes the use of a hierarchical model for a generative modeling task by introducing an intermediate latent variable to enforce the independence of the control and noise variable.", "The paper report extensive experimental results to validate the proposed hierarchical model.", "This type of framework of crude to fine hierarchical generative model has already been successfully introduced by StackGAN and it's recent variants.", "On the unsupervised disentangled feature learning, the framework provides incremental advancement by using beta-VAE in conjunction with GAN to use the best of both the worlds.", "Even though the proposed approach is similar to StackGAN, the experiments and the results mentioned in the paper are noteworthy.", "Questions to Authors:", "There are 2 main claims of novelty made in the paper.", "1. Architectural Biases:", "How is the approach different in comparison to the StackGAN and it's variable which also use multiple levels of crude to fine image generation?", "2. Unsupervised control variable discovery:", "This part is just the use of existing disentanglement VAEs to extract the control variables.", "So how does the paper try to make contributions to improve the disentangled features with the proposed method?", "Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?", "In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN)."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "arg-request_explanation", "arg-structuring_heading", "none", "arg-request_explanation", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 326, "sentences": ["This paper addresses the important / open problem of graph generation, and specifically in a conditional/transductive setting.", "Graph generations is a new topic, it is difficult, and has many important applications, for instance generating new molecules for drug development.", "As stated by the authors, this is a relatively open field: there are not many papers in this area, with most approaches today resorting to domain specific encodinings, or \"flattening\" of graphs into sequences to then allow for the use recurrence (like in MT); this which per se is an rather coarse approximation to graph topology representations, thus fully motivating the need for new solutions that take graph-structure into account.", "The setting / application of this method to graph synthesis of suspicious behaviours of network users, to detect intrusion, effectively a Zero-shot problem, is super interesting.", "The main architectural contribution of this paper are graph-deconvolutions, practically a graph-equivalent of CNN's depth-to-space - achieved by means of transposed structural matrix multiplication of the hidden GNN (graph-NN) activation - simple, reasonable and effective.", "While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.", "Results are provided on relatively new tasks so it's hard to compare fully to previous methods, but the authors do make an attempt to provide comparisons on synthetic graphs and intrusion detection data.", "The authors do published their code on GitHub with a link to the datasets as well.", "As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of \"edge-to-edge\" convolutions and generally the architectural choice related to the conditional GAN discriminator.", "Clarifications of these points, and more in general the philosophy behind the architectural choices made, would make this paper a much clearer accept.", "Thank you!", "ps // next my previous public comments, in detail, repeated ...", "--", "- the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 \"graph translator\".", "- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.", "- why do you need a conditional GAN discriminator, if you already model similarity by L1?", "Typically one would use a GAN-D() to model \"proximity\" to the source-distribution, and then a similarity loss (L1 in your case) to model \"proximity\" to the actual input sample, in the case of trasductional domains.", "Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.", "This is confusing to me.", "Please explain the logic for this architectural choice.", "-  could you please explain the setting for the \u201cgold standard\u201d experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_quote_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 327, "sentences": ["This paper proposes the Cramer-Wold autoencoder.", "The first contribution of the paper is to propose the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem.", "More specifically, in order to compute the Cramer-Wold distance, we first find the one dimensional projections of the distributions over random slices, and then compute the average L2 distances of the kernel density estimates of these projections over random slices.", "The second contribution of the paper is to develop a generative autoencoder which uses the Cramer-Wold distance to match the latent distribution of the data to the prior distribution.", "While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.", "My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?", "The paper points out that the main theoretical contribution is that in the case of the Gaussian distribution, the Cramer-Wold distance has a closed form.", "However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.", "The paper further uses this closed form property of the Cramer-Wold distance to propose the Cramer-Wold autoencoder with Gaussian priors.", "My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.", "Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.", "I believe the main advantages of methods such as WAE is that they can impose priors that do not have exact analytic forms."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_explanation", "none", "none", "none", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 328, "sentences": ["The paper introduces a novel regularized auto-encoder architecture called the Cramer-Wold AutoEncoders (CWAE).", "It's objective (Eq. 7) consists of two terms: (i) a standard reconstruction term making sure the the encoder-decoder pair aligns nicely to accurately reconstruct all the training images and (ii) the regularizer, which roughly speaking requires the encoded training distribution to look similar to the standard normal (which is a prior used in the generative model being trained).", "The main novelty of the paper is in the form of this regularizer.", "The authors introduce what they call \"the Cramer-Wold distance\" (for definitions see Theorems 3.1 and 3.2) which is defined between two finite sets of D-dimensional points.", "The authors provide empirical studies showing that the proposed CWAE method achieves the same quality of samples (measured with FID scores) as the WAE-MMD model [1] previously reported in the literature, while running faster (by up to factor of 2 reduction in the training time, as the authors report).", "While on the AE model / architecture side I feel the contribution is very marginal, I still think that the improvement in the training speed is something useful. Otherwise it is a nicely written and polished piece of work.", "Detailed comments:", "(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to \"improve the balance between two terms\", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].", "(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].", "In other words: it may be the case that there is a choice of a reproducing kernel k such that Eq. 2 of this paper is an estimate of MMD_k between two distributions based on the i.i.d. samples X and Y.", "Note that if it is indeed the case, this corresponds to the V-statistic and thus biased: in U-statistic the diagonal terms (that is i = i' and j = j' in forst two terms of eq 2) would be omitted.", "If this all is indeed the case, it is not surprising that the numbers the authors get in the experiments are so similar to WAE-MMD, because CWAE would be exactly WAE-MMD with a specific choice of the kernel.", "(3) The authors make a big deal out of their proposed divergence measure not requiring samples from the prior as opposed to WAE-MMD.", "However, WAE-MMD does not necessarily need to sample from the prior when used with Gaussian prior and Gaussian RBF kernel, because in this case the prior-related parts of the MMD can be computed analytically.", "In other words, if the computational advantage of CWAE compared to WAE-MMD comes from CWAE not sampling Pz, the computational overhead of WAE-MMD can be eliminated at least in the above-mentioned setting.", "(4) based on the name \"CW distance\" I would expect the authors to actually prove that it is indeed a distance (i.e. all the main axioms).", "(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.", "(6) What is image(X) in Remark 4.1?", "[1] Tolstikhin et al., Wasserstein Auto-Encoders, 2017."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_quote", "none", "none", "arg-request_edit", "none", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_clarification_label", "none_label"]}
{"abstract_id": 329, "sentences": ["Paper Contributions", "This paper introduces a new text generation scoring approach using BERT, called BERTScore.", "Using BERT embeddings and optionally idf scores, a greedy matching is performed between all reference and candidate words, with cosine similarity between vector representations as the scoring.", "From this, a precision, recall and F1 score can be derived.", "This notably outperforms BLEU, as well as other metrics, most but not all of the time.", "The paper offers a broad range of comparisons and analysis.", "Decision", "I'm leaning towards accepting the paper on the basis of the following.", "Strong points taken in consideration:", "- Simple, well-motivated metric that uses powerful BERT-style models, without being slow to compute either.", "- Good performance empirically on WMT. I'm less convinced on COCO since using the image is fair game there.", "- Code is provided, and it is simple and adaptable for future work.", "- Experimentation is detailed and reproducible.", "Weaker points taken in consideration:", "- Work conducted in parallel matches or exceeds the performance of BERTScore. This shouldn't necessarily be a reason to choose not to publish this work in my opinion, but it should be taken into consideration.", "I like that the authors were open and clear regarding this in their discussion.", "- The authors haven't come up with a recommendation for a single configuration of their approach. In one place they recommend F-BERT without idf, in another they argue for picking and choosing based on context, with little help about how to choose.", "I think practitioners are only going to be willing to switch away from BLEU, for example, if a single one-size-fits-all metric is proposed instead.", "I identify this ambiguity between BERTScore versions as an important weakness of the paper.", "- It's unclear throughout whether words or wordpieces are the main token being considered. Most discussion and definitions use \"words\", but in section 3, subsection Token Representation, it appears to be clearly stated that BERTScore uses a BERT model based on word pieces.", "I recommend adjust the language to be more consistent throughout. Also, scoring examples with word pieces would be more consistent with this as well, imo.", "Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.", "- Finally, I found some weaknesses in the Importance Weighting section (though this isn't too important since IDF isn't part of the recommended BERTScore I believe).", "The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set. This would add extra steps to using BERTScore though and make things more complicated in practice, but this should nevertheless probably be tried, or at least discussed in the paper.", "Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.", "So overall, I still think this deserves publication because it's valuable information for researchers, and the metric itself could be immediately useful to some as well.", "However, the weaknesses mentioned make me hesitate to fully endorse the work."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 330, "sentences": ["This paper addresses a problem that arises in \"universal\" value-function approximation (that is, reinforcement-learning when a current goal is included as part of the input);  when doing experience replay, the experience buffer might have much more representation of some goals than others, and it's important to keep the training appropriately balanced over goals.", "So, the idea is to a kind of importance weighting of the trajectory memory, by doing a density estimation on the goal distribution represented in the memory and then sample them for training in a way that is inversely related to their densities .", "This method results in a moderate improvement in the effectiveness of DDPG, compared to the previous method for hindsight experience replay.", "The idea is intuitively sensible, but I believe this paper falls short of being ready for publication for three major reasons.", "First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.", "Even if it's not possible to prove something about this strategy, it would be useful to just state a desirable property that the sampling mechanism should have and then argue informally that this mechanism has that property.", "As it is, it's just one point in a large space of possible mechanisms.", "I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.", "If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over \"nature\"'s choices in the MDP.", "Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.", "- I'm not sure we can conclude much from the results on fetchSlide (and it would make sense not to use the last set of parameters but the best one encountered during training)", "- What implementation of the other algorithms did you use?", "Third, the writing in the paper has some significant lapses in clarity.", "I was a substantial way through the paper before understanding exactly what the set-up was;  in particular, exactly what \"state\" meant was not clear.", "I would suggest saying something like s = ((x^g, x^c), g) where s is a state from the perspective of value iteration, (x^g, x^c) is a state of the system, which is a vector of values divided into two sub-vectors, x^g is the part of the system state that involves the state variables that are specified in the goal, x^c (for 'context') is the rest of the system state, and g is the goal. The dimensions of x^g and g should line up.", "- This sentence  was particularly troublesome:  \"Each  state s_t also includes the state of the achieved goal, meaning the goal state is a subset of the normal state. Here, we overwrite the notation s_t  as the achieved goal state, i.e., the state of the object.\"", "- Also, it's important to say what the goal actually is, since it doesn't make sense for it to be a point in a continuous space. (You do say this later, but it would be helpful to the reader to say it earlier.)"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none", "arg-request_clarification", "none", "none", "arg-request_edit", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 331, "sentences": ["Summary:", "This paper proposes a functional form to model the dependence of generalization error on a held-out test set on model and dataset size.", "The functional form is derived based on empirical observations of the generalizing error for various model and dataset sizes (sections O1, O2, and O3) and on certain necessary criteria (C1, C4 and C5).", "The parameters of the function are then fit using linear regression on observed data.", "The authors show that the regressed function \\(\\epsilon(m,n)\\) is able to predict the generalization error for various \\(m\\) and \\(n\\) reasonably accurately.", "Major Points:", "- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \\(\\epsilon(m,n)\\).", "I would ideally like to see results on more optimizers, at the very least for Adam, even if for fixed hyper-parameters.", "As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.", "If the form still holds true then the results from this work can be more reliably used for small-scale network development and in making trade-off choices (as discussed in section 8).", "- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.", "Minor Points:", "- It would be nice if more network architectures were analysed (such as VGG and DenseNets).", "- It would be nice if different stopping criteria were analysed.", "- It would greatly benefit the reader if eq. 5 were expanded.", "Overall, I think this is a well written paper and provides good insight into the behaviour of the error landscape as a function of model and dataset size.", "The paper\u2019s primary drawback is the restrictive setting under which the experiments are performed.", "Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).", "I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.", "Rebuttal Response", "I would like to thank the authors for their response.", "The results of additional experiments as described in Section 6.2 and in Figure 5 do indeed provide stronger evidence of the power-law form of the error function.", "In light of this, I have changed my original rating."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 332, "sentences": ["The paper proposes a new approach for quantizing neural networks.", "It looks at binary codes for quantization instead of the usual lookup tables because quantization approaches that rely on lookup tables have the drawback that during inference, the network has to fetch the weights from the lookup table and work with the full precision values, which means that the computation cost is remains the same as the non-quantized network.", "The paper presents FleXOR gates, a fast and efficient logic gate for mapping bit sequences to binary weights.", "The benefit of the gate is that the bit sequence can be shorter than the binary weights which means that the code length can be less than 1 bit per weight.", "The paper also proposes an algorithm for end-to-end training of the quantized neural networks.", "It proposes the use of tanh to approximate the non-differentiable Heaviside step function in the backward pass.", "Novelty", "The idea of using logic gates for dequantization is interesting and (as far as I know) novel.", "One can imagine, that specialized hardware build on this idea could very efficient for inference (in terms of energy cost).", "Writing", "The paper is very well written and completed with great visualizations and pseudocode.", "Kudos to the authors, I really enjoyed reading it.", "However, I do not think it is justified to go over the 8 page soft limit.", "I would recommend that the authors perhaps shorten section 3 or remove figure 9 to fit it into 8 pages.", "Significance/Impact", "The paper is motivated by the high computation cost of working with full precision values.", "But this paper also works with full precision weights, since it has a full precision scaling factor (alpha) and, as far as I understood, works with full precision values during forward propagation. This means that there likely are no computational savings when compared to lookup tables.", "The evaluation section lacks experiments that evaluate the computational savings. The baselines should include quantization methods based on lookup tables, and there should be a comparison of computational costs. The baselines that are presented (BWN etc.) offer a tradeoff between accuracy and computational costs, yet they are only compared in accuracy.", "I would strongly recommend including the computational cost of each method in the evaluation section.", "Overall assessment:", "While I enjoyed reading this paper, I am leaning towards rejection due to the shortcomings of the evaluation section."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_edit", "arg-structuring_heading", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 333, "sentences": ["This paper presents an adaptive exploration scheme that can reduce the complexity of per-task tuning.", "This goal is achieve by formulating the adapting scheme as a multi-arm bandit problem with the actual \"learning progress\" as a feedback signal.", "The paper is well written and easy to be understood.", "The strength of this paper is that 1) the proposed method is new in the sense that it invents an automatic way for exploration.", "2) The algorithm is simple yet effective by the experiment results the authors provide.", "Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.", "More explanations are needed."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 334, "sentences": ["This papers studies how to explore, in order to generate experience for faster learning of policies in context of RL.", "RL methods typically employ simple hand-tuned exploration schedules (such as epsilon greedy exploration, and changing the epsilon as training proceeds).", "This paper proposes a scheme for learning this schedule.", "The paper does this by modeling this as a non-stationary multi-arm bandit problem.", "Different exploration settings (tuple of choice of exploration, and the exact hyper-parameter), are considered as different non-stationary multi-arm bandits (while also employing some factorization) and expected returns are maintained over training.", "Arm (exploration strategy and hyper-parameter) is picked according to the return.", "The paper demonstrates results on the Atari suite of RL benchmarks, and shows results that demonstrate that their proposed search leads to faster learning.", "Strength:", "1. The paper tackles an interesting and important problem.", "The proposed solution is simple, yet effective.", "Shortcomings:", "1. The presentation is somewhat convoluted.", "The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.", "Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster. 2.", "I am confused by Figure 4, and in general with the relative rank metrics. Specifically, in Figure 4, is it that the proposed bandit approach not as good as picking a single hyper-parameter for the different settings (T=0.01, eps=0.01, omega=2.0)? Similarly, for Figure 2, a singe fixed z, seems to do better than the bandit versions.", "Why doesn't the proposed bandit algorithm not pick out the best hyper-parameter? How well", "would a simpler hyper-parameter search procedure (picking the best hyper-parameter after the first 2000 episodes)?", "3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix. An alternate organization that presents all the main results in the main body in a self-contained manner will help.", "4. Comparison with past works. I believe there are other existing works that should be cited and compared to. Using bandits to decide between different hyper-parameters is common (for example, see [A] for a service to do this with ML models), [B] uses improvements in accuracy as a way to pick between which question type to train on. Such past works should be cited and compared against. [A] https://ai.google/research/pubs/pub46180 [B] Learning by Asking Questions Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta and Laurens van der Maaten"], "labels": ["arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 335, "sentences": ["The paper proposes a learning-based adaptive compressed sensing framework in which both the sampling and the task functions (e.g., classification) are learned jointly end-to-end.", "The main contribution includes using the Gumbel-softmax trick to relax categorical distributions and use back-propagation to estimate the gradient jointly with the tas neural network.", "The proposed solution has the flexibility of able to be used in several different tasks, such as inverse problems ( super-resolution or image completion) or classification tasks.", "The paper is very well written.", "The paper locates itself well in current baselines and explains Experiments mostly well.", "However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:", "1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.", "The visualization and a thorough comparison were missing in MNIST classification.", "This baseline was also missing in image reconstruction.", "2) Compressive Sensing incorporates vast literature of algorithms focusing on different aspects of improvements; algorithms focused on classification and inverse problems.", "Even if done disjointly, how does the proposed joint learning is compared to those algorithms in these domains?", "3) Top row of Figure 3 nicely explains how the learned sampling paradigm performs compared to other mechanisms (such as uniform, random, low-pass). But there is no comparision against other non-fixed techniques."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_clarification", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 336, "sentences": ["*** Update ***", "I'd like to thank the authors for answering my questions, and I am satisfied with their response. I have read the other reviews for this paper as well, and I am keeping my score.", "This paper proposes BERTScore, a method for automatic evaluation of text.", "Their method uses BERT to produce contextualized word representations for the words in the reference and hypothesis.", "Then they compute the precision, recall, and F1 by greedily matching up words between the hypothesis and reference.", "To be more specific, for say recall they take each word in the reference and compute the cosine with all words in the hypothesis.", "Then they add up the largest cosine similarity for each word and average them together.", "Precision is defined similarly but with the roles of hypothesis and reference switched.", "F1 is then the harmonic mean of these two scores.", "They also experiment with using idf to weight importance.", "Their method is simple, but achieves very strong results and there are a ton of experiments in this paper (it is 41 pages).", "The focus is largely on metrics for MT, but they also evaluate on image captioning.", "The paper is also very thorough and many of the questions I had when reading it are answered (like effect of optimal matching, running time, etc.).", "The latter (running time) being one of the downsides of the method if it was to be used for fine-tuning MT systems. 40 times slower than BLEU, but I think this increased cost would be worth it and could be engineered around.", "Overall, I like the paper - it is simple and effective on its goal task of automatic evaluation for text generation.", "I think we are moving that way as a field and this paper proposes a useful method and is additionally a good study on the subject.", "A question I have is why the method doesn't perform well in certain cases. For instance, in Table 2 and 3 - some of the evaluations with tr and fi fall well below relative performance for other language pairs. Does this have to do with the quality of the representations in multilingual BERT? What is YiSi-1 doing, for instance for model selection of en-fi and en-tr that makes it have so much better performance?", "Edit: I also wonder if incorporating idf would be better if the values were computed by a larger corpus.", "I think it would make the most sense to compute these from the training data for the underlying BERT models. Since BERT itself is a function of this training data, it seems appropriate that these values would be as well (or perhaps at least a subset of this data).", "Missing citations:", "A citation to \"Beyond BLEU:Training Neural Machine Translation with Semantic Similarity\" from ACL 2019 should be incorporated into the related work. They use semantic similarity to fine-tune NMT systems with their own embedding-based (semantic similarity) metric and they found some nice properties from training in this way.", "Have you tried BERTScore on sentence similarity tasks? It's possible BERTScore could have strong performance and some readers may wonder this. There are evaluations on PAWS for paraphrase detection which I appreciated, but that is a little different.", "A citation to \"Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization\" from EMNLP 2019 should also be incorporated.", "This paper is a big boon to BERT score showing that it is a very helpful metric for fine-tuning summarization systems. They don't even need a cross-entropy term since BERTScore captures fluency so well. I'd like to see it for MT as well, but perhaps that is the next paper.", "Typos:", "The word \"language\" is misspelled twice in Appendix E."], "labels": ["arg-structuring_heading", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_result", "arg-request_edit", "none", "arg-structuring_heading", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 337, "sentences": ["In this paper, the authors propose a physical driven architecture of DeepSFM to infer the structures from motion.", "Extensive experiments on various datasets show that the model achieves the state-of-the-art performance on both depth and pose estimation.", "In general, the paper is clearly written but I still have several concerns.", "1.\tThe paper is easy to follow but the authors are expected to clarify the rationality in integration of the loss function.", "How the parameter of \\lambda_r, \\lambda_t, and \\lambda_r influence the performance.", "It would be better if the authors could present some analysis.", "2.\tThe experiments are rather insufficient.", "The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.", "3. The experiments in section 4.3 are also expected to be improved. It is difficult to draw a conclusion that the method is better than other ones based on such limited experiments."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_edit", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 338, "sentences": ["This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution that can be computed in closed form without drawing samples from the prior (but only when it is Gaussian).", "The primary contribution is the new CW statistical distance, which is the l2 distance between projected distributions, integrated over all possible projections (although not calculated as so in practice).", "Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.", "Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score).", "Some potential options include:", "1) Faster training times.", "It seems to me one potential advantage of the closed-form distance would be that the stochastic WAE-optimization can converge faster (due to lower-variance gradients).", "However, the authors only presented per-batch processing times as opposed to overall training time for these models.", "2) Stabler training.", "Perhaps sampling from the prior (as needed to compute statistical distances in the other WAE variants) introduces undesirable extra variance in the training procedure.", "The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.", "3) Usefulness of the CW distance outside of the autoencoder context.", "Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).", "Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?", "Without demonstrating any practical advance, this work becomes simply another one of the multitude of V/W-AE-variants that already exist.", "Other Comments:", "- While I agree that standard WAE-MMD and SWAE require some form of sampling to compute their respective statistical distance, a variant of WAE-MMD could be converted to a closed form statistical distance in the case of a Gaussian prior, by way of Stein's method or other existing goodness-of-fit measures designed specifically for Gaussians.", "See for example:", "Chwialkowski et al: https://arxiv.org/pdf/1602.02964.pdf", "which like CW-distance is also a quadratic-time closed-form distance between samples and a target density.", "Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.", "- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?", "After reading the revision: I have raised my score by 1 point and recommend acceptance."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_explanation", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "arg-structuring_summary", "arg-request_experiment", "arg-request_experiment", "none", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 339, "sentences": ["This work proposes an approach for explicitly placing information in a subset of the latent variables.", "The approach is to construct an auxiliary generative model that takes as input the set of latent variables subtracted from the target subset, which is used to model modified data samples that do not contain the desired information.", "Experiments focus on learning global information.", "The auxiliary model is then given data that have their global information destroyed via random shuffling of image patches.", "# Approach seems limited.", "- This approach seems very limited, as there must exist a known transformation that removes the desired information.", "Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)", "- Can this approach learn multiple factors as opposed to just two?", "- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.)", "# More ablations or experiments with comparable settings would be desirable.", "- What is the choice of beta in the beta-VAE training objective? Apart from 1.2, this isn't mentioned.", "My concern here is that beta might be affecting the result more than the proposed training algorithm.", "Can the proposed approach perform just as well without a modified objective?", "Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper. (e.g. this approach with normal VAE objective, and normal VAE objective without auxiliary task for the clustering experiment.)", "- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?", "# Related work.", "There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:", "- Tranforming autoencoders [1] also apply a transformation to the image, but the goal is to learn the factor corresponding to the transformation, rather than the complement as in this work.", "- An opposing approach for explicit information placement with a modified training procedure (where the target information is directly placed in the target subset and can handle multiple factors) is DC-IGN [2].", "I believe the DC-IGN approach is more general and can handle a superset of the tasks of this approach, without requiring an auxiliary decoder.", "Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?", "[1] Hinton, Geoffrey E., Alex Krizhevsky, and Sida D. Wang. \"Transforming auto-encoders.\" International Conference on Artificial Neural Networks. Springer, Berlin, Heidelberg, 2011.", "[2] Kulkarni, Tejas D., et al. \"Deep convolutional inverse graphics network.\" Advances in neural information processing systems. 2015.", "---- Update since rebuttal ----", "I thank the authors for clarifying how this work fits in with related works and clarifying the hyperparameters.", "I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.", "More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.", "The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation."], "labels": ["none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 340, "sentences": ["This work addresses the important problem of generation bias and a lack of diversity in generative models, which is often called model collapse.", "It proposed a new metric to measure the diversity of the generative model's \"worst\" outputs based on the sample clustering patterns.", "Furthermore, it proposed two blackbox approaches to increasing the model diversity through resampling the latent z. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white-box approaches.", "In terms of experiment setup, the authors chooses face generation as the area to investigate and measures the diversity by detecting the generated face identity.", "With the proposed methods, the authors showed that most STOA methods have a wide gap between the top p faces of the most popular face identities and randomly sampled faces.", "It further showed that the proposed blackbox approaches increases the proposed diversity metric without sacrificing image quality.", "The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.", "While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.", "For those reasons, I propose to REJECT this paper.", "Missing key experiments that will provide more motivation that 1. the new metric reflects human perception of diversity 2. the new metric works better than existing ones:", "1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.", "this is important since all your experiments rely on that assumption.", "2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.", "Missing assumptions about blackbox calibration approaches:", "1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?", "2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?", "A website that just exposes the image generation API may not allow you to ping their service 100k times to improve the generation diversity. If you are allowed to do that, it may be reasonable to assume that you can contact the API provider to get access to the rest of the model.", "Minor improvements that did not have a huge impact on the score", "1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.", "2. The statement \"IS, FID and MODE score takes both visual fidelity and diversity into account.\" under \"Evaluation of Mode Collapse\" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.", "3. You may want to consider stating the work as \"a pilot study\" (sec 6.) earlier in the abstract or in the introduction, so that the reader knows what to expect."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "none", "arg-request_clarification", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 341, "sentences": ["Privacy concerns arise when data is shared with third parties, a common occurrence.", "This paper proposes a privacy-preserving classification framework that consists of an encoder that extracts features from data, a classifier that performs the actual classification, and a decoder that tries to reconstruct the original data.", "In a mobile computing setting, the encoder is deployed at the client side and the classification is performed on the server side which accesses only the output features of the encoder.", "The adversarial training process guarantees good accuracy of the classifier while there is no decoder being able to reconstruct the original input sample accurately.", "Experimental results are provided to confirm the usefulness of the algorithm.", "The problem of privacy-preserving learning is an important topic and the paper proposes an interesting framework for that. However, I think it needs to provide more solid evaluations of the proposed algorithm, and presentation also need to be improved a bit.", "Detailed comments:", "I don\u2019t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.", "The decoder used to measure privacy is very important. Can you provide more detail about the decoders used in all the four cases?", "If possible, evaluating the privacy with different decoders may provide a stronger evidence for the proposed method.", "It seems that DNN(resized) is a generalization of DNN.", "If so, by changing the magnitude of noise and projection dimensions for PCA should give a DNN(resized) result (in Figure 3) that is close to DNN.", "If the two NNs used in DNN and DNN(resized) are different, I believe it\u2019s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.", "The abstract mentioned that the proposed algorithm works as an \u201cimplicit regularization leading to better classification accuracy than the original model which completely ignores privacy\u201d. But I don\u2019t see clearly from the experimental results how the accuracy compares to a non-private classifier.", "Section 2.2 mentioned how different kind of layers would help with the encoder\u2019s utility and privacy. It would be better to back up the argument with some experiments.", "I think it needs to be made clearer how reconstruction error works as a measure of privacy.", "For example, an image which is totally unreadable for human eye might still leak sensitive information when fed into a machine learning model.", "In term of reference, it\u2019s better to cite more articles with different kind of privacy attacks for how raw data can cause privacy risks.", "For the \u201cNoisy Data\u201d method, it\u2019s better to cite more articles on differential privacy and local differential privacy.", "Some figures, like Figure 3 and 4, are hard to read.", "The author may consider making the figures larger (maybe with a 2 by 2 layout), adjusting the position of the legend & scale of x-axis for Figure 3, and using markers with different colors for Figure 4."], "labels": ["none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-request_explanation", "arg-request_experiment", "none", "none", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_clarification", "none", "arg-request_edit", "arg-request_edit", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 342, "sentences": ["The suggested method proposes a technique to compress neural networks bases on PQ quantization.", "The algorithm quantizes matrices of linear operations, and, by generalization, also works on convolutional networks.", "Rather than trying to compress weights (i.e. to minimize distance between original and quantized weights), the algorithm considers a distribution of unlabeled inputs and looks for such quantization which would affect output activations as little as possible over that distribution of data.", "The algorithm works by splitting each column of W_ij into m equal subvectors, learning a codebook for those subvectors, and encoding each of those subvectors as one of the words from the codebook.", "The method provides impressive compression ratios (in the order of x20-30) but at the cost of a lower performance.", "Whether this is a valuable trade-off is highly application dependent.", "Overall I find the paper interesting and enjoyable.", "However, as I am not an expert in the research area, I can not assess how state of the art the suggested method is.", "There are a few other questions that I think would be nice to answer. I will try to describe them below:", "Suppose we have a matric W_{ij} with dimensions NxM where changing i for a given j defines a column.", "By definition, linear operation is defined", "y_i = sum_j W_ij x_j .", "Now say each column of matrix W is quantized into m subvectors.", "We can express W_ij in the following way:", "W_ij = (V^1_ij + V^2_ij + ... V^m_ij)x_j where V^m_ij is zero everywhere except for the rows covering a given quantized vector.", "For example, if W had dimensions of 8x16 and m=4,", "V^2_{3,j}=0, for all j", ", V^2_{4,j}=non_zero, V^2_{7,j}=non_zero, V^2_{8,j}=0, V^2_{i=4:8,j}=one_of_the_quantized_vectors.", "y_i = sum_j W_ij x_j = sum_k sum_j (V^k_ij) x_j =def= sum_k z^k_i where z^k are partial products: z^k_i=0 for i<k*N/m and i>(k+1)N/m", "Thus, the suggested solution effectively splits the output vector y_i into m sections, defines sparse matrices V^k_{ij} 1<=k<=m, and performs column-wise vector quantization for these matrices separately.", "Generally, it is not ovious or given that the current method would be able to compress general matrices well, as it implicitly assumes that weight W_{ij} has a high \"correlation\" with weights W_{i+kN/m,j} (which I call \"vertical\" correlation), W_{i,k+some_number} (which I call \"horizontal\" correlation) and W_{i+kN/m,k+some_number} (which I call \"other\" correlation).", "It is not given that those kind of redundancies would exist in arbitrary weight matrices.", "Naturally, the method will work well when weight matrices have a lot of structure and then quantized vectors can be reused.", "Matrices can have either \"horizontal\" or \"vertical\" redundancy (or \"other\" or neither).", "It would be very interesting to see which kind of redundancy their method managed to caprture.", "In the 'horizontal' case, it should work well when inputs have a lot of redundancy (say x_j' and x_j'' are highly correlated making it possible to reuse code-words horizontally within any given V^k: V^k_ij'=V^k_ij'').", "However, if thise was the case, it would make more sense to simply remove redundancy by prunning input vector x_j by removing either x_j' or x_j'' from it.", "This can be dome by removing one of the outputs from the previous layer.", "This can be a symptom of a redundant input.", "Another option is exploiting \"vertical\" redundancy: this happens when output y_i' is correlated with output y_{i'+N/m}. This allows the same code-word to be reused vertically.", "This can be a symptom of a redundant output.", "It could also be the case that compressibility could be further subtantially improved by trying different matrix row permutations.", "Also, if one notices that y_i' ir correlated with y_i'', it might make sense to permute matrix rows in such a way that both rows would end up a multiple N/m apart.", "It would be interesting to see how this would affect compressibility.", "The third case is when code words are reused in arbitrary cases.", "Generally, I think that answering the following questions would be interesting and could guide further research:", "1. It would be very interesting to know what kind of code-word reusa patterns the algorithm was able to capture, as this may guide further research.", "2. How invariance copressibility is under random permutations of matrix rows (thus also output vectors)?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_result", "arg-request_experiment", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_explanation_label"]}
{"abstract_id": 343, "sentences": ["The authors consider the use of tensor approximations to more accurately capture syntactical aspects of compositionality for word embeddings.", "Given two words a and b, when your goal is to find a word whose meaning is roughly that of the phrase (a,b) , a standard approach to to find the word whose embedding is close to the sum of the embeddings, a + b. The authors point out that others have observed that this form of compositionality does not leverage any information on the syntax of the pair (a,b), and the propose using a tensor contraction to model an additional multiplicative interaction between a and b, so they propose finding the word whose embedding is closest to a + b + T*a*b, where T is a tensor, and T*a*b denotes the vector obtained by contracting a and b with T. They test this idea specifically on the use-case where (a,b) is an adjective,noun pair, and show that their form of compositionality outperforms weighted versions of additive compositionality in terms of spearman and pearson correlation with human judgements.", "In their model, the word embeddings are learned separately, then the tensor T is learned by minimizing an objective whose goal is to minimize the error in predicting observed trigram statistics.", "The specific objective comes from a nontrivial tensorial extension of the original matricial RAND-WALK model for learning word embeddings.", "The topic is fitting with ICLR, and some attendees will find the results interesting.", "As in the original RAND-WALK paper, the theory is interesting, but not the main attraction, as it relies on strong generative modeling assumptions that essentially bake in the desired results.", "The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).", "Pros:", "- theoretical justification is given for their assumption that the higher-order interactions can be modeled by a tensor", "- the tensor model does deliver some improvement over linear composition on noun-adjective pairs when measured against human judgement", "Cons:", "- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.", "- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?", "- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper", "Some additional citations:", "- the above-mentioned ICLR paper provides a performant alternative to unweighted linear composition", "- the 2017 Gittens, Achlioptas, Drineas ACL paper provides theory on the linear composition of some word embeddings"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 344, "sentences": ["The paper presents an end-to-end methods for jointly training named entity recognition (NER) and relation extraction (RE).", "The model leverage pre-trained BERT language models, making it very fast to train.", "The methods is evaluated on 5 standard NER+RE datasets with good performances.", "Pros:", "- the paper is well written and very clear", "- the proposed model has two main advantages: (1) it is very fast to train due to the use of pre-trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)", "Cons:", "- I think the main source of improvement comes from the BERT representations used as input.", "As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.", "- Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train. This is not really surprising..."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 345, "sentences": ["This paper puts forth adversarial architectures for TTS.", "Currently, there aren't many examples (e.g. Donahue et al,  Engel et al. referenced in paper) of GANs being used successfully in TTS, so this papers in this area are significant.", "The architectures proposed are convolutional (in the manner of Yu and Koltun), with increasing receptive field sizes taking into account the long term dependency structure inherent in speech signals.", "The input to the generator are linguistic and pitch signals - extracted externally, and noise.", "In that sense, we are working with a conditional GAN.", "I found the discriminator design very interesting.", "As the comment below notes, it is a sort of patch GAN discriminator (See pix2pix, and this comment from Philip Isola - https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39) and that is could be quite significant in that it classifies at different scales.", "In the image world, having a single discriminator for the whole model would not take into account local structure of the images.", "Likewise, perhaps we can imagine something similar in the case of audio at varying scales - in fact, audio dependencies are even more long range.", "That might be one reason why the variable window sizes work here.", "The paper also presents to image analogues for metrics based on FID and the KID, with the features being taken from DeepSpeech2.", "I found the speech sample presented very convincing.", "In general, the architectures are also presented quite clearly, so it seems that we might be able to reproduce these experiments in our own practice.", "It is also promising that producing good speech could be achieved by a non-autoregressive or attention based architecture.", "The authors mention that they hardly encounter any issues with training stability and mode collapse. Is that because of the design of the multiple discriminator architecture?"], "labels": ["arg-structuring_summary", "none", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 346, "sentences": ["This paper tries to quantify how \"dense\" representations we need for a specific task -- more specifically, how many dimensions are needed from a given representation (for a given task) to achieve a percentage of the performance of the entire representation.", "The second thing the paper tries to quantify is how well representations learned for one task can be fine tuned for another.", "Experiments are conducted with 4 different representation technique on a dozen or so tasks.", "Quick summary: While I liked aspects of this -- including the motivation of having a lightweight way of understanding how well representations transfer across tasks, overall my concerns surrounding the methodology and some missing analysis leads me to believe this needs more work before it is ready for publication.", "Quality: Below average", "I believe the proposed techniques have some flaws which hurt the eventual method.", "There are also concerns about the motivations behind parts of the technique.", "Clarity: Fair", "There were some experimental details that were poorly explained but in general the paper was readable.", "Originality: Fair", "There were some nice ideas in the work but I remain concerned about aspects of it.", "Significance: Below average", "My concern is that the flaws in the method do not make it conducive to use as is.", "Strengths / Things I liked:", "+ I really liked the motivating problem of being able to (hopefully cheaply / efficiently) estimate transfer potential to understand how well representations will perform on a different task.", "+ Multiple representations and tasks experimented with", "Weaknesses / Things that concerned me:", "(In no specific order)", "- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.", "While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.", "This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance)", "Let's take an example: Say there is a single dimension of the representation that is a perfect predictor of a task.", "Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.", "To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.", "- (W2) Related to the last line: I did not see any experiments / analysis showing how stable these different numbers are across different runs of the representation technique. Nor did I see any error bars in the experiments.", "This again greatly concerned me as I am not certain how stable these metrics are.", "- (W3) Baselines for transfer learning: I felt this was another notable oversight.", "I would have liked to see results for both trivial baselines like random ranking as well as more informed baselines where we can estimate transfer potential using say k representation techniques, and then use that to help us understand how well it would do on the other representations.", "This latter baseline is a zero-cost baseline as it is not even dependent on the method.", "- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how \"precision\" and NDCG are used as metrics.", "Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the \"gold\" set. How is precision and NDCG calculated from this?", "More importantly I don't believe looking at rank alone is sufficient since that completely obscures the actual performance numbers obtained via transfer.", "In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.", "- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.", "(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)", "- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful", "-", "(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.", "I find this striking because I can easily come up with cheaper alternatives to get at this \"density\".", "For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.", "If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?", "Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?", "-", "(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets", "- (W8)", "The proposed  CLF weight difference method has some concerning aspects as well.", "For example say we had two task with exact opposite labels.", "They would have a very low weight difference score though they are ideal representations for each other.", "Likewise looking at a difference of weight vectors seems arbitrary in other ways as well."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 347, "sentences": ["This paper tackles vulnerability to poisoning.", "An important subtopic of adversarial ML.", "The authors propose using a GAN to generate poisoning data points, as an alternative to existing methods.", "While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).", "Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 348, "sentences": ["This paper presents a novel approach to bundle adjustment, where traditional geometric optimization is paired with deep learning.", "Specifically, a CNN computes both a multi-scale feature pyramid and a depth prediction, expressed as a linear combination of \"depth bases\".", "These values are used to define a dense re-projection error over the images, akin to that of dense or semi-dense methods.", "Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).", "By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.", "The paper is clear, well organized, well written and easy to follow.", "Even if the idea of joining BA / SfM and deep learning is not new, the authors propose an interesting novel formulation.", "In particular, being able to train the CNN with a supervision signal coming directly from the same geometric optimization process that will be used at test time allows it to produce features that  will make the optimization smoother and the convergence easier.", "The experiments are quite convincing and seem to clearly support the efficacy of the proposed method.", "I don't really have any major criticism, but I would like to hear the authors' opinions on the following two points:", "1) In page 5, the authors write \"learns to predict a better damping factor lambda, which gaurantees that the optimziation will converged to a better solution within limited iterations\".", "I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.", "The word \"guarantee\" usually implies that the effect can be somehow mathematically proved, which is not done in the paper.", "2) As far as I can understand, once the networks are learned, possibly on pairs of images due to GPU memory limitations, the proposed approach can be easily applied to sets of images of any size, as the features and depth predictions can be pre-computed and stored in main system memory.", "Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-request_clarification", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 349, "sentences": ["The authors consider the scenario of two agents, a demonstrator acting in an environment to achieve a goal, and a learner, which can also interact with the environment, but whose goal is to learn the demonstrator\u2019s policy by carrying out actions eliciting strong changes in the demonstrator\u2019s trajectory.", "The former is implemented as imitation learning, i.e. policy learning, the latter as curiosity driven RL.", "The authors are encouraged to review some of the related literature on optimal teaching, which also has developed a rich set of approaches to agent modeling, e.g. the work by Patrick Shafto.", "It may also be relevant to think about the relationship to active learning in IRL.", "I am not sure whether I would be able to implement and reproduce the presented work on the basis of the current manuscript including the appendix.", "It would be very helpful for the community to be able to do so.", "E.g., details on the the training of the demonstrators, their reward functions, and the behavior tracker.", "Particularly the \"fusion\" module remains extremely unclear.", "Overall, this is a nice paper, despite the fact that the example domains and problems considered are engineered strongly to allow for the proposed algorithm to be useful.", "Particularly for the claim of generalization to different environments, the details are all in the engineering of the particular grid world tasks, how they relate to each other and the sate representation used for the demonstrator s_d.", "I am not sure why it was submitted to ICLR and not the Annual Meeting of the Cognitive Science Society, though.", "Minor points:", "\u201cdiffers from this in two folds\u201d", "\u201cby generate queries\u201d"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-request_edit", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 350, "sentences": ["The authors suggest a method to create combined low-dimensional representations for combinations of pairs of words which have a specific syntactic relationship (e.g. adjective - noun).", "Building on the generative word embedding model provided by Arora et al. (2015), their solution uses the core tensor from the Tucker decomposition of a 3-way PMI tensor to generate an additive term, used in the composition of two word embedding vectors.", "Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.", "Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.", "Though the title promises a contribution to an understanding of word embedding compositions in general, they barely expound on the broader implications of their idea in representing elements of language through vectors.", "Their lack of willingness to ground their claims or decisions is even more apparent in two other cases.", "The authors claim that the Arora's RAND-WALK model does not capture any syntactic information. This is not true.", "The results presented by Arora et al. indeed show that RAND-WALK captures syntactic information, albeit to a lesser extent than other popular methods for word embedding (Table 1, Arora et al. 2015).", "Another unjustified choice by the authors is their choice of weighing the Tensor term (when it is being added to two base embedding vectors) in the phrase similarity experiment.", "The reason the authors provide for weighing the composition Tensor is the fact that in the unweighted version their model produced a worse performance than the additive composition.", "One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.", "Arora's generative model for word embeddings, on which the current paper is largely based upon, not only make the mathematical relationship among different popular word embedding methods explicit, but also by making and verifying explicit assumptions with regard to properties of the word embeddings created by their model, they are able to explain why low-dimensional embeddings provide superior performance in tasks that implicate semantic relationships as linear algebraic relations.", "Present work, however interesting with regard to its potential implications, strays away from providing such theoretical insights and suffices with demonstrating limited improvements in empirical tasks."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none", "arg-request_edit", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label"]}
{"abstract_id": 351, "sentences": ["This paper presents a set of statistical tools, that are applicable to quantitatively measuring the mode collapse of GANs.", "The authors consistently observe strong mode collapse on several state-of-the-art GANs using the proposed toolset.", "The authors analyze possible causes, and for the first time present two simple yet effective \u201cblack-box\u201d methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data.", "The writing and presentation are good.", "My concerns regarding this paper are as below.", "1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].", "2) The main contributions of this paper are not quite clear to me.", "3) Typos need to be corrected in next version, e.g., all equations should have punctuation mark at the end, all e.g., i.e., et al., etc. should be italic, format of references should be consistent.", "Based on my comments above, I decide to give the rate of WA for this paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "arg-request_typo", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 352, "sentences": ["Regularizing RKHS norm is a classic way to prevent overfitting.", "The authors note the connections between RKHS norm and several common regularization and robustness enhancement techniques, including gradient penalty, robust optimization via PGD and spectral norm normalization.", "They can be seen as upper or lower bounds of the RKHS norm.", "There are some interesting findings in the experiments. For example, for improving generalization, using the gradient penalty based method seems to work best.", "For improving robustness, adversarial training with PGD has the best results (which matches the conclusions by Madry et al.); but as shown in Figure 2, because adversarial training only decreases a lower bound of RKHS norm, it does not necessarily decrease the upper bound (the product of spectral norms).", "This can be shown as a weakness of adversarial training if the authors explore further and deeper in this direction.", "Overall, this paper has many interesting results, but its contribution is limited because:", "1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has been well studied by previous literature. This paper simply applies these results to deep neural networks, by treating the neural network as a big black-box function f(x) .", "Many of the results have been already presented in previous works like Bietti & Mairal (2018).", "2. In experiments, the authors explored many existing methods on improving generalization and robustness. However all these methods are known and not new.", "Ideally, the authors can go further and propose a new regularization method based on the connection between neural networks and RKHS, and conduct experiments to show its effectiveness.", "The paper is overall well written, and the introductions to RKHS and each regularization techniques are very clear.", "The provided experiments also include some interesting findings.", "My major concern is the lack of novel contributions in this paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 353, "sentences": ["This paper presents a classification method when the data consists of few clean labels and many noisy labels.", "The authors propose to construct a graph structure within each class and use graph convolutional network to determine the clean/noisy labels of samples in each class.", "The model is based on a binary cross entropy loss function in each class, which learns the probability of labels to be clean. And such \"clean\" probability is used as the measure of relevance score between the sample different classes.", "The idea of this paper is straightforward and the experimental results seem promising.", "The authors compare with several related methods and show the proposed method has better performance in few shot learning experiments.", "For the motivation of this methods, why would the graph be constructed within each class? If there is correlation between different classes, how could the model use such class-wise correlation to clean the label?", "Maybe I missed it, but how is the relevance score / predicted label determined for testing data given the graphs constructed in each class of training data?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_clarification", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 354, "sentences": ["This paper investigates an SGD variant (PowerSGD) where the stochastic gradient is raised to a power of $\\gamma \\in [0,1]$.  The authors introduce PowerSGD and PowerSGD with momentum (PowerSGDM).", "The theoretical proof of  the convergence is given and experimental results show that the proposed algorithm converges faster than some of the existing popular adaptive SGD techniques.", "Intuitively, the proposed PowerSGD can boost the gradient (since $\\gamma \\in [0,1]$) so it may be helpful for the gradient of the lower layers of a deep network which may be hit by the vanishing gradient issue.", "This may give rise to a faster convergence.", "So overall the idea makes sense but I have the following concerns.", "1.", "The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.", "At the first glance, it is $O(\\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\\frac{1}{\\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.", "In other words, when the number of iterations is large, the batch size will be large too.", "I consider this assumption unrealistic.", "Given that $T$ is typically very large (it is iterations, not epochs),  it will require a huge batch size, probably close to the whole training set.", "In this case, it is basically a GD, not SGD any more.", "That's why the rate is $O(\\frac{1}{T})$, which is the convergence rate of GD.", "I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.", "Actually in the experiments the authors never use an increasing batch size.", "Instead, a constant batch size 128 is used.", "Therefore,  the faster convergence demonstrated in the experiments can not be explained by Theorem 3.1 or Theorem 3.2.", "2. There are numerous inaccuracies in the proof given the supplementary material.", "For instance, in Eq.7,", "$\\nabla f(x) \\sigma(\\nabla f(x))$", "should be $\\nabla f(x)^{T} \\sigma(\\nabla f(x))$   The random variable $\\xi_{t}$ should be a scalar on training samples, not a vector, etc..  The authors should clean it up.", "3. It would be helpful to show the $\\gamma$ value on each experiment with different tasks.", "It would be good to know how $\\gamma$ varies across tasks.", "4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.", "5. The term \"PowerSGD\" seems to have been used by other papers."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_quote", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_experiment_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 355, "sentences": ["Authors use control theory to analyze and stabilize GAN's training.", "Their method, effectively, adds an L2 regularization to the output of the discriminator.", "I have some concerns regarding the novelty, analysis and also the experiments.", "- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?", "- In eq 9 in the dynamics of WGAN section, the discriminator should be restricted to Lip functions.", "This has not been considered in the analysis.", "- There are a few work in the literature that analyze local stability of GANs (e.g. https://arxiv.org/abs/1706.04156) as well as using some control theory for analyzing global stability of GANs (e.g. https://arxiv.org/abs/1710.10793).", "The connections of the proposed approach with existing literature should be better explained.", "- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-request_experiment", "none", "none", "none", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label"]}
{"abstract_id": 356, "sentences": ["This paper introduced a proximal approach to optimize neural networks by linearizing the network output instead of the loss function.", "They demonstrate their algorithm on multi-class hinge loss, where they can show that optimal step size can be computed in close form without significant additional cost.", "Their experimental results showed competitive performance to SGD/Adam on the same network architectures.", "1.", "Figure 1 is crucial to the algorithm design as it aims to prove that Loss-Preserving Linearization (LPL) preserves information on loss function.", "While the authors provided numerical plots to compare it with the SGD linearization, I personally prefer to see some analytically comparsion between SGD linearization and LPL even on the simplest case.", "An appendix with more numerical comparisons on other loss functions might also be insightful.", "2. It seems LPL is mainly compared to SGD for convergence (e.g. Fig 2).", "In Table 2 I saw some optimizers end up with much lower test accuracy.", "Can the authors show the convergence plots of these methods (similar to Figure 2)?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 357, "sentences": ["edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns.", "I think the paper is much more convincing now. I am happy to increase my rating to clear accept.", "I also agree with the introduction of the Chi vector, and with the use of the term of \"photometric BA\", since it was used before, even if it is unfortunate in my opinion.", "I thank the authors to replace reprojection by alignment, which is much clearer.", "---------------", "This paper presents a method for dense Structure-from-Motion using Deep Learning:", "The input is a set of images; the output is the camera poses and the depth maps for all the images.", "The approach is inspired by Levenberg-Marquardt optimization (LM): A pipeline extracting image features computes the Jacobian of an error function.", "This Jacobian is used to update an estimate of the camera poses.", "As in LM optimization, this update is done based on a factor lambda, weighting a gradient descent step and a Gauss-Newton step.", "In LM optimization, this lambda evolves with the improvement of the estimate.", "Here lambda is also predicted using a network based on the feature difference.", "If I understand correctly, what is learned is how to compute image features that provide good updates, how to predict the depth maps from the features, and how to predict lambda.", "The method is compared against DeMoN and other baselines with good results.", "I like the fact that the method is based on LM optimization, which is the standard method in 'geometric bundle adjustment', while related works consider Gauss-Newton-like optimization steps.", "The key was to include a network to predict lambda as well.", "However, I have several concerns:", "* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.", "The image features learned with the proposed method are re-used in an approach using a fixed lambda.", "If I understand correctly, there are 2 things wrong with that:", "- for GN optimization, lambda should be set to 0 - not a constant value.", "Several constant values should also have been tried.", "- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.", "Thus, the advantage of using a LM optimization scheme is not very convincing.", "Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.", "* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.", "Less critical concerns that still should be taken into account if the paper is accepted:", "- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.", "- the name 'Bundle Adjustment' is actually not adapted to the proposed method.", "'Bundle Adjustment' in 'geometric computer vision' comes from the optimization of several rays to intersect at the same 3D point, which is done by minimizing the reprojection errors.", "Here the objective function is based on image feature differences.", "I thus find the name misleading.", "The end of Section 3 also encourages the reader to think that the proposed method is based on the reprojection error.", "The proposed method is more about dense alignment for multiple images.", "More minor points:", "1st paragraph:", "Marquet -> Marquardt", "title of Section 3: revisitED", "1st paragraph of Section 3: audience -> reader", "caption of Fig 1: extractS", "Eq (2) cannot have Delta Chi on the two sides.", "Typically, the left side should be \\hat{\\Delta \\Chi}", "before Eq (3): the 'photometric ..' -> a 'photometric ..'", "1st paragraph of Section 4.3: difficulties -> reason", "typo in absolute in caption of Fig 4", "Eq (6): Is B the same for all scenes?", "It would be interesting to visualize it.", "Section 4.5: applies - > apply"], "labels": ["none", "none", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_quote", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_quote", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_edit", "arg-request_typo", "arg-request_edit", "none", "arg-request_typo", "arg-request_edit", "arg-request_typo", "arg-request_clarification", "arg-request_result", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_typo_label"]}
{"abstract_id": 358, "sentences": ["This paper presents ReMixMatch an improved version of MixMatch.", "The main contributions are the distribution alignment and the augmentation anchoring.", "Distribution alignment rescales the predictions based on the difference between the model marginals and the ground truth running average estimation.", "Augmentation anchoring instead of computing the guessed probabilities on unlabelled data as the average probabilities on transformed samples (as in MixMatch), it considers as guessed labels the average probabilities obtained from weak transformations (flip+crop) even when using stronger transformations (Autoaugment like).", "The paper is well written, has interesting experiments and very impressive results.", "However, there are some negative points that the authors should clarify:", "- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.", "- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.", "This is not so interesting, even though results are impressive.", "If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).", "Overall the paper is well presented and contributes to further improve the performance on semi-supervised learning.", "I there fore recommend it for acceptance.", "However, I would like to see in the paper a more general overview on the fact that strong transformations can further improve semi-supervised methods and ReMixMatch is a way to leverage those transformations.", "Additional comments:", "- Instead of using the rescaling trick for distribution alignment, what about enforcing the marginal distribution on the annotated data and the marginal distribution of the model to be similar with KL divergence? Would it be better or worse than the proposed approach?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_edit", "none", "none", "arg-request_edit", "arg-structuring_heading", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_explanation_label"]}
{"abstract_id": 359, "sentences": ["This paper proposes simple metrics for measuring the \"information density\" in learned representations.", "Overall, this is an interesting direction.", "However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.", "And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.", "+ The overarching questions that the authors set out to answer: How task-specific information is stored and to what extent this transfers, is inherently interesting and important.", "+ The proposed metrics and simple and intuitive.", "+ It is interesting that a few units seem to capture most task specific information.", "- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.", "As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.", "Yet the metrics proposed depend on supervision in the target domain.", "If we already have this, then -- as the authors themselves note -- it is trivial to simply try out different source datasets empirically on a target dev set.", "It is argued that this is an issue because it requires training 2n networks, where n is the number of source tasks.", "I am unconvinced that one frequently enough has access to a sufficiently large set of candidate source tasks for this to be a real practical issue.", "- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.", "The LSTM architecture used is reasonable, but it would be nice to see how much results change (if at all) with alternative architectures.", "- The CFS metric depends on a hyperparameter (the \"retention ratio\"), which here is arbitrarily set to 80% without any justification.", "- What is the motivation for the restriction to linear models? In the referenced probing paper, for example, MLPs were also used to explore whether attributes were coded for 'non-linearly'."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "arg-structuring_quote", "none", "none", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 360, "sentences": ["This paper proposes a new pipelined training approach to speedup the training for neural networks.", "The approach separates forward and backpropagation processes into multiple stages, cache the activation and gradients between stages, processes stages simultaneously, and then uses the stored activations to compute gradients for updating the weights.", "The approach leads to stale weights and gradients.", "The authors studied the relation between weight staleness and show that the quality degradation mainly correlates with the percentage of the weights being stale in the pipeline.", "The quality degradation can also be remedied by turning off the pipelining at the later training steps while overall training speed is still faster than without pipelined training.", "Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.", "Without the comparison it\u2019s not clear how much improvement this approach provides compared to existing work that perform stale updates."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 361, "sentences": ["\u2014 Summary", "The method extends [21], which proposes an unordered set prediction model for multi-class classification.", "For that problem, [21] can assume logistic outputs for all distinct classes.", "This work extends set prediction to the object detection task, where box identity is not distinct \u2014 this is handled by an additional model output that reasons about the most likely object permutations.", "The permutation predictions are used during training, but are not needed at inference time \u2014 as shown in Fig1 and Eq 7.", "Results are on detection of overlapping objects and a CAPTCHA toy summation example.", "\u2014 Clarity", "The exposition is not particularly clear in several places:", "- U^m in Eq 1 is undefined and un-discussed.", "What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.", "- The term p(w) disappears on the left hand side of Eq 2.", "- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.", "Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.", "Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).", "If so, why does term f2 in Eq 5 contain the permutation head output O2 and how do the two relate?", "- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.", "The dimensions of the convolutional feature map matter (probably need to be kept tractable).", "\u2014 Significance", "Key aspects of the model are not particularly clear, specifically about how the permutation prediction ( the key novelty here) is used to benefit training.", "\u2014 Term f2 in Eq5 uses w~ estimates, which appeared to be based on statistics from past SGD runs, yet also depends on the output of the permutation head O2. Am I misinterpreting the method?", "\u2014 In the paragraph right after Eq5, it\u2019s claimed that \u201cEmpirically, in our applications, we found out that estimation of the permutations from just f1 [in Eq5] is sufficient to train properly \u2026 by using the Hungarian algorithm\u201d. So then f2 term is not even used in. Eq5? If so, what is the significance of the permutation head other than adding an auxiliary loss?", "Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above \u2014 if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?", "While the failure of NMS to detect overlapping objects is expected, the experiments showing that perm-set prediction handles them well is interesting and promising.", "Solving the general case with larger images and many instances would increase the impact significantly \u2014 and likely require a combination of perm-set prediction and image tiling, although this is just a hypothesis.", "The Captcha toy example also shows some interesting behavior emerging \u2014 without digit-specific annotations (otherwise it would be multi-class classification setup from [21]), the model can handle the majority of summations correctly.", "\u2014 Experimental results", "The results are interesting proofs-of-concept but a few more experiments/answers would be helpful:", "- It still appears that PR curve in the high-precision regime (fig 3b) has lower precision than FRCNN/YOLO.", "Any idea as to why?", "- Ablation results on the effect of the permutation predictions vs Hungarian algorithm, etc would be helpful, as discussed above.", "- How sensitive is the method to seeing a certain cardinality? What if it never sees 3 pedestrians in an image, but only 1,2,4 will it fail to predict 3? Or alternatively, if we train a model that can handle up to 5-6 entities with examples than have <=4? What is the right way of data augmentation for this model (was there any and should there be?)", "- Given that values for U differ across applications, how sensitive is the output / how much sweeping did you have to do?", "-- Related work", "To the best of my knowledge it's representative.", "It would help to cite more recent work that decreases detector dependence on NMS.", "For example, \"Learning Non-Maximum Suppression\", Hosang, Benenson, Schiele, CVPR 2017 or \"Relation Networks for Object Detection\", by Hu et al, CVPR 2018 and references therein."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-request_clarification", "none", "none", "none", "none", "arg-request_clarification", "none", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "arg-request_result", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "none", "arg-request_explanation", "arg-request_experiment", "arg-request_result", "arg-request_result", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 362, "sentences": ["This paper proposed a method pGAN based on Generative Adversarial Networks to generate poisoning examples in order to degrade the performance of classifiers when trained on the poisoned training data.", "The authors evaluated pGAN on both synthetic datasets and commonly used MNIST and Fashion MNIST datasets in machine learning.", "The paper is self-contained and easy to read. My main concern is on the experiment results.", "The detailed questions are as follows:", "Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.", "Q2: Can the authors structure the experimental results with different sections? Currently it is just a single section which is difficult to read.", "Q3: The authors noticed that \u201cBut, as we decrease the value of \u03b1, the distribution of red points shifts towards the region where both green and blue distributions overlap\u201d.", "This observation is interesting as it finds that the poisoned input tends to lie on the overlap of two classes.", "But this can easily lead to a defense method: remove those training examples that are close to the other class.", "This defense mechanism can be used together with other sanitization approaches.", "So I would like to see how would pGAN perform in this case?", "Q4: The authors mentioned \u201cComparison with existing poisoning attacks in the research literature is challenging: Optimal poisoning attacks as in Munoz-Gonzalez et al. (2017) are computationally very expensive for the size of the networks and datasets used in our experiments in Fig. 2. \u201d .", "However, I can not agree because you can simply generate poisoned data and train the neural networks on the poisoned data regardless of the underlying approach that is targeted in generating the poisoned data.", "This would be an effective baseline to compare. (Correct me if I am wrong here.)", "I will change my score if the authors can address my concerns here.", "================================================================", "Thanks for the rebuttal. I am more convinced now."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_edit", "arg-structuring_quote", "none", "none", "none", "arg-request_experiment", "arg-structuring_quote", "none", "arg-request_experiment", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 363, "sentences": ["This paper presents a pretty cool idea for enabling \"adaptive\" kernels for CNNs which allow dramatic reduction in the size of models with moderate to large performance drops.", "In at least one case, the training time is also significantly reduced (2x).", "The best part about this paper is that the size of the models are much smaller; but the paper does offer any explanation of the value of this.", "For example, even a 1% drop in accuracy can be unacceptable; but in some applications (like cell phones and IOT devices) model size is critical.", "The authors' should add some wording to explain this value.", "The \"adaptive\"kernels the the authors talk about are really a new class of nonlinear kernels.", "It would be very interesting to see a discussion of the class of functions these nonlinear kernels represent.", "This kind of discussion would give the reader  motivation for the choice of function, ideas for how to improve in this class of functions, and insight into why it works.", "The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.", "It would be nice if the authors pointed to a git repository with their code an experiments.", "More importantly, the results presented are quite meager.", "If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.", "And the analysis of the \"dynamic range\" of the algorithim is missing.", "How do performance and model size trade off?", "How were the number of layers and kernels chosen?", "Was the 5x10x20x10 topology used for MNIST the only topology tried?", "That would be very surprising.", "What is the performance on all of the other topologies tried for the proposed algorithm?", "Was crossvalidation used to select the topology?", "If so, what was the methodology.", "Additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in Section 3; (2) omitted details, e.g., how much overlap exists between kernels (Figs. 1, 2, and 4 suggests there is no overlap - this should be made clear); and (3) poor grammar and nonstandard terminology, e.g., the authors' use of the word \"energy\" and the phrase \"degradation problem\".", "All of these issues should be addressed in a future version of the paper.", "Not sure why Eqns. 2 and 9 need any parentheses", ".  They should be removed."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_edit", "none", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_explanation", "none", "arg-request_experiment", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_result", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_edit", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 364, "sentences": ["The paper is really interesting.", "Set prediction problem has lots of applications in AI applications and the problem has not been conquered by deep networks.", "The paper proposes a formulation to learn the distribution over unobservable permutation variables based on deep networks and uses a MAP  estimator for inference.", "It has object detection applications.", "The results show that it can outperform YOLOv2 and Faster R-CNN in a small pedestrian detection dataset which contains heavy occlusions.", "The limitation is clearly stated in the last part of the paper that the number of possible permutations exponentially grows with the maximum set size (cardinality).", "In the author response period, I would like the author give more details about the pedestrian detection experiments, such as how many dense layers are used after ResNet-101, what are the training and inference time, is it possible to report results on PASCAL VOC (only the person class).", "The method is exciting for object detection funs.", "I would like to encourage the authors to release the code and let the whole object detection community overcome the limitation in the paper."], "labels": ["none", "none", "arg-structuring_summary", "none", "arg-structuring_summary", "none", "arg-request_clarification", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 365, "sentences": ["This paper proposed to use the duality gap sup_f V(f, g*) \u2013 inf_g V(f*, g) as a metric for GAN training.", "It proves that this metric is an upper bound of F-distance.", "It also proves a generalization bound for this metric.", "Simulation resultson MNIST, CIFAR10, etc. are reported.", "The contribution of this paper is incremental due to the following reasons.", "1) The duality gap is only an upper bound of the F-distance.", "This means that if the duality gap is zero then the learned distribution is the true distribution.", "However, the converse is not necessarily true: even if the algorithm starts with the true distribution, the duality gap may not be zero.", "Thus the metric is not a proper metric.", "The proof of the upper bound is straightforward.", "2) Another issue is the gap between the min-max formulation and the real training algorithm.", "As for GAN, due to the inexact update, it is not really solving the min-max problem.", "For the proposed metric, it is also impossible to solve sup_f V(f, g*) and inf_g V(f*, g) to reasonable accuracy.", "Thus what the algorithm is really doing, perhaps, is to optimizing a new loss which is the sum of the original loss and and an extra term.", "Viewing it as a \u201cduality gap\u201d seems to be far from the practical training.", "This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.", "3) The simulation is not convincing.", "The reported FID for CIFAR10 using WGAN-GP is 54.4, which seems to be a bit high.", "I\u2019m not sure whether it is due to parameter choice or due to weak D/G networks used in the simulation.", "If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 366, "sentences": ["This paper proposes a Frank-Wolfe based method, called DFW, for training Deep Network.", "The DFW method linearizes the loss function into a smooth one, and also adopts Nesterov Momentum to accelerate the training.", "Both techniques have been widely used in the literature for similar settings.", "This paper mainly focuses on the algorithm part, but only empirically demonstrate the convergence results.", "After reading the authors\u2019 feedback and the paper again, I think overall this is a good paper and should be of broader interest to the broader audience in machine learning community.", "In Section 6.1, the authors mention the good generalization is due to large number of steps at a high learning rate. Can we possibly get any theoretical justification on this?", "This paper uses multi class hinge loss as an example for illustration.", "Can this approach be applied for structure prediction, for example, various ranking loss?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "none", "arg-request_explanation", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 367, "sentences": ["Summary: This paper proposes a meta-learning solution for problems involving optimizing multiple loss values.", "They use a simple (small mlp), discrete, stochastic controller to control applications of updates among a finite number of different update procedures.", "This controller is a function of heuristic features derived from the optimization problem, and is optimized using policy gradient either exactly in toy settings or in a online / truncated manor on larger problems.", "They present results on 4 settings: quadratic regression, MLP classification, GAN, and multi-task MNT.", "They show promising performance on a number of tasks as well as show the controllers ability to generalize to novel tasks.", "This is an interesting method and tackles a impactful problem.", "The setup and formulation (using PG to meta-optimize a hyper parameter controller) is not extremely novel (there have been similar work learning hyper parameter controllers), but the structure, the problem domain, and applications are.", "The experimental results are through, and provide compelling proof that this method works as well as exploration as to why the method works (analyzing output softmax).", "Additionally the \"transfer to different models\" experiment is compelling.", "Comments vaguely in order of importance:", "1. I am a little surprised that this training strategy works.", "In the online setting for larger scale problems, your gradients are highly correlated and highly biased.", "As far as I can tell, you are performing something akin to truncated back back prop through time with policy gradients.", "The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.", "As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).", "Some comment as to this bias -- or even suggesting that it might exist would be useful.", "As of now, it is implied that the gradient estimator is unbiased.", "2. Second, even ignoring this bias, the resulting gradients are heavily correlated.", "Algorithm 1 shows no sign of performing batched updates on \\phi or anything to remove these corrections.", "Despite these concerns, your results seem solid.", "Nevertheless, further understanding as to this would be useful.", "3. The structure of the meta-training loop was unclear to me.", "Algorithm 1 states S=1 for all tasks while the body -- the overhead section -- you suggest multiple trainings are required ( S>1?).", "4. If the appendix is correct and learning is done entirely online, I believe the initialization of the meta-parameters would matter greatly -- if the default task performed poorly with a uniform distribution for sampling losses, performance would be horrible.", "This seems like a limitation of the method if this is the case.", "5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:", "5.1/Figure 1: I think there is an overloaded use of lambda? My understanding as written that lambda is both used in the grid search (table 1) to find the best loss l_1 and then used a second location, as a modification of l_2 and completely separate from the grid search?", "6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.", "It seems you performing controller optimization (optimizing phi), on the validation set loss, while also reporting scores on this validation set.", "This should most likely instead be a 3rd dataset.", "You have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.", "Given the low meta-parameter count of the I don't think this represents a huge risk, and baselines also suffer from this issue (hyper parameter search on validation set) so I expect results to be similar.", "7. Page 4: \"When ever applicable, the final reward $$ is clipped to a given range to avoid exploding or vanishing gradients\".", "It is unclear to me how this will avoid these.", "In particular, the \"exploding\" will come from the \\nabla log p term, not from the reward (unless you have reason to believe the rewards will grow exponentially).", "Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.", "This clipping will also introduce bias, this is not discussed, and will probably lower variance.", "This is a trade off made in a number of RL papers so it seems reasonable, but not for this reason.", "8. \"Beyond fixed schedules, automatically adjusting the training of G and D remains untacked\" -- this is not 100% true.", "While not a published paper, some early gan work [2] does contains a dynamic schedule but you are correct that this family of methods are not commonplace in modern gan research.", "9. Related work: While not exactly the same setting, I think [1] is worth looking at.", "This is quite similar causing me pause at this comment: \"first framework that tries to learn the optimization schedule in a data-driven way\".", "Like this work, they also lean a controller over hyper-parameters (in there case learning rate), with RL, using hand designed features.", "10. There seem to be a fair number of heuristic choices throughout.", "Why is IS squared in the reward for GAN training for example? Why is the scaling term required on all rewards?", "Having some guiding idea or theory for these choices or rational would be appreciated.", "11. Why is PPO introduced?", "In algorithm 1, it is unclear how PPO would fit into this?", "More details or an alternative algorithm in the appendix would be useful.", "Why wasn't PPO used on all larger scale models? Does the training / performance of the meta-optimizer (policy gradient  vs ppo) matter?", "I would expect it would.", "This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.", "12. \"It is worth noting that all GAN K:1 baselines perform worse than the rest and are skipped in Figure 2, echoing statements (Arjovsky, Gulrajani, Deng) that more updates of G than D might be preferable in GAN training.", "\" I disagree with this statement.", "The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.", "Arjovsky does discuss issues with training D to convergence, but I don't believe there is any exploration into multiple G steps per D step as a solution.", "13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.", "14: Claims in paper seem a little bold / overstating.", "The inception gain is marginal to previous methods, and trains slower than other baselines.", "This is also true of MNT section -- there, the best baseline model is not even given equal training time!", "There are highly positive points here, such as requiring less hyperparameter search / model evaluations to find performant models.", "15. Figure 4a. Consider reformatting data (maybe histogram of differences? Or scatter plot).", "Current representation is difficult to read / parse.", "Typos:", "page 2, \"objective term. on GANs, the AutoLoss: Capital o is needed.", "Page 3: Parameter Learning heading the period is not bolded.", "[1] Learning step size controllers for robust neural network training. Christian Daniel et. al.", "[2]http://torch.ch/blog/2015/11/13/gan.html", "[3] Understanding Short-Horizon Bias in Stochastic Meta-Optimization, Wu et.al.", "Given the positives, and in-spite of the negatives, I would recommend to accept this paper as it discusses an interesting and novel approach when controlling multiple loss values."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_clarification", "arg-request_explanation", "none", "none", "none", "none", "arg-request_explanation", "arg-request_clarification", "none", "none", "none", "none", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "none", "none", "arg-request_clarification", "arg-request_clarification", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-structuring_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 368, "sentences": ["This paper introduced a new architecture for input embeddings of neural language models: adaptive input representation (ADP).", "ADP allowed a model builder to define a set of bands of input words with different frequency where frequent words have larger embedding size than the others.", "The embeddings of each band are then projected into the same size.", "This resulted in lowering the number of parameters.", "Extensive experiments with the Transformer LM on WikiText-103 and Billion Word corpus showed that ADP achieved competitive perplexities.", "While tying weight with the output did not benefit the perplexity, it lowered the runtime significantly on Billion Word corpus.", "Further analyses showed that ADP gained performance across all word frequency ranges.", "Overall, the paper was well-written and the experiments supported the claim.", "The paper was very clear on its contribution.", "The variable-size input of this paper was novel as far as I know.", "However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.", "The weight sharing was also needed further investigation and experimental data on sharing different parts.", "The experiments compared several models with different input levels (characters, BPE, and words).", "The perplexities of the proposed approach were competitive with the character model with an advantage on the training time.", "However, the runtimes were a bit strange.", "For example, ADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4).", "The runtime of ADP seemed to lose in term of scaling as well to BPE.", "Perhaps, the training time was an artifact of multi-GPU training.", "Questions:", "1. I am curious about what would you get if you use ADP on BPE vocab set?", "2. How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 369, "sentences": ["The paper presents a new attack: Shadow Attack, which can generate imperceptible adversarial samples.", "This method is based on adding regularization on total variation, color change in each channel and similar perturbation in each channel.", "This method is easy to follow and a lot of examples of different experiments are shown.", "However, I have several questions about motivation and method.", "First, the proposed attack method can yield adversarial perturbations to images that are large in the \\ell_p norm.", "Therefore, the authors claim that the method can attack certified systems.", "However, attack in Wasserstein distance and some other methods can also do so. They can generate adversarial examples whose \\ell_p norm is large.", "I think the author should have some discussions about these related methods.", "Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1]. I hope to see some discussions about this.", "Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.", "[1] Salman, Hadi, et al. \"Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers.\" Neuips (2019)."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_explanation", "arg-request_result", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 370, "sentences": ["The authors make use of the theory of functional gradient, based on optimal transport, to develop a method that can promote the entropy of the generator distribution without directly estimating the entropy itself.", "Theoretical results are provided as well as necessary experiments to support their technique's outperformance in some data sets.", "I found that this is an interesting paper, both original ideal and numerical results."], "labels": ["arg-structuring_summary", "none", "none"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 371, "sentences": ["This paper proposed a new training framework to disentangle global structures from local structures based on Variational Autoencoders (VAEs). They first generate a transformed image by shuffl\u000fing the patches of the original image to destroy the global structures.", "The training task forces the model to reconstruct the original image and shuffled images from different latent variables, thus separating global long-range structural correlations and local patch-wise correlations .", "Instead of adjusting the objective function or model structure, the paper proposed a new and simple training framework to disentangle the global and local structures, which is novel.", "The experiment results are good on SVHN.", "Some visual inspection experiments on CIFAR10 are performed.", "The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.", "The rest experiments are all based on SVHN, which is too simple.", "More experiments based on other types of data sets with clear global structures such as faces or stop signs will be more convincing.", "In the digit dataset, the local and global structures are relatively easy to separate.", "However , in Table 1, the performance of VAE+Auxiliary is not better than two of the other methods.", "The idea in this paper is novel but experiments do not seem to be enough.", "More experiments on datasets with clear global and local structure separations with careful analyses are required to make the paper stronger."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 372, "sentences": ["This paper proposes use of intra-life coverage (an agent must visit all locations within each episode) for effective exploration in Atari games.", "This is in contrast of approaches that use inter-life coverage or curiosity metrics to incentivize exploration.", "The paper shows detailed results and analysis on 2 Atari games: Montezuma\u2019s Revenge and Seaquest, and reports results on other games as well.", "Strengths", "1. Intuitively, the idea of intra-life curiosity is reasonable.", "The paper pursues this idea and provides experimental evidence towards it on 2 Atari games.", "It is able to show compelling improvements on the challenging Montezuma\u2019s Revenge game.", "Weaknesses", "1. The two primary comparison points are missing:", "1a. Comparison to other exploration methods.", "A number of methods that use state visitation counts (also referred to as diversity, eg. [A,B]), or prediction error (also referred to as curiosity, eg [C]) have been proposed in recent years.", "It is important to place the contributions in this paper in context of these other works.", "A number of these references are missing and no experimental comparison to these methods has been made.", "1b. Comparison between inter and intra life curiosity.", "One of the central motivation is the utility of intra-life curiosity vs inter-life curiosity, yet no comparisons to this effect have been provided.", "2.", "Additionally, the paper employs a custom way of computing coverage (or diversity).", "It is in terms of location of agent on the screen, as opposed to featurization of the full game screen as used in prior works.", "It is possible that a large part of the gain comes from the clever design of the space for computing intrinsic exploration reward.", "The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).", "More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.", "The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.", "3. I will encourage investigation on a more varied set of tasks.", "Perhaps, also using some MuJoCo environments, or 3D navigation environments.", "Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.", "Additionally, all of these are still on Atari.", "[A] Diversity is All You Need: Learning Skills without a Reward Function Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine", "[B] EX2: Exploration with Exemplar Models for Deep Reinforcement Learning Justin Fu, John D. Co-Reyes, Sergey Levine", "[C] Curiosity-driven Exploration by Self-supervised Prediction Deepak Pathak, Pulkit Agrawal, Alexei A. Efros and Trevor Darrell International Conference on Machine Learning (ICML), 2017"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "arg-request_experiment", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 373, "sentences": ["Update to the Review after the rebuttal from the Authors:", "After carefully reviewing the responses by the authors especially on my concerns about the significance of solving an instance of a given problem and the improvement in the exposition of the ideas I would like to amend my earlier decision and recommend to accept.", "For completeness below is the original review.", "This paper introduces a framework to learn to generate solutions to online combinatorial optimization problems with worst case guarantees.", "The framework as the authors claim eliminates the need for manual hard to solve instance/data creation, which is necessary to teach the model to provide the aforementioned worst case guarantees.", "Therefore the main contribution of the paper can be said that this framework shows that it is possible to train a machine learning model, which can learn an algorithm to solve hard online combinatorial optimization problems and this training can be done without knowing much about the actual optimization problem domain.", "The only input required is the way to calculate the objective function of the actual problem.", "This contribution is demonstrated on two classes of problems: Ski-Rental and Fractional AdWords.", "The framework requires two neural networks one for solution generation agent and one for problem instance generation.", "These two networks are trained jointly from scratch and the underlying algorithm for the training is provided.", "Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.", "Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?", "This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.", "Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.", "Therefore I do not find being able to solve this problem as a supporting evidence for the contributions claimed.", "In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.", "I find this important because for combinatorial optimization usually scale matters a lot.", "While a small instance of a problem can be solve by a general purpose solver quickly a small increase in the problem size can turn out to be intractable.", "When proposing a machine learning approach to such problems I would expect the model to scale better than pure optimization approach so that there would be demonstrable benefit.", "Although the paper proposes an interesting framework I would argue that it is a \u201cgreen apple\u201d in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.", "Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.", "In order to clarify the exposition the following are some questions:", "1. Authors call the approach YaoGAN due to its structural similarity to GANs.", "I understand the fact that they are training two neural networks in an alternating scheme, which is similar to the GAN training.", "How can one evaluate the solutions generated by this framework similar to how GAN generators are evaluated?", "Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?", "2. The main technical contribution claim needs to be elaborated.", "I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.", "3. Authors claim there are two shortcomings of the previous method proposed in Kong et. al 2018.", "They need to elaborate how their method overcomes these issues better.", "4. Authors state that fractional relaxation of combinatorial mainly integer optimization problems, which is accurate.", "Yet their approach is only able to solve the fractional version of the AdWords problem.", "In addition I agree with the fact that although continuous relaxations to integer optimization problems might provide insightful directions they usually employed to to prove bounds on the heuristic approaches.", "Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.", "I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.", "What are the shortcomings?", "5.In Appendix A authors talk about no-regret dynamics, which are relevant.", "However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.", "6. In appendix C.2 authors provide additional plots for the Fractional AdWords problem.", "However, they retain from providing any intuition about them.", "In particular what is the conclusion to be drawn from Figure 5.", "This needs more elaboration. Is this way of training results expected? What is the lesson learned?", "7.In Figure 8 they provide example data from experience array.", "What are the significance of these examples? How they help us understand the problem instance generation was actually able to find interesting instances? What kind of dynamics are under covered?", "These are not directly revealed by only looking at the pictures one needs more explanation to support the claims."], "labels": ["arg-structuring_heading", "none", "arg-structuring_quote", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "arg-structuring_quote", "none", "arg-request_clarification", "arg-request_explanation", "arg-request_experiment", "arg-request_experiment", "arg-structuring_quote", "arg-request_experiment", "arg-structuring_quote", "none", "none", "none", "none", "arg-request_experiment", "arg-structuring_quote", "arg-request_explanation", "arg-structuring_quote", "none", "arg-request_explanation", "arg-request_explanation", "arg-structuring_quote", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "arg-request_clarification_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 374, "sentences": ["In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true.", "As the authors state, their notion of \"state-of-the-art\" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1.", "The question is, why one would exlude the mixture-of-softmax approach here?", "This is clearly misleading.", "The authors introduce the idea of past decoding for the purpose of regularization.", "It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.", "The results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank.", "Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets.", "The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.", "Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.", "It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications."], "labels": ["none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 375, "sentences": ["This paper looks to predict \"unstructured\" set output data.", "It extends Rezatofighi et al 2018 by modeling a latent permutation.", "Unfortunately, there is a bit of an identity crisis happening in this paper. There are several choices that do not follow based on the data the paper considers.", "1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\\pi | x_i, w) (1); this feels like a very odd choice to me.", "The outputs are either unordered sets, where you would have a permutation invariant (or exchangeable) likelihood, or they are ordered sequence where the order of the outputs does matter, as some are more likely than others.", "2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.", "The way the permutation, or the order of the data, accounts in the likelihood (2) does not make sense.", "Conditioned on the permutation of the set, the points are exchangeable.", "Let's just consider a 2 element \"set\" at the moment Y = (y_1, y_2).", "Order matters, so either this is being observed as pi=(1, 2) or pi=(2, 1), both of which depend on the input x. However, the likelihood of the points does not actually depend on the order in any traditional sense of the word.", "we have:", "p_\\pi((1, 2) | x, w) p_y(y_1 |  x, w, (1, 2)) p_y(y_2 |  x, w, (1, 2)) + p_\\pi((2, 1) | x, w) p_y(y_1 |  x, w, (2, 1)) p_y(y_2 |  x, w, (2, 1))", "*Note that in here (as in eq. 2) the output distribution p_y does not know what the index is of what it is outputting, since it is iid.* So what does this mean? It means that the order (permutation) can only affect the distribution in an iid (exchangeable, order invariant) way.", "Essentially the paper has just written a mixture model for the output points where there are as many components as permutations.", "I don't think this makes much sense, and if it was an intentional choice, the paper did a poor job of indicating it.", "3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.", "It is very unclear how the dependence on \\pi drops out when getting a MAP estimate of outputs in section 3.3.", "This needs to be justified.", "There are some stylistic shortcomings as well.", "For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs) .", "Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \\mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.", "But these and other points are minor.", "The paper should not be published until it can resolve or make sense of the methodological discrepancies between what it says it looks to do and what it actually does as described in points 1), 2), and 3) above."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 376, "sentences": ["The authors propose a new way to augment textual datasets for the task of sentiment analysis, in order to help the learning methods to generalize better by concentrating on learning the different that makes a difference.", "The main idea of the paper is to augment existing datasets with minimally counteractual versions of them, that change the sentiment of the documents.", "In this way, all spurious factors will naturally cancel out.", "The authors use the newly created datasets and show that indeed, the retrained algorithms on the augmented datasets generalize much better.", "The main contribution of the paper is the introduction of the idea of counterfactual datasets for sentiment analysis.", "Overall, I find the idea of the paper quite interesting and I\u2019m excited to use the datasets they have created.", "However, I think the relative novelty of the paper does not meet ICLR standards, and it\u2019s better suited as a whitepaper attached to an open dataset release."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label"]}
{"abstract_id": 377, "sentences": ["In the paper, the authors propose a pipelined backpropagation algorithm faster than the traditional backpropagation algorithm.", "The proposed method allows computing gradients using stale weights such that computations in different layers can be executed in parallel.", "They also conduct experiments to evaluate the effect of staleness and show that the proposed method is faster than compared methods.", "I have the following concerns:", "1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].", "2) Does the proposed method store immediate activations or recompute the activations in the backward pass?", "3) In the experiments, the accuracy values are too low for me. For example, resnet110 on cifar10 is 91.99% only, it should be around 93%, an example online", "https://github.com/akamaster/pytorch_resnet_cifar10.", "4) In the experiments, more comparisons with methods in [1] or [2] should be conducted given they are all parallelizing the backpropagation algorithm and achieve speedup in the training.", "5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.", "[1] Huo, Zhouyuan, et al. \"Decoupled parallel backpropagation with convergence guarantee.\" arXiv preprint arXiv:1804.10574 (2018).", "[2] Huo, Zhouyuan, Bin Gu, and Heng Huang. \"Training neural networks using features replay.\" Advances in Neural Information Processing Systems. 2018."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 378, "sentences": ["This paper is about using \"neural stethoscopes\", small complementary neural networks that are added to a main network which with their auxilary loss functions can measure suitability of features or guide the learning process.", "The idea is incremental to multi-task learning and enable, in a single framework, to validate intermediate features for additional related tasks.", "Moreover it can promote or suppress the correlation of such features to the tasks related to the main one.", "The framework is applied to the task of visual stability prediction of block towers.", "The paper builds upon Groth et al. 2018, adding the concept of local stability as correlated secondary task, used with the proposed neural stethoscopes.", "Experiments with an extension of ShapeStacks (Groth et al. 2018) dataset where the local stability is added to the global stability class, show that it is possibile increase the performance using the additional task.", "Moreover, it is shown that neural stethoscopes can suppress nuisance information when using a biased training dataset where the local and global stability are purposely inversely correlated.", "Strengths:", "+ A very nice paper, well written and easy to read. Figures are helpful and the structure is clear.", "+ The concept of neural stethoscope is interesting and simplify the concepts behind multitask learning.", "+ Experiments are convincing, interesting and there is some novelty in vision stability prediction.", "Weaknesses:", "- The novelty is limited related to multitask learning, thus it is an incremental paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 379, "sentences": ["The paper studies several different techniques for training GANs: the architecture chosen, the loss function of the discriminator and generator, and training techniques: normalization methods, ratio between updates of discriminator and generator, and regularization.", "The method is performing an empirical training study on three image datasets, modifying the training procedure (e.g. changing one of the parameters) and using different metrics to evaluate the performance of the trained network.", "Since the space of possible hyper-parameters , training algorithms, loss functions and network architecture is huge , the authors set a default training procedure, and in each numerical experiment freeze all techniques and parameters except for one or two which they modify and evaluate.", "The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.", "The authors recommend using non-saturated GANs loss and spectral normalization when training on new datasets, because these techniques achieved good performance metrics in most experiments.", "But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear if the improvement in performance is statistically significant, how robust it is to changes in other parameters etc.", "The authors also rely mostly on the FID metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.)", "The writing is understandable for the most part, but the paper seems to lack focus - there is no clear take home message.", "The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them, give mathematical formulations, or insights into their advantages/disadvantages, making it hard to the non-expert reader to understand what are these techniques and why are they introduced.", "With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 380, "sentences": ["This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors.", "The authors aim to reduce the distortion of each layer rather than the weight distortion.", "The proposed algorithm first selects the candidate codeword vectors using k-means clustering and fine-tune them via knowledge distillation.", "The authors verify the proposed algorithm by comparing it with existing algorithms for ResNet-18 and ResNet-50.", "Overall, I think that the proposed algorithm is easy to apply and the draft is relatively well written.", "Some questions and doubts are listed below.", "-In k-means clustering (E-step and M-step), is it correct to multiply \\tilde x to (c-v)?", "I think that the error arising from quantizing v into c is only affected by a subset of rows of \\tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, \u2026 rows of \\tilde x affect to the error.", "-Does minimizing reconstruction error minimizes the training loss (before any further fine-tuning) compared to na\u00efve PQ? If not,", "-Is there any guideline for choosing the optimal number of centroids and the optimal block size given a target compression rate?", "-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_explanation", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 381, "sentences": ["The paper presents a combination of evolutionary search methods (CEM) and deep reinforcement learning methods (TD3).", "The CEM algorithm is used to learn a Diagional Gaussian distribution over the parametes of the policy.", "The population is sampled from the distribution.", "Half of the population is updated by the TD3 gradient before evaluating the samples.", "For filling the replay buffer of TD3, all state action samples from all members of the population are used.", "The algorithm is compared against the plane variants of CEM and TD3 as well as against the evoluationary RL (ERL) algorithm.", "Results are promising with a negative result on the swimmer_v2 task.", "The paper is well written and easy to understand.", "While the presented ideas are well motivated and it is certainly a good idea to combine deep RL and evoluationary search, novelty of the approach is limited as the setup is quite similar to the ERL algorithm (which is still on archive and not published, but still...).", "See below for more comments:", "- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).", "- We are learning a value function for each of the first half of the population.", "However, the value function from the previous individual is used to initialize the learning of the current value function.", "Does this cause some issues, e.g., do we need to set the number of steps so high that the initialization does not matter so much any more? Or would it make more sense to reset the value function to some \"mean value function\" after every individual?", "- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label"]}
{"abstract_id": 382, "sentences": ["The submission proposes a method for hierarchical RL in multiagent settings.", "In particular it proposes to explicitly decouple training of a high-level and low-level controller with grounded the controller interface as goals in the environment to reach for the low-level controller.", "The model is trained via PPO with GAE and evaluated on a small set of multi agent locomotion tasks.", "The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.", "Furthermore, the experimental section does not compare to other forms of hierarchical approaches for MARL, and generally only provides a single comparison to PPO & MADDPG.", "To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission.", "A large part of making the MA system work well is based on reward shaping which nearly fills all of page 5.", "This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.", "The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).", "Regarding the challenges (and focus on learning simple tasks), reference [3] might be of interest to the authors.", "Minor", "- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.", "- Self-referential sentences in the supplementary materials (i.e. referral to itself)", "- Missing references on page 3", "- The egocentric velocity field is not described (section 5)", "- Section 3.1: maximize", "- The wording new paradigm in MARL might be unsuited given existing work on complex domains.", "\u2018Our proposed approach represents the first physics-based simulation of its kind that supports MARL.\u2019 This sentence remains unclear as the authors do not propose a simulation engine.", "- Text on experiment figures is much too small.", "[1] Andrew Levy, Robert Platt, and Kate Saenko. Learning Multi-Level Hierarchies with Hindsight. In International Conference on Learning Representations, 2019.", "[2] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient Hierarchical Reinforcement Learning. In Advances in Neural Information Processing Systems, pp. 3303\u20133313, 2018.", "[3] Ray Interference: a Source of Plateaus in Deep Reinforcement Learning Tom Schaul, Diana Borsa, Joseph Modayil and Razvan Pascanu"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_clarification", "arg-structuring_heading", "arg-request_edit", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 383, "sentences": ["This paper presents some experiments using random projections instead of embeddings from a 1-of-V encoding.", "Experiments on the Penn TreeBank benchmark data set show that in a feed-forward language modeling architecture similar to that of (Bengio, 2003), the random projections substantially reduce the number of parameters of the model while not harming perplexity too much.", "The paper would need to be improved substantially in order to appear at a conference like ICLR.", "First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.", "Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.", "First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.", "Second, the paper needs to use more state-of-the-art architectures.", "Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.", "Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.", "Changing the number/sizes of the network layers or using sparse weight matrices (perhaps with sparsity-inducing regularization) would be natural ways to reduce the parameter space.", "In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.", "Minor", "In the start of Section 3, it is not clear why having the projection be sparse is desired.", "Later, space (and time) efficiency is revealed as the motivation for the sparsity, but it would be helpful if the paper said this earlier.", "Equation 6 seems to have an error, the probability should be P(w_t | w_t-1...) instead of P(w_t , w_t-1...) if this is to represent the standard LM objective (the probability of the corpus).", "Sec 3.3: \"all models sare\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_typo_label"]}
{"abstract_id": 384, "sentences": ["This paper studies how the FiLM visual question answering (VQA) model answer questions involving the quantifier \u2018most\u2019.", "This quantifier is chosen for study because it cannot be expressed in first order logic (i.e., high-order logic is required), and secondly because there are two different algorithmic approaches to answering questions involving \u2018most\u2019 (cardinality-based strategy and pairing-based strategy).", "Experiments are performed by designing abstract visual scenes with controlled numerosity and spatial layouts, and applying methodologies from pyscholinguistics.", "The paper concludes that the model learns an approximate number system (ANS), consistent with the cardinality-based strategy, with implications for understanding the conditions under which existing VQA models should perform well or badly (and possibly for improving VQA models).", "Strengths:", "- The research question is clear and well-conceived. In general, it seems there are significant opportunities for better collaboration between the experimental psychology and machine learning communities, and this is a good example of the benefits.", "- The paper is clear, highly-focused, and well-written.", "Weaknesses:", "- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.", "For example, the section on \u201cRatios andWeber fraction\u201d argues that \u201cthese curves align well with the trend predicted by Weber\u2019s law\u201d, but does not explain how the experimental data would present if the alternative hypothesis (pairing-based strategy) was being used.", "What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?", "- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.", "While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.", "- In some ways it is not that surprising that the CNN more easily learns an approximate number system rather than a pairing-based algorithm, as the later would presumably need to learn a different convolutional filter for every possible spatial arrangement of the pairs (which would be very sample inefficient).", "Therefore, it might be interesting to consider, are there any circumstances under which the CNN would learn a pairing based algorithm?", "For example, what if the spatial configuration of the pairs was simplified, so they were always side-by-side at a fixed distance? If pairing-based algorithms emerged under simplified scenarios, this might have implications for the design of CNN filters (if we want models that are capable of learning these types of functions).", "Summary:", "I regard this as a good paper, with a couple of weakness that could be addressed as indicated."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "none", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 385, "sentences": ["Summary:", "This paper uses siamese networks to define a discriminative function for predicting protein-protein interaction interfaces.", "They show improvements in predictive performance over some other recent deep learning methods.", "The work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.", "Novelty:", "The main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN", "Clarity:", "- The paper is well written, with ample background into the problem.", "Significance:", "- Their method improves over prior deep learning approaches to this problem.", "However, the results are a bit misleading in their reporting of the std error.", "They should try different train/test splits and report the performance.", "- This is an interesting application paper and would be of interest to computational biologists and potentially some other members of the ICLR community", "- Protein conformation information is not required by their method", "Comments:", "- The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)", "-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred", "- The authors use a balanced ratio of positive and negative examples.", "The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones.", "Can they show performance at various ratios of positive:negative examples?", "In case there is a consistent improvement over prior methods, then this would be a clear winner"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_experiment", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 386, "sentences": ["This paper studies the problem of coordinating many strategic agents with private valuation to perform a series of common goals.", "The algorithm designer is a manager who can assign goals to various agents but cannot see their valuation or control them explicitly.", "The manager has a utility function for various goals and wants to maximize the total revenue.", "The abstract problem is well-motivated and significant and is an entire branch of study called algorithmic mechanism design.", "However often many assumptions have to be made to make the problem mathematically tractable.", "In this paper, the authors take an empirical approach by designing an RL framework that efficiently maximizes rewards across many episodes.", "Overall I find the problem interesting, well-motivated.", "The paper is well-written and contains significant experiments to support its point.", "However, I do not have the necessary background in the related literature to assess the significance of the methods proposed compared to prior work and thus would refrain from making a judgment on the novelty of this paper in terms of methodology.", "Here are some of my comments/questions to the author on this paper.", "(1) I want to clarify how the skills of the agents play a role in the problem setup. Does it show up in the expression for the manager's reward?", "In particular, does it affect the Indicator for whether a goal is completed Eq. (2) via a process that need not be explicitly modeled but can be observed via a feedback of whether or not the goal is completed?", "So in the case of resource collection example, the skill set is a binary value for each resource, whether it can be collected or not?", "(2) Related to the first point, the motivation for modeling the agents as maximizing their utility is the assumption that agents do not know their skills. I am wondering, is this really justified? Over the course of episodes, can the agents learn their skills based on the relationship between their intention and the goals they achieve? In the resource collection example, when they reach a resource and are not able to collect it, they understand that they do not have the corresponding skill.", "Is there a way to extrapolate the results from this paper to such a setting?", "(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).", "Are there alternate ways to overcome maintaining the UCB explicitly, especially for the number of time-steps?", "Some minor comments on the presentation.", "(1) What are the units for rewards in the plots? Is it the average per episode reward? It would be good to mention this in the caption.", "(2) There are a few typos in the paper.", "Some I could catch was,", "- Last line in Page 5: \"quantitative\" -> \"quantity\"", "- Page 8: skills nad preferences -> skills and preferences", "- Page 8: For which we combining -> for which we combine"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_explanation", "arg-structuring_heading", "arg-request_clarification", "arg-structuring_heading", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 387, "sentences": ["Given a network and input model for generating adversarial examples, this paper presents an idea to quantitatively evaluate the robustness of the network to these adversarial perturbations.", "Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.", "Detailed review below:", "- How does the performance of the proposed method scale wrt scalability? It will be useful to do an ablation study, i.e. keep the input model fixed and slowly increase the dimension.", "- Did you experiment with other MH proposal beyond a random walk proposal? Is it possible to measure the diversity of the samples using techniques such as the effective sample size (ESS) from the SMC literature?", "- What is the performance of the proposed method against \"universal adversarial examples\"?", "- The most interesting question is whether this method gives reasonable robustness estimates even for large networks such as AlexNet?", "- Please provide some intuition for this line in Figure 3: \"while the robustness to perturbations of size \u000f = 0:3 actually starts to decrease after around 20 epochs.\"", "- A number of attack and defense strategies have been proposed in the literature.", "Isn't it possible to use the proposed method to quantify the increase in the robustness towards an attack model using a particular defense strategy? If it is possible to show that the results of the proposed method match the conclusions from these papers, then this will be an important contribution."], "labels": ["arg-structuring_summary", "arg-request_experiment", "arg-structuring_heading", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 388, "sentences": ["This paper introduces a structured drop-in replacement for linear layers in a neural network, referred to as Kaleidoscope matrices.", "The class of such matrices are proven to be highly expressive and includes a very general class of sparse matrices, including convolution, Fastfood, and permutation matrices.", "Experiments are carried in a variety of settings: (i) can nearly replace a series of hand-designed feature extractor, (ii) can perform better than fixed permutation matrices (though parameter count also increased by 10%), (iii) can learn permutations, and (iv) can help reduce parameter count and increase inference speed with a small performance degradation of 1.0 BLEU on machine translation.", "This appears to be a solid contribution in terms of both theory and practical use.", "As I have not thought much about expressiveness in terms of arithmetic circuits (though I was unable to fully follow or appreciate the derivations, the explanations all seem reasonable) , my main comments are regarding experiments.", "Though there are experiments in different domains, each could benefit from some additional ablations, especially to existing parameterizations of structured matrices such as Fastfood, ACDC, and any of the multiple works on permutation matrices and/or orthogonal matrices.", "Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.", "There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.", "Pros:", "- The writing is easy to follow and concise, with contributions and place in the literature clearly stated.", "- The Kaleidoscope matrix seem generally applicable, both proven theoretically and shown empirically (experiments are spread across a wide range of domains).", "- The code includes specific C++ and CUDA kernels for computing K matrices, which will be very useful for adaptation.", "- The reasoning using arithmetic circuits seems interesting, and the Appendix includes a primer.", "Cons:", "- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.", "- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer? This comparison appears in other experiments, but seems to be missing here for some reason.", "It would lead to better understanding than only comparing to SincNet.", "- The setup for the learning to permute experiment is not as general as it would imply in the main text. The matrices are constrained so that an actual permutation matrix is always sampled, and the permutation is (had to be?) pretrained to reduce total variation for 100 epochs before jointly trained with the classifier.", "Though this is stated very clearly in the Appendix, I hope the authors can also communicate this clearly in the main text as it appears to be a crucial component of the experimental setup.", "Comments:", "- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?", "- There seems to be some blurring between the meaning of structure (used to motivate K matrices in the introduction) and sparsity (used to analyze K matrices).", "Structure might also include parameter sharing, orthogonality, and maybe other concepts.", "For instance, while Kaleidoscope matrices might include the subclass of circulant matrices, can they also capture the same properties or \"inductive bias\" (for lack of better word) as convolutional layers when trained?"], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_experiment", "none", "none", "arg-request_clarification", "arg-structuring_heading", "arg-request_experiment", "arg-request_explanation", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 389, "sentences": ["I really enjoyed reading the paper! The exposition is clear with interesting observations, and most importantly, the authors walk the extra mile in doing a theoretical analysis of the observed phenomena.", "Questions for the authors:", "1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled \u201cGenerative Ensembles for Robust Anomaly Detection\u201d makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.", "Their criteria also accounts for the variance in model log-likelihoods and is hence slightly different.", "2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.", "If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?", "3. Why does the constant image (all zeros) in Figure 9 (appendix) have such a high likelihood? It\u2019s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn\u2019t seem applicable.", "4. How much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images.", "5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN.", "This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.", "Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)"], "labels": ["none", "arg-structuring_heading", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label"]}
{"abstract_id": 390, "sentences": ["This paper proposes a hybrid machine learning algorithm using Gradient Boosted Decision Trees (GBDT) and Deep Neural Networks (DNN).", "The intended research direction on tabular data is essential and promising.", "However, the proposed technique does not seem to be handling the problem foundationally well.", "It seems heavily dependent on GBDT.", "It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.", "Moreover, I  don't think that the data sets in experiments are good enough to cover the importance and the nature of the problem.", "Pros:", "-This is a crucial line of research direction that aims to make DNNs applicable to many real-world problems (beyond speech and vision) in which discrete data and heterogeneous features exist such as engagement prediction, recommendation, and search.", "-The starting point of using GBDT seems like a good choice.", "-The Paper is mostly well written except occasional repetitions and missing acronym definitions.", "Cons:", "-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.", "I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.", "The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).", "This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me", "that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR.", "-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.", "-In the provided benchmark data sets the depth of the analysis seems to be enough.", "However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features (e.g., unstructured raw text for the search queries.) In such scenarios, it had been showed that wide-and-deep NNs perform decently.", "However such problems are entirely missing in the results section.", "I also think that this is a lost opportunity for the authors as they could be showing that it is the NN part contributing."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 391, "sentences": ["Verifying the properties of neural networks can be very difficult.", "Instead of", "finding a formal proof for a property that gives a True/False answer, this", "paper proposes to take a sufficiently large number of samples around the input", "point point and estimate the probability that a violation can be found.", "Naive", "Monte-Carlo (MC) sampling is not effective especially when the dimension is", "high, so the author proposes to use adaptive multi-level splitting (AMLS) as a", "sampling scheme.", "This is a good application of AMLS method.", "Experiments show that AMLS can make a good estimate (similar quality as naive", "MC with a large number of samples) while using much less samples than MC, on", "both small and relatively larger models", ".", "Additionally, the authors conduct", "sensitivity analysis and run the proposed algorithm with many different", "parameters (M, N, pho, etc), which is good to see.", "I have some concerns on this paper:", "I have doubts on applying the proposed method to higher dimensional inputs.", "In", "section 6.3, the authors show an experiments in this case, but only on a dense", "ReLU network with 2 hidden layers, and it is unknown if it works in general.", "How does the number of required samples increases when the dimension of input", "(x) increases?", "Formally, if there exists a violation (counter-example) for a certain property,", "and given a failure probability p, what is the upper bound of number of samples", "(in terms of input dimension, and other factors) required so that the", "probability we cannot detect this violation with probability less than p?", "Without such a guarantee, the proposed method is not very useful because we", "have no idea how confident the sampling based result is.", "Verification needs", "something that is either deterministic, or a probabilistic result with a small", "and bounded failure rate, otherwise it is not really a verification method.", "The experiments of this paper lack comparisons to certified verification", "methods. There are some scalable property verification methods that can give a", "lower bound on the input perturbation (see [1][2][3])", ".", "These methods can", "guarantee that when epsilon is smaller than a threshold, no violations can be", "found.", "On the other hand, adversarial attacks give an upper bound of input", "perturbation by providing a counter-example (violation).", "The authors should", "compare the sampling based method with these lower and upper bounds.", "For", "example, what is log(I) for epsilon larger than upper bound?", "Additionally, in section 6.4, the results in Figure 2 also does not look very", "positive - it unlikely to be true that an undefended network is predominantly", "robust to perturbation of size epsilon = 0.1. Without any adversarial training,", "adversarial examples (or counter-examples for property verification) with L_inf", "distortion less than 0.1 (at least on some images) should be able to find.", "It", "is better to conduct strong adversarial attacks after each epoch and see what", "are the epsilons of adversarial examples.", "Ideas on further improvement:", "The proposed method can become more useful if it is not a point-wise method.", "If given a point, current formal verification method can tell if a property is", "hold or not.", "However, most formal verification method cannot deal with a input", "drawn from a distribution randomly (for example, an unseen test example).", "This", "is the place where we really need a probabilistic verification method.", "The", "setting in the current paper is not ideal because a probabilistic estimate of", "violation of a single point is not very useful, especially without a guarantee", "of failure rates.", "For finding counter-examples for a property, using gradient based methods might", "be a better way.", "The authors can consider adding Hamiltonian Monte Carlo", "to", "this framework", "(See [4]).", "References:", "There are some papers from the same group of authors, and I merged them to one.", "Some of these papers are very recent, and should be helpful for the authors", "to further improve their work.", "[1] \"AI2: Safety and Robustness Certification of Neural Networks with Abstract", "Interpretation\", IEEE S&P 2018 by Timon Gehr, Matthew Mirman, Dana", "Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin Vechev", "(see also \"Differentiable Abstract Interpretation for Provably Robust Neural", "Networks\", ICML 2018.", "by Matthew Mirman, Timon Gehr, Martin Vechev.  They also", "have a new NIPS 2018 paper \"Fast and Effective Robustness Certification\" but is", "not on arxiv yet)", "[2] \"Efficient Neural Network Robustness Certification with General Activation", "Functions\"", ", NIPS 2018. by Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui", "Hsieh, Luca Daniel.", "(see also \"Towards Fast Computation of Certified Robustness for ReLU Networks\",", "ICML 2018 by Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh,", "Duane Boning, Inderjit S. Dhillon, Luca Danie.)", "[3] Provable defenses against adversarial examples via the convex outer", "adversarial polytope, NIPS 2018. by Eric Wong, J. Zico Kolter.", "(see also \"Scaling provable adversarial defenses\", NIPS 2018 by the same authors)", "[4] \"Stochastic gradient hamiltonian monte carlo.\" ICML 2014. by Tianqi Chen,", "Emily Fox, and Carlos Guestrin.", "============================================", "After discussions with the authors, they agree to revise the paper according to our discussions and my primary concerns of this paper have been resolved. Thus I increased my rating."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_edit", "arg-request_edit", "arg-request_edit", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 392, "sentences": ["The paper proposed an interesting algorithm and direction, which tries fill the gap of NN in tabular data learning.", "My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.", "Also, xgboost was the winning algorithm for many competitions for tabular data, would be good to compare the NN with properly optimised xgboost.", "In chapter 2, related work.", "The authors state that \"tree-based models still yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data.", "To me these two reasoning statements are not particularly convincing. One could also say:", "NN models yield two obvious shortages: (1) Hard to be integrated into complex end-to-end frameworks... (2) Hard to learn from streaming data...", "Actually, tree ensemble based algorithms, eg Hoeffding tree ensembles, are among the best performed algorithms for data streaming tasks."], "labels": ["none", "none", "arg-request_experiment", "arg-structuring_heading", "arg-structuring_quote", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 393, "sentences": ["The paper suggests a new regularization technique which can be added on top of those used in AWD-LSTM of Merity et al. (2017) with little overhead.", "This is a well-written paper with a clear structure.", "The experiments are presented in a clear and understandable fashion, and the evaluation seems thorough.", "The methodology seems sound, and the authors present the reader with all the information needed to replicate the experiments.", "I would only suggest evaluating this technique on AWD-LSTM-MoS of Yang et al. (2017) to get a more complete picture.", "References", "- Merity, S., Keskar, N.S. and Socher, R., 2017. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182.", "- Yang, Z., Dai, Z., Salakhutdinov, R. and Cohen, W.W., 2017. Breaking the softmax bottleneck: A high-rank RNN language model.", "arXiv preprint arXiv:1711.03953."], "labels": ["arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 394, "sentences": ["This manuscript applies transfer learning for protein surface prediction.", "The problem is important and  the idea is novel and interesting.", "However, the  transfer learning model is unclear.", "Pros:  interesting and novel idea", "Cons:  unclear transfer learning model, insufficient experiments.", "Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.", "It is unknown the used model is a new model or existing model.", "Besides, in the experiments, the proposed method is not compared to other transfer learning methods.", "Thus, the evidence of the experiments is not enough."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 395, "sentences": ["The paper makes a significant attempt at solving one of the practical problems in machine learning -- learning from many noisy and limited number of clean labels.", "This setting is presumably more practical than the setting of few-shot learning.", "Noisy labels are often abundantly available and investing in methods that can take the noise into account for building a discriminative model is quite timely.", "To be honest, the theoretical contribution of the paper is limited.", "The authors make use of the nearest neighbour graph obtained from a reduced-dimensional set of features to compute the weights of the noisy labels that must guide the predictive model.", "From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).", "However, that does not undermine the superior results the authors have received in the novel application they have targeted.", "I appreciate the effort that went validating these ideas with real-world datasets.", "In future, I would like to see a joint approach to such training, where the function g(), the nearest neighbour graph loss and the classification loss are all tied in the same objective function and are optimized jointly.", "The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 396, "sentences": ["TLDR: The function these deep set networks can approximate is too limited to call these networks universal equivariant set networks.", "Authors should scope the paper to the specific function family these networks can approximate.", "No baseline comparison with GraphNets.", "The paper proposes theoretical analysis on a set of networks that process features independently through MLPs + global aggregation operations.", "However, the function of interest is limited to a small family of affine equivariant transformations.", "A more general function is \\begin{equation} P(X)_i = Ax_i + \\sum_{j \\in N(x_i, X)} B_{(x_j, x_i)} x_j + c \\end{equation} where $N(x_i, X)$ is the set of index of neighbors within the set $X$. It is trivial to show that this function is permutation equivariant.", "Then, can the function family the authors used in the paper approximate this function?", "No.", "Can the proposed permutation equivariant function represent all function the authors used in the paper? Yes.", "1) If $B=0$, then the proposed function becomes MLP.", "2) If $A=0, N(x_i, X) = [n]$ and $B_{(x_j, x_i)} \\leftarrow B$, then this is $\\mathbf{1}\\mathbf{1}^TXB$, the global aggregation function.", "Also, this is the actual function that a lot of people are interested in.", "Let me go over few more examples.", "3) If $N(x_i, X) = $adjacency on a graph and $B_{(x_j, x_i)} \\leftarrow B$, then this is a graph neural network \"convolution\" (it is not a convolution)", "Example adjacency $N(x_i, X) = \\{j \\;| \\; \\|x_i - x_j\\|_p < \\delta, x_j \\in X\\}$. \\begin{equation} \\text{GraphOp}(X)_i = Ax_i + \\sum_{j \\in \\{j \\;| \\; \\|x_i - x_j\\|_p < \\delta, x_j \\in X\\}} Bx_j + c \\end{equation}", "4) If $x_i = [r,g,b,u,v]$ where $[r,g,b]$ is the color, $[u,v]$ is the pixel coordinate and $N(x_i, X) =$ pixel neighbors within some kernel size, $B(x_j, x_i)$ to be the block diagonal matrix only for the first three dimensions and 0 for the rest, then this is the 2D convolution.", "Again, the above function is a more general permutation equivariant function that can represent: a graph neural network layer, a convolution, MLP, global pooling and is one of the most widely used functions in the ML community, not MLP + global aggregation.", "Regarding the experiment metrics and plots:", "On the Knapsack test, the metric of interest is not the accuracy of individual prediction.", "Rather, whether the network has successfully predicted the optimal solution, or how close the prediction is to the solution.", "For example: success rate within the epsilon radius of the optimal solution while satisfying all the constraints.", "Fail otherwise.", "If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.", "Also, the authors should compare with few more graphnet + transmission layer (GraphNetST) baselines with the graph layers: $P(X)_i = Ax_i + \\sum_{j \\in N(x_i, X)} Bx_j + c$ and the same single transmission layer $\\mathbf{1}\\mathbf{1}^TXB$ in PointNetST.", "PointNet is a specialization of graphnets and GraphNetST should be added as a baseline with reasonable adjacency.", "Also experiment figures are extremely compact. Try using log scale or other lines to make the gaps wider.", "Minor", "I am quite confused with the name PointNetST.", "Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.", "The convention is B -> B', not A + B -> A'.", "In this case, A: PointNet, B: DeepSet", "Lemma 3 is too trivial.", "The paper is not very self contained.", "Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.", "P.2 power sum multi-symmetric polynomials.", "\"For a vector $x \\in R^K$ and a multi-index ...\" I think it was moved out of the next paragraph since  the same $x$ is defined again as $x \\in R^n$ again in the next sentence.", "Also, try using the consistent dimension for x throughout the paper, it confuses the reader."], "labels": ["arg-structuring_summary", "arg-request_edit", "none", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_edit", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_typo", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-request_experiment_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_typo_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 397, "sentences": ["(As a disclamer I want to point out I'm not an expert in GANs and have only a basic understanding of the sub-field, but arguably this would make me target audience of this paper).", "The authors presents a large scale study comparing a large number of GAN experiments, in this study they compare various choices of architechtures, losses and hyperparameters.", "The first part of the paper describes the various losses, architectures, regularization and normalization schemes; and the second part describes the results of the comparison experiments.", "While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.", "As far I can see the most important take home message of the paper can be summarized in \"one should consider non-saturating GAN loss and spectral normalization as default choices [...] Given additional computational budget, we suggest adding the gradient penalty [...] and train the model until convergence\".", "Pros:", "- available source code", "- large number of experiments", "Cons:", "- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show", "- not clear what the target audience of the first part (section 2) is, it is too technical for a survey intended for outsiders, and discusses subtle points that are not easy to understand without more knowledge, but at the same time seems unlikely to give additional insight to an insider", "- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type", "Some suggestions that I think could make the paper stronger", "- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.", "I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it. I would leave out some of the details, shortening the whole sections, and focus more on making a few of the concepts more understandable, and potentially leaving more space for a clearer description of the results", "- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?", "- \"the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once\"? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.", "It should probably be rephrased", "- at the start of section 3: what is an \"experiment\"?", "- in 3.1 towards the end of the first paragraph, what is a \"study\", is that the same as experiment or something different?", "- (minor) stating that lower is better in the graphs might be useful", "- (minor) typo in page 5 \"We use a fixed the number\""], "labels": ["none", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_edit", "arg-request_explanation", "arg-request_explanation", "arg-request_edit", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_edit_label", "arg-request_typo_label"]}
{"abstract_id": 398, "sentences": ["This paper proposes PowerSGD for improving SGD to train deep neural networks.", "The main idea is to raise the stochastic gradient to a certain power.", "Convergence analysis and experimental results on CIFAR-10/CIFAR-100/Imagenet and classical CNN architectures are given.", "Overall, this is a clearly-written paper with comprehensive experiments.", "My major concern is whether the results are significant enough to deserve acceptance.", "The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).", "I am not sure how novel the convergence analysis for PowerSGD is, and it would be nice if the authors could discuss technical challenges they overcome in the introduction."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label"]}
{"abstract_id": 399, "sentences": ["The authors are proposing an end-to-end learning-based framework that can be incorporated into all classical frequency estimation algorithms in order to learn the underlying nature of the data in terms of the frequency in data streaming settings and which does not require labeling.", "According to my understanding, the other classical streaming algorithms also do not require labeling but the novelty here I guess lie in learning the oracle (HH) which feels like a logical thing to do as such learning using neural networks worked well for many other problems.", "The problem formulation and applications of this research are well explained and the paper is well written for readers to understand.", "The experiments show that the learning based approach performs better than their all unlearned versions.", "But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.", "So, I am not sure if there are any new machine learning based frequency estimation algorithms."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 400, "sentences": ["This paper proposes two modifications for the MixMatch method [1] and achieves improved accuracy on a range of semi-supervised benchmarks.", "The first modification enforces the distribution of predicted labels to match the distribution of labeled data.", "The second modification is adding a learned data augmentation strategy, and adapting the method to work with strong data augmentation.", "The final method is titled ReMixMatch, and improves significantly over MixMatch, especially in low-data regime.", "The main contribution of the paper is really strong empirical results.", "The method achieves state of the art results or close to that on multiple benchmarks, with especially large gains in settings with very scarce labeled data, like 40 labels on CIFAR-10.", "Another important contribution is the learned data augmentation strategy, which as far as I understand is novel and overcomes some of the limitations of  existing learned data augmentation techniques.", "However, the explanation of the strategy wasn\u2019t very clear for me, and the authors didn\u2019t frame it as a major contribution.", "The main drawback of the paper is that it seems to be more engineering-focused, and doesn\u2019t provide much insight into semi-supervised learning.", "The paper can be summarized as adding two modifications to mix-match, and getting better results.", "The final method becomes fairly involved.", "Mix-Match is already an elaborate method, and ReMixMatch additionally introduces learned data augmentation, an additional loss term for matching label distributions between labeled and unlabeled data, consistency-loss, and a self-supervised loss (section 3.3).", "For the reasons above, I think the paper is borderline, but I am currently voting for acceptance based on the strong empirical performance.", "At the same time, I think the paper can be made stronger and more interesting to read, if the authors added some experiments aimed at understanding the proposed modifications.", "One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.", "For example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from Section 4.", "It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.", "For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.", "Just analyzing the learned data augmentation in different settings and adding more intuition for what happens would make the paper more insightful and interesting to read.", "On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it\u2019s being reported as 6.24.", "What is the reason for the difference?", "Another paper, [2], reports very competitive results on CIFAR-10 for 4k labels.", "I would recommend discussing these results briefly in the paper.", "At the same time the empirical performance of ReMixMatch is really impressive, and I don\u2019t think the results in [1] and [2] affect their significance.", "[1] MixMatch: A Holistic Approach to Semi-Supervised Learning", "David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel", "[2] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average", "Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_summary", "none", "arg-structuring_summary", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation", "arg-request_edit", "none", "arg-request_explanation", "none", "arg-request_edit", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 401, "sentences": ["[EDIT]: I have updated my score after the author response and paper revision.", "=============================", "[I was asked to step in as a reviewer last minute. I did not look at the other reviews].", "-------------------------------", "Summary", "-------------------------------", "This paper proposes to learn disentangled latent states under the GAN framework.", "The core idea is to partition the latent states into N partitions, and correspondly have N Siamese networks that pull the generated images with the same latent partition towards each other, along with a contrastive loss which ensures generated images with different latent partitions to be different.", "The authors experiment with two setups: in the \"unguided setup\" training is completely unsupervised, while in the \"guided\" setup, there is some weak supervision to encourage different partitions to learn different factors.", "-------------------------------", "Evaluation", "-------------------------------", "While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.", "This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.", "Results with weak supervision (their method for injecting weak supervision was very nice) are more impressive.", "However, there is no comparison against existing work.", "Learning disentangled representations with deep generative models is very much an active area.", "Here are some recent papers:", "https://openreview.net/references/pdf?id=Sy2fzU9gl", "https://arxiv.org/abs/1802.05822", "https://arxiv.org/abs/1802.05983", "https://arxiv.org/abs/1802.04942", "Importantly, there are no quantitative metrics.", "I do not think this work is ready for publication."], "labels": ["none", "arg-structuring_heading", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 402, "sentences": ["In this paper the authors proposed a new policy gradient method, which is known as the angular policy gradient (APG), that aims to provide provably lower variance in the gradient estimate.", "Here they presented a stochastic policy gradient method for directional control.", "Under the set of parameterized Gaussian policies, they presented a unified analysis of the variance of APG and showed how it theoretically outperform (in terms of having lower variance) than other state-of-the art methods.", "They further evaluated the APG algorithms on a grid-world navigation domain as well as the King of Glory task, and showed that the APG estimator significantly out-performs the standard policy gradient.", "In general I think this paper addressed an important issue in policy gradient in terms of deriving a lower variance gradient estimate. In particular the authors showed that under the parameterized marginal distribution, such as the angular Gaussian distribution, the corresponding APG estimate has a lower variance estimate than that of CAPG.", "Furthermore, I also appreciate that they evaluated these results in realistic experiments such as the RTS game domains.", "My only question is on the possibility of deriving realistic APG algorithms beyond the class of angular Gaussian policy.", "In terms of the layout of the paper, I would also recommend including the exact algorithm pseudo-code used in the main paper."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 403, "sentences": ["Dual Block-Coordinate Frank-Wolfe (Dual-BCFW) has been widely used in the literature of non-smooth and strongly-convex stochastic optimization problems, such as (structural) Support Vector Machine.", "To my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network, which employs a proximal-point method that linearizes not the whole loss function but only the DNN (up to the logits) to form a convex subproblem and then deal with the loss part in the dual.", "The attempt is not perfect (actually with a couple of issues detailed below), but the proposed approach is inspiring and I personally would love it published to encourage more development along this thread.", "The following points out a couple of items that could probably help further improve the paper.", "*FW vs BCFW*", "The algorithm employed in the paper is actually not Frank-Wolfe (FW) but Block-Coordinate Frank-Wolfe (BCFW), as it minimizes w.r.t. a block of dual variables belonging to the min-batch of samples.", "*Batch Size*", "Though the algorithm can be easily extended to the min-batch case, the author should discuss more how the batch size is interpreted in this case (i.e. minimizing w.r.t. a larger block of dual variables belonging to the batch of samples) and the algorithmic block (Algorithm 1) should be presented in a way reflecting the batch size since this is the way people use an algorithm in practice (to improve the utilization rate of a GPU).", "*Convex-Conjugate Loss*", "The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).", "All convex loss function can derive a dual formulation based on its convex-conjugate.", "See [1,2] for examples.", "It would be more insightful to compare SGD vs dual-BCFW when both of them are optimizing the same loss functions (either hinge loss or cross-entropy loss) in the experimental comparison.", "[1] Shalev-Shwartz, Shai, and Tong Zhang. \"Stochastic dual coordinate ascent methods for regularized loss minimization.\" JMLR (2013)", "[2] Tomioka, Ryota, Taiji Suzuki, and Masashi Sugiyama. \"Super-linear convergence of dual augmented Lagrangian algorithm for sparsity regularized estimation.\" JMLR (2011).", "*BCFW vs BCD*", "Actually, (Lacoste-Julien, S. et al., 2013) proposes Dual-BCFW to optimize structural SVM because the problem contains exponentially many number of dual variables.", "For typical multiclass hinge loss problem the Dual Block-Coordinate Descent that minimizes w.r.t. all dual variables of a sample in a closed-form update converges faster without extra computational cost.", "See the details in, for example, [3, appendix for the multiclass hinge loss case].", "[3] Fan, Rong-En, et al. \"LIBLINEAR: A library for large linear classification.\" JMLR (2008).", "*Hyper-Parameter*", "The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD."], "labels": ["none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_edit", "arg-structuring_heading", "arg-request_edit", "arg-structuring_heading", "none", "none", "arg-structuring_quote", "arg-request_experiment", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_quote", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_summary_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 404, "sentences": ["The authors propose to augment NMT with a grounded inventory of images.", "The intuition is clear and the premise is very tempting.", "The key architectural choice is to allow the transformer to use language embeddings to attend into a topic-image lookup table.", "The proportion is learned to balance how much signal comes from each source.", "Figure 4, attempts to investigate the importance of this sharing and its effects on performance.", "While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.", "For example, \"The old system of private arbitration courts is off the table\" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.", "It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.", "I trust that the authors did in fact achieve these results but I cannot figure out how or why.", "This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.", "In contrast, it does make sense that Multi30K would benefit from this architecture.", "As a minor note, were different feature extractors compared?", "The recent flurry of papers on multimodal transformers indicate that deeper resnet stacks correspond to improved downstream performance.", "Is that also true in this domain?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 405, "sentences": ["This paper studies the problem of generating contracts by a principal to incentive agents to optimally accomplish multiagent tasks.", "The setup of the environment is that the agents have certain skills and preferences for activities, which the principal must learn to act optimally.", "The paper takes a combined approach of agent modeling to infer agent skills and preferences, and a deep reinforcement learning approach to generate contracts.", "The evaluation of the approach is fairly thorough.", "The main novel contribution of the paper is to introduce the principal-agent problem to the deep multiagent reinforcement learning literature.", "My concerns are:", "- The paper should perform a literature search on related work from operations research, including especially principal-agent problems, which are not currently surveyed, and perhaps also optimal scheduling problems.", "- How do the problems introduced either map onto real applications or map onto environments studied in existing literature (such as in operations research)?", "- More details should be given on the mind tracker module.", "- Is it necessary to use deep reinforcement learning for contract generation?", "If the agent modeling is good, the optimal contracts look like they are probably simple to compute directly in the environments studied.", "Overall, the paper is somewhat interesting and relatively technically sound, but the contribution seems marginal. The problems studied seem pulled out a hat, when they could be situated in specific existing literature."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_explanation", "arg-request_edit", "arg-request_clarification", "none", "arg-structuring_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 406, "sentences": ["I believe that the authors have a solid contribution that can be interesting for the ICLR community.", "Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).", "Summary:", "The authors propose a new method called BA-Net to solve the SfM problem by explicitly incorporating geometry priors into a machine learning task.", "The authors focus on the Bundle Adjustment process.", "Given several successive frames of a video sequence (2 frames but can be extended up to 5), BA-Net jointly estimates the depth of the first frame and the relative camera motion (between the first frame and the next one).", "The method is based on a convolutional neural network which extracts the features of the different pyramid levels of the two images and in parallel computes the depth map of the first frame.", "The proposed network is based on the DRN-54 (Yu et al., 2017) as a feature extractor.", "This is complemented by the linear combination of depth bases obtained from the first image.", "The features and the initial depth then passed to the optimization layer called BA-layer where the feature re-projection error is minimized by the modified LM algorithm.", "The authors adapt the standard multi-view geometry constraints by a new concept of feature re-projection error in the BA framework (BA-layer) which they made differentiable.", "Differentiable optimization of camera motion and image depth via LM algorithm is now possible and can be used in various other DL architectures (ex. MVS-Net can probably benefit from BA-layer).", "The authors also propose a novel depth parametrization in the form of linear combination of depth bases which reduces the number of parameters for the learning task,", "enables integration into the same backbone net as used or feature pyramids and makes it possible to jointly train the depth generator and the BA-layer.", "Originally the proposed approach depicts the network operating in the two-view settings.", "The extensibility to more views is also possible and, as shown by authors, proved to improve performance.", "It is, however, limited by the GPU capacity.", "Overall, the authors came up with an interesting approach to the standard BA problem.", "They have managed to inject the multi-view geometry priors and BA into the DL architecture.", "Major comments regarding the paper:", "It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.", "Minor comments regarding the paper:", "-\tThe spacing between sections is not consistent.", "-\tFigures 1 is way too abstract given the complicated set-up of the proposed architecture.", "It would be nice to see more details on the subnet for depth estimator and output of the net.", "Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.", "- Talking about proposed formulation of BA use either of the following and be consistent across the paper:", "Featuremetric BA / Feature-metric BA / Featuremetric BA / \u2018Feature-metric BA\u2019", "-\tTalking about depth parametrization use \u2018basis\u2019 or \u2018bases\u2019 not both and clearly defined the meaning of this important notion.", "-\tAttention should be given to the notation in formulas (3) and (4).", "The projection function there is no longer accepts a 3D point parametrized by 3 variables.", "Instead only depth is provided.", "In addition, the subindex \u20181\u2019 of the point \u2018q\u2019 is not explained.", "-\tMore attention should be given to the evaluation section.", "Specifically to the tables (1 and 2) with quantitative results showing the comparison to other methods.", "It is not clear how the depth error is measured and it would be nicer to have the other errors explained exactly as they referred in the tables (e.g. ATE?).", "-\tHow the first camera pose is initialized?", "-\tIn Figure 2.b I\u2019m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?", "-\tAttention should be given to the grammar, formatting in particular the bibliography."], "labels": ["none", "none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-request_explanation", "arg-structuring_heading", "none", "none", "arg-request_result", "arg-request_edit", "none", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_typo_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "arg-request_clarification_label", "arg-request_clarification_label", "arg-request_edit_label"]}
{"abstract_id": 407, "sentences": ["The main idea behind the paper is to use random projections as the initial word representations, rather than the vocab-size 1-hot representations, as is usually done in language modeling.", "The benefit is that the matrix which projects words into embedding space can then be much smaller, since the space of random projections can be much smaller than the vocab size.", "The idea is an interesting one, but this work is at too much of a preliminary stage for a top-tier conference such as ICLR. In its present state it would make for a potentially interesting paper at a targeted workshop.", "More specific comments", "--", "The initial description of the language modeling problem assumes a particular decomposition of the joint probability, according to a particular application of the chain rule, but of course this is a modeling choice and not the only option (albeit the standard one).", "The main problem with the paper is the use of simple baseline setups as the only experimental configuration:", "o feedforward rather than recurrent network;", "o use of the Penn Treebank dataset only;", "o use of a small n for the n-grams.", "All or at least some of these decisions would need to be relaxed to make a convincing paper.", "The reasons for the use of the energy-based formulation are not clear to me.", "Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?", "Just before equation 6 it says that the resulting vector representation is the *sum* of all the non-zero entries.", "But there are some minus ones in the random projection?", "The PPL expression at the bottom of p.5 doesn't look right.", "The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.", "It looks like all the results are given on the test set. Did you not do any tuning on the validation data?", "The plots in figure 4 are too small.", "It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.", "The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.", "For example, there are lots of typos such as \"instead of trying to probability of a target word\"."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "arg-request_clarification", "arg-request_clarification", "none", "arg-request_result", "none", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_clarification_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 408, "sentences": ["This work proposes a hybrid VAE-based model (combined with an adversarial or maximum mean discrepancy (MMD) based loss) to perform timbre transfer on recordings of musical instruments.", "Contrary to previous work, a single (conditioned) decoder is used for all instrument domains, which means a single model can be used to convert any source domain to any target domain.", "Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.", "The instruments are often unrecognisable, although with knowledge of the target domain, some of its characteristics can be identified.", "The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.", "I have several further concerns about this work:", "* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.", "I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.", "* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?", "* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?", "This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.", "I appreciated that the one-to-one transfer experiments are incremental comparisons, which provides valuable information about how much each idea contributes to the final performance.", "Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.", "Other comments:", "* In the introduction, an adversarial criterion is referred to as a \"discriminative objective\", but \"adversarial\" (i.e. featuring a discriminator) and \"discriminative\" mean different things. I don't think it is correct to refer to an adversarial criterion as discriminative.", "* Also in the introduction, it is implied that style transfer constitutes an advance in generative models, but style transfer does not make use of / does not equate to any generative model.", "* Some turns of phrase like \"recently gained a flourishing interest\", \"there is still a wide gap in quality of results\", \"which implies a variety of underlying factors\", ... are vague / do not make much sense and should probably be reformulated to enhance readability.", "* Introduction, top of page 2: should read \"does not learn\" instead of \"do not learns\".", "* Mor et al. (2018) do actually make use of an adversarial training criterion (referred to as a \"domain confusion loss\"), contrary to what is claimed in the introduction.", "* The claim that training a separate decoder for each domain necessarily leads to prohibitive training times is dubious -- a single conditional decoder would arguably need more capacity than each individual separate decoder model.", "I think all claims about running time should be corroborated by controlled experiments.", "* I think Figure 1 is great and helps a lot to distinguish the different domain translation paradigms.", "* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. \"matching samples\").", "* Section 3.1, \"amounts to optimizing\" instead of \"amounts to optimize\"", "* Higgins et al. (2016) specifically discuss the case where beta in formula (1) is larger than one.", "As far as I can tell, beta is annealed from 0 to 1 here, which is an idea that goes back to \"Generating Sentences from a Continuous Space\" by Bowman et al. (2016).", "This should probably be cited instead.", "* \"circle-consistency\" should read \"cycle-consistency\" everywhere.", "* MMD losses in the context of GANs have also been studied in the following papers:", "- \"Training generative neural networks via Maximum Mean Discrepancy optimization\", Dziugaite et al. (2015)", "- \"Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy\", Sutherland et al. (2016)", "- \"MMD GAN: Towards Deeper Understanding of Moment Matching Network\", Li et al. (2017)", "* The model name \"FILM-poi\" is only used in the \"implementation details\" section, it doesn't seem to be referred to anywhere else. Is this a typo?", "* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?", "* The descriptor distributions in Figure 3 don't look like an \"almost exact match\" to me (as claimed in the text).", "There are some clearly visible differences.", "I think the wording is a bit too strong here."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "none", "arg-request_edit", "arg-request_typo", "arg-request_edit", "none", "arg-request_experiment", "none", "none", "arg-request_typo", "none", "none", "arg-request_edit", "arg-request_typo", "arg-structuring_heading", "none", "none", "none", "arg-request_clarification", "arg-request_explanation", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "arg-request_clarification_label", "arg-request_explanation_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 409, "sentences": ["Problem and contribution:", "The paper studies if the Visual Question answering model \u201cFILM\u201d from Perez et al (2018) is able to decide if \u201cmost\u201d of the objects have a certain attribute or color.", "For this it tries to mimic the setup used to test human abilities in the study by Pietroski et al. (2009).", "The main contribution of this is work is a discussion of how a model could solve the problem of deciding \u201cmost\u201d and the study which shows that the studied model has some ability to do this.", "From this the paper concludes that the model is likely to have some approximate number system.", "Strengths:", "1.\tThe paper looks at a new angle to study and characterize CNN models in general, and VQA models in particular by looking into the psycholinguistic literature experimental setup studied with human subjects.", "2.\tThe paper studies different variants of controlling for different factors (e.g. pairing data points, area used, different training data and pre-trained vs. trained from scratch CNN models)", "3.\tIt is interesting to see that the models performance reasonably aligns with the curve predicted by \u201cWeber\u2019s law\u201d.", "Weaknesses:", "4.\tNumber of objects vs. ratios is not disentangled: While the paper clarifies that not only a smaller number of objects are used, it would be interesting to understand if similar conclusions hold if only the same number or about the same number of total objects are used but the ratios change (at least for more extreme ratios, 1:2, this seems to be the case as they achieve 100% accuracy).", "5.", "The paper only focusses on a single VQA model (FILM) which limits the understanding if this observation is specific to this model; what about other models such as the one from Hudson & Manning (2018), or Relation Networks (Santoro et al) or even simpler baselines: A system which two attention mechanisms (without normalizations) which are sum pooled and then compared would sort of explicitly encode the idea of the APN system.", "It would be valuable to compare them to see how different systems (can) solve this task.", "I would expect that the architecture favors certain capabilities; e.g. Relation Networks might lead more to a paring-based strategy. Or Zhang et al. (2018) might be able to exploit explicit counting to solve the task.", "6.\tThe \u201cmost\u201d ability or APN ability seems to be highly related to accumulation in neural networks.", "The paper FiLM uses global max-pooling and I am wondering if this affect this ability.", "7.\tThe study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data.", "7.1.", "Maybe beyond the scope of this work, but it would be interesting to understand how much training data different models need to obtain this capability.", "8.\tFor evaluation: Are there distractors, i.e. elements which don\u2019t belong to set A or B? If not, how would distractors affect it.", "9.\tClarity:", "9.1. The equation between equation (1) and (2) misses a number [I will call it 1.5 for now]", "9.2.\tIn formula (1.5) \u201c<=>\u201d seems to be used at different levels (?) it would be good to use brackets to make clear which level \u201c<=>\u201d refers to.", "Minor:", "10.\tThe title suggests that the paper studies multiple VQA models but only a single model is studied.", "Conclusion:", "The paper looks into an interesting direction to study CNN models but has some limitations including studying only a single VQA model type, limited to artificially generated images."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-request_experiment", "arg-structuring_heading", "none", "arg-request_experiment", "none", "none", "arg-request_clarification", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_clarification", "arg-structuring_heading", "arg-request_typo", "arg-request_edit", "arg-structuring_heading", "arg-request_edit", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 410, "sentences": ["This paper proposes an additional loss term to use when training an LSTM LM.", "The authors argue that, intuitively, we want the output distribution to retain some information about the context, or \"past\".", "Given this, they use the output distribution as input to a one layer network that must predict the current token.", "The loss for this network is incorporated as an additional term used when training the LM.", "The authors show that by adding this loss term they can achieve SOTA (for single softmax model) perplexity on a number of LM benchmarks.", "The technical contribution is proposing a new loss term to use when training a language model.", "The idea is clear, simple, and well explained, and it seems to be effective in practice.", "One drawback is that it is highly specific to language models.", "Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.", "In addition, there is not much theoretical justification for it, it seems like a one-off trick.", "The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?", "Although it is specific to language models, there are a few reasons it might be of broader significance:", "- It falls in the recent line of work in incorporating auxiliary losses for various tasks.", "This idea has touched many problems and seen success in practice.", "- Perhaps it can be applied to other sequence models.", "For example in encoder-decoder models, the decoder can be thought of as a conditional LM.", "Experiments are comprehensive and rigorous.", "They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.", "Pros:", "- New SOTA for single softmax model on LM benchmarks.", "- Simple, clearly explained idea.", "- Demonstrates effectiveness of auxiliary losses.", "- Rigorous experiments.", "Cons", "- Trick is specific to LM.", "- No large corpus results."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 411, "sentences": ["For the task of predicting interaction contact among atoms of protein complex consisting of two interacting proteins, the authors propose to train a Siamese convolutional neural network, noted as SASNet, and to use the contact map of two binding proteins\u2019 native structure.", "The authors claim that the proposed method outperforms methods that use hand crafted features; also the authors claim that the proposed method has better transferability.", "My overall concern is that the experiment result doesn\u2019t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn\u2019t really fit in the \u201ctransfer\u201d learning scenario.", "Also, the compared methods don\u2019t really use the validation set from the complex data for training at all.", "Thus the experiment comparison is not really fair.", "2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn\u2019t include any significance of the sampling.", "Specifically, the testing dataset is fixed.", "A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.", "Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.", "Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can\u2019t capture while SASNet can.", "Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.", "Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.", "Overall the paper is well written, and I do think the paper could be much stronger the issues above are addressed.", "Some minor issues:", "1)\ton page 4, Section 3, the first paragraph, shouldn\u2019t \u201cC_p^{val} of 55\u201d be \u201cC_p^{test} of 55\u201d?", "2)\tIt is not clear what the \u201creplicates\u201d refer to in the experiments.", "3)\tSome discussion on why the \u201cSASNet ensemble\u201d would yield better performance would be good; could it be overfitting?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "arg-request_experiment", "none", "arg-structuring_heading", "arg-request_typo", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 412, "sentences": ["The paper focuses on the stability prediction task on the ShapeStacks dataset.", "Specifically, the paper creates a new extension to the dataset, and it proposes the use of \"Neural Stethoscopes\" framework to analyze deep neural nets' physical reasoning of local stability v.s. global stability.", "It is shown in the paper neural nets tend to be misled by local stability when the task is to predict global stability.", "Then the paper utilizes the proposed framework to de-bias the misleading correlation to achieve a state-of-the-art on the dataset.", "The paper is very well-written and easy to follow.", "The main idea is simple and the experiments are detailed.", "Specifically on the task of stability prediction, it is quite interesting to know that neural nets can be misled by visual cues (local stability).", "However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of \"Neural Stethoscopes\" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 413, "sentences": ["Summary:", "The authors look at the problem of exploration in deep RL.", "They propose a \u201ccuriosity grid\u201d which is a virtual grid laid out on top of the current level/area that an Atari agent is in.", "Once an agent enters a new cell of the grid, it obtains a small reward, encouraging the agent to explore all parts of the game.", "The grid is reset (meaning new rewards can be obtained) after every roll out (meaning the Atari agent has used up all its lives and the game restarts).", "The authors argue that this method enables better exploration and they obtain an impressive score on Montezuma\u2019s Revenge (MR).", "Review:", "The paper contains an extensive introduction with many references to prior work, and a sensible lead up to the introduced algorithm.", "The algorithm itself seems to work well and some of the results are convincing.", "I am a bit worried about the fact that the agents have access to their history of locations (\u201cthe grid\u201d).", "The authors mention that none of the methods they compare against has this advantage and it seems that in a game that rewards exploration directly (MR) this is a large advantage.", "The authors comment on this advantage in section 3 and found that removing intrinsic rewards hurt performance significantly.", "Only removing the grid access made results on MR very unstable.", "However in order to compute the intrinsic rewards, it still seems necessary to access the location of the agent, meaning that implicitly the advantage of the method is still there.", "I was wondering if the authors find that the agents are forcibly exploring the entire environment during each rollout? Even if the agent knows what/where the actual goal is. There is a hint to this behaviour in section 4, on exploration in sparse domains.", "The future work section mentions some interesting improvements, where the agent position is learned from data. That seems like a promising direction that would generalise beyond Atari games and avoids the advantage.", "Nits/writing feedback:", "- There is no need for such repetitive citing (esp paragraph 2 on page 2).", "Sometimes the same paper is cited 4 times within a few lines.", "While it\u2019s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.", "- I think the comparison between prior lifetimes and humans mastering a language doesn\u2019t hold up and is distracting", "##", "##", "Revision:", "The rebuttal does little to clarify open questions:", "1. Both reviewer 2 and I commented on the ablation study regarding the grid but received no reply.", "2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.", "3. The authors argue in their rebuttal that \"the grid\" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm.", "This seems contradictory."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 414, "sentences": ["The paper presents proof that the DeepSets and a variant of PointNet are universal approximators for permutation equivariant functions.", "The proof uses an expression for equivariant polynomials and the universality of MLP.", "It then shows that the proposed expression in terms of power-sum polynomials can be constructed in PointNet using a minimal modification to the architecture, or using DeepSets, therefore proving the universality of such deep models.", "The results of this paper are important.", "In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.", "For example, here is an alternative and clearer route presenting the same result: one may study the simple case of having single input channel, for which the output at index \"i\" of an equivariant polynomial is written as the sum of all powers of input multiplied by a polynomial function of the corresponding power-sum.", "This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.", "Generalizing this to the multi-channel input as the next step could make the proof more accessible.", "The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.", "Given the fact that it applies the same MLP independently to individual set members, it is obvious that it is not universal equivariant (for example, consider a function that performs a fixed permutation to its input), and I fail to see why the paper goes into the trouble of having theorems and experiments just to demonstrate this point. If there were any other objectives beyond this in the experiments could you please clarify?", "Finally, could you give a more accurate citation (chapter-page number) for the single-channel version of Theorem 2.?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_clarification", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 415, "sentences": ["This paper introduces a new generative poisoning attack method against machine learning classifiers.", "The authors propose pGAN with three components to maximum the error of classification and guarantee undistinguished poisoning data for the discriminator.", "The experimental results show that the hyperparameter \\alpha significantly affects the poisoning data distribution and pGAN leads to specific error in a classification task.", "This paper should be weekly accepted, considering the following aspects.", "Positive points: (1) The experiments seem solid.", "The overall performance with different parameters and the corresponding error type have been evaluated.", "(2) The error-specific and performance-control characteristics of pGAN seem to be interesting.", "(3) The paper is well organized.", "Negative points: (1) The authors should provide more justification on equation-3.", "Why do the authors directly average different loss for the discriminator and the classifer?", "(2) The function of the discriminator is not very clear, especially for the classification error test.", "Does the discriminator exclude the poisoning data according to certain rule?", "It would make more sense if the classification error measured from the data the discriminator selects.", "(3) pGAN can produce error-specific attack without sufficient justifications.", "Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?", "(4) For the error-specific attack task, it would be better to provide an ablation experiment.", "For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \\alpha=0) or typical pGAN when they compare with the label-flip operation.", "Please explain which component contribute to the error-specific inclination."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_explanation", "arg-request_experiment", "none", "arg-request_explanation", "arg-request_experiment", "arg-request_experiment", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_explanation_label"]}
{"abstract_id": 416, "sentences": ["Summary.", "The authors empirically investigate the influence of the architecture and the capacity of an NN-model on the transferability of adversarial examples.", "They also study the influence of the smoothness.", "From the obtained results, they propose the smoothed gradient attack showing improvements on the transferability of adversarial examples.", "Pros.", "* Robustness of neural nets is a challenging problem of interest for ICLR", "* The paper is well written", "* The experimental study is convincing", "* The experimental results for the smoothed gradient attacks are promising", "Cons.", "* The results of the experimental study are somehow expected", "* the idea of smoothing gradients is not new", "Evaluation.", "The experimental study of the transferability of adversarial examples is well designed.", "Experimental protocol is convincing.", "The smoothed gradient attacks improve many previously proposed attacks.", "Therefore, my opinion is rather positive. But, as a non expert in the field, I am not completely convinced by the novelty of the approach.", "Some details.", "Typos: That l8 abstract; systems l9 intro; and l2 related work; directly evaluation l2 Section4, must has l-10 p4;", "* the choice \\sigma = 15 in Section 6.2 should be justified by the following study", "* \\sigma is not given in Figure 3(a)"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_explanation", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 417, "sentences": ["This paper addresses the problem of adversarial transferability, i.e. the ability that an adversarial example generated by one model can successfully fool another model.", "There are numerous papers on this topic recently, such as Fawzi'15, Liu'17, Dong'18, Athalye'18...", "The authors propose tot study two types of factors that might influence transferability: model-specific parameters and smoothness of loss surface for constructing adversarial examples.", "Two experimental studies are made for each influence factor from existing architectures.", "Another attack strategy aiming at smoothing the loss surface is proposed, an experimental evaluation shows the effectiveness of the proposed method.", "Pros", "-the proposed experimental studies can be interesting to the community", "-many interesting illustrations are provided.", "Cons", "-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better", "-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.", "-Only two influence factors are studied, again the paper would be more interesting with a more general study", "The paper has an interesting potential but seems a bit limited in its present form."], "labels": ["arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 418, "sentences": ["This paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution.", "Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10.", "This is an interesting observation because the prevailing assumption is that density models can distinguish inliers from outliers.", "However, this phenomenon is not encountered when comparing MNIST and NotMNIST.", "The SVHN/CIFAR10 phenomenon has also been shown in concurrent work [1].", "Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?", "It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.", "For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood.", "Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.", "For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting.", "The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.", "This paper is well written.", "I think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired.", "[1] Choi, H. and Jang, E. Generative Ensembles for Robust Anomaly Detection.", "https://arxiv.org/abs/1810.01392", "Pros:", "- Interesting observation of density modelling shortcoming", "- Clear presentation", "Cons:", "- Lack of a strong explanation for the results or a solution to the problem", "- Lack of an extensive exploration of datasets"], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 419, "sentences": ["This paper used the concept based on channel deficiency to derive a variational bound similar to variational information bottleneck.", "Theoretical analysis shows that this bound is an lower bound on the VIB objective.", "The empirical analysis shows it outperforms VIB in some sense.", "I think this paper's contribution is rather theoretical than practical.", "The experiments section can be improved in the following aspect:", "-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines", "- I(Z;Y) vs I(Z;X) graph is typically used in a VIB setting.", "In the paper's variational deficiency setting, although plotting I(Z;Y) vs I(Z;X) is necessary, it would be also helpful for the authors' to plot Deficiency vs I(Z;X), because this is what new objective is trading-off.", "- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.", "- How do the paper estimate I(Z;Y) and I(Z;X) for plotting these figures? Does the paper use lower bound or some estimators? It should be made clear in the paper since these are non-trivial estimations.", "Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:", "- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016", "It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.", "It would also be helpful for the authors to do a comparison or connection section with this paper.", "I like the paper in general, but given it still has some space for improvement, I would keep my decision as boarder line for now."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_edit", "none", "arg-request_experiment", "none", "arg-request_clarification", "none", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_edit_label"]}
{"abstract_id": 420, "sentences": ["This paper provides some insights on influence of data distribution on robustness of adversarial training.", "The paper demonstrates through a number of analysis that the distance between the training an test data sets plays an important role on the effectiveness of adversarial training.", "To show the latter, the paper proposes an approach to measure the distance between the two data sets using combination of nonlinear projection (e.g. t-SNE), KDE, and K-L divergence.", "The paper also shows that under simple transformation to the test dataset (e.g. scaling), performance of adversarial training reduces significantly due to the large gap between training and test data set.", "This tends to impact high dimensional data sets more than low dimensional data sets since it is much harder to cover the whole ground truth data distribution in the training dataset.", "Pros:", "- Provides insights on why adversarial training is less effective on some datasets.", "- Proposes a metric that seems to strongly correlate with the effectiveness of adversarial training.", "Cons:", "- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.", "- The marketing phrase \"the blind-spot attach\" falls short in delivering what one may expect from the paper after reading it.", "The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.", "For some dataset, this is beyond a spot, it could actually be huge portion of the input space!", "Minor comments:", "- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.", "Though the paper is not suggesting that, it would help to clarify it in the paper.", "Furthermore, it would help if the paper elaborates why the distance between the test and training dataset is smaller in an adversarially trained network compared to a naturally trained network.", "- Are the results in Table 1 for an adversarially trained network or a naturally trained network?", "Either way, it could be also interesting to see the average K-L divergence between an adversarially and a naturally trained network on the same dataset.", "- Please provide more visualization similarly to those shown in Fig 4."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_explanation", "arg-request_clarification", "arg-request_experiment", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_edit_label"]}
{"abstract_id": 421, "sentences": ["Summary", "The authors make three major contributions that improve MixMatch and achieve state-of-the-art in a semi-supervised image classification task.", "The major contributions include: (1) distribution alignment to calibrate the predicted distribution of unlabeled data; (2) augmentation anchoring to allow more aggressive data augmentation; and (3) CTAugment to train the augmentation policy alongside the semi-supervised model.", "The authors conduct experiments on SVHN, CIFAR-10 and STL, and show significant improvements over the MixMatch baseline.", "They also show good results (15.08% error rate) of training with 40 labeled data, in spite of very high variation.", "In the ablation study, they show the error rate drops as K (number of augmentation) increases.", "They also conduct ablation studies on the design choices of their method.", "Decision", "The decision for this paper is borderline, tending towards a weak accept.", "Overall, the paper proposes some simple but interesting ideas, e.g. distribution environments.", "However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.", "As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.", "The tendency to accept is due to the overall strong results.", "Strength", "1. Significant improvement over MixMatch baseline.", "2. The proposed augmentation anchoring and distribution alignment can be easily integrated into existing work.", "3. The proposed CTAugment method lifts the burden of training an RL data augmentation policy.", "Weakness", "1. The objective of the update equation of CTAugment\u2019s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.", "In other words, the objective of the update equation encourages higher weights for the distortion parameter that leads to lower variation in the predicted distribution.", "However, the idea of aggressive data augmentation is to generate data that has high variation in the model prediction, and then penalize the variation in the form of consistency loss.", "The variation induced by aggressive augmentation is the root of the consistency loss that helps regularize the model.", "2. The authors should provide ablation study and analysis of their CTAugment.", "For example, they should compare with simple random augmentation policy.", "It is also recommended to show the learned weights of the distortion parameter.", "Also does larger K value when applied for vanilla MixMatch approach the results in ReMixMatch?", "3. The authors should provide more detail of the setting in the ablation study.", "For example, the setting of \u201cNo strong aug.\u201d and \u201cNo weak aug.\u201d are not clear.", "4. The authors hypothesize that \u201cstronger augmentation can result in disparate predictions, so their average may not be a meaningful target.\u201d However, they do not show any analysis to support this hypothesis.", "5. It is recommended to evaluate the method on larger datasets such as CIFAR-100.", "It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.", "Minor Comments", "1. For Table 2 and Table 3, it should be \u201cerror rate\u201d rather than \u201caccuracy\u201d.", "2. How is the loss weight \u03bbr tuned in the 40 labeled setting? How are the hyper-parameters tuned in general?"], "labels": ["arg-structuring_heading", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_edit", "arg-request_clarification", "arg-request_edit", "none", "none", "arg-request_experiment", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_explanation_label"]}
{"abstract_id": 422, "sentences": ["PROS:", "- The text is very well written, with a good balance between mathematical details and intuitions.", "- I really like the high-level description of the algorithms and proof techniques", "CONS:", "to be completely honest, I am not sure I have learnt anything new from the paper.", "1) the proof techniques are very standard", "2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors: a. large deviation principles b. the larger the swapping rate, the better (which motivated Dupuis & al to consider the infinite swapping limit.) and c. Bakri & al methodology to prove convergence relying on the carre du champ is by now very standard and the proofs of the paper are only minor adaptations.", "I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.", "REMARKS:", "1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions."], "labels": ["arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_clarification", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 423, "sentences": ["Summary", "-------", "This paper describes a model for musical timbre transfer which builds on recent developments in domain- and style transfer.", "The proposed method is designed to be many-to-many, and uses a single pair of encoders and decoders with additional conditioning inputs to select the source and target domains (timbres).", "The method is evaluated on a collection of individual note-level recordings from 12 instruments, grouped into four families which are used as domains.", "The method is compared against the UNIT model under a variety of training conditions, and evaluated for within-domain reconstruction and transfer accuracy as measured by maximum mean discrepancy.", "The proposed model seems to improve on the transfer accuracy, with a slight hit to reconstruction accuracy.", "Qualitative investigation demonstrates that the learned representation can approximate several coarse spectral descriptors of the target domains.", "High-level comments", "-------------------", "Overall, this paper is well written, and the various design choices seem well-motivated.", "The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.", "Specifically, the authors introduced an extra input (control) to encode the pitch class and octave information during encoding.", "I infer that this was necessary to achieve good performance, but it would be instructive to see the results without this additional input, since it does in a sense constitute a form of supervision, and therefore limits the types of training data which can be used.", "While I understand that quantifying performance in this application is difficult, I do find the results difficult to interpret.", "Some of this comes down to incomplete definition of the metrics (see detailed comments below).", "However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).", "The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?", "What is the criteria for bolding here?", "It would be helpful if these scores could be calibrated in some way, e.g., with reference to MMD/KNN scores of random partitions of the target domain samples.", "Since the authors do additional information here for each sample (notes), it would be possible to pair generated and real examples by instrument and note, rather than (in addition to) unsupervised, feature-space pairing by MMD.", "This could provide a slightly stronger version of the comparison in Figure 3, which shows that the overall distribution of spectral centroids is approximated by transfer, but does not demonstrate per-sample correspondence.", "Detailed comments", "-----------------", "At several points in the manuscript, the authors refer to \"invertible\" representations (e.g., page 4, just after eq. 1), but it seems like what they mean is approximately invertible or decodable.", "It would be better if the authors were a little more careful in their use of terminology here.", "In the definition of the RBF kernel (page 4), why is there a summation?", "What does this index? How are the kernel bandwidths defined?", "How exactly are reconstruction errors calculated: using the NSGT magnitude representation, or after resynthesis in the time domain?"], "labels": ["arg-structuring_heading", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_experiment", "none", "arg-structuring_heading", "none", "arg-request_clarification", "arg-request_clarification", "arg-request_result", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_clarification", "arg-request_clarification", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 424, "sentences": ["This paper proposes to enable GAN based TTS in the time domain with the careful designs of the (non-autoregressive) generator and discriminator.", "There have been various trials of GAN-TTS but not so many success and I'm glad to hear that the proposed method seems to enable GAN-TTS with fast inference thanks to the non-autoregressive property.", "The method also proposes new objective measures inspired by the image recognition network based on the high-level features generated by end-to-end ASR, which is also another important contribution of this paper.", "My concern for this paper is reproducibility.", "Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.", "Apart from that, the paper is well written overall by well describing the trend of GAN studies in the image processing and the application of such image processing oriented GAN techniques to TTS."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 425, "sentences": ["The paper proposes a framework for learning interpretable latent representations for GANs.", "The key idea is to use siamese networks with contrastive loss.", "Specifically, it decomposes the latent code to a set of knobs (sub part of the latent code).", "Each time it renders different images with different configurations of the knobs.", "For example, 1) as changing one knob while keeping the others, it expects it would only result in change of one attribute in the image, and 2) as keeping one knob while changing all the others, it expects it would result in large change of image appearances.", "The relative magnitude of change for 1) and 2) justifies the use of a Siamese network in addition to the image discriminator in the standard GAN framework.", "The paper further talks about how to use inductive bias to design the Siamese network so that it can control the semantic meaning of a particular knob.", "While I do like the idea, I think the paper is still in the early stage.", "First of all, the paper does not include any numerical evaluation.", "It only shows a couple of examples.", "It is unclear how well the proposed method works in general.", "In addition, the InfoGAN work is designed  for the same functionality.", "The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 426, "sentences": ["The authors propose learnable \"kaleidoscope matrices\" (K-matrices) in place of manually engineered structured and sparse matrices.", "By capturing \"all\" structured matrices in a way that can be learned, and without imposing a specific structure or sparsity pattern, these K-matrices can improve on existing systems by * capturing more structure (that was not handled by the existing manually engineered architecture), * running faster than dense implementations.", "The claim that \"all\" structured matrices can be represented efficiently is a strong one, and in section 2.3 the authors make it clear what they mean by this.", "Although the proof is long and beyond the expertise of this reviewer, the basic explanation given in section 2.3 makes their point clear for the non-expert reader.", "The balance of the paper empirically tests the claims of learnable structure and efficiency.", "On the basis that these experiments essentially bear out the claims of the paper, I selected to accept the paper.", "Weaknesses:", "1. Regarding the ISWLT translation task result:", "With this dataset, it's a bit of a stretch to say there was \"only a 1 point drop in BLEU score\". That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement.", "There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 427, "sentences": ["This paper investigates the impact of stale weights on the statistical efficiency and performance in a pipelined backpropagation scheme that maximizes accelerator utilization while keeping the memory overhead modest.", "The paper proposes to combine pipelined and non-pipelined training in a hybrid scheme to address the issue of significant drop in accuracy when pipelining is deeper in the network.", "The performance of the proposed pipelined backpropagation is demonstrated on 2 GPUs using ResNet with speedups of up to 1.8X over a 1-GPU baseline and a small drop in inference accuracy.", "The paper is well written and easy to follow.", "The proposed idea is interesting and its effectiveness is well demonstrated with a promising speed and a small drop in accuracy.", "The proposed approach is compared to two existing works:  PipeDream [1] and GPipe [2].", "Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.", "Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.", "Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.", "Minor comment: An interesting line of work is that of [3] which could be included in the discussion.", "Overall, the proposed approach is interesting and is shown to achieve promising results.", "However, memory overhead is still an issue compared to existing method.", "[1] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efficient pipeline parallel DNN training, 2018. URL http://arXiv:1806.03377.", "[2] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism, 2018. URL http://arXiv:1811.06965.", "[3] Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Jorgen Thelin, Nikhil Devanur, Ion Stoica: Blink: Fast and Generic Collectives for Distributed ML. arXiv:1910.04940, 2019."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_summary", "none", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 428, "sentences": ["This paper presents a method for interactive agent modeling that involves learning to model a demonstrator agent not only through passively viewing the demonstrator agent, but also through interactions from a learner agent that learns to probe the environment of the demonstrator agent so as to maximally change the behavior of the demonstrator agent.", "The approximated demonstrator agent is trained through standard imitation learning techniques and the learning or probing agent is trained using reinforcement learning.", "The mind of the demonstrating agent is modeled as a latent space representation from a neural net.", "This latent space representation is used as the reinforcement learning signal for the learner (probing) agent similar to the curiosity driven techniques where larger changes in the representation of mind are sought out since they should lead to larger differences in demonstrator agent behavior.", "The authors test this in several gridworld environments as well as a sorting task and show that their method achieves superior performance and generalizes better to unseen states and task variations compared to several baseline methods.", "General comments, in no particular order:", "1. The authors should provide more details on how the hand-crafted demonstrator agents were made.", "I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?", "2. The demonstrated tasks are (gridworld and algorithmic) which are very simple RL taks with low-dimensional (non-visual) state-spaces .", "It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.", "3. The core premise behind training the learner agent with RL is using a curiosity driven approach to train a probing policy to incite new demonstrator behaviors by maximizing the differences between the latent vectors of the behavior trackers at different time steps.", "Because the latent vector is modeled as a non-linear function, distances between latent vector representations do not necessarily correspond to similar distances between behavior policies (for example, KL distances between two policy distributions).", "Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.", "4. The biggest flaw that I see in this method is the practicality of it's use.", "This method relies on the ability to obtain or gain access to a demonstration agent to learn from.", "In very simple tasks, such as the one presented here, the authors were able to hard-code their own demonstration agent.", "However, in harder tasks, this will not be feasible.", "If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.", "In reality, for sufficiently difficult tasks, a human would be the demonstration agent (as is done in most robotics tasks).", "In practice, imitation learning from a human works well since the learning can be done offline (i.e., post-hoc after a set of demonstrations are collected from the human).", "However, this task requires the learning to be interactive and thus the demonstrator needs to be present during the learning.", "Interactively learning from a human becomes a problem if the learning takes tens of thousands of episodes of training since a human cannot reasonably be expected to be present for that amount of time.", "Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?", "5. My previous comment relates mainly to the application of improved imitation learning.", "However, I do think this is still very useful in the context of multi-agent reinforcement learning for collaborative and competitive tasks (sections 4.6 and 4.7).", "I think this method demonstrates a method for improved collaborative and/or competitive performances given the fact that you already have a single agent with a learned policy.", "Overall, I think the paper presents a really nice idea of how to improve modeling of agents.", "essentially, a learner agent learns how to probe a demonstrator agent to provide more information about what's being demonstrated and prevent over-fitting to a set of fixed demonstrations.", "This work sounds novel to me from a reinforcement learning perspective, however, I'm not well versed on theory of mind research."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 429, "sentences": ["The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier \"most\" (\"most of the dots are red\").", "It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too.", "This is consistent with human behavior.", "Strengths:", "* The introduction lays out an ambitious program of comparing humans to deep neural networks.", "* The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items.", "Weaknesses:", "* The architecture of the particular model is described very briefly, and at multiple points there\u2019s an implication that this is an investigation of \u201cdeep learning models\u201d more generally, even though those models may vary widely.", "While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.", "I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks).", "* I found it difficult to follow the theoretical motivation for performing the work.", "The goal seems to be to test whether the network is performing the task in way that \"if not human-like, at least is cognitively plausible\".", "I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.", "Later in the same paragraph, the authors argue that \"in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable\".", "This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge \"some degree of\" is really neither here nor there).", "In general, I don't understand why we would want a visual question answering system that returns approximate answers --", "isn't it better to have it count exactly how many red dots there are compared to non-red dots?", "* The authors assume that explicit counting is not \"likely to be learned by the 'one-glance' feed-forward-style neural network\" evaluated in the paper. What is this statement based on? Why would a \"one-glance\" network have trouble counting objects?", "(What is a \u201cone-glance network\u201d?)", "* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can \"learn and utilize higher-level concepts than mere pattern matching\".", "What is \"pattern matching\" and how does it differ from \"higher-level concepts\"?", "* Why would the pairing strategy in a neural network be affected by the clustering of the objects?", "I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.", "Minor comments:", "* Is the definition of \"most\" really a central piece of evidence for \"the apparent importance of a cardinality concept to human cognition\"? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here.", "* Please use the terms \"interpretation\" and \"verification\" consistently.", "* \"One over the other strategy\" -> \"one strategy over the other\".", "* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_edit", "none", "none", "arg-request_clarification", "none", "none", "none", "arg-request_experiment", "arg-request_clarification", "arg-request_explanation", "none", "arg-request_clarification", "arg-request_explanation", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_edit", "arg-request_typo", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_quote_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_typo_label", "arg-request_typo_label", "none_label"]}
{"abstract_id": 430, "sentences": ["Summary", "The paper presents a novel approach for learning a generative model where different factors of variations can be independently manipulated.", "The method is build upon  the GAN framework where the latent variables are divided into different subsets (chunks) which are expected to encode information about high-level factors of variation.", "To this end, a Siamese Network for each chunk is trained with a contrastive loss minimizing the distance between generated images sharing the same factor (the latent variables in the chunk are equal), and maximizing the distance between pairs where the latent variables differ.", "Given that the proposed model fails in this fully-unsupervised setting, the authors propose to add weak-supervision into the model by forcing the Siamese networks to  focus only on particular aspects of generated images (e.g, color, edges, etc..).", "This is achieved by applying  a basic transformation  over the input images in order to remove specific information.", "The evaluation of the  proposed model is carried out using the MS-Celeb dataset where the authors provide qualitative results.", "Methodology", "*Disentangling generative factors without explicit labels is a challenging and interesting problem.", "The idea of dividing the latent representation in different subsets and using a proxy task involving triplets of images has been already explored in [3].", "However, the use of Siamese networks in this context is novel and sound.", "*As shown in the reported results, the proposed method fails to learn meaningful factors in the unsupervised setting.", "However, the authors do not provide an in-depth discussion of this phenomena.", "Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.", "*The strategy proposed to introduce weak-supervision is too ad-hoc.", "I agree that using cues such as the average color of an image can be useful if we want to model basic factors of variation.", "However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.", "*As far as I understand, the transformations applied to the input images (e.g, edge detection) must be differentiable (given that it is necessary to backpropagate the gradient of the contrastive loss through the generator network).", "If this is the case, this should be properly discussed in the paper.", "Moreover, given that the amount of differentiable transformations is reduced, this also limits the application of the proposed method for more interesting scenarios.", "*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.", "How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?", "Have the authors considered to use categorical or binary variables?", "The use of the contrastive loss sounds more appropriate in this case.", "Experimental results", "*The experimental section is too limited.", "First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.", "For example, when the edge extractor is used, what kind of information is modeled by the latent variables? Is it consistent across different samples?", "Moreover, it is not clear why the authors have limited the evaluation to the case where only two \u201cchunks\u201d are used.", "In principle, the method could be applied with many more subsets of latent variables and then manually inspect them to check it they are semantically meaningful (see [2])", "*As previously mentioned, there are many recent works addressing the same problem from a fully-unsupervised perspective [1,2,3].", "All these works provide quantitative results evaluating the learned representations by using them to predict real labels (e.g, attributes in the CelebA data-set).", "The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.", "This could clarify the advantages of the weakly-supervised strategy compared to unsupervised approaches.", "Review summary", "+The addressed problem (learning disentangled representations without explicit labeling) is challenging and interesting.", "+The idea of using a proxy task (contrastive loss with triplets of generated images) is somewhat novel and promising.", "- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.", "-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications", "-The experimental section do not clarify the benefits of the proposed approach.", "In particular, the qualitative results are too limited and no quantitative evaluations is provided.", "[1] Variational Inference of Disentangled Latent Concepts from Unlabelled Observations (Kumar et al, ICLR 2018)", "[2] Beta-vae: Learning basic visual concepts with a constrained variational framework. (Higgins et. al, ICLR 2017)", "[3] Disentangling Factors of Variation by Mixing Them. (Hu et. al, CVPR  2018)"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 431, "sentences": ["This paper proposed several extensions to the Neural LP work.", "Specifically, this paper addresses several limitations, including numerical variables, negations, etc.", "To efficiently compute these in the original Neural LP framework, this paper proposed several computation tricks to accelerate, as well as to save memory.", "Experiments on benchmark datasets show significant improvements over previous methods, especially in the case where numerical variables are required.", "I think overall the paper is written clearly, with good summarization of existing works. Also I like the simple but effective tricks for saving the computation and memory.", "One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.", "For example, if rules contain quantifiers, how would this be extended?", "Minor comments:", "1) 4.1,  \u201cO(n^2/2)\u201d -- just put O(n^2) or simply write as n^2/2.", "2) How are the rules from in Eq (2)? i.e., how is \\beta_i selected for each i?", "In the extreme case it would be all the permutations.", "3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "arg-request_edit", "arg-request_explanation", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label"]}
{"abstract_id": 432, "sentences": ["This paper provides exact bounds on the risk when training a two-layer neural network in an asymptotic regime.", "Namely, the paper considers training under the square-loss objective, a two-layer neural network with $h$ hidden units on inputs of dimension $d$ and training on $n$ samples.", "The asymptotic regime is considered by making all of $d$, $h$, $n$ go to $\\infty$, in a way that the ratio $d/n$ approaches $\\gamma_1$ and the ratio $h/n$ approaches $\\gamma_2$.", "This paper considers the following scenarios of training described below, where the data is generated from a linear model on Gaussian inputs and with a zero-mean noise.", "The emphasis of the results is on understanding when a \"double descent\" type phenomenon occurs (\"Double descent\" is a recently coined phenomenon in literature where the risk, as a function of the \"complexity of the model\", initially has a classical U-shape behavior, but eventually decreases again once the complexity of the model exceeds the number of training points.)", "1. Training only the second layer: The risk is first decomposed into a bias and a variance term.", "An exact bound on the variance term of the risk is obtained.", "While the exact nature of the bound is rather complex to parse, the takeaway is that a double descent phenomenon is observed in terms of $\\gamma_2$, namely, the risk blows up when $h \\approx n$, but decreases as $h$ is increased beyond $n$.", "2. Training only the first layer: Two different regimes are considered here, depending on the scale of initialization, called \"vanishing\" and \"non-vanishing\" initializations.", "In both regimes, the risk is independent of $\\gamma_2$, that is, the risk does not depend on number of hidden units (although the risk bounds are different and there is an additional assumption in the case of non-vanishing initialization to ensure that the initialized network computes the zero function).", "In other words, a \"double descent\" phenomenon is not observed in this setting.", "Recommendation:", "I recommend \"weak acceptance\".", "The paper extends prior works that obtain asymptotic risk bounds on linear models to the setting of two-layer neural networks (where only one layer is trained).", "However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.", "Technical Comments:", "- I felt that while it is valuable to have exact bounds on the risk, the form of the bounds are quite complex and hard to parse (especially in Thm 4, case of training only the second layer).", "Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.", "So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.", "- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement \"the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks.\"", "- Another future direction that could be included in discussions is the setting where both layers are trained simultaneously."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label"]}
{"abstract_id": 433, "sentences": ["This paper introduces the study of the problem of frequency estimation algorithms with machine learning advice.", "The problem considered is the standard frequency estimation problem in data streams where the goal is to estimate the frequency of the i-th item up to an additive error, i.e. the |\\tilde f_i - f_i| should be minimized where \\tilde f_i is the estimate of the true frequency f_i.", "Pros:", "-- Interesting topic of using machine learned advice to speed up frequency estimation is considered", "-- New rigorous bounds are given on the complexity of frequency estimation under Zipfian distribution using machine learned advice", "-- Experiments are given to justify claimed improvements in performance", "Cons:", "-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.", "-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i\u2019s themselves. While in some applications this might be natural, this is certainly very restrictive in situations where f_i\u2019s are updated not just by +/-1 increments but through arbitrary +/-Delta updates, as in this case it might be more natural to assume that the distribution of the queries might be proportional to the frequency that the corresponding coordinate is being updated, for example.", "-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.", "-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher\u201918.", "-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.", "Other comments:", "-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label"]}
{"abstract_id": 434, "sentences": ["*CAVEAT*", "I must caveat that this paper is out of my comfort zone in terms of topic, so my review below should only be taken lightly.", "It also explaina the brevity of my review.", "My apologies to the authors and other reviewers.", "*Paper summary*", "The authors design a set architecture, which is equivariant to permutations on the input.", "They show the simplest such set architecture, which preserves equivariance, while being a universal approximator.", "Nicely this architecture relies on a correction to PointNet, called PointNetST, which they show is not equivariant universal.", "Furthermore, they run experiments on a few toy examples demonstrating that their system performs well.", "*Paper decision*", "I have decided to give this paper a weak accept, since it contains both theory and nice experiments.", "To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.", "For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.", "*Supporting arguments*", "- The paper is written clearly. This said, it requires a great deal of effort to follow the maths if you are not already fluent in a lot of the ideas used in the paper (this includes myself).", "- I think the structure of the paper is fine for this sort of work. Perhaps at the beginning it would be more useful to spend more time on a roadmap of the results presented in the paper and to explain the exact significance of why the reader should want to continue reading.", "- I think the selection of experiments is nice, containing both regression and classification. What would have been nicer would be to perform some sort of ablation study, where the authors studied how the representational capacity of the network changed as a result of them introducing the universal linear transmission layer.", "- A direct theoretical and experimental comparison between PointNet and PointNetST would have been useful for me to understand the impact of the change that the authors introduce. *", "Questions/notes for the authors*", "- Please answer my concerns in the support arguments", "- Where is the conclusion section?"], "labels": ["arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "arg-request_edit", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_clarification_label"]}
{"abstract_id": 435, "sentences": ["Summary", "The authors introduce kaleidoscope matrices (K-matrices) and propose to use them as a substitute for structured matrices arising in ML applications (e.g. circulant matrix used for the convolution operation).", "The authors prove that K-matrices are expressive enough to capture any structured matrix with near-optimal space and matvec time complexity.", "The authors demonstrate that learnable K-matrices achieve similar metrics compared to hand-crafted features on speech processing and computer vision tasks, can learn from permuted images, achieve performance close to a CNN trained on unpermuted images and demonstrate the improvement of inference speed of a transformer-based architecture for a machine translation task.", "Review", "The overall quality of the paper is high.", "The main contribution of the paper is the introduction of a family of matrices called kaleidoscope matrices (or K-matrices) which can be represented as a product of block-diagonal matrices of a special structure. Because of the special structure, the family allows near-optimal time matvec operations with near-optimal space complexity for structured matrices which are commonly used in deep architectures.", "The proposed approach is novel. It gives a new characterization of sparse matrices with optimal space complexity up to a logarithmic term.", "Moreover, the proposed characterization is able to learn any structured matrix and matvec time complexity of the K-matrix representation is near-optimal matvec time complexity of the structured matrix.", "Even though in the worst-case complexity is not optimal, the authors argue that for matrices that are commonly used in machine learning architectures (e.g. circulant matrix in a convolution layer) the characterization is optimal.", "This results in a new differentiable layer based on a K-matrix that can be trained with the rest of an architecture using standard stochastic gradient methods.", "However, it is worth noting that the reviewer is not an expert in the field, and it is hard for him to compare the proposed approach with previous work.", "The paper is generally easy to follow.", "Even though the introduction of K-matrices requires a lot of definitions, they are presented clearly and Figure 1 helps to understand the concept of K-matrices.", "The experimental pipeline is also clear.", "Given the special structure of the family, the reviewer might guess that having K-matrices can slow down the training, i.e. it might require more epochs to achieve the reported results compared to baselines.", "Providing training plots might increase the quality of the paper.", "The experimental results are convincing.", "First, the authors show that K-matrices can be used instead of a handcrafted MFSC featurization in an LSTM-based architecture on the TIMIT speech recognition benchmark with only a 0.4% loss of phoneme error rate.", "Then, the authors evaluate K-matrices on ImageNet dataset.", "In order to do so, they compare a lightweight ShuffleNet architecture which uses a handcrafted permutation layer to the same architecture but with a learnable K-matrix instead of the permutation layer.", "The authors demonstrate the 5% improvement of accuracy over the ShuffleNet with 0.46M parameters with only 0.05M additional parameters of the K-matrix and the 1.2% improvement of accuracy over the ShuffleNet with 2.5M parameters with only 0.2M additional parameters of the K-matrix.", "Next, the authors show that K-matrices can be used to train permutations in image classification domains.", "In order to demonstrate so, they take the Permuted CIFAR-10 dataset and ResNet-18 architecture, insert a trainable K-matrix at the beginning of the architecture and compare against ResNet-18 with an inserted FC-layer (attempting to learn the permutation as well) and ResNet-18 trained on the original, unpermuted CIFAR-10 dataset.", "With K-matrix, the authors achieve a 7.9% accuracy improvement over FC+ResNet-18 and only a 2.4% accuracy drop compared to ResNet-18 trained on the original CIFAR-10.", "Finally, the authors demonstrate that K-matrices can be used instead of the decoder\u2019s linear layers in a Transformer-based architecture on the IWSLT-14 German-English translation benchmark which allows obtaining 30% speedup of the inference using a model with 25% fewer parameters with 1.0 drop of BLEU score.", "Overall, the analysis and the empirical evaluations suggest that K-matrices can be a practical tool in modern deep architectures with a variety of potential benefits and tradeoffs between a number of parameters, inference speed and accuracy, and ability to learn complex structures (e.g. permutations).", "Improvements", "1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis.", "2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_result", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_result", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 436, "sentences": ["The paper proposes a method to disentangle latent variables for certain factors of interest in an image by considering the original input image and a transformation of the image where information about the factors of interest is removed.", "The generative process is then modeled by having two latent variables --  the first responsible for generating the transformed image whereas both latent variables are responsible for generating the original input image.", "This inductive bias naturally enforces that the second latent variable will not model the information which the first needs to reconstruct the transformed image, due to the VAE objective penalizing redundancy in information present in the latents.", "The paper demonstrates this in one setting where the transformation is random shuffling of image patches, which should remove the global information of the original input image.", "The methodology of the paper was concise and easy to follow.", "The simple inductive bias presented in the paper for disentangling local and global information is very interesting.", "It is not obvious that shuffling image patches at a particular scale would lead to complete loss of global information, but the paper does show results on SVHN and CIFAR10 for which global information is sufficiently disentangled.", "The results for digit identity clustering were great for showing the correlation between their learnt global information and label information.", "The paper introduced their model as a general purpose strategy for placing desired information in latent variables using auxiliary tasks, but focus was directed to the global vs local line of analysis.", "While giving examples for what kind of information can be removed, the authors mentioned that color to gray-scale might be one possibility.", "It would have been interesting to see this and other possibilities explored in the paper.", "I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.", "It is mentioned in the paper that having a single inference network for the posterior as opposed to the factorized one is conceivable.", "I would be curious to see an analysis of how that works out as compared to the separate encoders case.", "Overall, the paper has a novel idea which is well motivated and executed in terms of experiments."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_summary", "none", "arg-request_experiment", "arg-request_experiment", "arg-structuring_quote", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 437, "sentences": ["This article presents experiments on medium- and large-scale language modeling when the ideas of adaptive softmax (Grave et al., 2017) are extended to input representations.", "The article is well written and I find the contribution simple, but interesting.", "It is a reasonable and well supported increment from adaptive softmax of Grave et al. (2017).", "My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.", "I understand that for two matrices A and B we have rank(AB) <= min(rank(A), rank(B)), and we are not making the small-sized embeddings richer when backprojecting to R^d, but", "have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?", "References", "Joulin, A., Ciss\u00e9, M., Grangier, D. and J\u00e9gou, H., 2017, July. Efficient softmax approximation for GPUs. In International Conference on Machine Learning (pp. 1302-1310)."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 438, "sentences": ["This paper proposes an interesting extension to the Neural LP framework for learning numerical rules in knowledge graphs.", "The proposed method can handle predicates involving the comparison of the numerical attribute values.", "The authors demonstrate its effectiveness on both synthetic knowledge graphs and the parts of existing knowledge graphs which consider numerical values.", "I recommend the paper to be rejected in its current form for the following 3 reasons:", "(1) Although the idea of making numerical rules differentiable is interesting, the current proposed method can only deal with one form of numerical predicate, which is numerical comparison.", "The limitation to such a special case makes the paper somewhat incremental.", "(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.", "Although the proposed method does a good job in synthetic experiments, outperforming existing methods by a large margin, its performance on the numerical variants of Freebase/DBPedia dataset does not show consistent significant improvement.", "The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.", "(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.", "A good place to start with is to visualize(print out) the learned numerical rules and see if they make any sense.", "The experiment section needs significant improvement, especially when there is space left.", "The authors can consider improving the paper based on the above drawbacks.", "I encourage the authors to re-submit the paper once it's improved."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_experiment", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 439, "sentences": ["GANs (generative adversarial network) represent a recently introduced min-max generative modelling scheme with several successful applications.", "Unfortunately, GANs often show unstable behaviour during the training phase.", "The authors of the submission propose a functional-gradient type entropy-promoting approach to tackle this problem, as estimating entropy is computationally difficult.", "While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:", "1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\\theta_{old}}, ...", "2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.", "3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.", "4. Differentiation w.r.t. functions (or more generally elements in normed spaces) is a well-defined concept in mathematics, including the notions of Gateaux, Frechet and Hadamard differentiability.", "It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.", "While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.", "---", "After paper revisions:", "Thank you for the updates.", "The submission definitely improved. I have changed my score to '6: Marginally above acceptance threshold'; the suggested regularization can be a useful heuristic for the GAN community."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-structuring_heading", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 440, "sentences": ["This paper gives a theoretical analysis of an interesting statistical physics technique known as replica exchange.", "The basic idea is that Langevin dynamics at low temperature is slow to converge, and that one could potentially boost the convergence by alternating between low and high temperature.", "At the extreme one could imagine running in parallel a random search and a gradient descent, and ``teleporting\" the gradient descent algorithm whenever the random search algorithm finds a point with better value.", "This makes a lot of sense and it is nice to see a theoretical analysis of this.", "The mathematics are sound, but I do not know whether it is an appropriate submission for ICLR.", "One comment from the math side: it would be interesting (albeit probably difficult) to study kappa in (3.10) as a function of a. In particular at face value it looks like one only benefits from taking a larger, so why not study the limiting behavior of a->infty? What is the limiting value of kappa? Can you perform those calculations in the convex case at least?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 441, "sentences": ["This paper seeks to separate \"causal\" features from ones with spurious correlations in the context of natural language machine learning tasks.", "The proposed approach is to ask human annotators to alter examples in a minimal way that changes the label.", "Thereby the humans separate out the causal features (those changed) from the spurious or irrelevant features (those left unchanged).", "Experiments show that classifiers trained on the original data perform poorly on the altered data and vice versa, but (unsurprisingly) training on the union of the two datasets results in a classifier that performs well in both cases.", "Furthermore, training an SVM on the original results in irrelevant attributes (such as movie genre) being weighted, whereas these weights are largely removed when training on the union of the datasets.", "This suggests that the augmented training data results in weighting the \"right\" features more.", "Overall, I think this paper should be accepted because it makes several interesting contributions: It proposes an interesting approach, shows intriguing experimental results, and produces an interesting dataset (size ~2k) that may be useful for future testing.", "The main limitation of the paper is that the evidence is largely circumstantial.", "The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.", "My suggestion for a further experiment would be to apply the movie review classifiers to, say, book reviews -- something where the task is fundamentally the same but the context is different. If the classifier trained on the union of the original and altered datasets performs better than a classifier trained on only on dataset, then that is strong evidence that this approach yields better extrapolation."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 442, "sentences": ["The reinforcement learning tasks with sparse rewards are very important and challenging.", "The main idea of this work is to encourage intra-life novelty.", "The authors introduce the curiosity grid and the intrinsic reward term so that the agent can explore toward unvisited states at every episode.", "However, the results are not enough to be accepted to ICLR having a very high standard.", "In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.", "There are some RL algorithms reported to be better than A2C.", "For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.", "=================================================================================================", "I've read the rebuttal. I updated my score but still not vote for accept.", "This paper is not my main research area.", "Very unfortunately, this paper was assigned to me.", "The main issue of this paper is the fair comparisons with other works.", "However, I don't have enough knowledges to judge this point.", "So please assess this paper with other reviewers comments."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-structuring_heading", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label"]}
{"abstract_id": 443, "sentences": ["The contributions of this paper are in the domain of policy search, where the authors combine evolutionary and gradient-based methods.", "Particularly, they propose a combination approach based on cross-entropy method (CEM) and TD3 as an alternative to existing combinations using either a standard evolutionary algorithm or a goal exploration process in tandem with the DDPG algorithm.", "Then, they show that CEM-RL has several advantages compared to its competitors and provides a satisfactory trade-off between performance and sample efficiency.", "The authors evaluate the resulting algorithm, CEM-RL, using a set of benchmarks well established in deep RL, and they show that CEM-RL benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.", "It is a pity to see that the authors provide acronyms without explicitly explaining them such as DDPG and TD3, and this right from the abstract.", "The parer is  in general interesting, however the clarity of the paper is hindered", "by the existence of several typos, and the writing in certain passages can be improved. Example of typos include  \u201can surrogate gradient\u201d, \u201c\"an hybrid algorithm\u201d,  \u201cmost fit individuals are used \u201d and so on\u2026", "In the related work the authors present the connection between their work and contribution to the state of the art in a detailed manner.", "Similarly, in section 3 the authors provide an extensive background allowing to understand their proposed method.", "In equation 1, 2 the updates of  \\mu_new and \\sigma_new uses \\lambda_i, however the authors provide common choices for \\lambda without any justification or references.", "The proposed method is clearly explained and seems convincing.", "However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.", "1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate\u2026) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.", "2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark", "The rebuttal provided by the authors is convincing."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_typo", "none", "arg-request_typo", "none", "none", "arg-request_edit", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 444, "sentences": ["The paper presents a new attack, called the shadow attack, that can maintain the imperceptibility of adversarial samples when out of the certified radius.", "This work not only aims to target the classifier label but also the certificate by adding large perturbations to the image.", "The attacks produce a 'spoofed' certificate, so though these certified systems are meant to be secure, can be attacked.", "Theirs seem to be the first work focusing on manipulating certificates to attack strongly certified networks.", "The paper presents shadow attack, that is a generalization of the PGD attack.", "It involves creation of adversarial examples, and addition of few constraints that forces these perturbations to be small, smooth and not many color variations.", "For certificate spoofing the authors explore different spoofing losses for l-2(attacks on randomized smoothing) and l-inf(attacks on crown-ibp) norm bounded attacks.", "Strengths: The paper is well written and well motivated.", "The work is novel since most of the current work focus on the imperceptibility and misclassification aspects of the classifier, but this work addresses attacking the strongly certified networks.", "Weakness: It would be good to see some comparison to the state of the art"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 445, "sentences": ["This is a paper of the verification of neural networks, i.e. check their robustness, and the main contribution here is to tackle it as a statistical problem adressed with multi-level splitting Monte Carlo approach.", "I found the paper well motivated and original, resulting in a publishable piece of research up to a few necessary adjustments.", "These concern principally notation issues and some potential improvements in the writing.", "Let me list below some main remarks along the text, including also some typos.", "* In the introduction, \"the classical approach\" is mentioned but to be the latter is insufficiently covered. Some more detail would be welcome.", "* page 2, \"predict the probability\": rather employ \"estimate\" in such context?", "* \"linear piecewise\": \"piecewise linear\"?", "* what is \"an exact upper bound\"?", "* In related work, no reference to previous work on \"statistical\" approaches to NN verification. Is it actually the case that this angle has never been explored so far?", "* I am not an expert but to me \"the density of adversarial examples\" calls for further explanation.", "* From page 3 onwards: I was truly confused by the use of [x] throughought the text (e.g. in Equation (4)) .", "x is already present within the indicator, no need to add yet another instance of it. Here and later I suffered from what seems to be like an awkward attempts to stress dependency on variables that already appear or should otherwise appear in a less convoluted way.", "* In Section 4, it took me some time to understand that the considered metrics do not require actual observations but rather concern coherence properties of the NN per se.", "While this follows from the current framework, the paper might benefit from some more explanation in words regarding this important aspect. * In page 6, what is meant by \"more perceptually similar to the datapoint\"?", "* In the discussion: is it really \"a new measure\" that is introduced here? *", "In the appendix: the MH acronym should better be introduced, as should the notation g(x,|x') if not done elsewhere (in which case a cross-reference would be welcome).", "Besides this, writing \"the last samples\" requires disambiguation (using \"respective\"?)."], "labels": ["arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_typo", "arg-request_typo", "arg-request_clarification", "none", "arg-request_clarification", "none", "none", "none", "arg-request_edit", "arg-request_clarification", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_explanation_label", "arg-request_typo_label", "arg-request_explanation_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 446, "sentences": ["In this paper, the authors consider CNN models from the lens of kernel methods.", "They build upon past work that showed that such models can be seen to lie in appropriate RKHS, and derive upper and lower bounds for the kernel norm.", "These bounds can be used as regularizers that help train more robust neural networks, especially in the context of euclidean perturbations of the inputs, and training GANs.", "They show that the bounds can also be used to recover existing special cases such as spectral norm penalizations and gradient regularization.", "They derive generalization bounds from the point of view of adversarial learning, and report experiments to buttress their claims.", "Overall, the paper is a little confusing.", "A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.", "It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).", "It might be nice to carefully delineate the authors' work from the former, and present their contributions.", "Page 4: Other Connections with Lower bounds: The first line \" \"we may also consider ... \". This line is vague. How will you ensure the amount of deformation is such that the set \\bar{U} is contained in U ?", "Page 4 last paragraph: \"One advantage ... complex architectures in practice\" : True, but the tightness of the bounds *do* depend on \"f\" (specifically the RKHS norm).", "It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?", "eqn (8): use something else to denote the function 'U'.", "You used 'U' before to denote the set.", "eqn (12): does \\tilde{O} hide polylog factors? please clarify."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit", "arg-request_clarification", "none", "arg-request_explanation", "arg-request_edit", "arg-request_edit", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_clarification_label"]}
{"abstract_id": 447, "sentences": ["This paper introduces policy gradient methods for RL where the policy must choose a direction (a.k.a., the navigation problem).", "Mapping techniques from \"non-directional\" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).", "The authors propose to sample directly on the sphere, using the fact that the likelyhood of an angular Gaussian r.v. has *almost* a closed form and its gradient can almost be computed, up to some normalization term (the integral which is constant in the standard Gaussian case).", "This can be seen as a variance reduction techniques.", "The proofs are not too intricate, for someone used to variance reduction (yet computations must be made quite carefully).", "The result is coherent, interesting from a theoretical point of view and the experiment are somehow convincing.", "The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited..."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 448, "sentences": ["This paper proposes, analyzes, and empirically evaluates PowerSGD (and a version with momentum), a simple adjustment to standard SGD algorithms that alleviates issues caused by poorly scaled gradients in SGD.", "The rates in the theoretical analysis are competitive with those for standard SGD, and the empirical results argue that PowerSGD algorithms are competitive with widely used adaptive methods such as Adam and RMSProp, suggesting that PowerSGD may be a useful addition to the armory of adaptive SGD algorithms.", "Overall I recommend acceptance of this paper, although I think there may be a couple of places where the authors overclaim a bit on the theoretical side.", "Specifically:", "\u2022", "The convergence analysis assumes a batch size equal to T, the number of steps of PowerSGD.", "This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.", "If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).", "\u2022", "In Remark 3.4.3, the authors claim that another point of difference between their results and Yan et al.'s (2018) is that Yan et al. assume bounded gradients, an assumption that is not satisfied for e.g., mean squared error (MSE).", "But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem min_\u03b2 (1/N)\u03a3_n (y_n \u2013 x_n \u2022\u00a0\u03b2)^2 with the minibatch gradient estimator computed over randomly chosen minibatches B: \\hat g = (1/|B|) \u03a3_{n \\in B} x_n (y_n \u2013 x_n \u2022\u00a0\u03b2).", "As the norm of \u03b2 goes to infinity, so does the expected norm of the error of \\hat g. I'm not saying this is a particularly big deal, just that it's not an improvement over Yan et al.'s result.", "That aside, this seems like good work that could have a significant impact on practice.", "A couple of other minor points:", "\u2022", "It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.", "It would be nice to see some discussion (or at least speculation) on why that is.", "\u2022\u00a0Not all of the arrows in Figure 1 are pointing to the right lines.", "\u2022\u00a0In the abstract, it might be good to clarify that the exponentiation is elementwise."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-structuring_quote", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-request_explanation", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 449, "sentences": ["This paper gives a perspective of GANs from control theory, where it is well established in which cases a dynamical system (which can be described analytically as a function of time) is stable or not (by looking at the roots of the denominator of the so-called transfer function in control theory).", "It is interesting that the analysis using this framework on simple examples is in line with known results in the GAN literature (Dirac GAN).", "Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.", "For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).", "In my understanding, the authors present these results to justify the validity of the approach.", "However, this limits the novelty of the results relative to existing literature.", "The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph) .", "Moreover, if I understand correctly the WGAN analysis does not take into account that G and D are non-linear, and it is unclear if these can be done.", "I am also wondering if the comparison with the baselines is fair.", "In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.", "In particular, FID of ~30 on CIFAR10 for the baselines is notably higher then current reported results on this dataset (e.g. Miyato et al. 2018; Chavdarova et al. 2019).", "In my opinion, the authors could start from the existing state of the art implementations on this dataset, and report if negative feedback (NF) improves upon.", "As the approach uses NF is derived specifically for unstable dynamics, it is not clear to me how adding it would affect the training if the dynamics *is stable*. In this context, I consider that for example, the work by Balduzzi et al. 2018 may be relevant as it describes that the dynamics of games (the Jacobian) has two components, one of which describes the oscillating behavior (Hamiltonian game); whereas most games are a mix of oscillating and non-oscillating dynamics.", "It is not clear to me if NF would improve stability/performances in general games.", "As the authors\u2019 main claim is improved stability I am curious to see more detailed analysis on real-world datasets (e.g. multiple seed runs, 2nd-moment estimates over iterations as in Chavdarova et al. 2019).", "In summary, although the proposed perspective seems promising given the presented results and it is interesting, in my opinion, it does not provide novel insights nor obtains current state-of-the-art results on CIFAR10, or guarantee that if pursuing it would allow for solving current issues of GAN training.", "--- Minor ---", "- Abstract: \u2018converge better\u2019 it is not clear to me in what sense (faster/better final performances)", "- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t", "-  Page 3, Sec. 3: I think citing works that also focus on Dirac-GAN would motivate better why you focus on Dirac-GAN in this paper (e.g. writing `as in Mescheder et al. 2018`)", "- Page 4: infinity - infinite", "- Page 5: can also.. explains ->  explain"], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "none", "arg-request_explanation", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_edit", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "arg-request_clarification_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 450, "sentences": ["[Edit] I changed my rating from 4 to 5 based on the author responses.", "=======", "This paper proposed a GAN that learns a disentangled factors of variations in unsupervised (or weakly-supervised) manner.", "To this end, the proposed method incorporates a contrastive loss together with Siamese network, which encourages the generator to output smaller variations in samples if they are drawn by varying the same latent factors.", "The proposed idea is evaluated on simple datasets such as MNIST and centered faces, and show that it is able to learn disentangled latent codes by incorporating some heuristics.", "Although the paper presents an interesting and reasonable idea, I think the paper is incomplete and in the proof-of-concept stage.", "In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper.", "In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.", "In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation."], "labels": ["none", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 451, "sentences": ["MEASURING DENSITY AND SIMILARITY OF TASK RELEVANT INFORMATION IN NEURAL REPRESENTATIONS", "Summary:", "This work attempts to define two kinds of metrics (metrics for information density and for information similarity) for the sake of automatically detecting similarity between tasks so that transfer learning can be done more efficiently.", "The concepts are clearly explained, and the metric for information density seems to match up with intuitions coming out of forward selections approaches.", "The metric for information transfer seems to be the commonplace metric that other works default to when they show that pre-trained representations are effective on downstream tasks.", "It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.", "The problem addressed (automatic similarity scoring of tasks) is important for transfer learning, and thus the results have potential to be very impactful if they generalize to other kinds of tasks; as is, they seem to apply only to classification tasks, but that is a good step.", "Pros:", "Clearly written; experiments on the datasets chosen do seem to suggest that the proposed methods have potential.", "Brings in nice intuition from forward feature selection.", "An important problem with potential for high impact.", "Cons:", "It is not clear to me that the classifier difference metric is well-defined.", "Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?", "Is it not the case that classifier weights could come out quite different despite the tasks being quite similar if the linear classifiers learned to capitalize on dissimilar, yet equally fruitful patterns in the input features?", "Do you have thoughts on how this could be applied outside the context of sentence representations and further outside the context of classification?", "Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.", "These classification datasets are often so close, that I do wonder whether even simpler methods would work just as well.", "For example, clustering on bags-of-words might also show that SST, SST-fine, and IMDb are close/similar/transferable.", "The same could be said for SICK and SNLI.", "It would be nice to see a comparison to such baselines in order to get a sense of how the proposed methods give insights that other unsupervised or supervised methods might give just as well.", "Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against."], "labels": ["arg-structuring_quote", "arg-structuring_heading", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "arg-request_clarification", "arg-request_explanation", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label"]}
{"abstract_id": 452, "sentences": ["This paper proposes a multi-agent hierarchical reinforcement learning algorithm so that multiple humanoid robots can navigate in multi-agent settings (e.g. avoid collisions, collaboration, chase and escape) in a physically simulated environment.", "The key difference of this paper with the prior work on MARL is that it used an accurate physics simulation of humanoid robots.", "This is the main reason of using the hierarchical RL.", "In general, I like this paper.", "It is an important step towards multi-agent learning in complex physical environments.", "The results look appealing, too.", "However, I voted for \"Weak Reject\" for two reasons.", "First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.", "The combination of these two methods seems straightforward.", "Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.", "I do not understand the \"deep integration of MARL and HRL\" that is claimed in the Introduction.", "I also do not agree with another claim that \"We consider the simulation and training environment to be another novel contribution... few simulator support more than one agent, at most 2\".", "In most of the simulators that I am familiar with, such as Mujoco, Bullet, DART, it is straightforward to add multiple simulated robots.", "Second, the writing can be greatly improved.", "Almost half of the technical details are buried in \"8. Supplementary material\".", "Since it is not fair to use \"Supplementary material\" as a way to extend the page limit, I will make my judgement of the paper solely based on the contents up to Section 7.", "In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.", "I think that these are important details and may also be the contributions of this paper.", "Most of these should be moved to the main text.", "Here are some more suggestions on writing:", "1) Certain paragraphs in the main text can be significantly shortened, such as the reward shaping in Section 5.2.", "2) It would be great if the paper can clearly define the experiments: \"waypoint\", \"oncoming\", \"mall\", and \"bottleneck\".", "3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example, promiss->promise week signal->weak signal missing citation [?] in page 3 reuse the same symbol v_{com} for agent's velocity and desired speed in eq(3)"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_clarification", "arg-request_clarification", "none", "none", "arg-request_edit", "arg-request_edit", "arg-request_edit", "arg-structuring_heading", "arg-request_edit", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 453, "sentences": ["This paper proposes an extension of NeuralLP that is able to learn a very restricted (in terms of expressiveness) set of logic rules involving numeric properties.", "The basic idea behind NeuralLP is quite simple: traversing relationships in a knowledge graph can be done by multiplicating adjacency matrices, and which rules hold and which ones don't can be discovered by learning an attention distribution over rules from data.", "The idea is quite clever: relationships between numeric data properties of entities, such as age and heigh, can also be linked by relationships such as \\leq and \\geq, and those relations can be treated in the same way as standard knowledge graph relationship by the NeuralLP framework.", "A major drawback in applying this idea is that the corresponding relational matrix is expensive to both materialise, and use within the NeuralLP framework (where matrices are mostly sparse).", "To this end, authors make this process tractable by using dynamic programming and by defining such a matrix as a dynamic computation graph by means of the cumsum operator.", "Furthermore, authors also introduce negated operators, also by defining the corresponding adjacency matrices by means of computation graphs.", "Authors evaluate on several datasets - two real world and two synthetic - often showing more accurate results than the considered baselines.", "One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.", "Another concern I have is that the expressiveness of the learned rules can be somehow limited, but this paper seems like a good star towards learning interpretable rules involving multiple modalities.", "Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 454, "sentences": ["Summary:", "The authors take two tasks,sentiment analysis and natural language inference, and identify datasets for them which they counterfactually augment it by asking people over the Amazon Mechanical Turk Platform to change either the sentiment (in the case of sentiment analysis) or the nature of relationship in the NLI task by making minimal changes to the text that produce the targeted changes.", "Authors find that popular models trained on either fail on the other dataset while the models trained on both actually generalize much better.", "This is because the original sample and its counterfactual pair the label changed , has the difference in the text that matters to the change and this pair could reduce spurious correlations that models might find in the data distribution.", "Pros:", "This is a very interesting experiment and certainly the dataset that will be released would be extremely valuable to the community.", "The one part (I dont have much NLP background but I do have a causality background) that I like most is that the new text generated are counterfactual in some real sense with respect to a real world generating process - that is people modifying text with changed targets.", "A lot of existing work that claim to do counterfactual changes do not specify assumptions about the generating mechanism.", "For counterfactuals to be valid they have to be intervention on the actual generating mechanism (or an assumed one) acting on a given unit (latent) that produced the current sample.", "The paper in that respect (even if it does not explicitly specify relationship between counterfactuals and generating mechanisms) tries to be faithful to a \"strict causal notion\" by actually asking people to modify the text.", "Cons:", "- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the \"people\" in amazon turk were substituting.", "-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?", "-  In figure 6, it appears that BERT is sensitive to the domain - does it mean that it is bad ? - Authors indicate that ideally it must not be so. Because Table 3 results seem to indicate that BERT performs the best in almost all the cases .", "-  Can the authors highlight the best performances in each case in the Tables by a bold face.", "It helps easily eye ball the best performing model."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_clarification_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 455, "sentences": ["Summary", "Authors present a decentralized policy, centralized value function approach (MAAC) to multi-agent learning.", "They used an attention mechanism over agent policies as an input to a central value function.", "Authors compare their approach with COMA (discrete actions and counterfactual (semi-centralized) baseline) and MADDPG (also uses centralized value function and continuous actions)", "MAAC is evaluated on two 2d cooperative environments, Treasure Collection and Rover Tower.", "MAAC outperforms baselines on TC, but not on RT.", "Furthermore, the different baselines perform differently: there is no method that consistently performs well.", "Pro", "- MAAC is a simple combination of attention and a centralized value function approach.", "Con", "- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.", "- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.", "- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.", "- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).", "It is unclear how the model actually operates and uses attention during execution.", "Reproducibility", "- It seems straightforward to implement this method, but I encourage open-sourcing the authors' implementation."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "arg-structuring_heading", "none", "none", "none", "arg-request_result", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 456, "sentences": ["Overview: This work is an interesting work to understand the generalization capabilities of a two layered neural network in a high dimensional setting (samples, features and neurons tend to infinity).", "It studies the conditions under which the \"double descent phenomenon\" may be observed.", "Summary: The work shows that in two layered neural networks with non-linearity", "1) the double descent phenomenon of the bias-variance decomposition may be observed when the second layer weights are optimized assuming that the first layer weights are constant.", "2) the bias-variance decomposition does not exhibit double descent when optimizing only the first layer with both vanishing and non-vanishing initialization of weights.", "3) For vanishing initalization of weights for the first layer with non-linear activation , the gradient flow solution is asymptotically close to a two layered linear network.", "It is independent of overparametrization.", "However, the condition for this is smooth activation and the result does not hold for ReLU activation.", "4) For non-vanishing initilization of the weights for the first layer with non-linear activation, the gradient flow solution is well approximated by a kernel model.", "However, the risk is independent of overparametrization.", "I believe this is an interesting work that needs to be accepted."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 457, "sentences": ["This paper introduces a new approach to solve optimization problems without relying on any human-provided data beyond the specification of the optimization problem itself.", "The approach is inspired by the two-player zero-sum game paradigm and follow a generative adversarial network (GAN) setting.", "One network is trained to output the optimal behavior for a given problem, while the other is trained to output difficult instances of the given problem.", "These two networks are trained simultaneously and compete against the other until some equilibrium is achieved.", "This approach is tested on two small problems for which the optimal behavior is known and seems to perform near theoretical optimality.", "I weakly reject this paper because although the approach is indeed interesting, the paper is lacking some structure, as described below:", "- The paper clearly mentions that no optimization of the training setup or the hyperparameters has been done because the authors are not interested in extending ML techniques.", "However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.", "It is thus unclear if the approach is robust against different hyperparameter settings.", "- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.", "- Section 1.1 presents results with too many details without introducing the problem.", "I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.", "- One task is presented in Section 2 \"Preliminaries\" while the other task is presented in Section 4 \"AdWords\".", "It is hard to follow the flow of ideas present in the paper when similar things are not together.", "I would suggest restructuring the paper into a more classical structure such as: <intro without detailed results - previous work & problematic - approach taken with more details for reproducibility - description of the two tasks - description of experiments with more details for reproducibility - results - conclusion>.", "- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.", "It is very hard to understand sentences referring to this.", "- This work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms.", "It is hard to support this motivation when no experiments are done in its favor.", "- No comparison has been made between their approach and other previous approaches.", "We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio.", "It is thus very hard to know if this new approach brings any improvement to previous work.", "Below are a few things that were not considered to make a decision, but are only details that would make the paper slightly better:", "- typo at the beginning of section 3.1: missing 'be' in  \"This can either *be* by an ...\"", "- typo at the beginning os section 4:  missing 'be' in \"... the algorithm must irrevocably *be* allocated to ...\"", "- Axis' names to the different plots in the Figures would help understand them better.", "Also, the description of some figures could benefit more details that could be taken off from the text."], "labels": ["none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_quote", "none", "none", "none", "none", "arg-request_edit", "arg-structuring_quote", "none", "arg-request_edit", "none", "none", "arg-structuring_quote", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 458, "sentences": ["This paper studied a random projection of word embeddings in neural language modeling.", "Instead of having |V| x m embeddings, the author(s) represented a word with a random, sparse, linear combination {1, 0, -1} of k vector of size m.", "The experiment on PTB dataset showed that k had to be somewhat close to |V| in order to achieve the comparable perplexity to a feed-forward NLM.", "Overall, I am not sure what we could gain from this research direction.", "The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).", "In addition, the fact that the random projections preserved the inner product (centered at zero) was probably not desirable.", "It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).", "The experiments were quite extensive on the hyper-parameters and showed how the models performed under different settings.", "However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).", "I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).", "PTB also has a very unnatural vocabulary distribution as pointed out in [2].", "Thus, it might be helpful to test the result on another dataset (e.g. WikiText).", "Other comments", "1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.", "2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).", "3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?", "3. Some typos", "- \"... is that instead of trying to probability ...\" => \"... tying ...\"", "- \"... All models sare trained ...\" => \"... are ...\"", "- \"... Tho get the feature ...\" => ?", "References", "[1] S. Arora et al., 2016. Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "[2] S. Merity et al., 2016. Pointer Sentinel Mixture Models", "[3] Y. Gal et al., 2015. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-request_experiment", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_clarification", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 459, "sentences": ["Summary: This paper introduces a new Neural Network training procedure, designed for tabular data, that seeks to leverage feature clusters extracted from GBDTs.", "Strengths: The idea of leveraging feature groups in a neural network structure; the novelty of the RESE model;", "Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.", "Minor typos:", "(abstract)", "- \"NN has achieved\" => \"Neural Networks have achieved\"", "- \"performances\" => performance", "- \"explicitly leverages\" => \"explicitly leverage\"", "Questions:", "- (top of p. 2) What exactly is the difference between \"implicit feature combinations\" and \"explicit (?), expressive feature combinations\"", "- (top of p. 2) \"encourage parameter sharing\" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]", "- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.", "Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?"], "labels": ["arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-structuring_summary", "arg-request_clarification", "none", "arg-request_explanation", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 460, "sentences": ["This paper addresses a novel variant of AutoML, to automatically learn and generate optimization schedules for iterative alternate optimization problems.", "The problem is formulated as a RL problem, and comprehensive experiments on four various applications have demonstrated that the optimization schedule produced can guide the task model to achieve better quality of convergence, more sample-efficient, and the trained controller is transferable between datasets and models.", "Overall, the writing is quite clear, the problem is interesting and important, and the results are promising.", "Some suggestions:", "1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?", "More discussions on these questions can be very helpful to further understand the proposed method.", "2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.", "3. Any plan for open source ?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 461, "sentences": ["This paper proposes to use codes and codebooks to compress the weights.", "The authors also try minimizing the layer reconstruction error instead of weight approximation error for better quantization results.", "Distillation loss is also used for fine-tuning the quantized weight.", "Empirical results on resnets show that the proposed method has a good compression ratio while maintaining competitive accuracy.", "This paper is overall easy to follow.", "My main concern comes from the novelty of this paper.", "The two main contributions of the paper:", "(1) using codes and codebooks to compress weights; and", "(2) minimizing layer reconstruction error instead of weight approximation error", "are both not new.", "For instance, using codes and codebooks to compress the weights has already been used in [1,2].", "A weighted k-means solver is also used in [2], though the \"weighted\" in [2] comes from second-order information instead of minimizing reconstruction error.", "In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].", "Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.", "It is not clear how the compression ratio in table 1 is obtained.", "Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).", "Can the authors provide an example to explain how to compute the compression ratio?", "[1].", "Model compression as constrained optimization, with application to neural nets.", "part ii: quantization.", "[2]. Towards the limit of network quantization.", "[3]. Efficient and Accurate Approximations of Nonlinear Convolutional Networks.", "[4]. ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_clarification", "arg-request_result", "arg-request_result", "arg-request_explanation", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 462, "sentences": ["This paper has problems with clarity/polish and experimental design that are sufficiently severe to merit rejection by themselves.", "Regarding clarity/polish:", "I am generally not super picky about these things, but there does have to be some standard.", "This paper looks very hastily put together, especially pages 7 and 8.", "There are many typos and unclear statements.", "Just a few examples:", "> Generative Adversarial Networks (GANs) are powerful framework for (in the abstract)", "> be a good metric to evolution the difference (in the abstract)", "> In the past few years, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are impactful because it has shown lots of great results for many AI tasks, (first sentence)", "> It means that there is no an unanimous metric to represent the difference between the true data distribution and the generated distribution", "What does this mean? People have mostly settled on using FID for this.", "> It is also difficult to know whether the generated distribution is close to the true distribution, and this is often observed by human eyes.", "Isn't this just restating the point made in the first sentence?", "Regardless, nobody really uses human evaluation anymore - so this is just not correct.", "> It means that if the original generator and discriminator are random, it is difficult to confirm that the generator and discriminator can converge to the ideal conclusion by training with given data.", "But this paper doesn't propose a way to solve that problem, so it's strange to mention this here in this way.", "These issues would maybe be excusable if not for the totally inadequate experimental validation.", "A non-exhaustive list of methodological problems with the (single) experiment:", "1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs have inter-run variance larger than the difference in score reported in Fig 1 and 2.", "2. The models have not been trained for long enough.", "3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which probably leads to:", "4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,", "and rendering the third claim from the introduction (\"We propose an new algorithm with the new metric which demonstrates better results than state-of-the-art algorithms.\") completely untrue.", "In light of these other issues, I haven't checked the proofs."], "labels": ["none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-structuring_quote", "arg-structuring_quote", "arg-structuring_quote", "arg-structuring_quote", "none", "arg-structuring_quote", "none", "none", "arg-structuring_quote", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 463, "sentences": ["This paper proposes a novel view for stabilising GANs from the perspective of control theory.", "This view provides new insights into GAN training and may inspire future research along this direction.", "This paper is overall well written, with a smooth introduction of background material that might be less familiar for machine learning researchers.", "There are places that need further clarification, but I think the proposed direction is promising.", "Questions about the method:", "- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective. For example, could the proposed regulariser be interpreted as imposing certain constraint on the spectrum of Jacobian?", "- Does section 2.2 depend on the assumption of linear dynamics?", "- Does the E in eq.7 come from eq. 4?", "- Could you give some intuition for the paragraph above section 3.4, about the different form of inputs when treating D and G as dynamics?", "For consistency, it is perhaps better to keep the dependency of p_D and p_G on x explicit (same for eq. 10), unless this is intended?", "- My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters?", "Experiments:", "- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018). However, Mescheder et al. (2018) uses DCGAN for CIFAR10, which raises further questions about the scores on this dataset:", "- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper. In Figure 5 of this paper, they are clearly below 6.", "What\u2019s the reason for this discrepancy?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "arg-request_clarification", "arg-request_explanation", "arg-request_edit", "none", "arg-structuring_heading", "none", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 464, "sentences": ["This paper seems to be an exposition on the primary performance affecting aspects of generative adversarial networks (GANs).", "This can possibly affect our understanding of GANs, helping practitioners get the most in their applications, and perhaps leading to innovations that positively affect GAN performance.", "Normally, expositions such as this I find difficult to recommend for publication.", "In these times, one can find \"best practices\" with a reasonable amount of rigor on data science blogs and such.", "An exposition that I would recommend for publication, would need to exhibit a high sense of depth and rigor for me to deem it publication worthy.", "This paper, for me, achieves this level of quality.", "The authors start off by giving a precise, constrained list of hyperparameters and architectural components that they would explore.", "This is listed in the title and explained in detail in the beginning of the paper.", "The authors are right in explaining that they could not cover all hyperparameters and chose what I feel are quite salient ones.", "My one ask would have been a survey of how activations might affect performance.", "I sense that everyone has settled upon LeakyReLUs for internal layers, but a survey of that work and experimentation within the authors' framework would have been nice.", "The authors then explain the metrics for evaluation and datasets.", "The datasets offered a healthy variety for typical image recognition tasks.", "It would be interesting to see what these metrics would reveal when applied to other types of data (e.g. scientific images).", "The  authors explain, with graphs, the results of the loss, normalization, and architectures.", "I feel the discussion on loss was rushed, and I gained no insight on what the authors thought was a prominent difference between the three losses studied.", "Perhaps the authors had no salient observations for loss, but explicitly stating such would be useful to the reader.", "The only observation I gained as far as this is that non-saturating loss would possibly be stable across various datasets.", "Regularization and normalization are discussed in much more detail, and I think the authors made helpful and interesting observations, such as the benefits of spectral normalization and the fact that batch normalization in the discriminator might be a harmful thing.", "These are good takeaways that could be useful to a vast number of GANs researchers.", "For architectures to be a main pillar of the paper, I feel that this area could have been explored in greater detail.", "I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.", "Unless I am misunderstanding something, it seems that the authors simply tested one more architecture, for the express purpose of testing whether their observations about normalization would hold.", "As a bonus, the authors bring up some problems they had in making comparisons and reproducing results.", "I think this is an extremely important discussion to have, and I am glad that the authors detailed the obstacles in their journey. Hopefully this will inspire other researchers to avoid adding to the complications in this field.", "The graphs were difficult to parse.", "I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.", "In addition, different charts presenting only one loss function, with their spectral normalization and gradient penalty variants, would have made the effects of the normalization more obvious on the FID distribution graphs.", "If this can be changed before publication, I would strongly suggest it.", "I appreciate that the authors provided source code via GitHub.", "However, in the future, the authors should be careful to provide an anonymous repository for review purposes.", "I had to be careful not to allow myself to focus on the author names which are prominent in the repository readme, and one of whom has his/her name in the GitHub URL itself.", "I didn't immediately recognize the names and thus it was easy for me not to retain them or focus on them.", "However, if it had been otherwise, it might have risked biasing the review.", "In all, I think this is a good and useful paper from which I have learned and to which I will refer in the future as I continue my research into GANs and VAEs.", "I would suggest changing the title to be more appropriate and accurate (the researchers are primarily focused on showing the positive and negative effects of normalization across various loss functions and architectures).", "But altogether, I believe this is a paper worth publishing at ICLR."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-request_experiment", "arg-request_experiment", "arg-structuring_summary", "arg-structuring_summary", "arg-request_experiment", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_edit", "none", "none", "none", "none", "none", "none", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 465, "sentences": ["The paper proposes an end-to-end joint model for named entity recognition (NER) and relation extraction (RE), using pre-trained language models.", "The model is very simple, with the key is to use BERT and take NER output as input to RE.", "The experimental results show the model, without the need for handcrafted features, get state-of-the-art results on five datasets.", "Although the paper is well written and shows good results, I would reject the paper because:", "- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.", "- the good performance seems to be from BERT rather than the model's structure (table 2 suggests that). I thus think the contribution of the paper is pretty not significant.", "I think the paper does not fit this conference. It is better to be presented in a Demonstration section at a *ACL conference."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 466, "sentences": ["In this paper, the authors claim that they are able to update the generator better to avoid generator mode collapse and also increase the stability of GANs training by indirectly increasing the entropy of the generator until it matches the entropy of the original data distribution using functional gradient methods.", "The paper is interesting and well written.", "However, there is a lot of work coming out in the field of GANs currently, so I am not able to comment on the novelty of this regularization approach, and I am interested to know how this method performs when compared to other techniques to avoid mode collapse such as feature matching and mini-batch discrimination, etc."], "labels": ["arg-structuring_summary", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label"]}
{"abstract_id": 467, "sentences": ["The aim of this paper is to solve SAT instances using a CNN architecture.", "SAT instances are represented using an efficient encoding of boolean matrices.", "The overall idea is to decompose an input SAT instance into simpler ones, and to train the neural model on simpler instances using an existing solver for labeling these instances.", "Based on satisfaction probabilities induced from simpler formulas, the architecture predicts a partial assignment which is fed to the existing solver for deriving the satisfiability result.", "Arguably, the topic of \u201clearning to solve SAT instances\u201d is very interesting, by coupling results from neural networks and SAT solvers.", "This work is inspired from the landmark paper on NeuroSAT, and the experimental results look promising.", "However, since the framework is focused on solving random SAT problems (especially random 3-SAT instances), the paper is missing a detailed description of this active research topic in AI and the SAT community (see e.g. [1,2]).", "Notably, the problem of generating realistic random k-SAT instances has long been considered as one of the most important challenges in SAT research [3].", "Importantly, modern random k-SAT instances are not only characterized by their number of variables, and their ratio  #clauses / #variables, but with an additional \u201cstructure\u201d which mimics real-world, industrial instances (see e.g. [4]).", "Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.", "Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.", "How do \u201cwe choose a specific number of assignments based on prediction probabilities\u201d?", "Unless I missed something, the output of the CNN architecture is a probability value that the input formula is SAT, so I don\u2019t really see how this can be related to prediction probabilities of assignments.", "This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.", "The example at the end of section 3.3 is not very helpful: namely, the CNF formula $(x_2) \\land (\\neg x_2)$ is clearly unsatisfiable, so how can the model predict that it is satisfiable with 80% probability? And, if we try here $x_2 = 1$, we immediately get $\\bot$ (the unsat CNF), but not $x_1$ (which was already assigned to $0$).", "Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.", "The Z3 solver is mainly focused on solving SMT instances [5], not random k-SAT instances which, by the way, is a common track in annual SAT competitions (see e.g. [6]).", "To this point, generic SAT solvers such as MiniSAT [7] and Glucose [8] are able to solve in few seconds some random 3-SAT instances with thousands of variables and tens of thousands of clauses (see e.g. [4]).", "So, the motivating assertion \u201c[...] state-of-the-art solvers do not yet scale to large, difficult formulas, such as ones with hundreds of variables and thousands of clauses\u201d in the introduction of the paper, is not totally correct.", "To sum up, I would recommend to compare the CNNSAT architecture with well-known SAT solvers such as MinSAT, Glucose, March, or Dimetheus [9] which has been one of the strongest solvers in recent years for tackling random instances.", "Also, as mentioned above, it would be interesting to incorporate some structures (such as, for example, community attachments or popularity-similarities) in SAT instances, in order to estimate whether CNNSAT could handle pseudo-industrial problems.", "[1] D. Mitchell, B. Selman, H. Levesque, Hard and easy distributions of SAT problems, in: Proceedings of the 10th National Conference on Artificial Intelligence, AAAI\u201992, 1992, pp. 459\u2013465.", "[2] Nudelman, E., Leyton-Brown, K., Hoos, H. H., Devkar, A., & Shoham, Y. Understanding random SAT: Beyond the clauses-to-variables ratio. In 10th International Conference on Principles and Practice of Constraint Programming (CP\u201904), pp. 438\u2013452.", "[3] B. Selman, H.A. Kautz, D.A. McAllester, Ten challenges in propositional reasoning and search, in: Proceedings of the 15th International Joint Conference on Artificial Intelligence, IJCAI\u201997, 1997, pp. 50\u201354.", "[4] J. Gir\u00e1ldez-Cru and J. Levy. Generating sat instances with community structure. Artificial Intelligence, 238:119 \u2013 134, 2016.", "[5] The 2014 SMT Competition https://satassociation.org/jsat/index.php/jsat/article/download/122/114", "[6] The 2018 SAT Competition http://sat2018.forsyte.tuwien.ac.at/index.php?cat =results", "[7] N. E\u00e9n, N. S\u00f6rensson, An extensible SAT-solver, in: Proceedings of the 6th International Conference on Theory and Applications of Satisfiability Testing, SAT\u201903, 2003, pp. 502\u2013518.", "[8] ] G. Audemard, L. Simon, Predicting learnt clauses quality in modern SAT solvers, in: Proceedings of the 21st International Joint Conference on Artificial Intelligence, IJCAI\u201909, 2009, pp. 399\u2013404", "[9] Dimetheus https://www.gableske.net/dimetheus"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_clarification", "none", "arg-request_edit", "arg-request_explanation", "arg-request_experiment", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 468, "sentences": ["Summary: The paper studies the problem of training deep neural networks in the distributes setting while ensuring privacy.", "Each data sample is held by one individual (e.g., on a cell phone), and a central algorithm trains a learning model on top of this data.", "In order to protect the privacy of the individuals, the paper proposes the use of multi-layer encoders (E) over the raw data, and then send them across the server.", "The privacy is ensured by exemplifying the inability to reconstruct the original data from the encoded features, via running a reverse deep model (X).", "The notion of privacy is quantified by the Euclidian distance between the reconstructed vector via the best X and the original feature vector, maximized over E. The overall framework resembles a GAN, and the paper calls it RAN (Reconstructive Adversarial Network).", "Positive aspects: The problem of training privacy preserving deep models over distributed data has been a significant and important challenge.", "The current solutions that adhere to differential privacy based approaches are not yet practical. In my view, it is a very important research question.", "Negative aspects: One major concern I have with the paper is the notion of privacy considered.", "The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.", "There has been a large body of work which shows that weaker attacks like membership attacks can be equally damaging, ii) Privacy is a worst-case guarantee.", "I do not see the GAN style approach taken by the paper, ensures this."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 469, "sentences": ["The paper deals with further development of RAND-WALK model of Arora et al. There are stable idioms, adjective-noun pairs and etc that are not covered by RAND-WALK, because sometimes words from seemingly different contexts can join to form a stable idiom.", "So, the idea of paper is to introduce a tensor T and a stable idiom (a,b) is embedded into v_{ab}=v_a+v_b+T(v_a, v_b,.) and is emitted with some probability p_sym (proportional to exp(v_{ab} times context)).", "The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.", "Finally, there exists an expression, PMI3(u,v,w), that shows the correlation between 3 words, and that can be estimated from the data directly.", "It is proved that Tucker decomposition of that tensor gives us all words embeddings together with tensor T. Thus, from the latter we will obtain a tool for finding embeddings of idioms (i.e. v_a+v_b+T(v_a, v_b,.)).", "Theoretical analysis seems correct (I have not checked all the statements thoroughly, but I would expect formulations to be true).", "The only problem I see is that phrase similarity part is not convincing.", "I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 470, "sentences": ["The authors proposed an AutoLoss controller that can learn to take actions of updating different parameters and using different loss functions.", "Pros", "1. Propose a unified framework for different loss objectives and parameters.", "2. An interesting idea in meta learning for learning loss objectives/schedule.", "Cons:", "1. The formulation uses REINFORCE, which is often known with high variance.", "Are the results averaged across different runs? Can you show the variance?", "It is hard to understand the results without discussing it.", "The sample complexity should be also higher than traditional approaches.", "2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?", "3. Why do you set S=1 in the experiments? What\u2019s the importance of S?", "4. I think it is quite surprising the AutoLoss can resolve mode collapse in GANs.", "I think more analysis is needed to support this claim.", "5. The evaluation metric of multi-task MT is quite weird. Normally people report BLEU, whereas the authors use PPL.", "6. According to https://github.com/pfnet-research/chainer-gan-lib, I think the bested reported DCGAN results is not 6.16 on CIFAR-10 and people still found other tricks such as spectral-norm is needed to prevent mode-collapse.", "Minor:", "1. The usage of footnote 2 is incorrect.", "2. In references, some words should be capitalized properly such as gan->GAN."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_clarification", "none", "arg-request_result", "arg-request_explanation", "none", "arg-request_explanation", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 471, "sentences": ["Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs for multimodal NMT.", "Their approach enables visual information to be integrated into large-scale text-only NMT.", "Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines.", "Strengths:", "- This paper is well motivated and well written. I especially like how they use external paired sentence-image data from Multi30k to learn weak pairs for sentences in machine translation.", "- Experimental results are convincing. I like how low-resource translation is included as a priority in their experiments.", "Weaknesses:", "- Do you have any explanations as to why the number of images, if too large, actually hurts translation performance? Is it because more images also leads to a higher chance of noisy images?", "- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.", "- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.", "- Why are there missing BLEU scores and the number of parameters in Table 1?", "### Post rebuttal # ##", "Thank you for your detailed answers to my questions."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_experiment", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 472, "sentences": ["Summary:", "This paper proposes a way to train a manager agent which would manage a bunch of worker agents to achieve a high-level goal.", "Each worker has its own set of skills and preferences and the manager tries to assign sub-tasks to these agents along with bonuses such that the agents can even perform tasks that are not preferred by them.", "Authors achieve this by training a manager which tracks the skills and preferences of the agents on the fly.", "Authors have done an extensive analysis of the proposed approach in two simple domains: resource collection and crafting.", "Major comments:", "This paper focuses on multi-agent settings with self-interested agents.", "The problem formulation and the solution are novel enough.", "Experiments are on toy domains with very few goals and sub-task dependencies.", "However, authors have done a good job in doing an extensive analysis of the proposed approach.", "1.\tCan you comment about the scalability of the proposed solution when the number of possible subtasks increases? When the sub-task dependency graph size increases?", "2.\tWhat is the reason for using rule-based agents in all the experiments? It would have been more useful if all the analysis are done with RL agents rather than rule-based agents.", "It would also make the paper stronger.", "3.\tAre the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper.", "I would increase my rating if the authors are willing to release the code to reproduce all the results reported in the paper.", "Minor comments:", "1.\tPage 3, line 9: \u201ctypical\u201d -> \u201ctypically\u201d", "2.\tPage 3, \u201cintention\u201d section: \u201cBased on the its reward ..\u201d Check grammar.", "3.\tPage 5, last line: \u201cthe total quantitative is 10\u201d check grammar.", "4.\tPage 8, conclusions, second line: \u201cnad\u201d -> \u201cand\u201d", "5.\tPage 8, conclusions, 4th line: \u201ccombing\u201d -> \u201ccombine\u201d"], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_clarification", "arg-request_edit", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_typo_label", "none_label", "arg-request_edit_label", "arg-request_typo_label", "arg-request_typo_label"]}
{"abstract_id": 473, "sentences": ["This papers tackles the following question. Is it possible to learn the \"most\" complex instance of a class of (combinatorial) problem while finding (or recovering) algorithms with strong minimax rate.", "This is very interesting and clearly a nice line of work (in theory though).", "The techniques used rely on GANs since it can be shown that finding the best (random) algorithm and the worst (deterministic) instance is equivalent to finding the worst random instance against the best deterministic algorithm.", "This is actually a direct consequence of any minmax theorem in game theory; the authors decided to credit that result to Yao (I tend to *strongly* disagree with that point as, even if he stated this fact in CS, this result was quite standard several decades before him - anyway.).", "Then this idea is evaluated in two examples.", "A toy problem (the ski rental) and a more or less concrete ones (adwords pb of Mehta).", "This is the major disappointment in the paper.", "The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract \"find algorithms with strong worst-case guarantees for online combinatorial optimization problems\".", "So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).", "I believe that this paper is thus not in its final form and could be largely improved."], "labels": ["arg-structuring_summary", "none", "arg-structuring_summary", "none", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 474, "sentences": ["The paper considers an actor-critic scheme for multiagent RL, where the critic is specific to each agent and has access to all other agents' embedded observations.", "The main idea is to use an attention mechanism in the critic that learns to selectively scale the contributions of the other agents.", "The paper presents sufficient motivation and background, and the proposed algorithmic implementation seems reasonable.", "The proposed scheme is compared to two recent algorithms for centralized training of decentralized policies, and shows comparable or better results on two synthetic multiagent problems.", "I believe that the idea and approach of the paper are interesting and contribute to the multiagent learning literature.", "Regarding cons:", "- The critical structural choices (such as the attention model in section 3.2) are presented without too much justification, discussion of alternatives, etc.", "- The experiments show the learning results, but do not provide a peak \"under the hood\" to understand the way attention evolved and contributed to the results.", "- The experiments show good results compared to existing algorithms, but not impressively so."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 475, "sentences": ["The paper is well written and the main contribution, a methodology to find \u201cblind-spot attacks\u201d well motivated and differences to prior work stated clearly.", "The empirical results presented in Figure 1 and 2 are very convincing.", "The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.", "Why for example not using a simple score based on the histogram, or even the mean distance?", "Of course providing a single measure would allow to leverage that information during training.", "However, in its current form this seems rather complicated and computationally expensive (KL-based).", "As stated later in the paper the histograms themselves are not informative enough to detect such blind-spot transformation.", "Intuitively this makes a lot of sense given that the distance is based on the network embedding and is therefore also susceptible to this kind of data.", "However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue."], "labels": ["none", "none", "none", "arg-request_clarification", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 476, "sentences": ["The paper proposes a neural net implementation of counterfactual regret minimization where 2 networks are learnt, one for estimating the cumulative regret (used to derive the immediate policy) and the other one for estimating a cumulative mixture policy.", "In addition the authors also propose an original MC sampling strategy which generalize outcome and external sampling strategies.", "The paper is interesting and easy to read. My main concern is about the feasibility of using a neural networks to learn cumulative quantities.", "The problem of learning cumulative quantities in a neural net is that we need two types of samples:", "- the positive examples: samples from which we train our network to predict its own value plus the new quantity, but also:", "- the negative examples: samples from which we should train the network to predict 0, or any desired initial value.", "However in the approach proposed here, the negative examples are missing.", "So the network is not trained to predict 0 (or any initial values) for a newly encountered state.", "And since neural networks generalize (very well...) to states that have not been sampled yet, the network would predict an arbitrary values in states that are visited for the first time.", "For example the network predicting the cumulative regret may generalize to large values at newly visited states, instead of predicting a value close to 0.", "The resulting policy can be arbitrarily different from an exploratory (close to uniform) policy, which would be required to minimize regret from a newly visited state.", "Then, even if that state is visited frequently in the future, this error in prediction will never be corrected because the target cumulative regret depends on the previous prediction.", "So there is no guarantee this algorithm will minimise the overall regret.", "This is a well known problem for exploration (regret minimization) in reinforcement learning as well (see e.g. the work on pseudo-counts [Bellemare et al., 2016, Unifying Count-Based Exploration and Intrinsic Motivation] as one possible approach based on learning a density model).", "Here, maybe a way to alleviate this problem would be to generate negative samples (where the network would be trained to predict low cumulative values) by following a different (possibly more exploratory) policy.", "Other comments:", "- It does not seem necessary to predict cumulative mixture policies (ASN network).", "One could train a mixture policy network to directly predict the current policy along trajectories generated by MC.", "Since the samples would be generated according to the current policy \\sigma_t, any information nodes I_i would be sampled proportionally to \\pi^{\\sigma^t}_i(I_i), which is the same probability as in the definition of the mixture policy (4).", "This would remove the need to learn a cumulative quantity.", "- It would help to have a discussion about how to implement (7), for example do you use a target network to keep the target value R_t+r_t fixed for several steps?", "- It is not clear how the initialisation (10) is implemented.", "Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_clarification", "none", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "arg-structuring_quote_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 477, "sentences": ["This paper proposes a pair of LSTM networks, one of which estimates the current strategy at iteration t+1 and the other estimates the average strategy after t iterations.", "By using these networks within a CFR framework, the authors manage to avoid huge memory requirements traditionally needed to save cumulative regret and average strategy values for all information sets across many iterations.", "The neural networks are trained via a novel sampling method with lower variance/memory-requirements that outcome/external sampling, and are amendable to continual improvement by warm-starting the networks based on cloned tabular regret values.", "Overall, the paper is well-written with clear definitions/explanations plus  comprehensive ablation-analyses throughout, and thus constitutes a nice addition to the recent literature on leveraging neural networks for IIG.", "I did not find many flaws to point out, except I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed.", "Typo:  \"care algorithm design\" -> \"careful algorithm design\""], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_experiment", "arg-request_typo"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-request_experiment_label", "arg-request_typo_label"]}
{"abstract_id": 478, "sentences": ["The submission proposes a new method for agent design to learn about the behaviour of other fixed agents inhabiting the same environment.", "The method builds on imitation learning (behavioural cloning) to model the agent\u2019s behaviour and reinforcement learning to learn a probing policy to more broadly explore different target agent behaviours.", "Overall, the approach falls into the field of intrinsic motivation / curiosity-like reward generation procedures but with respect to target agent behaviour instead of the agent\u2019s environment.", "While learning to model the target agent\u2019s inner state, the RL reward is generated based on the difference of the target agent\u2019s inner state between consecutive time steps.", "The approach is evaluated against a small set of baselines in various toy grid-world scenarios and a sorting task and overall performs commensurate or better than the investigated baselines.", "Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.", "It would be highly beneficial to evaluate these aspects.", "Furthermore, it would be beneficial to provide more information about the baselines; in particular the type of count-based exploration.", "For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs to not only evaluate performance but also robustness.", "Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).", "One additional aspect pointing towards the necessity of further evaluation is the strong dependence of performance on the dimensionality of the latent, internal state (Fig.4).", "Minor issues:", "- Reward formulations for the baselines as part of the appendix.", "- Same scale for the y-axes across figures"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-request_experiment", "arg-request_clarification", "arg-request_edit", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 479, "sentences": ["Pros", "Solid technical innovation/contribution:", "- The paper proposed a novel method FICM that bridged the intrinsic reward in DRL with optical flow loss in CV to encourage exploration in an environment with sparse rewards.", "To the best of my knowledge, this was the first paper proposed to use moving patterns in two consecutive observations to motivate agent exploration.", "Balanced view:", "- The authors discussed both the advantages of FICM and settings that FICM might fail to perform well, and conducted experiments to better help the readers understand such nuances.", "Such balanced view should be valuable to RL communities in both academia and industry.", "Clarity:", "- In general this was a very well-written paper, I had no difficulty in following the paper throughout.", "The proposed method (FICM) was clearly motivated, and the authors provided good coverage of related works.", "Notably, the authors reviewed two relevant methods upon which FICM was motivated, which made the paper self-contained.", "Cons", "Experiments:", "- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).", "It would be interesting to compare FICM against simpler exploration baselines such as epsilon-greedy or entropy regularization.", "- I\u2019d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.", "Significance of the innovation:", "- The proposed exploration method seemed to be applicable with a particular RL setting: the environment changes could be represented through consecutive frames (e.g., video games), and optical flow could be used to interpret any object displacements in such consecutive frames.", "And as the authors discussed, even under such constraints the applicability of proposed method depends on how much changes of the environment were relevant to the goal.", "Reproducibility:", "- Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.", "Summary", "A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited."], "labels": ["arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "arg-structuring_summary", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 480, "sentences": ["This paper addresses the problem of building models for NLP tasks that are robust against spurious correlations in the data by introducing a human-in-the-loop method: annotators are asked to modify data-points minimally in order to change the label.", "They refer to this process as counterfactual augmentation.", "The authors apply this method to the IMDB sentiment dataset and to SNLI and show (among other things) that many models cannot generalize from the original dataset to the counterfactually-augmented one.", "This contribution is timely and addresses a very important problem that needs to be addressed in order to build more robust NLP systems.", "Because, however, of a few limitations, I recommend weak acceptance.", "My main hesitation comes from a lack of clarity about the main lesson we have learned.", "In particular, if the goal is to use this method to augment the data we use to train NLP systems in order to make them more robust, it seems that the time cost of the process will be prohibitive.", "On the other hand, perhaps these methods could be used to identify the kind of spurious correlations that models tend to rely on, which could then be used in a more automated data augmentation process.", "If that's the goal, however, a more detailed error analysis would need to be included.", "A few small comments:", "* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.", "I would love to see a more detailed investigation of what annotators usually did.", "For instance, a reason that hypothesis-only models do well is that certain words are very predictive of certain labels (e.g. \"not\" and contradiction).", "Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?", "That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.", "* The BiLSTM they use is very small (embedding and hidden dimension 50).", "Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.", "It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.", "Some very minor / typographic comments:", "* abstract: \"with revise\" should be \"with revising\"", "* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause", "* page 2, \"We show that...\" I'd break this into two sentences to make it easier to parse.", "* Table 3: I would make two columns for each model with accuracy on original versus revised.", "With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "arg-request_clarification", "arg-request_clarification", "none", "arg-request_clarification", "arg-request_experiment", "arg-request_edit", "arg-request_edit", "arg-request_clarification", "arg-structuring_heading", "arg-request_typo", "arg-request_edit", "arg-request_typo", "arg-request_clarification", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label"]}
{"abstract_id": 481, "sentences": ["The paper develops a new 'convolution' operation.", "I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.", "p2-3, Section 3.1 - I found the equations impossible to read. What are the subscripts over?", "In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??", "Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?", "Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?", "Experimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost.", "It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.", "It would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions."], "labels": ["arg-structuring_summary", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-request_clarification_label", "arg-request_explanation_label", "arg-request_clarification_label", "arg-request_explanation_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label"]}
{"abstract_id": 482, "sentences": ["This paper introduces deficiency bottleneck for learning a data representation and represent  complicated channels using simpler ones.", "This problem has a natural variational form that can be easily implemented from VIB.", "Experiments show good performance comparing to VIB.", "This paper is well-written and easy to read.", "The idea using KL divergence creating a deficiency channel to learn data representation is very natural.", "It is interesting that this formulation could be understood as minimizing a regularized risk gap of statistical decision problems, which justifies the usage of deficiency bottleneck (eq.9).", "My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.", "However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.", "For example, how does the method compare with (variants of) Variational Autoencoder?", "A discussion on this or some empirical evaluations would be nice."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 483, "sentences": ["The paper introduces adaptive kernels (that adapts its weights as a function of image content) to the framework of CNN.", "The benefit of adaptive kernels is the reduction of memory usage (at training and at the inference time) as well as training speedups (up to 2x).", "The kernels are evaluated on two datasets MNIST and CIFAR10", "I like the idea of building models that are memory efficient at training and at evaluation time.", "However, the evaluation of the proposed adaptive kernels is rather limited.", "In order to improve the paper, the authors could take into consideration the following points:", "1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?", "2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?", "3. Traditional convolutional kernels together with max pooling operations ensures some degree of translation invariance.", "How big is the generalization gap for the tested models when adaptive kernel is used?", "4. How sensitive are the results to the number of adaptive kernels in the layers.", "5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?", "6. On CIFAR10 the results seem to be worse that other methods.", "However, it is important to note that the Adaptive Kernels CNN has way less parameters.", "It would be interesting to see how the performance of adaptive kernels based CNNs scales with the number of parameters.", "7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.", "8. The authors acknowledge the similarities (and some differences) with Brabandere et al (2016).", "It might be beneficial to include comparison to this approach in the experimental section.", "Moreover, given the similarities, it might be good to discuss the differences in the approaches in the introduction section.", "9. The ideas presented in the paper seems related to general concept of hypernetworks, where one network learns (or helps to learn) paramenters of the other network.", "It would be nice to position the ideas from the paper w.r.t. this line of research too.", "10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).", "I like the drawings, however, the font on the drawings is too small - making it hard to read.", "Some typos:", "1. the difficult to train the network", "2. table 2: Dynamic -> Adaptive?", "Overall, the paper presents interesting ideas with some degree of originality.", "I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "none", "none", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_experiment", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_edit", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_typo_label", "none_label", "arg-request_edit_label"]}
{"abstract_id": 484, "sentences": ["The paper proposes a new joint learning algorithm that works for two tasks, NER and RE.", "The model is based on a pre-trained BERT model, which provides the word vectors of the input word sequence.", "Then it solves two tasks with two network branches: the first branch minimizes the loss for NER, and the second branch minimizes the loss for RE.", "The second branch uses entity labels predicted by the first branch, so joint learning may benefit both tasks.", "The design of the architecture is novel, but it is also not groundbreaking.", "Each network branch is from known structures, but the combination is not proposed before.", "The submission has evaluated the proposed algorithms on four datasets and improved SOTA performances.", "The ablation study justifies the design details.", "The writing is generally clear.", "Now critics:", "Ablation study:", "1. As pointed by one public comment, the ablation study should show how much improvement is from BERT vectors.", "2. I'd like to see another ablation study of whether RE helps NER. If you remove the RE component, does the NER performance suffer?", "Writing:", "3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?"], "labels": ["arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-request_result", "arg-request_experiment", "arg-structuring_heading", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-structuring_heading_label", "arg-request_explanation_label"]}
{"abstract_id": 485, "sentences": ["The paper proposes a new way to generate adversarial images that are perturbed based on natural images called Shadow Attach.", "The generated adversarial images are imperceptible and have a large norm to escape the certification regions.", "The proposed method incorporates the quantities of total variation of the perturbation, change in the mean of each color channel, and dissimilarity between channels, into the loss function, to make sure the generate adversarial images are smooth and natural.", "Quantitative studies on CIFAR-10 and ImageNet shows that the new attack method can generate adversarial images that have larger certified radii than natural images.", "To further improve the paper, it would be great if the authors can address the following questions:", "- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?", "- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.", "In my opinion, to support the above claim, shouldn\u2019t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?", "- From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?", "- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.", "A smaller dissimilarity suggests a greater similarity between channels.", "- Lambda sim and lambda s are used interchangeably. Please make it consistent.", "- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_experiment", "arg-request_explanation", "arg-request_edit", "none", "arg-request_edit", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_quote_label", "arg-request_edit_label", "none_label", "none_label"]}
{"abstract_id": 486, "sentences": ["The paper proposed a new pipelined training strategy to fully utilize the memory and computational power to speed up the training process.", "In order to overcome the generalization degradation of the proposed method, the authors further introduced the so-called hybrid method to combine their proposed pipelined method and normal training.", "The pipelined method is interesting.", "For the pipelined process itself, it is similar to model parallelization.", "For the method proposed by the paper,  it is like the async-SGD method.", "The paper merged these two ideas together but did not solve the problem from async-SGD, i.e. with a large number of processes, the generalization performance degrades (in the paper, it is so-called \"stages\").", "Even with the hybrid method, the accuracy still drops.", "Also, the sentence, \"We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy.\", is confusing. If I use data parallelization, the gain should be also around 2.", "The ResNet on Cifar-10 results are not convincing. The normal accuracy of ResNet20 on Cifar-10 is around 92 but the paper reported 91.1%.", "Based on this, I think the paper has some room for improvement."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-structuring_summary", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 487, "sentences": ["Summary", "This paper derives a new policy gradient method for when continuous actions are transformed by a normalization step, a process called angular policy gradients (APG).", "A generalization based on a certain class of transformations is presented.", "The method is an instance of a Rao-Blackwellization process and hence reduces variance.", "Detailed comments", "I enjoyed the concept and, while relatively niche, appreciated the work done here and do believe it has clear applications.", "I am not convinced that the measure theoretic perspective is always necessary to convey the insights, although I appreciate the desire for technical correctness. Still, appealing to measure theory does reduces readership, and I encourage the authors to keep this in mind as they revise the text.", "Generally speaking it seems like a lot of technicalities for a relatively simple result: marginalizing a distribution onto a lower-dimensional surface.", "The paper positions itself generally as dealing with arbitrary transformations T, but really is about angular transformations (e.g. Definition 3.1).", "The generalization is relatively straightforward and was not too surprising given the APG theory.", "The paper would gain in clarity if its scope was narrowed.", "It's hard for me to judge of the experimental results of section 5.3, given that there are no other benchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.", "Def 4.4: \"a notion of Fisher information\" -- maybe \"variant\" is better than \"notion\", which implies there are different kinds of Fisher information", "Def 3.1 mu is overloaded: parameter or measure?", "4.4, law of total variation -- define", "Overall", "This was a fun, albeit incremental paper.", "The method is unlikely to set new SOTA, but I appreciated the appeal to measure theory to formalize some of the concepts.", "Questions", "What does E_{pi|s} refer to in Eqn 4.1?", "Can you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)", "Experiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian on the angle?", "Suggestions", "Paragraph 2 of section 3 seems like the key to the whole paper -- I would make it more prominent.", "I would include a short 'measure theory' appendix or equivalent reference for the lay reader.", "I wonder if the paper's main aim is not actually to bring measure theory to the study of policy gradients, which would be a laudable goal in and of itself.", "ICLR may not in this case be the right venue (nor are the current results substantial enough to justify this) but I do encourage authors to consider this avenue, e.g. in a journal paper.", "= Revised after rebuttal =", "I thank the authors for their response.", "I think this work deserves to be published, in particular because it presents a reasonably straightforward result that others will benefit from.", "However, I do encourage further work to", "1) Provide stronger empirical results (these are not too convincing).", "2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-request_clarification", "arg-request_clarification", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "arg-request_clarification", "arg-request_clarification", "arg-request_explanation", "arg-structuring_heading", "arg-request_edit", "arg-request_edit", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "arg-request_clarification_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-structuring_heading_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 488, "sentences": ["The authors study the generalization error of two-layer neural nets, where an asymptotic point of view is taken.", "Their main results can be summarized as follows.", "1. If only the second layer is optimized, they observe the double-descent phenomenon.", "2. However, if only the first layer is optimized, the double-descent is not observed.", "This shows that recent results for certain linear models (e.g. Song, Montanari 2019) do not directly transfer to neural networks.", "As the authors point out, however, if a different scaling is used in the asymptotics, double descent might still be observed.", "I see the following strengths of the paper.", "-This is a very well-written paper with a clear message.", "-The result is important and gives new insights into the generalization properties of neural networks.", "In my view, this is an interesting contribution, which should be accepted.", "---------", "Thank you for your response. I will leave the rating unchanged."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 489, "sentences": ["The authors proposed a Modulated Variational auto-Encoders (MoVE) to perform musical timbre transfer.", "The authors define timbre transfer as applying parts of the auditory properties of a musical instrument onto another.", "It replaces the usual adversarial translation criterion by a Maximum Mean Discrepancy (MMD) objective.", "By further conditioning our system on several different instruments, the proposed method can generalize to many-to-many transfer within a single variational architecture able to perform multi-domain transfers.", "Some detailed comments are listed as follow,", "1 The implementation steps of the proposed method (MoVE) are not clear.", "Some details are missing, which is hardly reproduced by the other researchers.", "2 The experimental settings are not reasonable.", "The current experimental settings are not matched with the practice environment.", "3 The proposed method can transfer the positive knowledge.", "However, some negative knowledge information can be also transferred.", "So how to avoid the negative transferring?", "4 For the model, the optimization details or inferring details are missing, which are important for the proposed model."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 490, "sentences": ["1) Summary", "This paper proposes a method for learning an agent by interacting and probing an expert agents behavior.", "This method is composed of a policy that learns to imitate an expert\u2019s action, and a policy that challenges the expert in order to get it to take multiple possible routes to solve a task.", "The two policies share a \u201cbehavior tracker\u201d that models the expert\u2019s behavior, and communicates it to both policies being learned.", "The probing policy is optimized using a curiosity-driven reward in order to get the expert take trajectories the probing policy has not seen before.", "In experiments, the authors perform experiments to show how the learned agent can generalize to unseen configurations in the corresponding environments in which the agents were trained, and also use the proposed technique in a sorting task in which the method generalizes to longer arrays to be sorted.", "2) Pros:", "+ Neat idea for exploring an experts behavior by changing the environment surrounding it (probing it).", "+ Cool experiments for applicability.", "+ Well written paper and easy to understand.", "3 Comments:", "- Equation 1 typo?:", "To my understanding, in curiosity driven exploration, the exploration is driven based on how well the next state can be predicted by the agent.", "In equation 1, different time steps are being compared, m^t and m^{t-1}, but the comparison should be between the predicted time step t and real time step t. Can the authors clarify why different time steps are compared in the equation?", "- Baseline missing: Random actions from expert", "A simple baseline to compare against could be to simply force the expert to take a few random actions during its trajectory and let the imitator learn from these.", "Comparing against this baseline could serve as evidence that we need to actually learn the probing agent to acquire a more optimal policy.", "- Baseline missing: Simple RNN policies that communicate hidden states.", "Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.", "While optimizing the curiosity reward the hidden states could be used as well.", "If successful, this baseline can show that we actually need to model the \u201cbehavior\u201d with a separate network.", "- Ablation study for the importance of fusion:", "The authors have a \u201cfusion\u201d layer within the imitator and probing policies.", "An ablation study showing that this layer is actually necessary is missing from the paper.", "- Generalizability argument", "The authors claim that they show a single starting configuration for the agents during training, and different starting configurations during testing.", "While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.", "It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.", "A more drastic change of the environment could make for a stronger argument.", "4) Conclusion:", "Overall, I like the idea of having a policy that tries to figure out the general behavior of a demonstrator by probing it.", "Having said that, I feel this paper needs to improve in the aspects mentioned above.", "If the authors present more convincing evidence that successfully address the comments above, I am willing to increase my score."], "labels": ["arg-structuring_heading", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "none", "arg-request_clarification", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 491, "sentences": ["I want thank the authors for solving this long-standing GAN challenge in raw waveform synthesis.", "With all due respect, previous GAN trials for audio synthesis are inspiring, but their audio qualities are far away from the state-of-the-art results.", "Although the speech fidelity of GAN-TTS is still worse than WaveNet and Parallel WaveNet from the posted sample, it has begun to close the significant performance gap that has existed between autoregressive models and GANs for raw audios.", "Overall, this is a very good paper with significant contributions to the filed.", "Detailed comment:", "1, In WaveNet, the conditional features (linguistic / mel-spectrogram) are added as bias terms in the convolutional layers.", "Did the authors tried this alternative architecture for the generator, which uses the white noisy z as network input (similar as flow-based models, e.g., Parallel WaveNet) and the conditional features as bias term in the convolutional layers?", "2, Could the authors comment the importance of serval architecture choices in this work?", "From Table 1, it seems to me that the ensemble of random window discriminators is the most important (perhaps the only important) contributing factor for the success.", "For example, the MOS score was boosted from 1.889 to 4.213 by replacing a single full discriminator to the ensemble of RWDs.", "3, The notations in Eq. (1) and (2) are messy.", "Although I can figure their meaning from the context, one may clarify certain notations if they appear at the first time.", "4, The stable training (NO model collapses) is pretty impressive.", "Could the authors shed some light on the potential reason? Does the ensemble of RWD regularizes the training?", "What's your experience for training FullD (does not have random window ) and cRWD_1 (only has one random window discriminator) ?", "Are they still very stable?", "Also, could the authors comment on the importance of large batch size -- 1024 for stable training of GAN-TTS?", "5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).", "Yamamoto et al. Probability Density Distillation with Generative Adversarial Networks for High-Quality Parallel Waveform Generation. 2019.", "=== update ===", "Thank you for the detailed response.", "2,  Thanks for the elaboration.", "4,  It would be very interesting to see an analysis of model stability with smaller batch sizes."], "labels": ["none", "none", "none", "none", "arg-structuring_heading", "none", "arg-request_clarification", "arg-request_explanation", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_edit", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_summary_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_clarification_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 492, "sentences": ["The privacy definition employed in this work is problematic.", "The authors claim that \"Privacy can be quantified by the difficulty of reconstructing raw data via a generative model\".", "This is not justified sufficiently.", "Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.", "The proposed method is not appropriately compared with the other methods in experiments.", "In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct.", "At the specific point that the proposed method is evaluated with MNIST and Sound, it achieves better utility and better \"privacy\".", "However, the Pareto front of the proposed method is concentrated on a specific point.", "For example, the proposed method does not achieve high \"privacy\" as \"noisy\" does.", "In this sense, the proposed method is not comparable with \"noisy\".", "In my understanding, this concentration occurs because the range of \\lambda is inappropriately set.", "This kind of regularization parameter should be exponentially varied so that the privacy-utility Pareto front covers a wide range. --", "Minor:", "In Eq. 1, the utility is evaluated as the probability Yi=Yi'.", "What randomness is considered in this probability?", "In Eq 2, privacy is defined as maxmin of |Ii - Ii'|.", "Do you mean privacy guaranteed by the proposed method is different for each data? This should be defined as expectation over T or max over T.", "In page 4. \"The reason we choose this specific architecture is that an exactly reversed mode is intuitively the mode powerful adversarial against the Encoder.\" I could not find any justification for this setting. Why \"exactly reversed mode\" can be the most powerful adversary? What is an exactly reversed mode?", "Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.", "The resulting model would thus be highly affected by the setting of n and k.", "How can you choose k and n?"], "labels": ["none", "none", "none", "arg-request_explanation", "none", "none", "none", "none", "none", "none", "none", "arg-request_edit", "arg-structuring_heading", "none", "arg-request_clarification", "none", "arg-request_clarification", "arg-request_explanation", "none", "none", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 493, "sentences": ["Summary:", "This paper looks at the MARL problem in high-dimensional continuous control settings.", "To improve learning in this multi-agent setting, they propose to pre-train a lower-level policy that takes as input foot-step goals and is executed for a fixed number of timestep, thereby simplifying both the learning and exploration.", "I'm a bit unsure of how to evaluate this paper.", "On the one hand, I believe it has several contributions:", "- Proposing a new MARL - continuous control environment", "- Proposing a new lower-level policy for high-demensional continuous control environments, including how to learn it", "- Using it to perform MARL in this environment", "On the other hand, it is hard to say what the _main_ contribution is, which in turn makes it difficult to evaluate whether the experimental evaluation is sufficient:", "Clearly, a main part of the paper is the work done to construct the hierarchical setup, including goal space, observation space and reward functions.", "However, this work, as far as I can tell, is separate from the MARL problem.", "Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.", "On the other hand, there is the application of the hierarchical setup to the MARL problem.", "However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.", "Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.", "I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:", "- Why does temporal correlation reduce the non-stationarity of the MARL problem?", "- Why does structured exploration reduce the number of network parameters that need to be learned?", "- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?", "In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.", "Edit:", "Thank you for your response.", "Unfortunately, I don't feel like it sufficiently addresses my questions and concerns.", "I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.", "Regarding my questions: I understand where the temporal correlation is coming from in an HRL setting.", "However, what was not clear to me is how this reduces the non-stationarity of MARL.", "I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.", "And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.", "I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.", "I want to re-iterate that I think that the submitted work by the authors is impressive and can provide valuable insights, but I believe it requires more work and more relevant baselines."], "labels": ["arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation", "arg-structuring_heading", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 494, "sentences": ["The authors extend an existing approach to adaptive softmax classifiers used for the output component of neural language models into the input component, once again allowing tying between the embedding and softmax.", "This fills a significant gap in the language modeling architecture space, and the perplexity results bear out the advantages of combining adaptively-sized representations with weight tying.", "While the advance is in some sense fairly incremental, the centrality of unsupervised language modeling to modern deep NLP (ELMo, BERT, etc.) implies that perplexity improvements as large as this one may have meaningful downstream effects on performance on other tasks.", "Some things I noticed:", "- One comparison that I believe is missing (I could be misreading the tables) is comparing directly to Merity et al.'s approach (adaptive softmax but fixed embedding/softmax dimension among the bands). Presumably you're faster, but is there a perplexity trade-off?", "- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.", "- The loss by frequency-bin plots are really fantastic.", "You could also try a scatterplot of log freq vs. average loss by individual word/BPE token.", "- Do you have thoughts as to why full-softmax BPE is worse than adaptive softmax word level?", "That goes against the current (industry) conventional wisdom in machine translation and large-scale language modeling that BPE is solidly better than word-level approaches because it tackles the softmax bottleneck while also sharing morphological information between words."], "labels": ["arg-structuring_summary", "none", "none", "arg-structuring_heading", "arg-request_experiment", "arg-request_edit", "none", "arg-request_experiment", "arg-request_explanation", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "arg-structuring_heading_label", "arg-request_explanation_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "arg-request_experiment_label"]}
{"abstract_id": 495, "sentences": ["Gradient-free evolutionary search methods for Reinforcement Learning are typically very stable, but scale poorly with the number of parameters when optimizing highly-parametrized policies (e.g. neural networks).", "Meanwhile, gradient-based deep RL methods, such as DDPG are often sample efficient, particularly in the off-policy setting when, unlike evolutionary search methods, they can continue to use previous experience to estimate values.", "However, these approaches can also be unstable.", "This work combines the well-known CEM search with TD3 (an improved variant of DDPG).", "The key idea of of this work is in each generation of CEM, 1/2 the individuals are improved using TD3 (i.e. the RL gradient).", "This method is made more practical by using a replay buffer so experience from previous generations is used for the TD3 updates and importance sampling is used to improve the efficiency of CEM.", "This work shows, on some simple control tasks, that this method appears to result in much stronger performance compared with CEM, and small improvements over TD3 alone.", "It also typically out-performs ERL.", "Intuitively, it seems like it may be possible to construct counter-examples where the gradient updates will prevent convergence.", "Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).", "The justification that the method of Khadka & Tumer (2018) cannot be extended to use CEM, since the RL policies do not comply with the covariance matrix is unclear to me.", "Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.", "In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.", "It sees like the more important distinction is that, in this approach, the information flows both from ES to RL and vice-versa, rather than just from RL to ES.", "One view of this method would be that it is an ensemble method for learning the policy [e.g. similar to Osband et al., 2016 for DQN].", "This could be discussed and a relevant control would be to keep a population (ensemble) of policies, but only update using RL while sharing experience across all actors.", "This would isolate the ensemble effect from the evolutionary search.", "Minor issues:", "- The ReLU non-linearity in DDPG and TD3 prior work is replaced with tanh.", "This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.", "- The paper is over the hard page limit for ICLR so needs to be edit to reduce the length.", "Osband I, Blundell C, Pritzel A, Van Roy B. Deep exploration via bootstrapped DQN. InAdvances in neural information processing systems 2016 (pp. 4026-4034)."], "labels": ["none", "none", "none", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "arg-request_experiment", "none", "arg-structuring_heading", "none", "arg-request_edit", "arg-request_edit", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "arg-structuring_summary_label", "none_label", "none_label", "arg-structuring_quote_label", "arg-request_experiment_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_experiment_label", "arg-request_edit_label", "none_label"]}
{"abstract_id": 496, "sentences": ["This paper provides an approach to use visual information to improve text only neural machine translation systems.", "The approach creates a \"topic word to images\" map using an existing image aligned translation corpora.", "Given a source sentence, the model extracts relevant images, extracts their Resnet features and fuses them with the features generated from the word sequence.", "The decoder uses these fused representation to generate the target sentence.", "Overall, I like the approach, seems like it can be easily augmented to existing NMT systems.", "One of the claims of the paper was to be able to use monolingual image aligned data.", "However image captioning datasets are not mentioned.", "It would make sense to use image captioning data to create the image lookup.", "Also, what will be the performance of a standard image captioning system on the task ?", "I believe it will not be great, but I think for completeness, you should add such a baseline.", "Minor comments:", "1. What is M in Algorithm 1 ?", "2. First paragraph in related work is very unrelated to the current subject, please remove."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_experiment", "arg-request_experiment", "arg-request_experiment", "arg-structuring_heading", "arg-request_explanation", "arg-request_edit"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-request_experiment_label", "arg-request_explanation_label", "none_label", "arg-structuring_heading_label", "arg-request_clarification_label", "none_label"]}
{"abstract_id": 497, "sentences": ["This paper combines the global and local stability prediction and tries to get interpretable results using the stethoscope design, which is actually a weighted subbranch for the main branch.", "There are several concerns regarding the proposed framework.", "1) How to choose \\lambda?", "A better design could be a learnable \\lambda.", "Instead of just one scalar value, it could be better to learn a map of \\lambdas, which indicates the distribution of local stability and how it is related to global stability.", "The visualization of the \\lambda map might be more interpretable for understanding the stability prediction.", "2) The global stability prediction does not have a consistent correlation with the local stability prediction, as shown by the easy and hard examples.", "This complex relationship will confuse the network during the training.", "That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.", "This is hard to provide a meaningful interpretation of the task."], "labels": ["arg-structuring_summary", "arg-structuring_heading", "arg-request_clarification", "none", "arg-request_experiment", "arg-request_experiment", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "arg-request_explanation_label", "arg-request_experiment_label", "arg-request_experiment_label", "arg-request_experiment_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 498, "sentences": ["This paper suggests a quantization approach for neural networks, based on the Product Quantization (PQ) algorithm which has been successful in quantization for similarity search.", "The basic idea is to quantize the weights of a neuron/single layer with a variant of PQ, which is modified to optimize the quantization error of inner products of sample inputs with the weights, rather than the weights themselves.", "This is cast as a weighted variant of k-means.", "The inner product is more directly related to the network output (though still does not account for non-linear neuron activations) and thus is expected to yield better downstream performance, and only requires introducing unlabeled input samples into the quantization process.", "This approach is built into a pipeline that gradually quantizes the entire network.", "Overall, I support the paper and recommend acceptance.", "PQ is known to be successful for quantization in other contexts, and the specialization suggested here for neural networks is natural and well-motivated.", "The method can be expected to perform well empirically, which the experiments verify, and to have potential impact.", "Questions:", "1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.", "Does it pose a difficulty? How does it compare to other methods?", "2. Can you elaborate on the issue of non-linearity? It is mentioned only briefly in the conclusion. What is the difficulty in incorporating it? Is it in solving equation (4)? And perhaps, how do you expect it to effect the results?"], "labels": ["arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-request_explanation", "arg-request_explanation", "arg-request_explanation"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "arg-request_explanation_label"]}
{"abstract_id": 499, "sentences": ["The paper considers 'replica exchange' Langevin dynamics.", "These methods are very popular among practitioners, and developing some theory backing the empirical successes is an important goal.", "Unfortunately this paper offers only weak results.", "- The first 6 pages set up the general formalism. This is textbook material adapted to the current problem.", "- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.", "- Page 8 gives a Poincare inequality.", "Again, this follows from known results.", "More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.", "- Similar comments hold for the following pages.", "They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
{"abstract_id": 500, "sentences": ["This paper studies the problem of learning from multiple tasks and additional noisy data.", "The proposed representation learning method first assigns each noisy data a relevance score using the topological information.", "Then the authors propose to minimize a combination of the loss of a class-prototype learning loss and a cosine classifier learning loss to learn a good representation generator g_theta.", "The empirical study validates the effectiveness of the proposed method.", "I have the following comments,", "1. The studied problem that learning from few-shot data and large-scale noisy data is interesting. According to the experimental results, the proposed method seems to be promising.", "2. The learning procedure is confusing.", "It is highly recommended to provide the pseudocode of the proposed method.", "3. Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_clarification"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-request_explanation_label"]}
{"abstract_id": 501, "sentences": ["The paper presents a method of learning representations that is based on minimizing \"deficiency\" rather than optimizing for information sufficiency.", "While perfect optimization of the sufficiency term in IB is equivalent to minimizing deficiency, the thesis of the paper is that the variational upper bound on deficiency is easier to optimize, and when optimized produces better (more compressed representations), while performing equally on test accuracy.", "The paper is well written and easy to read.", "The idea behind the paper (optimizing for minimizing deficiency instead of sufficiency in IB) is interesting, especially because the variational formulation of DB is a generalization of VIB (in that VIB reduces to VDB for M=1).", "What takes away from the paper is that while perfect optimization of IB/sufficiency is equivalent to perfect optimization of DB, it is not clear what happens when perfection is not achieved.", "Further, the authors claim that DB is able to obtain more compressed representations (But is the goal a compressed representation, or an informative one?).", "The paper would also benefit from evaluation of the representation itself, and comparison to other non-information bottleneck based algorithms."], "labels": ["arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "arg-request_clarification", "arg-request_experiment"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "none_label", "arg-request_experiment_label"]}
{"abstract_id": 502, "sentences": ["Quality/clarity:", "- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.", "Starting with S and i: I guess S and i are both simply varying-length sequences in U.", "- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).", "Originality/Significance:", "I have certainly never seen a ML-based paper on this topic.", "The idea of 'learning' prior information about the heavy hitters seems original.", "Pros:", "It seems like a creative and interesting place to use machine learning.", "the plots in Figure 5.2 seem promising.", "Cons:", "- The formalization in Paragraph 3 of the Intro is not very formal. I guess S and i are both simply varying-length sequences in U.", "- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).", "-In describing Eqn 3 there are some weird remarks, e.g. \"N is the sum of all frequencies\". Do you mean that N is the total number of available frequencies? i.e. should it be |D|? It's not clear to me that the sum of frequencies would be bounded if D is not discrete.", "- Your F and \\tilde{f} are introduced as infinite series. Maybe they should be {f1, f2,..., fN}, i.e. N queries, each of which you are trying to be estimate.", "- In general, you have to introduce the notation much more carefully. Your audience should not be expected to be experts in hashing for this venue! !", "'C[1,...,B]' is informal abusive notation.", "You should clearly state using both mathematical notation AND using sentences what each symbol means.", "My understanding is that that h:U->b, is a function from universe U to natural number b, where b is an element from the discrete set {1,...,B}, to be used as an index for vector C. The algorithm maintains this vector C\\in N^B (ie C is a B-length vector of natural numbers). In other words, h is mapping a varying-length sequence from U to an *index* of the vector C (a.k.a: a bin). Thus C[b] denotes the b-th element/bin of C, and C[h(i)] denotes the h(i)-th element.", "- Still it is unclear where 'fj' comes from. You need to state in words eg \"C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \\in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.\"", "- What I don't understand is how fj is dependent on h. When you say \"at the end of the stream\", you mean that given S, we are analyzing the frequency of a series of sequences {i_1,...,i_N}? - Sorry, it's just confusing and I didn't really understand \"Single Hash Function\" from Sec 3.2 until I started typing this out.", "- The term \"sketch\" is used in Algorithm1, like 10, before 'sketch' is defined!!", "-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).", "Conclusion:", "Honestly, this paper is very difficult to follow.", "However to sum up the idea: you want to use deep learning techniques to learn some prior on the hash-estimation problem, in the form of a heavy-hitter oracle.", "It seems interesting and shows promising results, but the presentation has to be cleaned up for publication in a top ML venue.", "******", "Update after response:", "The authors have provided improvements to the introduction of the problem setting, satisfying most of my complaints from before. I am raising my score accordingly, since the paper does present some novel results."], "labels": ["arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_clarification", "none", "none", "none", "none", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "arg-structuring_heading", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "none_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label"]}
{"abstract_id": 503, "sentences": ["The paper explores how the architecture, smoothness of the decision boundary and test accuracy of a model impacts the transferability of examples produced from it.", "The paper provides a couple of novel insights, such as the asymmetry when transferring adversarial examples from one model to another.", "In addition, a novel method is proposed to enhance the transferability of adversarial examples from any model, through using smoothed gradients.", "The experiments seem to show that the effect is rather large, and also makes the examples more robust to other transformations such as JPEG compression.", "Overall, these are interesting insights that could lead to further developments in making models more robust to adversarial examples.", "In particular, deriving adversarial examples that are both transferable and resilient to certain usual image transformations shows that the scope of the issue with adversarial examples may be even greater than what is understood today.", "The paper is rather clear.", "Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.", "Some examples (there are way too many to report them all):", "- \"Transfer-based attackS ... since they ...*", "- \"of adversarial exampleS ...\"", "- \"from model A can transfer to model B\"", "- \"less transferable than *those from* a shallow model\"?", "- \"investigations, We \": don't capitalize", "- \"the averaging *has* a smoothing effect\"", "- \"our motivation are\"", "- \"contributed it to\"", "- \"available *to the* adversary\"", "- \"crafting adversarial perturbationS\"", "- \"directly evaluation\"", "- \"be fixed 100\"", "Pros:", "- Transferability and robustness of adversarial examples is a very important problem", "- Interesting insights, esp. the construction and evaluation of examples that are more resilient to certain image transformations", "- Experimental results are convincing", "Cons:", "- Contribution overall may be a bit limited", "- Grammatical errors and odd formulations all over the place"], "labels": ["arg-structuring_summary", "arg-structuring_summary", "arg-structuring_summary", "none", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-request_typo", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "arg-structuring_summary_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_typo_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-request_edit_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label"]}
{"abstract_id": 504, "sentences": ["Pros:", "- The finding that SVHN has larger likelihood than CIFAR according to networks is interesting.", "- The empirical and theoretical analyses are clear, seem thorough, and make sense.", "- Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian).", "Cons:", "- The premises of the analyses are not very convincing, limiting the significance of the paper.", "- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.", "In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.", "- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images.", "At least this limitation should be pointed out in the paper.", "- Some parts of the paper feel long-winded and aimless.", "[Quality]", "See above pros and cons.", "A few less important disagreement I have with the paper:", "- I don't think Glow necessarily is encouraged to increase sensitivity to perturbations.", "The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.", "- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.", "[Clarity]", "In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.", "Section 2 background takes too much space.", "Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.", "Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.", "A few editorial issues:", "- On page 4 footnote 2, as far as I know the paper did not define BPD.", "- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.", "[Originality]", "I am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel.", "However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:", "V\u00edt \u0160kv\u00e1ra et al. Are generative deep models for novelty detection truly better?", "^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.", "A part of the paper's contribution (section 5 conclusion) seem to overlap with others' work.", "The section concludes that if the second dataset has small variances, it will get higher likelihood.", "But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).", "[Significance]", "The paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly.", "However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.", "According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that \"lies within\" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?", "Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much."], "labels": ["arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-request_edit", "none", "arg-structuring_heading", "arg-structuring_heading", "arg-structuring_heading", "none", "none", "none", "arg-structuring_heading", "none", "none", "none", "none", "arg-structuring_heading", "arg-request_edit", "arg-request_typo", "arg-structuring_heading", "none", "arg-request_edit", "none", "none", "none", "none", "none", "arg-structuring_heading", "none", "none", "arg-request_experiment", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "none_label", "arg-structuring_heading_label", "none_label", "none_label", "arg-request_explanation_label", "none_label"]}
{"abstract_id": 505, "sentences": ["I vote to reject the paper at this stage, mainly because of the following three points:", "1) The motivation is unclear and overall structure of the paper is confusing.", "It should be better motivated why one should use the duality gap as an upper bound for the \"F-distance\".", "Minimizing the F-distance as is usually done seems like the more direct and simple approach.", "Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial.", "2) The presentation is not professional, hard to follow and the submission overall looks very rushed:", "- In equations, please use \\inf, \\sup, and \\text{...} for text such as distance, data, ...", "- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).", "What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.", "- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.", "- The writing looks very rushed, and should be improved.", "For example, I have trouble understanding the sentence \"So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets.\" in the introduction.", "- The aspect ratio in Fig. 5 should be fixed.", "3) The experiments are completely preliminary and not reasonable:", "- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).", "There are countless open pytorch implementations on GitHub which out-of-the-box produce much better results.", "- The shown inception scores are far from state-of-the-art.", "It is unclear, why one should use the proposed duality gap GAN."], "labels": ["none", "none", "none", "none", "none", "none", "arg-request_edit", "none", "arg-request_explanation", "none", "none", "arg-request_explanation", "arg-request_edit", "none", "none", "none", "none", "none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], "pred_labels_truncated": false, "pred_labels": ["arg-structuring_heading_label", "none_label", "arg-request_explanation_label", "none_label", "none_label", "none_label", "arg-request_typo_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "arg-request_edit_label", "none_label", "none_label", "none_label", "none_label", "none_label"]}
