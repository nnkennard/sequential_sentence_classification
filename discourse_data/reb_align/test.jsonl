{"abstract_id": 339, "sentences": ["We thank reviewer3 for the comprehensive review.", "We would like to address each of the reviewer's concerns individually. (This will include some discussions above only seen by authors, reviewers, area chairs and program chairs.)", "# Is the approach really limited?", "It is true that that the transformation that removes the desired information must be known before hand which is the main assumption in the paper.", "Other conceivable transformations are (i) removal of colour information by converting image to grey scale, (ii) removal of orientation information with random rotation and positional shift, (iii) removal of temporal correlation using shuffling in a time-series data and etc.", "We hope that that this paper could ignite a discussion around what transformation can be created to impose prior knowledge into the model.", "These prior transformation could be something that we observed in biology, for example, we observe global-local information disentanglement in our perception.", "Is there other hard-coded disentanglement in biology?", "This is rather an interesting problem in our opinion.", "Can this method learn more factor than just two?", "It is conceivable that there could be more than one information of interests that get destroyed in a transformation.", "For example, one latent factor could model middle-range correlations if the transformation remove long-range correlations through shuffling process and short-range correlations get destroyed through a blurring process (e.g. local smoothing transformation).", "Another two factors could represent long-rang and short-range correlations.", "What if the desired factors are not clearly disjoint and collectively exhaustive?", "This is an interesting question.", "We do not think that our current approach can disentangle continuous features.", "In a future work, there could be an auxiliary task method that can create continuous latent variables.", "We hope that this paper create interesting open problems for future research.", "# More ablations or experiments with comparable settings would be desirable.", "In our experiments, we found that the disentanglement of global and local information is very robust to different values of beta.", "In experiment 1.2 we use beta=1.0 which is the same as using the original VAE objective.", "However, beta does affect the quality of the generative samples (blurriness).", "For experiment 1.1, different beta produce similar disentanglement results, we use beta=20 to produce the figures as it created nicest looking samples.", "We uses beta=40 for all clustering experiments which had been searched from beta=\\{1, 10, 20, 30, 40, 50, 60\\} for the best digit identity clustering results.", "Thanks to reviewer3, we incorporated this information into the revision.", "Regarding the clustering result, we believe that the resulting accuracy number cannot be used to compare the quality of the clustering methods.", "We observe that the global structure contains more information than just digit identity.", "It also contains information such as whether or not there are distracting digits in the image.", "We are not concern with improving upon baseline but rather to confirm that our method can disentangle global-local information and the further analysis have shown that the grouping corresponds to more than just the digit identity.", "The use of 30 clusters helps us identify the grouping of other types of global information in addition to the digit identity.", "Therefore, the identity clustering performance does not directly translate into the ability to disentangle local and global variables.", "# Related work", "We would like to thank reviewer 3 for suggesting the related works that we have missed. These were incorporated in the revision.", "As discussed in the comments above (visible only to authors and area chairs), there is an overhead regarding grouping of data into batches in DC-IGN approach.", "We agree that DC-IGN could potentially perform the same task as our model or more.", "However, our method can reduce the effort of needing to group the data or use labelled data by instead thinking more about prior knowledge (transformation function) of the entire dataset.", "The contributions of this paper are", "(i) Suggest that there is another method of imposing prior knowledge into algorithmic design of the latent variable model.", "We believe this can be categorised as a self-supervised learning approach (a kind of unsupervised learning) which have not been explored much in the context of the latent variable model.", "(ii) Show that it can be used to disentangle global and local information through experiments."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 340, "sentences": ["Thank you for your careful reading and comments.", "Q1: Missing assumptions about the black-box calibration approaches", "We thank R1 and R2 for endorsing the merit of our proposed black-box calibration.", "The black-box calibration assumes no read/write to model weights or availability training data, but access to the sampling of random seed.", "The black-box calibration is useful for both model user and API owner.", "Model owner: We suppose that the dense mode happens to be close to a specific training image, thus violating privacy.", "The model owner would like to calibrate the model to alleviate the mode collapse.", "In such a situation, training data may no longer be accessible since it contains private information, e.g. human faces or person images.", "Retraining consumes much time and energy, especially for complex models trained on a huge dataset.", "Besides, we empirically validate that the dense mode is not caused by imbalanced data or randomness during initialization/optimization.", "So retraining won't work for dense-mode alleviation.", "Our proposed black-box calibration has an advantage over retraining with minimum time and energy cost and no touching training data.", "Moreover, the calibration can target any dense mode for alleviation.", "API owner: For enterprise users having access to the face image generation service via cloud API, they are given the ping service for a huge number of times or not even restricted.", "Black-box calibration enables the API owner to customize the model's sampling process to meet the users' needs.", "Q2: Missing key experiments that will provide more motivation that 1. face identity can be used as a proxy for face image diversity; 2. applying our proposed metric to the training datasets should show no gap between $\\mathcal{R}_{obs}$ and $\\mathcal{R}_{ref}$:", "1. Face identity as a proxy for face image diversity", "We would like to clarify that we are not using the identity label as a proxy.", "Instead, we are using the embedding features obtained from the neural network trained on the face recognition task.", "We claim that the embedding features have rich semantics of all kinds of facial attributes, e.g. age, gender, race and so on.", "The rich semantics of the face embedding feature can be validated by its strong transferability on other visual tasks, e.g. gender/race classification and age regression.", "Prior studies [Savchenko, Andrey V, \"Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output ConvNet\" (2019)] have shown that transfer learning using neural networks pretrained on face recognition can produce highly effective results for gender recognition and age estimation.", "2. Applying our metric on the training set of FFHQ", "FFHQ is a public face dataset contains $56,138$ images, without repeating identities.", "We first randomly pick $1k$ images to form the S set and sort the S set according to the number of neighbors within distance 0.3.", "We choose the sample at percentile $0.01\\%, 0.1\\%, 1\\%, 10\\%, 20\\%, 30\\%, 40\\%, 50\\%, 60\\%, 70\\%, 80\\%, 90\\%$. We conduct the neighboring analysis on these selected samples.", "We still observe a gap between $\\mathcal{R}_{obs}$ and $\\mathcal{R}_{ref}$, which demonstrates that FFHQ dataset has dense mode, even without repeating identities.", "Furthermore, we would like to clarify that our metric is proposed to measure the collapse of GAN's learned distribution.", "We have empirically shown in the paper that the mode collapse still occurs despite balanced training data.", "You can check the details in the appendix of the paper, the paragraph of \"Applying Our Proposed Metric on FFHQ\".", "Q3: Minor improvements", "1. Proof or citation for the flaws of FID", "There is a recently published survey paper that can back our claim. It is [Ali Borji, \"Pros and Cons of GAN Evaluation Measures\" (Arxiv 18)]", "2. The contradiction between the two statements", "We use the word \"loss of diversity\" since IS's measuring of diversity is limited.", "E.g., on ImageNet with 1000 classes, it can not rule out the case when then generator simply repeating the same image for each different class.", "3. We take your advice and will address this piece of work as a \"pilot study\" in the final version."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 341, "sentences": ["We thank the comments with cares and insights, which are helpful for improving the quality and readability of our paper.", "We are glad that you support our paper.", "We have addressed all the comments as follows:", "Response #1: In the revision, we had added a new experiment to zoom in on two categories for clearer utility visualization.", "In particular, we show the DNN\u2019s deep features and RAN\u2019s Encoder output to illustrate how they push the features to cluster with the \u201ccar with/without road\u201d & \u201csailboat with/without water\u201d images in the feature space.", "Response #2: We agree that we should provide more details about the decoders.", "Generally, we set the Decoder to mirror the Encoder's architecture.", "That is, we assume a powerful adversary that knows the Encoder in training.", "Because the Encoders are different for different tasks, the Encoders are different too.", "In particular, we select the architecture of Encoder plus Classifier to be LeNet for MNIST, Ubisound and Har, to be AlexNet for CIFAR-10, and to be VGG-16 for ImageNet.", "The architectures of Encoder in four cases are different, so the Decoder is varied as well.", "In the revision, we have added above explanations about Decoder in Section 2.3 and in experiment settings of Section 3.", "Response #3: We agree that the description of three baselines should be more precise, especially the DNN and DNN(resized) baseline.", "In the revision, we have added explanations on the difference/similarity between DNN (resized) and DNN baselines. And explain why we include them as baselines to compare RAN against in Section 3.1.", "Response #4: We have added more explanations in Section 3.1 about how \u201cthe proposed algorithm works as an implicit regularization leading to better classification accuracy than the original model which completely ignores privacy\u201d.", "As shown in Figure 3, the utility of RAN\u2019s Encoder output is higher than that of DNN.", "Here the DNN model stands for the non-private feature extractor followed by a non-private classifier.", "Response #5: We agree that it is necessary to conduct experiments to compare RAN\u2019s performance concerning privacy and accuracy with/without a different kind of layers so that we can back up the argument mentioned in Section 2.2.", "On the one hand, we have already conducted exhaustive micro-benchmark experiments to determine the current design of RAN.", "For example, we select different model architectures (layers and building blocks), weight updating schemes of different parts (when and how to update Encoder, Decoder and Classifier) and settings of some important hyper-parameters (the setup of \u201cn\u201d epochs and \u201ck\u201d steps, learning rate) to select the empirically optimized one.", "However, we only present the most important results in this paper due to the space limit.", "On the other hand, for all the arguments in Section 2.2, we have added the citation to support them.", "Response #6: We agree that it is important to justify how the reconstruction error works as a measure of privacy in this paper.", "In the revision, we have added the following explanation and justification on privacy quantification in Section 1, Section 2, Section 4 and Section 5.", "First, there is no single standard definition of data privacy-preserving and corresponding adversary attacks.", "And a fundamental problem is the natural privacy-utility tradeoff which is affected by different data privacy-preserving methods.", "We note that our principal contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attacker and privacy quantification.", "Second, finding the right measurement for privacy is an open problem in itself.", "To evaluate RAN, one has to pick some quantifications.", "In the present paper, we chose the \u201creconstructive error\u201d because it is the most intuitive one to measure the risk of original data disclosure given perturbed data (Encoder output).", "Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a defined application.", "For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN\u2019s Encoder output, and the sensitive patterns founded from the raw data, in an object recognition task.", "Response #7: Thanks for pointing out the citation problem in Section 3.1.", "In the revision, we have added explanation and cited more articles about several attacks for how the raw data can cause privacy risks in Section 1.", "For example, underlying correlation detection, re-identification and other malicious mining.", "As for the \u201cNoisy Data\u201d method, we have added the citation on differential privacy in Section 3.1.", "Response #8: We have re-plotted Figure 3 and Figure 4 to improve the readability."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 342, "sentences": ["We thank Reviewer 1 for their insightful questions and suggestions.", "We agree that Product Quantization (PQ) is key to get \u201cimpressive compression ratio\u201d while maintaining competitive accuracy, provided that there is some special structure and redundancy in the weights and the way we quantize them.", "Which kind of redundancy does our method capture?", "As rightfully stated by Reviewer 1, choosing which elementary blocks to quantize in the weight matrices is crucial for the success of the method (what the Reviewer calls \u201chorizontal/vertical/other\u201d correlation)", ".", "In what follows, let us focus on the case of convolutional weights (of size C_out x C_in x K x K).", "As we state in our paper: \u201cThere are many ways to split a 4D matrix in a set of vectors and we are aiming for one that maximizes the correlation between the vectors since vector quantization-based methods work the best when the vectors are highly correlated", "\u201d", ".", "We build on previous work that have documented the *spatial redundancy* in the convolutional filters [1], hence we use blocks of size K x K. Therefore, we rely on the particular nature of convolutional filters to exploit their spatial redundancy.", "We have tried other ways to split the 4D weights into a set of vectors to in preliminary experiments, but none was on par with the proposed choice.", "We agree with Reviewer 1 that the method would probably not yield as good a performance for arbitrary matrices.", "Using row permutations to improve the compressibility?", "This is a very good remark.", "Indeed, redundancy can be artificially created by finding the *right* permutation of rows (when we quantize using column blocks for a 2D matrix).", "Yet in our preliminary experiments, we observed that PQ performs systematically worse both in terms of reconstruction error and accuracy of the network that when applying a random permutation to a convolutional filter.", "This confirms that our method captures the spatial redundancy of the convolutional filters as stated in the first point.", "[1] Exploiting linear structure within convolutional networks for efficient evaluation, Denton et al."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 343, "sentences": ["We are grateful to the reviewer for their time and effort in reading our paper and providing feedback.", "Generative model assumptions: our model is an expansion of the original RAND-WALK model of Arora et. al., with the purpose of accounting for syntactic dependencies.", "The additional assumptions we include and the concentration phenomena we prove theoretically are verified empirically in section 5, so our results do hold up on real data.", "Use on downstream tasks: we believe that capturing syntactic relationships using a tensor can be useful for some downstream tasks, since our results in the paper suggest that it captures additional information above and beyond the standard additive composition.", "However, as the main goal of this paper is to introduce and analyze the model, we defer more application-focused analysis to future work.", "Interaction between arbitrary word pairs: our model introduces the tensor in order to capture syntactic relationships between pairs of words, such as adjective-noun and verb-object pairs.", "While it might be interesting to try to capture interactions between all pairs of words, that is not justified by our model and we didn't explore it.", "However, we also trained our model using verb-object pairs, and we have updated section 5 as well as the appendix to include these additional results.", "Comparison to Arora, Liang, Ma ICLR 2017: we appreciate the suggestion to include a comparison with the SIF embedding method of Arora et. al., as this method is also obtained from a variant of the original RAND-WALK paper.", "We have updated Table 2 and the discussion in section 5 to include these additional results.", "As reported in their paper, the SIF embeddings yield a strong baseline for sentence embedding tasks, and we find the same to be true in the phrase similarity task for adjective-noun phrases (not so for verb-object phrases).", "However, we find that we can improve upon the SIF performance by addition of the tensor component from our model. (We note that we have just used the tensors trained in our original model; it is possible that combining the model in SIF and syntactic RAND-WALK more carefully could give even better results.)", "Additional citations: we have updated the paper to include both additional citations."], "labels": ["global_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 344, "sentences": ["Hello,", "We would like to thank you for reviewing our paper.", "Also, thank you for your comment about the clarity of the writing, we spent a lot of effort ensuring the paper was easy to read.", "Regarding the suggested ablation,", "This comment was also made in the official blind review #2.", "We also responded to this suggestion in the public comment. For your convenience, we have copied our response here:", "In our model, BERT is more than a source of contextual word embeddings as we fine-tune all of its ~110M parameters during training.", "Simply replacing BERT with distributed embeddings and a character-CNN or LSTM wouldn\u2019t allow us to determine the effect of contextualized embeddings because we would simultaneously be removing the majority of our model\u2019s trainable parameters.", "Nevertheless, we performed the suggested ablation by swapping BERT for GloVe embeddings (300 dimensional) and found that NER performance dropped from 89.46% to 40.33% and RE performance fell from 66.83% to 14.44% on the test set of the ConLL04 corpus (note that we had to increase the learning rate by 10X to get the model to converge).", "If you were to somehow control for this drop in model capacity, say by adding in an LSTM network, the ablated model would closely match this paper [1], whom we outperform by ~3% overall on the CoNLL04 corpus.", "This paper is not cited in Table 1 as they report macro-averaged F1 scores, while most other papers (including the current state-of-the-art [2]) report micro-averaged F1 scores, as we did.", "Finally, it is well known that contextual embeddings outperform distributed embeddings on a wide range of NLP tasks, including NER [3].", "The aim of our study wasn\u2019t to compare contextual vs. distributed embeddings but on how to successfully integrate BERT into a state-of-the-art joint NER and RE architecture.", "Regarding your comments:", "\u201cI think the main source of improvement comes from the BERT representations used as input.\u201d", "\u201c[...] the contributions of this paper are to show that using BERT representations as input [\u2026]\u201d", "We would like to clarify that we are not simply using BERT representations as input.", "We are integrating BERT as part of our model architecture and fine-tuning it along with the task-specific parameters (as stated in the second to last paragraph of the introduction).", "For the particular problem of joint NER and RE, we found this to be critical.", "For example, early on in our experiments we tested using BERT as a feature extractor vs. fine-tuning the entire architecture and found that performance dropped to ~42.82% (from 78.15%) on the CoNLL04 corpus.", "Integrating BERT as part of our model (as opposed to simply using its embeddings as inputs) allowed us to swap recurrent architectures common in joint NER and RE models in favour of simple and shallow task-specific architectures composed of feed forward neural networks.", "This reduced training times while improving performance (see our response to official review #1 for more details).", "Again, thanks for your constructive comments!", "[1] https://link.springer.com/chapter/10.1007/978-3-030-15712-8_47", "[2] https://arxiv.org/abs/1905.05529", "[3] https://arxiv.org/abs/1802.05365"], "labels": ["global_context", "global_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 345, "sentences": ["Thank you for your comments.", "Please refer to the joint response in regards to training stability and mode collapse."], "labels": ["global_context", "single_context"], "confs": [1.0, 1.0]}
{"abstract_id": 346, "sentences": ["We thank the reviewer for their insightful and constructive feedback.", "Re: (W1 & W2) Adversely affected by rotations", ">", "While the CFS is not invariant to rotations, it is a surprising, and empirically noteworthy, finding that all 4 different ways of producing representations consistently encode a dozen tasks very succinctly.", "This is in line with some early work that observed that certain characteristic properties like length [1][2], sentiment [3], presence/absence of brackets [4] are encoded in a single dimension in the space.", "Some of these findings can be attributed to the additive property of the cell state of the LSTMs c_t = f_t c_{t-1} + h_t {c\u2019}_{t}, which is free from matrix rotations.", "As previously noted, this empirically also results in single cells of the LSTM being interpretable.", "To just give one example, LSTM cell state can increment by a fixed amount at every time step and can count the number of tokens reliably (i.e the string length) [1][4].", "Re: (W3) Baselines for transfer learning:", "> The random baseline (i.e a random ordering of candidate task) is compared in figure 3 (and all the plots in the appendix), where we plot the accuracy boost using the best task till now in the produced recommendation of candidate tasks using different methods.", "We can clearly see that the random ordering is much worse compared to informed metrics that use representations.", "Upon your suggestion, we would also add this random baseline in table 2 as well.", "Re: (W4) Metrics for ranking of transfer don't make sense (and some are missing).", "How is precision and NDCG calculated", "> To compute the gold set, we first train a neural network for each of the candidate tasks and then use the pre-trained sentence encoder (part of the network) from the candidate task to fine-tune on the target task.", "The ranked list (in the decreasing utility of transfer learning gain) is then considered the \u2018gold\u2019 set.", "We further compare our recommendations of candidate tasks generated using CFS and classifier weight difference methods against the gold ranked list.", "Precision@K, Reciprocal Rank and NDCG are among the popular information retrieval metrics to compare a recommended list against a gold ranked list.", "These metrics are meaningful in our case, for instance, Reciprocal Rank tells us how many tasks we need to consider as per our recommendation before we hit the highest performing candidate task.", "Figure 3 presents the accuracy boost using the best task till now in the produced recommendations for the candidate tasks using different methods.", "Regarding missing values:", "As we explain in the paper, classifier weight difference metric is only applicable in cases", "where the number of features between the tasks are of the same size.", "Thus, 2 sentence input tasks and 1 sentence input tasks cannot be compared using the metric.", "Re: (W5) Multi-task learning", "> Our goal is somewhat orthogonal to the multitask learning setting where all the tasks are jointly trained.", "We, instead, focus on how the task-specific information is present in popular sentence representations, and how this could be used to assess transfer potential among tasks.", "Re: (W6) Motivation for CFS", "> There is a rich literature concerning what information is captured in the representations.", "Further, there are a few initial works that show that certain characteristics like length [1][2], sentiment [3], presence and absence of tokens like brackets [4] are densely captured in a single dimension of the representation space.", "In a similar spirit, we wanted to quantitatively study this surprising phenomenon, and we were curious about how densely is information encoded in representations.", "Re: (W7) Alternatives to CFS / Computational concerns", "> We agree that LARS/LASSO could act as potential ways to attain reduced dimensions.", "For our use case, we found the greedy forward selection computationally fast enough (of the order of a few minutes), and we observed a significant portion of accuracy captured in a very few dimensions.", "We would definitely explore this further, and add a detailed analysis on computational efficiency of different methods to reduce dimensions.", "Re: (W8) The proposed  CLF weight difference method has some concerning aspects as well. For example say we had two task with exact opposite labels.", "They would have a very low weight difference score though they are ideal representations for each other", "> You are right. For the very same reason, we take the inverse of the difference of normalized absolute classifier weights (Section 4.2).", "References:", "1.\u201cWhy Neural Translations are the Right Length\u201d :http://www.aclweb.org/anthology/D16-1248.pdf", "2. On the Practical Computational Power of Finite Precision RNNs for Language Recognition: https://arxiv.org/abs/1805.04908", "3. Learning to Generate Reviews and Discovering Sentiment: https://arxiv.org/abs/1704.01444", "4. Visualizing and Understanding Recurrent Networks : https://arxiv.org/abs/1506.02078"], "labels": ["global_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context", "global_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 347, "sentences": ["Thank you very much for the feedback.", "We have updated the paper and included a new section (4.3) showing how pGAN attacks bypass 4 different defence mechanisms, including outlier detection (as in Paudice et al. 2018a), the PCA-based defence in Rubinstein et al. 2009 (Antidote), Sever (Diakonikolas et al ICML 2019), and label sanitization", "(Paudice et al. 2018b)", ".", "From the reviewer\u2019s comments we noticed that, perhaps, the submitted paper, may not have sufficiently clearly explained that the approach is already targeting defences based on outlier detection and in particular that proposed in Paudice et al. 2018a.", "We already assume that the defender is in control of a fraction of trusted (clean) data points to train the outlier detector, which is a strong assumption in favour of the defender.", "To make this point clearer, we have also updated Figure 2 in the paper, showing the performance of pGAN for alpha = 0, i.e. when no detectability constraints are considered.", "In the Figure, we can observe that both for MNIST and FMNIST the outlier detection is capable of detecting many poisoning points and the effect of the attack is reduced compared with the results for alpha = 0.1.", "Different outlier-detection-based defences have already been proposed in the literature, such as Steinhardt et al. 2017 (\u201cCertified defenses for data poisoning attacks\u201d), Koh et al. 2018 (\u201cStronger data poisoning attacks break data sanitization defenses\u201d) or Paudice et al. 2018a, to cite some.", "In our experiments we chose the scheme proposed by Paudice et al. 2018a, as it assumes a stronger model for the defender (as mentioned before), which, in our opinion helps to validate the effectiveness of pGAN to craft successful poisoning attacks even in cases where the defender is in control of a fraction of trusted (clean) data points.", "Label sanitization (as proposed in Paudice et al. 2018b) completely fails to defend against pGAN attack, as shown in Figure 8 (right).", "As pGAN produces poisoning points that are correlated, the KNN-based algorithm proposed to do the relabelling is not capable of detecting the poisoning points.", "Moreover, some of the genuine points from the target class are incorrectly relabelled, making the problem even worse.", "The PCA-based defence proposed by Rubinstein et al. 2009 (Antidote) is also not capable of mitigating pGAN attack.", "The detectability constraints included in our model prevents this defence to detect the generated poisoning points.", "In the supplement we have included an analysis of the sensitivity of this algorithm to the threshold to discard training points.", "We can observe that the error increases as we increase this threshold.", "The \u201cSever\u201d defence (Diakonikolas et al. 2019 ICML) is also not robust against pGAN attack.", "In Figure 8 (left) we can observe that the defence performs worse than the outlier detector and that, when the algorithm is not under attack, the performance slightly decreases, as the algorithm is removing genuine data points that are significant for the training process.", "For FMNIST, Sever outperforms the outlier detector when the number of poisoning points is reduced, although the degradation of the algorithm as we increase the fraction of poisoning points is faster compared to the outlier detector and the PCA-based defence.", "In the supplement we included the sensitivity analysis w.r.t. the parameter that controls the fraction of points to be discarded.", "We can observe that, in this case, the difference in performance is not significant for the different values explored for this threshold.", "In summary, the revised paper (see the new version uploaded) now provides a comprehensive comparison of different defence mechanisms and shows the effectiveness of pGAN to bypass all of them.", "First in Figure 2 we show the effect of the attack for different values of alpha tested against the outlier-detection-based defence.", "Then, we have provided an empirical evaluation of pGAN against 4 different defence mechanisms both in MNIST and FMNIST, showing how our attack bypasses all of these defences.", "We thank the reviewer for this valuable comment, which has certainly helped us to improve the paper.", "We hope that the score can be revised to reflect this improvement.", "Thank you very much."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 348, "sentences": ["We thank the reviewer for the comments and appreciation, and would like to answer the reviewer\u2019s questions as follows:", "Q1. The use of the word \u201cguarantees\u201d is imprecise:", "Thanks for pointing out this.", "We have adjusted the word.", "A theoretical analysis will be an interesting future work.", "Q2. Whole sequence reconstruction results:", "Our current implementation only allows up to 5 images in a single 2015 TITANX GPU with 12GB memories.", "This is because we implemented the whole pipeline using tensorflow in python, which is memory inefficient, especially during training.", "Each image takes about 2.3GB memory on average, and most of the memory is consumed by the CNN features and matrix operation.", "But it is straightforward to concatenate multiple 5-frame segments to reconstruct a complete sequence, which is demonstrated in the comparison with CodeSLAM in Figure 7 of the revised version.", "It is also straightforward to implement our BA-Layer in CUDA directly to reduce the memory consumption of matrix operation and push the number of frames."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 349, "sentences": ["Thank you for your comments and suggestions. Please see our responses below.", "1. Related work", "Thanks for pointing out this. We have added discussion about the optimal teaching and active IRL.", "2. More implementation details", "We have provided more details in the revision and plan to release our code.", "Regarding your questions: i) demonstrators policies are implemented by search algorithms; ii) the behavior tracker is an LSTM with 128 hidden units; iii) fusion module produces a 32-dim attention vector corresponding to 32 feature maps from the state encoder, and each element of that vector is used to reweight one of the feature map in order to reshape the state feature.", "3. I am not sure why it was submitted to ICLR and not the Annual Meeting of the Cognitive Science Society", "We think this is appropriate for ICLR as we propose a novel deep RL approach to improve representation learning for agent modeling.", "Having said that, it could be an interesting future work to study how humans perform probing in the perspective of cognitive science.", "4. Typos", "Thanks for point out the typos. We have fixed them in the revision."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 350, "sentences": ["We thank the reviewer for reading and evaluating our submission.", "Additive composition vs. tensor: as discussed in our introduction (and illustrated by the qualitative results in Tables 1 and 2), we believe that linear addition of two word embeddings may be an insufficient representation of the phrase when the combined meaning of the words differs from the individual meanings.", "Syntactically related word pairs such as adjective-noun and verb-object pairs can have this property."], "labels": ["global_context", "multiple_context", "single_context"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 351, "sentences": ["Thank you for your careful reading and comments!", "Q1: We are evaluating the proposed metrics on more recent GAN-based models you suggested and will update the results once the results become available.", "Q2: The main contribution is listed as follows:", "1. A pilot study of mode collapse existence in GAN.", "2. Metric to detect mode collapse in GAN models without any labels (ground truth or pseudo-labels).", "2. Black-box plug-and-play model collapse calibration.", "Thank you again for your careful reading and kindly identifying the typos in our paper! We will fix these typos and meticulously proofread our article."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 352, "sentences": ["We thank the reviewer for his comments.", "We address the comments about novelty in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ), for instance concerning the relationship to previous work, and the regularization penalty ||f||_M we propose.", "More detailed comments are addressed below.", "**", "weakness of adversarial training", "As noted in our general response, our ||f||_M regularization approach empirically yields models with a more useful certified generalization guarantee in the presence of adversaries on Cifar10, while PGD adversarial training would likely require local verification of robustness around each test example, and we are not aware of useful guarantees on adversarial generalization for such models.", "We agree that this aspect is not clear in the current submission, and we will improve it in the next version.", "*", "* relationship with traditional RKHS regularization", "There is indeed no question that kernel methods/RKHSs have been widely used for regularization of non-linear functions, for over 20 years now, however these methods typically rely on solving convex optimization problems using the kernel trick, or various kernel approximations (such as random Fourier features).", "Separately, defining RKHSs that contain neural networks has indeed been the study of previous work, such as Bietti and Mairal (2018) or Zhang et al. (2016; 2017), however these only study theoretical properties of the kernel mapping and the RKHS norm, or derive convex learning procedures to replace training neural networks.", "Our approach is quite different, in that we leverage these insights to obtain practical regularization strategies for generic neural networks.", "** new regularization methods", "In addition to the ||f||_M lower bound penalty discussed in our general response, we note that combined approaches based on lower bound + upper bound methods are also novel to the best of our knowledge, and in particular we found combining robust optimization techniques with spectral norm constraints to be quite successful in many of the small data scenarios considered (see Table 1).", "We will happily clarify some of these points in an updated version of the paper."], "labels": ["global_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 353, "sentences": ["We would like to thank the reviewer for the positive feedback.", "We reply to the the two questions below.", "Q1: For the motivation of this method, why would the graph be constructed within each class? If there is a correlation between different classes, how could the model use such class-wise correlation to clean the label?", "R1: The most general graph would be constructed based on image and text similarities combined.", "Here, we pre-filter with text similarity, i.e., label names, and then build the graph based on visual similarities.", "This permits (a) to significantly reduce the size of the graph and hence the complexity and (b) to reduce the noise during the cleaning task.", "We agree that operating on the more complex graph could be the subject of future research, but a significantly different method would be required and the gain of the correlation is not granted.", "Q2: Maybe I missed it, but how is the relevance score / predicted label determined for testing data given the graphs constructed in each class of training data?", "R2: There is no relevance score assigned to the test data.", "Relevance scores are only used during training.", "In particular, we build per-class graphs using the training data, assign each training example a relevance score (Section 4), and train a classifier using the training data and the corresponding relevance scores (Section 5).", "Now given a test image, a prediction is simply made by the classifier; no data or relevance scores are used.", "See also pseudo-code in response R1 to reviewer 3."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 354, "sentences": ["We thank you for your comments and hope that the following response will address your concerns.", "1. We did stated both in the Theorem statements and Remark 3.4 that the a large batch size $B_t=T$ is used for the convergence proof.", "This means the effective rate of convergence is $O(1/\\sqrt{T})$ as pointed out by the reviewer.", "This rate matches the currently best known rate of convergence for SGD (see, e.g. Ge et al., COLT'15).", "We have now made this very clear in both Remarks 3.3 and 3.5.", "Please see changes highlighted in blue and also our response to Reviewer 2 on novelty of the convergence analysis.", "If our response addresses your main concern, we sincerely hope you that you can reconsider your score.", "For your other points, we have made the following changes in the paper.", "2. We have checked and fixed a few typos in the paper.", "Please note that we wrote  $\\nabla f(x)\\cdot \\sigma (\\nabla f(x))$ in eq. (7) as a dot product.", "So it is the same as $\\nabla f(x)^{T} \\sigma(\\nabla f(x))$. This notation was explained in the notation section.", "If you have any remaining concerns, please let us know.", "3. We have added the values for chosen $\\gamma$ in the updated version (see caption of Figure 1).", "4. We plan to add SGD to the experiments, but this may take a while to complete, especially for some of the experiments.", "We promise to do so in the final version.", "5. We were not aware of this at the time of submission.", "We have changed this to PoweredSGD. If you have any alternative suggestions, please let us know.", "We summarize the main contributions of the paper as follows:", "- In the theoretical part, we provided more concise convergence rate analysis for stochastic momentum methods in the non-convex setting.", "This was made possible by a sharp estimate of the accumulated momentum terms (Lemma B1).", "We believe this is an important but under-explored topic (Yan et al., 2018).", "- In the experimental part, we empirically showed that the proposed optimisation algorithms have potential to solve realistic problems.", "We are not claiming these variants will outperform all other methods in all training cases, but we sincerely believe that the results are promising.", "In particular, we have demonstrated their potential benefits of mitigating gradient vanishing and combining other techniques for accelerating optimization.", "We do admit the gap between our theoretical analysis and experiments in the sense that the analysis does not account for the initial acceleration observed in many experiments.", "We think this is a very interesting question for future research and hope that this paper can motivate further research in this area.", "We agree with your intuition that this may have something to do with $\\gamma\\in (0,1)$ boosting the gradients."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 355, "sentences": ["Thanks for your comments.", "Below we address the detailed comments.", "In particular, we clarify the potential misunderstanding on the linearity of the discriminator and added new state-of-the-art results by applying negative feedback to SN-GAN.", "Q1: About novelty and analysis:", "A1: As agreed by both Reviewer #2 and Reviewer #3, in this paper, our contributions are twofold: (1) a unified and promising framework to model the stability of GANs using control theory, (2) we propose to use the negative feedback to stabilize GANs.", "First, using control theory, the dynamics of GANs can be modeled as transfer functions with Laplacian transformation, and various existing methods (e.g., Negative Momentum and Reg-GAN) can be considered as certain controllers that are widely used in control theory.", "Moreover, through control theory, the stability of GANs can be easily inferred from the poles of the transfer function, instead of analyzing the complicated jacobian matrix of the dynamics as discussed in Sec. 4.1 and Appendix A&C. We argue that our method is distinct from existing method, which is well-discussed in our response to Q4.", "Second, our perspective also provides a promising direction that can further benefit the training dynamics of GANs using advanced control methods (e.g., nonlinear control and  modern control theory [*2]) to improve both the stability and the convergence speed of GAN.", "As a concrete example, we propose to use the most widely-used negative feedback control method to stabilize GAN's dynamics and the empirical results demonstrate the effectiveness of NF-GAN as shown in Sec. 4&6.", "Exploring advanced control methods is our important future work.", "We updated the empirical results on the state-of-the-art model in the revision, where we applied our proposed NF-GAN to the SN-GAN [*6].", "We can see that NF-GAN can successfully address the potential unstable issues of SN-GAN and achieve state-of-the-art inception score on CIFAR-10.", "More details can be found in our response to the common concern and our revised paper.", "Q2: Linear discriminator and extending the analysis to realistic settings:", "A2: Thanks.", "We indeed extended the analysis of Dirac GAN to the more realistic setting in Sec. 3.2, where the discriminator is NOT linear.", "In this part, we analyzed the dynamics of WGAN in the function space following [*1], i.e., we directly modeled $D(t, x)$ and $G(t, z)$ for all $x$ and $z$. It avoids the nonlinearity issue caused by the neural network, and both G and D are linear dynamics, at least locally around the equilibrium, as discussed in Sec. 3.2 and Appendix D in the revision.", "Fig. 2 (right) provides a diagram of the unregularized WGAN.", "In practice, we use the gradient descent method in the parameter space to approximate the dynamics in the functional space to efficiently solve the optimization problem.", "Recent advances in modeling GAN in the functional space [*5] provide powerful tools to bridge the gap and we leave it as our future work.", "We updated the discussion in Sec. 3.2 in the revision to make this clearer.", "Q3: The Lip constraints on the discriminator:", "A3: Actually, our method also applies to WGAN with Lipschitz constraints (vanilla WGAN).", "Existing work [*3] states that vanilla WGAN diverges and we provide theoretical and empirical evidence that our method helps vanilla WGAN converge.", "Theoretically, to address this comment, we added Theorem 1 (See in the Appendix D) that states the dynamics of $D$ with Lipschitz constraint follows Eqn. (10) *around the equilibrium*. Therefore, the stability analysis and our proposed method in Sec. 4 still applies to vanilla WGAN because control theory mainly focuses on the stability *around the equilibrium* [*2].", "Empirically, as suggested by R#3, we built a vanilla WGAN baseline using the SN-GAN [*6] framework, whose Lipschitz constraints are satisfied through spectral normalizations.", "We compared SN-GAN (WGAN loss) and NF-SN-GAN (WGAN loss) and obtained a significant improvement on both the stability and the final results (IS from 3.29 to 8.28, See details in our post for common concerns).", "It demonstrates that our method helps vanilla WGAN converge, which is consistent with our theoretical analysis.", "Q4: Related work:", "A4: Thanks for pointing out the related work. In fact, our method is distinct from these methods.", "For the first paper (i.e., Gradient descent GAN optimization is locally stable) analyzed the stability of GANs using the Jacobian matrix and adopted a regularization term to stabilize GANs similarly to [*4].", "Instead, we adopted a different method to model the dynamics from control theory.", "The difference has been discussed in Sec. 1 and Sec. 5", "For the second paper, the authors used the Lyapunov function, which is different from our framework, to analyze the stability of GANs.", "Besides, their method fails to scale-up to large datasets such as CIFAR-10 because of computational issues.", "Q5: Empirical results:", "A5: Theoretically, Reg-GAN is also a stable training method for GANs but it is computationally less efficient than NF-GAN (ours), as illustrated in Fig. 4.", "Empirically, we can achieve better results compared to Reg-GAN as illustrated in Table 1 (top).", "Moreover, we also also advanced the state-of-the-art results based on practical GANs (SN-GAN).", "The inception score on CIFAR-10 is improved from 8.22 to 8.45.", "See details in Table 1 (bottom) in the revision and our post about common concerns.", "[*1] Johnson, Rie, and Tong Zhang. \"Composite Functional Gradient Learning of Generative Adversarial Models.\" International Conference on Machine Learning. 2018.", "[*2] Khalil, Hassan K. \"Nonlinear systems.\" Upper Saddle River (2002).", "[*3] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. \"Which Training Methods for GANs do actually Converge?.\" International Conference on Machine Learning. 2018.", "[*4] Mescheder, Lars, Sebastian Nowozin, and Andreas Geiger. \"The numerics of gans.\" Advances in Neural Information Processing Systems. 2017.", "[*5] Johnson, Rie, and Tong Zhang. \"Composite Functional Gradient Learning of Generative Adversarial Models.\" International Conference on Machine Learning. 2018.", "[*6] Miyato, Takeru, et al. \"Spectral Normalization for Generative Adversarial Networks.\" (2018)."], "labels": ["global_context", "other_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 356, "sentences": ["We thank the reviewer for their comments and suggestions.", "We answer below:", "1. As the reviewer accurately points out, we choose to always employ the hinge loss for DFW in this paper because it gives an optimal step-size.", "In the new version of the paper, we have included additional baselines on the SNLI data set.", "This provides more empirical comparisons between the performance of CE and SVM for different optimizers.", "2. In appendix B.2 of the paper, we have added the convergence plot for all methods on the CIFAR data sets.", "In some cases the training performance can show some oscillations.", "We emphasize that this is the result of cross-validating the initial learning rate based on the validation set performance: sometimes a better behaved convergence would be obtained on the training set with a lower learning rate.", "However this lower learning rate is not selected because it does not provide the best validation performance (this is consistent with our discussion on the step size in section 6)."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 357, "sentences": ["We thank the reviewer for the comments and appreciate that the reviewer likes our idea of including optimization in the network. But our contribution is beyond adopting Levenberg-Marquardt instead of Gauss-Newton.", "We would like to clarify several things to address the reviewer's concerns:", "Q1. The advantages of Levenberg-Marquardt over Gauss-Newton is unclear (the main reason for rejection):", "Firstly, we want to clarify that our contribution is beyond improving the Gauss-Newton optimization to Levenberg-Marquardt.", "More importantly, our contribution is the combination of conventional multi-view geometry (i.e. joint optimization of depth and camera poses) and end-to-end deep learning (I.e. depth basis generator learning and feature learning).", "This contribution is achieved by our differentiable LM optimization that allows end-to-end training.", "Secondly, we agree with the reviewer that comparing with the Gauss-Newton algorithm will be interesting and have updated such a comparison in Appendix B in the revised version according to the reviewer\u2019s suggestions:", "1. We retrained the whole pipeline with Gauss-Newton, to make sure the features are learned specifically for Gauss-Newton.", "2. We compared with various constant lambda values to see how the performance varies along with lambda.", "Note that we also fine-tune the network to make sure the features fit different lambda.", "In Table 4 of the revised version (Appendix B), our method outperforms the Gauss-Newton algorithm in the last column.", "This is because the objective function to be optimized is non-convex, and the vanilla Gauss-Newton method might get stuck at saddle point or local minimum.", "This is why the Levenberg-Marquardt algorithm is the standard choice for conventional bundle adjustment.", "In Figure 6 of the revised version (Appendix B), our method also consistently performs better than different constant lambda values.", "This is because the value of lambda should be adapted to different data and optimization iterations.", "There is no \u2018optimal\u2019 constant lambda for all data and iterations.", "Q2.", "Comparison with CodeSLAM:", "We have included that in Figure 7 of the revised version (Appendix E).", "Since there is no public code for CodeSLAM, we cite its results directly from the CodeSLAM paper.", "Q3. The state vector Chi is not defined for the proposed method.", "The Chi is defined in Section 3 as the vector containing all camera poses and point depths.", "Since our method also solves for these unknowns as in classic methods, we did not redefine the Chi.", "But in the revised version we have recapped the definition of Chi when introducing our method at the beginning of Section 4.", "Q4. Should the paper be called Bundle Adjustment?:", "The term \u2018Bundle Adjustment\u2019 is originally used to refer to the joint optimization of 3D scene points and camera poses by minimizing the reprojection error.", "The keyword Bundle comes from the fact that a bundle of camera view rays pass through each of the 3D scene points.", "Multiple recent works, e.g. [Engel et al., 2017,Delaunoy and Pollefeys, 2014], have generalized it to \u201cphotometric BA\u201d where scene points and camera poses are optimized together by minimizing the photometric error.", "Our method is along this line.", "But we further improve the photometric error to featuremetric error.", "Each 3D scene point is still constrained by a bundle of camera view rays, though the error function has been changed.", "So we believe it is justified to call this method feature-metric BA.", "But we agree with the reviewer that the word \u2018reprojection\u2019 is misleading when we introduce our feature-metric BA and the photometric BA.", "So we use the word \u2018align\u2019 as the reviewer suggested and use \u2018reprojection\u2019 only for the geometric BA.", "Q5. Is B the same for all scenes?:", "In the revised version, We added Figure 8 to visualize of the term B in Equation 7 (Page 6) for different scenes.", "We can clearly see that it is scene dependent.", "Q6.Typos:", "We have fixed all the typos as suggested in the revised version.", "We thank the reviewer for raising the score.", "We submitted the response and the revision until the last minute because a lot of extra works have been done for the revision, and we want to ensure the correctness and completeness.", "But we will have a better-planned schedule for the next ICLR to fit the purpose of openreview."], "labels": ["multiple_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 358, "sentences": ["1. The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.", "A: While ReMixMatch comprises many components (some of which are new), we believe our ablation study justifies the reason why each component exists. If there are additional ablation experiments that you think would be helpful for us to run, please let us know.", "2. As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.", "A: We actually found that using stronger augmentations in MixMatch resulted in divergence.", "We mention this in the paper (\"Since MixMatch uses a simple flip-and-crop augmentation strategy, we were interested to see if replacing the weak augmentation in MixMatch with AutoAugment would improve performance but found that training would not converge.\") but will emphasize this more in the next draft.", "We also found in our ablation study that using only strong augmentation (i.e., replacing weak augmentations with strong augmentations) resulted in very poor performance for ReMixMatch, suggesting that anchoring towards a weaker augmentation is important.", "We will update the labels in the ablation table to make this more clear.", "3. What about enforcing the marginal distribution on the annotated data and the marginal distribution of the model to be similar with KL divergence?", "A: We tried this approach in initial experiments and found that it performed poorly.", "Using the KL loss also introduces a scalar multiplier hyperparameter for this loss term.", "We spent some time tuning this hyperparameter and were unable to obtain good results, so we chose to use the proposed version which does not introduce such a hyperparameter.", "It may be that further investigation into this form of a loss could be fruitful."], "labels": ["single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 359, "sentences": ["Thank you for your feedback. We are glad to know that you find the problem inherently interesting and important.", "Re: no exploration of encoder architectures is performed", "> We are not sure if we understand this completely.", "Just to clarify, we do compare 4 different sentence encoders [1][2][3][4] which display a fair amount of variety in ways which sentence representations can be computed.", "For instance, SkipThought vectors [1] use bi-GRU based encoder-decoder model to reconstruct the surrounding sentences.", "ParaNMT [2] and InferSent [3] use different LSTM based architectures to perform back-translation and textual entailment respectively.", "Lastly, SIF [4] is a tf-idf based weighted average of individual GloVe word representations.", "One of the key findings of our work is that task-specific information is captured succinctly for a majority of 13 different NLP tasks across 4 different choices of encoder architectures.", "1. Skip-Thought Vectors (https://arxiv.org/pdf/1506.06726.pdf)", "2. PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations (https://arxiv.org/pdf/1711.05732.pdf)", "3. Supervised Learning of Universal Sentence Representations from Natural Language Inference Data (https://arxiv.org/pdf/1705.02364.pdf)", "4. A Simple but Tough-to-Beat Baseline for Sentence Embeddings (https://openreview.net/forum?id=SyK00v5xx)", "Re: Utility of the methods is a bit unclear", "> We agree that our approach to estimate transfer potential reaps true benefits only when n is large.", "However, this is not uncommon in scenarios like machine translation, where there are hundreds of potential language pairs that could be used as candidate tasks.", "Furthermore, we believe (although acknowledge that this is subjective) that curiosity-driven questions about how the information is encoded are interesting: while they might not be useful in a way that is easily measurable by quantifiable metrics, they provide insights that can help guide future work.", "Re: CFS metric depends on a hyperparameter (the \"retention ratio\")", "> Sorry about the lack of clarity!", "To clarify, we used the elbow method (used to find an appropriate number of clusters for clustering) and observed that the \u2018elbow\u2019 in the accuracy vs dimensions plot was around the 80% accuracy mark for most tasks, and hence, we used 80% as the retention ratio.", "We will discuss this process and test with different retention ratios in the final version.", "Re: motivation for the restriction to linear models?", "> Our motivation to use linear models is to keep the setup simple and fast.", "As the classifiers are able to extract task-specific information and reliably estimate transfer potential; changing to a different classifier like MLP, we believe, shouldn\u2019t affect our results in a significant way.", "However, we will empirically verify this, and discuss this in the camera-ready/future versions of the paper."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 360, "sentences": ["We thank the reviewer for pointing out the potential similarity between our pipelined approach and the asynchronous update approach.", "Pipelined backpropagation is similar to model parallelism but it addresses the resource underutilization issue in model parallelism.", "However, asynchronous update (e.g., asycn-SGD in Dean et al. [1]) usually utilizes a parameter server to keep track of model parameters (weights) while our pipelined method does not use any parameter server.", "Furthermore, each accelerator obtains a replica of a full model in asycn-SGD training while each accelerator contains only a part of the model in our pipelined method, on the assumption that the full model does not fit into the memory of a single accelerator.", "The async-SGD in Dean et al. [1] still falls into data parallelism because each accelerator has a replica of the full model.", "On the other hand, our approach falls into pipelined parallelism.", "Thus, we focused our comparison to related work on two similar approaches: PipeDream and GPipe, both utilizing pipelined parallelism.", "Nonetheless, we will expand the related work section to more explicitly compare to data parallelism and non-pipelined approaches to model parallelism (i.e., expand on the first paragraph of related work).", "[1]  Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. 2012. Large scale distributed deep networks. In Proceedings of the 25th International Conference on Neural Information Processing Systems"], "labels": ["global_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 361, "sentences": ["U^m in Eq 1:", "This term is clearly defined at the beginning of section 3, first paragraph, line 6 \u201cU is the unit of hyper-volume in ...\u201d .", "U^m is simply U powered by the cardinality variable.", "In section 3.3, in the paragraph after Eq.7 line 2, we explain the mechanism to obtain U. For each experiment, we also report the tuned value for U.", "- The term p(w) in Eq 2:", "Note Eq. 2 calculates the posterior, i.e., p(w|D) and according to the Bayes theorem, it is p(w|D)\u00a0\\propto\u00a0p(D|w)p(w)", "which\u00a0is\u00a0simply\u00a0Eq. 1.", "- Eq 5 confusion :", "To explain Eq.5 and Eq. 6, the training works as follows:", "At each iteration k-1, the network predicts the outputs, i.e., O1, O2 and \\alpha.", "We first solve a discrete optimization to find the permutation (matching) between the predictions at k-1 and the ground truth (GT).", "Then, we use this permutation to order GT and back-propagate the losses to update w at iteration k. Please note that cardinality loss does not depend on this permutation variable.", "- The network architecture :", "Note that our described methodology can be applied to any network architecture.", "In ALL our experiments, we use Res-net 101 (mentioned several times on page 6 and 8).", "We\u00a0only\u00a0need to define the number of outputs and use the set loss defined in Eq. 5 and 6.", "For example, for the set size with maximum cardinality 4, we need 5 outputs\u00a0( \\alpha)\u00a0for cardinality m = {0, 1,...4}.\u00a0If the state of each set is 5 for the detection experiment, we need 4*5=20 outputs for the state  loss (O1).", "For the permutation (O2), we need 4!=24 outputs.", "For each output we have a loss defined for each experiment\u00a0in the text.", "- the permutation to benefit training:", "We refer the reviewer to the experiment we have already included in Appendix titled \u201cAn additional baseline experiment\u201d, which unfortunately we could not include in the main manuscript due to space constraints.", "We use a baseline model with no permutation, which is exactly same as [21], to train the network for the detection task.", "The results show the model is not able to learn this task, hence highlighting the need for permutation prediction for a complete set prediction network.", "Even if we remove the permutation head, O2, from our model, we still need to calculate the permutation using Eq. 5 and use it for backpropagation in Eq. 6.", "However the model in [21] completely ignore the permutation in its formulation.", "Therefore, it cannot learn the detection task.", "- Term f2 in Eq5 uses w~ estimates:", "Your interpretation is indeed correct.", "Given the predictions of O1 and O2 using statistics from past SGD runs, we want to find the best permutation.", "There are indeed m! way to assign GT set elements to the predictions.", "We solve this optimization to find the best one.", "- the significance of the permutation:", "Even if we don\u2019t use f2 for the estimation of the best permutation, we can use \\pi* as ground truth for updating its loss in Eq. 6.", "- classifying permutations:", "Classifying the permutations provides the extra information about the structure of the problem, e.g. there exist a single order which matters or it can be several different orders or the problem is orderless.", "We simply do not ignore the permutations from Hungarian by allowing the network to learn them.", "We refer you to the experiment we have already included in Appendix titled \u201cDetection & Identification results\u201d, where we used the predicted permutations to identify the bounding boxes for similar looking objects across different test images", "-  larger images and many instances:", "We agree.", "We leave this as future work, as it would require an engineering effort that departs from the main purpose of our paper, which is to show theoretically how to construct a network that can work with sets instead of tensors.", "- sensitivity to seeing a certain cardinality:", "During the training, we do the data augmentation (by cropping, flipping etc) to ensure the network sees enough sample for each cardinality.", "We will include this detail in the text.", "- Related work", "We are happy to include these references.", "But these works are orthogonal to the main subject of our paper.", "In our paper the goal is to introduce a framework to output a set using neural networks and we used the detection task as one of the set prediction examples.", "We would like to add few comment about these two works:", "These approaches try to learn a pairwise relationship between the boxes outputted using the conventional proposal based detection approaches.", "First a) they need to introduce extra pairwise network or heavier computation to learn these pairwise relationship b) they assume the relationship is pairwise between bounding boxes.", "Out framework is a single stage approach which uses a conventional convnet backbones with no extra computation.", "Since it is end-to-end prediction of boxes, we don\u2019t enforce any pairwise or higher order relationships between the outputs.", "We all rely on the layer of neural nets to capture high level relationship between outputs before predicting them."], "labels": ["multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 362, "sentences": ["Thank you very much for your feedback. We have revised and updated the paper following your suggestions.", "Here are our reply to your comments:", "(Q1) Thank you very much for the suggestion.", "We have included experimental results evaluating pGAN on CIFAR-10.", "The results are shown in Figure 4 in the updated version of the paper.", "(Q2) Following your recommendation, we have structured the section with the experimental results in different subsections.", "(Q3) The observation from the reviewer is correct: pGAN aims to inject poisoning points in regions that are close to the decision boundary, especially in those where the data distributions overlap more.", "Points that are far away from the decision boundary may be detected by the discriminator, outlier detection or other defensive algorithms that could be used.", "Defences aiming to remove points that are close to the decision boundary could be effective to remove poisoning points generated with pGAN, but these defences will suffer from a significant loss in performance, especially when the algorithm is not under attack.", "For example, in SVMs, support vectors are points that are close to the decision boundary.", "If we remove these points, as suggested by the reviewer, we would obtain different support vectors that would lead to suboptimal solutions with significantly degraded performance.", "With regards to addressing the more general point of the performance of pGAN when state-of-the-art defences are used, we have updated the paper to include a new section (4.3) where we show that pGAN is capable of bypassing 4 different defence mechanisms.", "This supports the effectiveness of our attack.", "(Q4) Munoz-Gonzalez et al. (2017) showed an experiment using a Convolutional neural network with 450,000 parameters, trained with 1,000 training points and injecting 10 poisoning points.", "In our case, for the experiment with MNIST in Figure 2, we used a deep neural network with more than 40,000,000 parameters, 1,000 training points, injecting up to 400 poisoning points.", "As the reviewer can observe the scale of the experimental evaluation is significantly different.", "The computational complexity of the attack in Munoz-Gonzalez et al. (2017) makes the experimental evaluation intractable for the settings considered in our experiments.", "On the other side, Paudice et al. (2018a) showed that, in many cases, if we don\u2019t consider appropriate detectability constraints, the attack points generated by optimal attack strategies formulated as bilevel optimization problems can be effectively filtered out with appropriate outlier detection, resulting in blunt attacks.", "This is not the case for pGAN, which is capable of bypassing different defences, including the outlier detection scheme proposed by Paudice et al. (2018a).", "Although defences based on outlier detection can be bypassed, as shown by Koh et al. (2017) (Stronger poisoning attacks break data sanitization defences), the complexity of the bilevel problem significantly increases compared to Munoz-Gonzalez et al. (2017).", "Thus, applying the attack strategy proposed by Koh et al. (2017) is also computationally intractable in our experimental settings.", "One of the main advantages of pGAN is the possibility of generating poisoning attacks at scale with detectability constraints capable of targeting large deep networks, where strategies relying on bilevel optimization have a limited applicability.", "Please, let us know if there are aspects that remain unclear or that require further clarification.", "Thank you very much."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 363, "sentences": ["The best part about this paper is that the size of the models are much smaller; but the paper does offer any explanation of the value of this.", "For example, even a 1% drop in accuracy can be unacceptable; but in some applications (like cell phones and IOT devices) model size is critical.", "The authors' should add some wording to explain this value.", "R) Thank you this good observation (Added to the paper)", "The \"adaptive\"kernels the the authors talk about", "are really a new class of nonlinear kernels.", "It would be very interesting to see a discussion of the class of functions these nonlinear kernels represent.", "This kind of discussion would give the reader  motivation for the choice of function, ideas for how to improve in this class of functions, and insight into why it works.", "The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.", "It would be nice if the authors pointed to a git repository with their code an experiments.", "R) Now we have a pytorch version of the code at https://github.com/adapconv/adaptive-cnn", "With MNIST and CIFAR.", "More importantly, the results presented are quite meager.", "If this is a method for image recognition,", "1)\tit would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.", "R) We added a new experiment for a real life application; testing different topologies.", "2)", "And the analysis of the \"dynamic range\" of the algorithim is missing.", "R) New data was added to the paper exercising multiple topologies, in a wider range of applications.", "3) How do performance and model size trade off?", "R)     A new experiment added to the paper shows the accuracy degradation vs model compression", "4) How were the number of layers and kernels chosen?", "R)We started with the original topology replacing convolutional kernels by the Adaptive kernels, then we reduced kernel by kernel, retraining the model each time to match the accuracy (with small drop).", "But our proposal is not the topology is the new type of filters, so many topologies can be improved using this type of filters, for instance an Adaptive ResNet.", "5) Was the 5x10x20x10 topology used for MNIST the only topology tried?", "R)We tested many, and we think that we can continue reducing the model, but our purpose is not to present a topology, our purpose was to show the advantages of Adaptive convolutions, having a model 66X smaller, 2X less MAC operations and trained 2X faster give us the clue that many researchers can explore on their own topologies and get benefits of it.", "6) That would be very surprising.", "What is the performance on all of the other topologies tried for the proposed algorithm?", "R) A table comparing different topologies is included in the new version of the paper.", "7) Was crossvalidation used to select the topology?", "If so, what was the methodology.", "We started with the reference topology like: ResNet18, LeNet, etc. then we reduce the number of kernels and layers keeping similar accuracy.", "Additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in Section 3; (2) omitted details, e.g.,", "1)how much overlap exists between kernels (Figs. 1, 2, and 4 suggests there is no overlap - this should be made clear);", "R)That is right, there is not overlap.", "and (3) poor grammar and nonstandard terminology, e.g., the authors' use of the word \"energy\" and the phrase \"degradation problem\".", "All of these issues should be addressed in a future version of the paper.", "R) Terms like \u201cenergy\u201d  were removed from the paper.", "We didn\u2019t invent the terminology \u201cdegradation problem\u201d it was used here https://arxiv.org/pdf/1512.03385.pdf, you want us to remove it?", "(Fixed on the paper)", "Not sure why Eqns. 2 and 9 need any parentheses", ".  They should be removed.", "(Fixed on the paper)"], "labels": ["single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 364, "sentences": ["We appreciate AnonReviewer3\u2019s recognition of our work.", "- Network details", "We only replace the last fc layer of ResNet-101 with a new fc layer mapping to 49 (5+20+24 = 49) outputs for calculating cardinality, states and permutation (the choice of these numbers explained in our response to R2).", "- inference time", "We also performed extra experiment on accuracy and inference time between different detectors (on the same machine and GPU) reported here:", "Faster R-CNN: AP=0.68, Inference time=101 ms", "YOLO\u00a0v2: AP=0.68, Inference time=12.3 ms", "YOLO\u00a0v3: AP=0.70, Inference time=18.2 ms", "Our network: AP=0.75, Inference time=15.1 ms", "- test on PASCAL VOC", "We observed PASCAL VOC dataset include many images with more than 4 persons.", "Considering the images include up to 4 persons only, we might not have enough training data to train ResNet-101 network."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 365, "sentences": ["Thanks for your attention to our work.", "1) For the first problem that the duality gap is only an upper bound of F-distance.", "Our logic is that: a) There exists a condition s.t. duality gap = 0.", "b) If duality gap = 0, then the generator is the best one that can generate the true distribution.", "May be in the algorithm, we will miss the best generator because we do not get the equilibrium.", "2) Our method may encounter the same problem as the traditional algorithm.", "It is a kind of Markov chain to train the Loss. And the essence of the algorithm is in fact to solve $\\sup_f \\inf_{g^*} V(f, g^*)$ and $\\inf_g \\sup_{f^*}V(f^*, g)$. We should consider some better algorithm to solve it.", "3) For the experiments, we will do some modification and improve our network."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 366, "sentences": ["We thank the reviewer for their comments.", "We provide answers below:", "* \u201cThe DFW linearizes the loss function into a smooth one, and also adopts Nesterov momentum to accelerate the training.\u201d", "We would like to clarify this statement: one of the key ideas of the DFW algorithm is not to linearize the loss function $\\mathcal{L}$, but only the model $f$.", "* \u201cBoth techniques have been widely used in the literature for similar settings\u201d.", "We wish to clarify the main technical contributions of this paper, since the SVM smoothing and the application of Nesterov acceleration are not the main novelty of this work.", "We discuss the summary of contributions (available at the end of section 1 of the paper) in the context of technical novelty.", "- Employing a composite framework allows us to use an efficient primal-dual algorithm.", "As stated by Reviewer 1, this is novel in the context of deep neural networks: \u201cTo my knowledge, the submission is the first sound attempt to adapt this type of Dual-based algorithm for optimization of Deep Neural Network [..]\u201d.", "- Crucially, our approach yields an update at the same computational cost per iteration as SGD and with the same level of parallelization.", "In contrast, in the closest approach to ours, the algorithm of Singh & Shawe-Taylor (2018) can only process a single sample at a time.", "This results in an approach whose runtime is virtually multiplied by the batch-size (it would be slower by two orders of magnitude in typical classification settings, including for the experiments of this paper).", "- We do not mean to claim that the application of Nesterov acceleration is a technical novelty in itself.", "However, its use is subtle in our case (see appendix A.7) and it is empirically crucial for good performance, hence its mention in the paper.", "- To the best of our knowledge, the hyper-parameter free smoothing approach that we propose in this work is novel (but is not the main contribution).", "We have adapted the abstract and summary of contributions to focus on the main novelty, which is an optimization algorithm for deep neural networks with an optimal step-size at the same computational cost per iteration as SGD.", "If the reviewer remains concerned by a lack of novelty, we would be grateful if he/she could provide specific references so that we can compare them in detail with the DFW algorithm."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 367, "sentences": ["Thanks for the detailed and encouraging feedback! We reply all comments below (relevant ones are put together):", ">> Comments #1, #11", "We mainly account the success of this simple training strategy to the simplicity of the model, the relatively low dimensionality of our input features, and the simplified action space (though all  three suffice to obtain a good controller in the current settings).", "They make the training of the controller much easier compared to other RL tasks with higher dimensional features or larger output space.", "We have added the detailed PPO-based training algorithm in Appendix A.1.", "While AutoLoss is amenable to different policy optimization algorithms, we empirically find PPO performs better on NMT, but REINFORCE performs better on GANs.", "As to the online setting, thanks for pointing us to the \u201cshort-horizon bias\u201d paper.", "We have indicated in the revision the existence of this bias -- this bias was observed on the GAN task -- overtraining G can increase IS in a short term, but may lead to divergence in a long term as G becomes too strong.", "On the other hand, we didn\u2019t observe it harms on NMT task noticeably.", "We hypothesize the tradeoff is insignificant on NMT, as in our multi-task setting, slightly over-optimizing one task objective usually does not have irreversible negative impact on the MT model (as long as the other objectives are optimized appropriately later on).", ">> Comments #2, #3", "We\u2019d like to clarify that S=1 is consistent in the overhead section and Algorithm.1.", "S controls how many sequences to generate to perform a (batched) policy update (i.e. S is the batch size), and we set S=1 for all tasks.", "Only T differs across tasks, but we always update \\phi whenever a reward is generated.", "Back to comment #2: for regression and classification, we have experimented with larger S and found the improvement marginal.", "As each reward is generated via an independent experiment, the correlations among gradients are unobvious.", "For large-scale tasks, we use memory replay to alleviate correlations in online settings (please see Algorithm 2 in Appendix A.1 in our revised version).", "Performing batched update with a larger S might help reduce correlations; However, a large S, as a major drawback, requires performing ST (S>>1) steps of task model training, in order to perform one step of controller update.", "This yields better per-step convergence, but longer overall training (wallclock) time for the controller to converge.", "There might exist sweet spots for S where one can achieve both good per-step convergence and short training time, but we skip the search of S and simply use S=1 as it performs well.", "It is worth noting that some recent literature uses a stochastic estimation of the policy gradient with batch size 1 as well, and report strong empirical results [1].", "[1] Efficient Neural Architecture Search via Parameter Sharing.", "ICML 2018", ">> Comment #4", "We observe the controller performance on all 4 tasks are insensitive to initialization.", "A good initialization (e.g. in NMT, equally assigning probabilities to each loss at the start of the training) indeed leads to faster learning, but most experiments with random initializations manage to converge to a good optima,", "thanks to \\epsilon-greedy sampling used in training.", ">> Comment #5", "They are the same -- there is a typo leading to confusion in the sentence \u201c...in Figure 1 where we set different \\lambda in l_2 = \\lambda |\\Theta|_2...\u201d; which should be \u201c...in Figure 1 where we set different \\lambda in l_2 = \\lambda |\\Theta|_1...\u201d.", "We have fixed it in the latest version.", ">> Comment #6", "Please see the last paragraph in page 5.", "For regression, classification and NMT, we split data into 5 partitions D_{train}^C, D_{val}^C, D_{train}^T, D_{val}^T, D_{test}. AutoLoss uses D_{train}^C and D_{val}^C to train the controller.", "Once trained, the controller guides the training of a new task model on another two partitions D_{train}^T, D_{val}^T. Trained task models are evaluated on D_{test}. Baseline methods use the union of D_{train}^C, D_{val}^C, D_{train}^T, D_{val}^T for training/validation.", "For GANs that do not need a validation or test set, we follow the same setting in [1] for all methods.", "[1] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. ICLR 2016.", ">> Comment #7", "Thanks for pointing out -- we apologize for misusing \u201cexploding or vanishing gradients\u201d and have revised the paper to be accurate.", "We simply intended to clip the reward to reduce variances, and fount it effectively improved training.", ">> Comment #8, #9", "Thanks for pointing us to these two works.", "In [1], the authors investigate several features and develop a controller that can adaptively adjust the learning rate of the ML problem at hand, similarly in a data-driven way.", "In [2], the authors propose to manually balance the training of G and D by monitoring how good G and D are, assessed by three quantities and realized by simple thresholding.", "By contrast, AutoLoss offers a more generic way to parametrize and learn the update schedule.", "Hence, AutoLoss fits into more problems (as we\u2019ve shown in the paper).", "We have appropriately revised the two claims and cited them in the latest version.", ">> Comment #10", "Empirically, IS^2 or IS do not make much difference on the performance.", "The scaling term is a flexible parameter that controls the scale of the reward which we do not tune very much though.", ">> Comment #12", "Yes, in WGAN, it is preferable to train the critic till optimality.", "We have revised the statement for accuracy -- we observe in our experiments, for DCGANs with the vanilla GAN objective (JSD), more generator training than discriminator training generally performs better (but this may not be an effective hint for other GAN objectives as they behave very differently).", ">> Comment #13", "We have added Appendix A.8 to disclose all hyperparameters.", "All code and model weights used in this paper will be made available.", ">> Comment #14", "We\u2019ve revised our statements to be more accurate: for all GANs and NMT experiments, we observe AutoLoss reaches better final convergence; For GAN 1:1, GAN 1:9, AutoLoss trains faster; for NMT experiments, AutoLoss not only trains faster but also converges better.", "We\u2019d like to clarify that for all our GANs and NMT experiments, the stopping criteria of an experiment is either divergence or when we don\u2019t observe improvement of convergence for 20 continuous epochs.", "This is why in Fig.2, Fig.3(L) and Fig.4(c), it looks like that different methods are given different training time.", ">> Comment #15", "We have update Figure.4(b) to a scatter plot, and fixed mentioned typos in the current version."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 368, "sentences": ["We thank the reviewer for the comments!", "Q: \u201cADP and ADP-T runtimes were very close on WikiText-103 dataset but very different on Billion Word corpus (Table 3 and 4)\u201d", "The differences in training time are due to the size of the models: Weight tying saves a lot more parameters for the Billion Word model due to the larger vocab compared to the WikiText-103 models which have a smaller vocab.", "On WikiText-103, tying saves 15% of parameters (Table 3, ADP vs ADP-T, 291M vs 247M) and training time is reduced by about 13%.", "On Billion Word, tying saves 27% of parameters (Table 4) and training time is reduced by about 34%.", "The slight discrepancy may be due to multi-machine training for Billion Word compared to the single machine setup for WikiText-103.", "Q1: \"I am curious about what would you get if you use ADP on BPE vocab set?\"", "We tried adaptive input embeddings with BPE but the results were worse than softmax.", "This is likely because 'rare' BPE units are in some sense not rare enough compared to a word vocabulary.", "In that case, the regularization effect of assigning less capacity to 'rare' BPE tokens through adaptive input embeddings is actually harmful.", "Q2: \"How much of the perplexity reduction of 8.7 actually come from ADP instead of the transformer and optimization?\"", "For WikiText-103 (Table 3) we measured 24.92 on test with a full softmax model (a 5.2 PPL improvement over the previous SOTA).", "This corresponds to a Transformer model including our tuned optimization scheme.", "Adding tied adaptive input embeddings (ADP-T) to this configuration reduces this perplexity to 20.51, which is another reduction of 4.4 PPL."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 369, "sentences": ["Thanks for your constructive feedback.", "We have modified the paper to include some of the experiments you have suggested.", "Please find our detailed response below:", "[R4: First, the proposed attack method can yield adversarial perturbations to images that are large in the \\ell_p norm.", "Therefore, the authors claim that the method can attack certified systems.", "However, attack in Wasserstein distance and some other methods can also do so.", "They can generate adversarial examples whose \\ell_p norm is large.", "I think the author should have some discussions about these related methods.]", "Thank you for pointing us out to the missing related work which we have included in the revision.", "Indeed, the Wasserstein attack and the other previously mentioned non-$\\ell_p$ bounded attacks are alternatives for producing quasi-imperceptible non-$\\ell_p$ bounded adversarial examples.", "Any of these methods can alternatively be used for generating non $\\ell_p$ bounded attacks.", "However, one major advantage of our attack method over the Wasserstein attack may be its simplicity and scalability.", "Per your suggestion, we ran experiments using the Wasserstein attack.", "The authors of [1] suggest that the Wasserstein PGD attack works best when the attacker takes PGD steps in $ell_p$-norm directions and then project the noise back onto the Wasserstein ball.", "We used their official implementation and adapted it to attack the Randomized Smoothed classifier.", "Based on the official implementation, after every 10 iterations, if the attack is not successful, we increase the radius of the wasserstein ball in which the noise is projected back onto.", "Consequently, the attack is always able to reach a comparable, but slightly weaker, spoofed certified radii (~ 67% that of the shadow attack) at the cost of slightly more perceptible adversarial noise in difficult cases.", "Note that the reason that the examples are more perceptible than those from [1] is that they are made to produce large certified radii and not only cause misclassification (i.e., the entire Gaussian augmented batch needs to get misclassified.) A comparison of the resulting images and average certified radii of those images can be found in the following anonymized link:", "https://docs.google.com/spreadsheets/d/1F0P8aOD_5aiVjW3CrR49fudz4EgrORz7v4t0ZIJEBAo/edit?usp=sharing.", "[1] Wong et al., \u201cWasserstein Adversarial Examples via Projected Sinkhorn Iterations\u201d.", "[R4: Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].", "I hope to see some discussions about this. Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.]", "Per your request, we have attacked the work of [2] and reported results of attacking the pre-trained SmoothAdv classifiers (available in [3]) in Appendix B.", "Similar to the non-adversarially trained smooth classifier included in the original submission, we can produce adversarial examples for the SmoothAdv classifier which on average produce larger certified radii than their natural example counterpart.", "Also, in the revised document, we have expanded the caption of Table 1 to make sure that it is clear what a certified is and that a larger radii is better.", "[2]. Salman et al., \u201cProvably Robust Deep Learning via Adversarially Trained Smoothed Classifiers\u201d, NeurIPS 2019", "[3]. https://github.com/Hadisalman/smoothing-adversarial"], "labels": ["global_context", "global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 370, "sentences": ["Thank you very much for the comment.", "We believe that functional gradient based methods have much room to explore, and it is our hope that many aspects of GANs can be analyzed using this philosophy."], "labels": ["global_context", "global_context"], "confs": [1.0, 1.0]}
{"abstract_id": 371, "sentences": ["We would like to thank Reivewer 1 for your time and the review.", "We agree that more experiments based on other types of dataset will make the result stronger which we hope to perform in a follow up work.", "However, we believe that the current results already give substantial evidences that the method successfully disentangle local and global structure.", "While the performance of VAE+Auxiliary in digit identity clustering is not higher than two of the other methods, we found that the grouping can corresponds to other global features such as how many digits are in the image and the global-colour style.", "Therefore, the lower clustering accuracy does not mean that the method poorly disentangle local and global information but rather suggesting that the digit identity clustering is an incomplete evaluation metric for unsupervised clustering.", "We hope that reviewer 1 see that, in the context of this paper, the experiments have substantially fulfilled their proposed of showing that the method can disentangle global and local information as intended."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 372, "sentences": ["Due to the overlap between reviewer comments, we decided to address all concerns in a single response (please see above)."], "labels": ["other_context"], "confs": [1.0]}
{"abstract_id": 373, "sentences": ["Thank you for your review.", "Please also see our high-level clarification above which we believe can help in better interpretation of our contribution.", "Some specific responses below:", "-- \u201cproposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.\u201d", "Please refer to our overall comments on this question (and also a few more details in reply to Reviewer#1\u2019s similar question).", "-- Comment on scale / speed for large instances of combinatorial optimization:", "The point of this work is only to see if ML can find optimal algorithms, and not about doing it faster than the known theoretical algorithms.", "Note that this is not similar to the case of solving an offline combinatorial problem via integer programming or other solvers, since our problems are online, i.e., the instance is not known beforehand, so there is no comparison to such \u201cgeneral-purpose\u201d solvers.", "Thus we don't compare to the running time of offline solvers, but to the worst-case competitive ratio of the optimal online algorithms.", "As mentioned in the comment, this approach may eventually lead to finding optimal or near-optimal algorithms for a problem (not an instance of a problem) for which no algorithm is known -- but this is outside the scope of this work future work.", "Again drawing the analogy of playing Go, the objective is mostly on training an agent that can make competitive moves rather than very fast moves, and there is no known \u201cgeneral-purpose\u201d strategy to accomplish this.", "*Please also see reply to reviewer #2 on a similar question of evaluating against other methods*", "-- \u201cSki Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, i.e. problem instances are not generated by use of a machine learning model, which is one of the main claims the authors are making.\u201d", "Please see our high-level clarification on top.", "(4) We agree with the reviewer that in many cases there is a gap between solving the discrete problem and the fractional problem.", "In general it is an established approach to solve the fractional problem and use additional techniques such as rounding to fill the gap.", "As to AdWords, although the discrete problem naturally corresponds to the real world scenario, we do not consider fractional AdWords below the bar", "compared to discrete AdWords in terms of difficulty", ".", "The optimal CR bound and the adversarial distribution are the same for both cases, and the optimal algorithms basically have the same structure.", "One may arguably say that the optimal algorithm for the fractional problem has richer structure as in the fractional problem the action space is much larger as we can fractionally assign each ad to many advertisers.", "As to the shortcomings of our techniques and why we pick the fractional problem, note that the GAN framework needs the computation of the discriminator network (i.e. the algorithm agent in our context) to be differentiable in order to update the generator network (i.e. the adversary in our context) during training.", "This poses difficulties if we ask the algorithm agent to make discrete decisions via sampling or rounding since it will not be differentiable.", "This doesn\u2019t mean that our high-level framework (i.e. training the algorithm and adversary networks simultaneously) is doomed, since we can use other ML techniques (e.g. reinforcement learning) to implement our framework, but in general sampling and rounding will lead to much more work during training, so we pick the GAN structure in this work.", "(5) We know from theory that if the algorithm player runs a no-regret dynamic (e.g. MWU) and the adversary player responds with the worst input for the algorithm in each round, then the algorithm player converges to the optimal algorithm, and the uniform distribution over the adversary player\u2019s responses gives the adversarial distribution.", "However, we cannot really follow this approach as the space of algorithms is infinite and we cannot run a MWU on this space, and in general it is also hard or impossible to find the absolute worst input in each round.", "In the practical framework, the algorithm player uses a neural network, and the adversary network tries its best to come up with a bad (but not necessarily worst) input each round.", "Thus we don\u2019t have all the clean theoretical guarantees anymore, but the intuition should still largely hold (as our empirical result suggests).", "(6) We updated the appendix to address this. See \u201cTraining convergence\u201d in Appendix D.2", "(7) We updated the appendix to address this. See \u201cAdversarial distribution\u201d in Appendix D.2"], "labels": ["global_context", "global_context", "global_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 374, "sentences": ["We thank the reviewer for reading the paper and the comments.", "As already stated in the comments below, our claim of state-of-the-art in the original manuscript pertains to models with a single softmax, which we clearly state in section 4.1.", "We will update the abstract to remove any confusion.", "As suggested by multiple reviewers, we have performed further experiments by incorporating our Past Decode Regularization (PDR) in the mixture-of-softmax (AWD-LSTM-MoS) model of (Yang et al. 2017).", "We use the same model sizes as used in the paper.", "As shown below, we observe gains of 0.4 and 1.0 perplexity points for PTB and WT2, while with dynamic evaluation the gains are 0.4 in both cases.", "AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)", "Penn Treebank with finetuning -", "56.2/53.8", "||  56.5/54.4", "Penn Treebank with dynamic evaluation -", "48.0/47.3", "||  48.3/47.7", "WikiText-2 with finetuning -", "63.0/60.5", "||  63.9/61.5", "WikiText-2 with dynamic evaluation -", "42.0/40.3", "||  42.4/40.7", "Note that, we performed very limited hyperparameter tuning in the vicinity of the hyperparameters used by (Yang et al. 2017) and a more exhaustive search is likely to lead to better gains.", "Thus, the gains due to PDR generalize to more complex models like AWD-LSTM-MoS+PDR.", "We can justify PDR theoretically as an inductive bias on the language model.", "The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform.", "Similarly, the distribution of the first word given the second word will be far from uniform.", "A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one.", "In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the \"true second word\" and decode the distribution of the first word.", "Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram.", "We believe language modeling is a fundamental problem in NLP and our work continues a long stream of papers that have achieved steadily lower perplexities over the past few years.", "We evaluated our approach on two standard datasets that have been used as a benchmark in most of these papers.", "As suggested by multiple reviewers, we have conducted further experiments on the Gigaword corpus to test PDR on larger corpora.", "Specifically,  we use a 2-layer LSTM with hidden dimension 1024 and a word embedding dimension of 1024.", "We truncated the vocabulary by keeping approximately 100k words with the highest frequency and used the same validation and test sets as (Yang et al. 2017).", "We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 points in the test perplexity.", "Note that we tuned the PDR loss coefficient very coarsely and tuning it further could lead to higher gains.", "We will update the manuscript with these additional results and discussion and post it shortly.", "Yang et al. 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv:1711.03953."], "labels": ["global_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 375, "sentences": ["We thank Maxwell for some clarification.", "We believe AnonReviewer2 misunderstood some of the concepts and we will try to clarify them here and update the paper accordingly.", "- predicting unordered sets", "The assumption is what is available as GT is a set.", "This means we cannot infer any specific ordering from GT.", "The proposed framework is very flexible as we don\u2019t need to enforce the problem to be necessarily orderless  (although it can be).", "The reason we would like to learn  p_m(\\pi | x_i, w) is to infer the nature of the problem.", "However, excluding the main experiment in supplementary material, we did enforce the problem to be orderless by removing O2 and the permutation loss.", "This is equivalent to assume p_m(\\pi | x_i, w) is uniform (order does not matter) in Eq.2 and you can see O2 and its loss will be eliminated from Eq. 5 and 6.", "However, we still require to solve Eq. 5 to find the best permutation based on f1 only, which is equivalent to use Hungarian to solve the assignments.", "We also disagree with R3 that the problem is either unordered sets or there exist only one order to be correct.", "There can exist multiple orders to be true, but not all.", "This can be inferred by learning p_m(\\pi | x_i, w) from samples derived during training by Eq. 5.", "- permutation in the likelihood (2) does not make sense:", "In addition to what is explained by Maxwell, I add this clarification:", "p_y(y_1 | x, w, (1, 2)) means the first output is assigned to the first ground truth, while p_y(y_1 | x, w, (2, 1)) mean the first output is assigned to the second ground truth.", "These two scenarios are acctally generate very different gradient.", "The same argument can be extended to p_y(y_2 | x, w, (1, 2)) and p_y(y_2 | x, w, (2, 1)).", "- the dependence on \\pi drops out when getting a MAP estimate of outputs:", "The permutation takes into the account when there is loss and a GT to compare as GT  annotations are permutated to be assigned to the outputs.", "During inference, we don\u2019t have loss and GT.", "We just have the predicted outputs, e.g. cardinality, states and premutation and the order which we want to show the states will not change the value of the states.", "We hope to have clarified all the technical misunderstandings.", "We would like to point the reviewer again to our impressive results in the detection problem and ask him/her to reconsider his/her rating if the technical concerns are now clear."], "labels": ["global_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 376, "sentences": ["We thank the reviewer for taking the time to consider our paper and appreciate that you are excited to use our counterfactually-augmented dataset.", "While degrees of novelty and the relevant sorts of novelty are a matter of opinion we respectfully assert our view that new ideas, the new resource that we present, and the scientific insights derived from our experiments, are precisely the sorts of novelty that should be sought by conferences.", "We respectfully disagree with the reviewer\u2019s suggestion that a fundamentally distinct resource warrants only a whitepaper.", "We politely point out that many conferences have entire dedicated tracks, and even best paper awards for resources, and that many seminal papers of pivotal importance to the field make precisely this sort of contribution (e.g. ImageNet).", "Additionally we point out that the resource is not the only novel idea here.", "Of chief importance here is the intellectual contribution casting the problem of learning \u201csuperficial associations\u201d coherently in the language of intervention, and producing a dataset that addresses counterfactuals in a real sense (as pointed out more eloquently by R1).", "Moreover, our experiments shed insights about the price to be paid for relying less on spurious associations and our updated experiments (inspired by R3\u2019s suggestions) show that our methods result in improved performance out-of-sample on a variety of datasets.", "We hope that you might be willing to reconsider our contributions in light of the significance and uniqueness of the dataset, the insights of our experiments and the demonstrated out-of-domain robustness."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 377, "sentences": ["1", ".", "We thank the reviewer for pointing out papers [1] and [2].", "We will definitely cite them in the paper and include a discussion in related work on how our scheme compares to that proposed in the two papers.", "In essence, our scheme is different than [1] in two key aspects: (1) we pipeline both the forward and backward passes of the backpropagation while [1] pipelines only the backward pass.", "Further, equation (9) in [1] suggests that while weight updates use delayed gradients, the delayed weights (W^(t-K+k)) are used for the weight gradient calculation.", "This is essentially similar to weight stashing used in PipeDream, which we compared to in our paper.", "Thus, our scheme has the advantage of a smaller memory footprint.", "The follow up work in [2] attempts to reduce the memory footprint through feature replay (i.e., re-computing activations during backward pass, similar to GPipe).", "Our scheme saves the activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization of the accelerators (GPUs).", "We will edit the related work section to include the above discussion.", "2.\tThe method proposed in our paper stores immediate activations, which is mentioned in Section 3 of the submission.", "3.", "We appreciate the pointer to the better performance of ResNet-110.", "We trained the network for only 164 epochs with a batch size of 100, which is probably the reason that its inference accuracy is lower than expected.", "Should we adopt the hyperparameters (a batch size of 128) and more training epochs (200 epochs) as shown at https://github.com/akamaster/pytorch_resnet_cifar10 , our ResNet-110 baseline reached 93.59% in inference accuracy, and the pipelined ResNet-110 reached 92.88% in inference accuracy.", "The speedup obtained is 1.73X, slightly higher than the 1.71X obtained in our paper, which could be caused by the batch size increase that makes the GPU process more efficient.", "The exact inference accuracy of the model is somewhat orthogonal to our study.", "It is the trend of the decline in inference accuracy with pipelining is what we study and this trend exists with both our hyperparameters and those at https://github.com/akamaster/pytorch_resnet_cifar10.", "Nonetheless, it is relatively easy for us to update the results in the paper with these new hyperparameters.", "4.", "Indeed, comparisons to the results in [1][2] would be interesting.", "However, since the scheme in [1] employ weight stashing as PipeDream does and in [2] utilizes re-computing activations, as in GPipe, our comparisons to PipeDream and GPipe subsume comparisons to [1][2], particularly given the space limitations of submission.", "5.", "We appreciate such detailed and rigorous convergence analysis provided in [1] and [2].", "The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing or micro-batching, is simpler and does converge.", "The paper does achieve this goal, on a number of networks.", "Given the limited space provided, it would be difficult to fit a convergence analysis in our paper."], "labels": ["other_context", "other_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 378, "sentences": ["Thank you for your review.", "We believe it captures the essence of what we are proposing and are delighted with the overall very positive assessment.", "As regards the weakness mentioned, we accept the characterisation of our work as incremental in the sense that it draws together a number of known techniques from areas such as multi-task and adversarial learning.", "We argue, however, that the contribution of our submission lies exactly in the unification of these approaches into the stethoscope framework, which lends itself to targeted representation analysis and modification.", "We showcase stethoscopes to provide insights into a particular application domain, stability prediction in intuitive physics, but believe that the methods presented here will provide a ready toolkit for researchers addressing a variety of challenges in network interpretability and (de)biasing."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 379, "sentences": ["Thank you for the time. We would like to take this opportunity to correct some factually incorrect statements below.", "[Q] The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.", "[A]  We respectfully disagree.", "To our knowledge, this is only the second work which attempts to fairly and systematically compare GANs in a large-scale setting.", "The main conclusions of our work (about NS-GAN, spectral normalization, and gradient penalty) hold across several datasets and architectures.", "[Q] But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.),", "[A] We again respectfully disagree -- both LSUN and CelebaHQ are used for the first time in such a large-scale evaluation.", "In fact, none of the techniques were previously evaluated on CelebaHQ.", "Furthermore, even if some data sets, such as LSUN, were used previously, the comparison to other works was always done by the authors of the new method usually with additional changes, such as architectural decisions and optimization tricks.", "[Q] Not clear if the improvement in performance is statistically significant, how robust it is to changes in other parameters etc.", "[A] In this we take care of systematically evaluating various design decisions.", "While the space of design decisions is too large to search over, we focus on the main design choices and provide some conclusions in this context.", "Performance improvements obtained by both spectral norm and gradient penalty are statistically significant as seen in the plots -- the performance with respect to the baseline is far outside of the two standard errors of the median in most settings.", "[Q] The authors also rely mostly on the FID metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.)", "[A] FID was shown to correlate well with perceived image quality (e.g. precision) and mode coverage (recall).", "The evidence can be found in [1], and [2].", "As such, a reduction in FID corresponds both to improved image quality, as well as improved mode coverage.", "IN practice, a 10% drop in FID is visible to a human, and samples can be seen in the Appendix.", "While it is not a perfect metric, it is arguably useful for sample-based relative comparison of generative models.", "[1] https://arxiv.org/abs/1711.10337", "[2] https://arxiv.org/abs/1806.00035", "[Q] The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them, give mathematical formulations, making it hard to the non-expert reader to understand what are these techniques and why are they introduced.", "[A] Most of these are described in Section 2 (in particular, discussion on regularization and penalties is in Section 2.2).", "Describing all aspects of these techniques would require substantially more space and hence we refer to the original work for precise formulation.", "[Q] With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.", "[A] We respectfully disagree: we believe that for GAN practitioners our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most resnet tricks do not matter, etc.", "All of these insights are supported by a fair and unbiased rigorous experimental process.", "On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 380, "sentences": ["We thank Reviewer 3 for raising important questions.", "We answer them below.", "Using \\tilde x in the E- and M-steps.", "We agree with Reviewer 3 that \u201cthe error arising from quantizing v into c is only affected by a subset of rows of \\tilde x\u201d.", "However, we solve Equation (2) with this proxy algorithm for two reasons.", "First, using the full \\tilde x matrix allows to factor the computation of the pseudo-inverse of \\tilde x and thus allows for a much faster algorithm, see answer to Reviewer 2 and the details of the M-step in the paper (as well as footnote 2).", "Second, early (and slow) experiments suggested that the gains were not significant when using the right subsets of \\tilde x in this particular context.", "Minimizing the reconstruction error", "Our method results in both better reconstruction error and better training loss than na\u00efve PQ *before* any finetuning.", "As we state in the paper, applying naive PQ without any finetuning to a ResNet-18 leads to accuracies below 18% for all operating points, whereas our method (without any finetuning) gives accuracy around 50% (not reported in the paper, we will add it in the next version of our paper).", "Choosing the optimal number of centroids/blocks size", "There is some rationale for the block size, related to the way the information is structured and redundant in the weight matrices (see in particular point 1 of answer to Reviewer 1).", "For instance, for convolutional weight filters with a kernel size of 3x3, the natural block size is 9, as we wish to exploit the spatial redundancy in the convolutional filters.", "For the fully-connected classifier matrices and 1x1 convolutions however, the only constraint on the block size if to be a divisor of the column size.", "Early experiments when trying to quantize such matrices in the row or column direction gave similar results.", "Regarding the number of centroids, we expect byte-aligned schemes (256 centroids indexed over 1 byte) to be more friendly for an efficient implementation of the forward in the compressed domain.", "Otherwise, as can be seen in Figure 3, doubling the number of centroids results in better performance, even if the curve tends to saturate around k=2048 centroids.", "As a side note, there exists some strategies that automatically adjust for those two parameters (see HAQ for example).", "Comparison with pruning and low-rank approximation", "We argue that both pruning and low-rank approximation are orthogonal and complementary approaches to our method, akin to what happens in image compression where the transform stage (e.g., DCT or wavelet) is complementary with quantization. See \u201cDeep neural network compression by in-parallel pruning-quantization\u201d, Tung and Mori for some works investigating this direction."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context", "single_context", "single_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 381, "sentences": ["We thank the reviewer for raising useful points which helped us a lot improving the paper.", "The main point of the reviewer is that the novelty of our approach is limited with respect to the Evolutionary RL (ERL) algorithm, and that improvement is sometimes small.", "These remarks helped us realize that we had to better highlight the differences between our approach and ERL, both in terms of concepts and performance.", "We did so by replacing Figure 1, which was contrasting CEM-RL to CEM, with a figure directly contrasting CEM-RL to ERL.", "We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies, described either in the main text or in appendices.", "By the way, the ERL paper is now published at NIPS, but it was not the case yet when we submitted ours. We updated the corresponding reference.", "The reviewer seems to consider that each actor in our CEM-RL algorithm comes with its own critic (the reviewer says value function), which would raise a value function initialization issue.", "Actually, this is not the case: there is a single TD3 critic over the whole process, and gradient steps are applied to all the selected actors from that single critic.", "This has been clarified in the text by insisting on the unicity of this critic.", "We agree with the reviewer that the importance mixing did not provide the sample efficiency improvement we expected, and we can only provide putative explanations of why so far.", "Nevertheless, we believe this mechanism still has some potential and is currently overlooked by most deep neuroevolution researchers, so we decided to keep the importance mixing study in Appendix B rather than just removing it."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 382, "sentences": ["We thank the reviewer for their time and comments on the work.", "Concerning including more comparison and ablations in the paper, we have performed an extended analysis of our method to the baselines across many environments.", "See Figures 2,3,5 for more learning curve results and baseline comparisons and Figure 6 for qualitative metric analysis.", "We show that our method outperforms the baselines across multiple environments.", "In the paper, we include many details on the environment rewards and design as we consider these simulation tasks part of the contribution of the work.", "The simulation tasks contain robotic humanoid characters that need to learn how to navigate given egocentric vision.", "No other simulation is available that combines these challenges.", "The simulation will be released with the work for others to use and build on multi-agent learning methods.", "We have reviewed the provided references and have included them in the paper."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 383, "sentences": ["* models that get scores in the ~80 ppl range for Penn Treebank are important.", "we agree with the advice but not with the justification.", "We explain why in the general response: our goal is not to get good language models, but to use language modelling as a setting to test a property of a mechanism that is proposed.", "The perplexity becomes a way to observer the effect of a mechanism and not the goal itself.", "Moreover, (not in this case but) the architectures used to achieve better scores on given datasets are so over-parametrized that it's hardly reasonable to assume that the improvement justifies the cost of accommodating huge models overfitted to a particular dataset (and sometimes to a particular dataset configuration)", "That said, we agree that using different architectures would strengthen our point and make the paper more convincing.", "Also, using different datasets would help us demonstrate that the effect of the proposed mechanism is data-independent.", "We are also considering it's application to a different set of tasks in the future.", "We did follow reviewer recommendations and performed experiments with LSTMs and QRNN (slightly faster) along with WikiText (which is larger but not intractable), unfortunately we couldn't accommodate all the analysis and changes in time.", "* its parameter-reduction approaches against other compression and hyperparameter optimization techniques.", "We recognize that the focus on parameter reduction was perhaps counter productive to making the goal or this work clear.", "It is a byproduct of the technique, but modelling discrete distributions without prior knowledge of how many classes one might encounter is the main issue we are trying to solve. We could do that by using character-level or sub-word tokens, but again, the goal is not --solely-- language modelling as a task.", "The mechanism is applicable to settings where the number of possible input patterns is too large to instantiate as a parameter table (embeddings), but where the number of patterns that actually occur could actually more \"reasonable\". Meaning that as long as the \"world\" is not random uniform, we can make predictions."], "labels": ["single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 384, "sentences": ["Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:"], "labels": ["global_context"], "confs": [1.0]}
{"abstract_id": 385, "sentences": ["We thank the reviewer for their comments.", "We address their comments individually below.", "> The work is more suitable for a bioinformatics audience though, as the bigger contribution is on the particular application, rather than the model / method itself.", "The main contribution of this paper is the representation of the protein interaction data in the input layer of the CNN", "Response: The main contribution and novelty of this paper is the demonstration of the transferrability and power of the learned representation, and thus is a good fit for ICLR.", "We use the application of protein interface prediction as a test case for this, but applications can range widely from drug discovery, to RNA folding, to small molecule quantum mechanical calculations.", "As many of these tasks are very data-poor, this demonstrated transferrability opens up novel avenues through which these problems can be tackled.", "> - Their method improves over prior deep learning approaches to this problem.", "However, the results are a bit misleading in their reporting of the std error.", "They should try different train/test splits and report the performance.", "Response: We do use different subsets of the train set for different replicates.", "However, the train and test sets cannot be mixed as they come from different data distributions (P_r and P_p) and we are trying to show we can transfer with no retraining from P_r to P_p.", "Thus our reported metrics are correct and justified for this problem, though we have clarified the exact nature of the replicates in the text to ensure this is not misleading.", "> - The authors should include citations and motivation for some of their choices (what sequence identity is used, what cut-offs are used etc)", "Response: We do provide citations for these choices.", "See the second and third paragraphs of section 3 on page 4 for motivation/citations for sequence identity and cut-offs used, respectively.", "> -", "The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred", "Response: Fout et al. and Sanchez-Garcia et al. are feature engineering approaches -- they both use high-level features as inputs to their models (not atomic coordinates).", "Sanchez-Garcia et al. use a tree ensemble model that has no end-to-end learning aspects at all.", "Another popular pure feature engineering approach is PAIRPred (Minhas et al., Protein 2014), which uses an SVM trained on high-level features.", "However, we do not compare to them as their performance on C_p^{test} (0.863)  was already superseded in Fout et al.\u2019s work.", "IntPred [Northey et al., Bioinformatics 2017] addresses the binding site prediction problem (given one protein, which residues can be interfacial with any other protein), which is different than the problem we present.", "> - The authors use a balanced ratio of positive and negative examples.", "The true distribution of interacting residues is not balanced -- there are several orders of magnitude more non-interacting residues than interacting ones.", "Can they show performance at various ratios of positive:negative examples?", "In case there is a consistent improvement over prior methods, then this would be a clear winner", "Response: We can demonstrate consistent performance at different ratios of positive:negative examples.", "Running tests on C_p^{test} at 1:3, 1:5, and 1:10 ratios demonstrate no significant impact on performance (0.889 [0.882 +/- 0.012], 0.889 [0.882 +/- 0.011], and 0.895 [0.886 +/- 0.015], respectively).", "The AUROC metric we use is insensitive to class imbalance, and thus is a good measure to use when evaluating on datasets with varying amounts of imbalance."], "labels": ["global_context", "other_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 386, "sentences": ["Thank you for your reviews.", "Here are our responses to your questions:", "1. Clarify how the skills of agents play a role in the problem setup", "We clarify the definition of skills and how it influences the manager\u2019s decision as follows.", "i) As defined in Section 3, an agent\u2019s skill depends on its state transition probabilities and its policy.", "The state transition probabilities define if a resource can be collected by an agent (i.e., whether the \u201ccollect\u201d action executed by this agent will have real effect), and it is equivalent to a binary value for each resource in Resource Collection.", "The agent\u2019s skill also depends on its policy because it affects how fast an agent can achieve a goal.", "E.g., when the agent has a suboptimal policy, it may not be able to reach a goal within the time limit even though it actually can collect the resource if given more time.", "ii) The skills are completely hidden from the manager.", "It can be inferred by the manager based on the performance history, and also on the estimated worker policies by IL.", "However, only checking whether a goal is reached is not sufficient to determine skills.", "Failing to reach a goal may be a result of several reasons -- it may be because i) the bonus in the contract is too low, ii) the contract terminates prematurely before the agent can reach the goal, or iii) the assigned task depends on another task which has not been finished yet.", "So the manager needs to infer agents\u2019 skills, preferences, and the task dependency jointly through multiple trials.", "2. Is maximizing utility justified?", "Maximizing utility is actually the setup in similar problems in economics.", "Just like those problems (e.g., mechanism design), this paper focuses on scenarios where agents won\u2019t truthfully or clearly reveal its skills and preferences to the manager, and do not always behave optimally.", "As we stated in the paper, maximizing utility is more realistic, and typically the span of the decision making process of the manager is much shorter than the time needed for improving worker agents.", "Let\u2019s consider a simple scenario.", "An agent is unable to collect a certain kind of resource.", "By maximizing its utility, it may still accept the contract and go to that resource.", "Once a resource is occupied by this agent, other agents can no longer collect it according to our setting.", "This means that the resource will never be really collected.", "As an empirical evidence,  you may compare the S2 and S3 settings with S1 in Resource Collection.", "In S2 and S3, workers may prefer a task that it can not perform, which should never happen in the case of maximizing return.", "As a result (shown in Figure 4b and Figure 4c), the training difficult significantly increases.", "3. Are there alternate ways to overcome maintaining the UCB explicitly, especially for the number of time-steps?", "Yes, there are ways to overcome this.", "First, we can define small time intervals instead of maintaining statistics for each step (i.e., combining statistics in every dT consecutive steps will reduce the complexity to 1 / dT of the original size).", "Note that this has been done in results shown in Appendix C.1, where dT also means that for every dT steps, the manager can only change the contracts once.", "Second, we may define a maximum number of steps to be considered in the performance history, which can be determined by the upper bound of the execution time for a subtask, and can be smaller than the step limit of the whole episode.", "4. What are the units for rewards in the plots?", "It is the average per episode.", "The reward is defined as in Section 5.1.1 and Section 5.1.2 without any rescaling.", "We have added this in the caption.", "5. Typos", "Thank you for pointing out these typos. We will fix them in the next revision."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 387, "sentences": ["We thank you for your useful feedback and suggestions for additional experiments, and are glad you found the connection we draw between verification and rare event estimation to be an interesting idea.", "1. \"How does the performance of the proposed method scale wrt scalability? It will be useful to do an ablation study, i.e. keep the input model fixed and slowly increase the dimension.\"", "This is a great question and something we have been looking into.", "As a first step, we have run a new experiment at a higher scale with the CIFAR-100 dataset and a far larger DenseNet-40/40 architecture as discussed in the response to Reviewer 1.", "We see our approach still performs very effectively on this larger problem, for which most existing verification approaches would struggle due to memory requirements (see also our new comparisons in Section 6.4).", "We are now working on doing an ablation study on the size of the input dimension x, but it is unlikely we will be finished with this before the end of the rebuttal period due to the fact that it will require a very large number of runs to generate.", "2. \"Did you experiment with other MH proposal beyond a random walk proposal?\"", "That\u2019s an excellent idea and a topic for future research.", "We didn\u2019t experiment with a MH proposal beyond a random walk because this was the simplest thing to try and it already worked well in practice.", "As well as different proposals, we have also been thinking about the possibility to instead use a more advanced Langevin Monte Carlo approach to replace the MH, which we expect to mix more quickly as the chains are guided by the gradient information.", "3. \"What is the performance of the proposed method against 'universal adversarial examples'?\"", "\u201cUniversal adversarial examples\u201d refers to a method for constructing adversarial perturbations that generalize across data points for a given model, often generalizing across models too.", "Our method does not give a measure of robustness with respect to a particular attack method - it is attack agnostic.", "It measures in a sense the \u201cvolume\u201d of adversarial examples around a given input, and so if this is negligible then the network is robustness to any attack for that subset of the input space, whether by a universal adversarial example or another method.", "All the same, investigating the use of our approach in a more explicitly adversarial example setting presents an interesting opportunity for future work.", "4. \"The most interesting question is whether this method gives reasonable robustness estimates even for large networks such as AlexNet?\"", "This is an important point to address.", "As previously mentioned, we have extended the experiment of section 6.3 to use the much larger DenseNet-40/40 architecture on CIFAR-100 and we see that our method still performs admirably.", "See the updated paper and our response to Reviewer 1 above.", "5. \"Please provide some intuition for this line in Figure 3: 'while the robustness to perturbations of size epsilon=0.3 actually starts to decrease after around 20 epochs.'\"", "The epsilon used during the training method of Wong and Kolter (ICML 2018) is annealed from 0.01 at epoch 0 to 0.1 at epoch 50.", "It\u2019s interesting from Figure 5 that the network is made robust to epsilon = 0.1 and 0.2 by training to be robust using a much smaller epsilon.", "The network appears to become less robust for epsilon = 0.3 as the training epsilon reaches 0.1.", "So this a counterintuitive result that training using a smaller epsilon may be better for overall robustness.", "One hypothesis for this is that the convex outer adversarial polytope is insufficiently tight for larger epsilon.", "Another hypothesis may be that training with a lower epsilon has a greater effect on the adversarial gradient at an input, as the training happens on a perturbation closer to that input.", "6. \"A number of attack and defense strategies have been proposed in the literature.", "Isn't it possible to use the proposed method to quantify the increase in the robustness towards an attack model using a particular defense strategy? If it is possible to show that the results of the proposed method match the conclusions from these papers, then this will be an important contribution.\"", "It is possible to quantify the increase in robustness using a particular defense strategy, as we do in section 6.4 for the robust training method of Wong and Kolter (ICML 2018).", "We find that our method is in agreement with theirs.", "To quantify the increase in \u201crobustness\u201d with respect to a particular attack method, you can simply record the success of the attack method over samples from the test set as the training proceeds.", "This will not, however, be a reliable measure of robustness as the network can be trained to be resistant to the attack method in question while not being resistant to attack methods yet-to-be devised (the adversarial \u201carms race\u201d).", "We believe that what we really desire is an attack agnostic robustness measure, such as the method in our work."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 388, "sentences": ["We thank the reviewer for their encouraging feedback and thoughtful comments on our work.", "Regarding the permutation learning experiment, in response to the feedback, we have revised the main text to clarify the setup.", "The core of the experiment is the ability to denoise permuted images using some representation of the permutation set.", "In order to do this successfully, it is necessary for such a representation to have certain properties such as inducing a distribution over permutations.", "We have implemented and added a comparison to the Gumbel-Sinkhorn method (Mena et al., 2018), which is a customized representation for permutations with these properties, and requires similar techniques (unsupervised objective, permutation sampling, etc.) in order to learn the latent structure.", "The ResNet classifier on top can be viewed primarily as a way to evaluate the quality of the learned permutation; both of these representations are capable of learning the right latent structure, with test accuracies of 93.6 (Kaleidoscope) and 92.9 (Gumbel-Sinkhorn) respectively.", "The highlight of this experiment is that the K-matrix representation also comes with the requisite properties for this learning pipeline, despite not being explicitly designed for permutation learning.", "Regarding comparison to a dense matrix for the speech experiment, in Table 5 (Appendix B.1.2), we compare the use of K-matrices in the raw-features speech model with several other classes of matrices, including dense matrices.", "For instance, we find that, while using a trainable dense matrix slightly outperforms just using the fixed FFT (0.3% drop in test phoneme error rate), using a K-matrix instead of a dense matrix yields a further improvement of 0.8% in the phoneme error rate.", "Regarding ease of training and hyperparameter tuning, we would like to re-emphasize that for all experiments, all hyperparameters for training were kept the same as those for training the default model architecture, other than those we explicitly mentioned as being tuned.", "In particular, we did not modify any hyperparameters (such as number of epochs, optimizer, or learning rate) for the ShuffleNet and DynamicConv experiments.", "For the TIMIT speech experiment, we tune only the \u201cpreprocessing layer\u201d learning rate.", "This is because the default speech pipeline already uses different learning rates for different portions of the network, so there is no clear choice a priori for the learning rate of the \u201cpreprocessing layer\u201d (note that most methods, including K-matrices, do not seem to be overly sensitive to the choice of this learning rate).", "Thus, in these experiments, K-matrices can be used as a drop-in replacement for linear layers without significant tuning effort.", "Regarding structure and sparsity: We use \u201cstructure\u201d in the context of structured matrices to mean matrices with a fast (subquadratic) multiplication algorithm.", "Structured matrices have a sparse factorization with total NNZ on the order of the number of operations required in the multiplication.", "This connection was known in the algebraic complexity community, and formalized by De Sa et al. (2018).", "Regarding the inductive bias encoded by K-matrices: the building block of K-matrices is a butterfly matrix, which encodes the recursive divide-and-conquer structure of many fast algorithms such as the FFT.", "Analyzing the precise effects of the inductive bias imposed by K-matrices is an exciting question for future work."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 389, "sentences": ["Thanks again, Reviewer #1, for your thoughtful comments.", "We respond to your other comments below.", "1.  \u201cIt seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?\u201d", "This is an interesting idea, but we are not sure it is applicable.", "If one looks closely at Figure 2 (b), there are still blue and black histogram bars (denoting CIFAR-10 train and test instances) covering the entirety of SVHN\u2019s support (red bars).", "2.  \u201c[The constant input]\u2019s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn\u2019t seem applicable.\u201d", "See general response #2.", "3.  \u201cHow much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images.\u201d", "We have not tested non-image data, since images are the primary focus of work on generative models, but this is an interesting area for future work.", "4.  \u201cSamples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.\u201d", "This is a very good point.", "See our response to Shengyang Sun\u2019s comment below.", "We see think this phenomenon has to do with concentration of measure and typical sets, but we do not yet have a rigorous explanation.", "5.  \u201cThere seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)\u201d", "We have fixed the spacing in the latest draft :)"], "labels": ["other_context", "other_context", "single_context", "single_context", "single_context", "single_context", "other_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 390, "sentences": ["Thanks for your efforts in reviewing our paper and the valuable comments, but we have different opinions about your comments.", "1. Comments about the contributions and novelty", "As we emphasized many times in our paper, the success of DNN in domains such as image, speech and text, is built on the comprehensive exploration of the locality-based patterns, which motivates us to first find such patterns of features in tabular data automatically and then build up NN architecture based on these discovered patterns.", "This is the core idea of this paper.", "Thus, GBDT is just a tool we adopt to mine the patterns and do feature grouping since GBDT is an efficient and convenient method for these pre-processing tasks: 1) GBDT is very fast.", "In most experiments, the total time cost of GBDT part in TabNN is about several minutes, while the NN part often needs several hours for training.", "2) the learning of GBDT is just based on statistical information over full dataset.", "Thus, GBDT can learn the stable and robust feature combinations.", "We can definitively replace GBDT with other methods, such as feature correlations, as long as they can achieve better performance then GBDT.", "Regarding the comments asking for the comparison with GBDT, we consider that they are not comparable since we are not inventing a model to beat GBDT, instead, we are developing a model to cover the scenarios not suitable for GBDT such as some applications need online updating.", "This point has also been emphasized in our paper.", "2. Heavy feature engineering and ad-hoc practical steps", "We are not sure why you conclude this point.", "TabNN is a fully end-to-end learning approach with no need of an extra feature engineering step.", "And as stated in the paper, the design of TabNN follows two principles: \\emph{to explicitly leverages expressive feature combinations} and \\emph{to reduce model complexity}. We cannot agree there are ad-hoc parts in the proposed model. Could you explain this with more details?", "3. Benchmark Dataset and Compared with Deep and Wide (D&W) NNs", "As stated in Section 2, D&W NNs and many related models can work well with high dimensional sparse features, which are usually in the form of one-hot encoding converted from categorical features.", "Actually, these NNs perform very well in such datasets, even better than GBDT.", "In contrast, the proposed TabNN works better on another kinds of tabular data, with numerical features and low-cardinality categorical features.", "Since there are many dummy dimensions in one-hot encoding, TabNN is hard to learn the useful features combinations from them.", "Therefore, TabNN and D&W NNs are orthogonal with each other.", "We can use them independently according to the feature types of data. And they can be used together for the data with mixed feature types.", "Therefore, we did not conduct any experiment on data with high-cardinality categorical features.", "We will state this clearer in the paper."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 391, "sentences": ["4. \"The experiments of this paper lack comparisons to certified verification", "methods. There are some scalable property verification methods that can give a", "lower bound on the input perturbation (see [1][2][3])", ".", "These methods can", "guarantee that when epsilon is smaller than a threshold, no violations can be", "found.", "On the other hand, adversarial attacks give an upper bound of input", "perturbation by providing a counter-example (violation).", "The authors should", "compare the sampling based method with these lower and upper bounds.", "For", "example, what is log(I) for epsilon larger than upper bound?\"", "The three references and the follow-up work that you cite give different methods for obtaining a certificate-of-guarantee that a datapoint is robust in a fixed epsilon l_\\infty ball, with varying levels of scalability/generality/ease-of-implementation.", "For those datapoints where they can produce such a certificate", ", the minimal adversarial distortion is lower-bounded by that fixed epsilon.", "This is important work to be sure, but we view it as predominantly orthogonal to ours, for which we define robustness differently, as the \u201cvolume\u201d of adversarial examples rather than the distance to a single adversarial example.", "We actively argue that the minimal adversarial distortion is not a reliable measure of neural network robustness in many scenarios, as it is dictated by the position of a single violation, and conveys nothing about the amount of violations present.", "Despite these being two different definitions of robustness, to try and demonstrate some comparisons between the two, we extended experiment 6.4 (already using Wong and Kolter (ICML 2018) [3]) and compared the fraction of samples for which I = P_min to the fraction that could be certified by Wong and Kolter for epsilon in {0.1, 0.2, 0.3}. We found that it wasn\u2019t possible to calculate the certificate of Wong and Kolter for epsilon = 0.2/0.3 for all epochs, or epsilon = 0.1 before a certain epoch, due to its exorbitant memory usage.", "This significant memory gain thus indicates that our approach may still have advantages when used as a method for approximately doing more classical verification, even though this was not our aim.", "Please see the updated paper for full details.", "5. \"Additionally, in section 6.4, the results in Figure 2 also does not look very", "positive - it unlikely to be true that an undefended network is predominantly", "robust to perturbation of size epsilon = 0.1. Without any adversarial training,", "adversarial examples (or counter-examples for property verification) with L_inf", "distortion less than 0.1 (at least on some images) should be able to find.\"", "You are correct that without any robustness training it is possible to find adversarial examples with distortion less than 0.1 for some inputs.", "This is indeed what our results show in Figure 5 in the appendices, illustrating our metric for individual samples.", "You can see for several samples that were not initially robustness to eps=0.1 perturbations (log(I) > log(P_min)), the value of log(I) decreases steadily as the robust training procedure is applied.", "It does appear, however, that the network is predominantly robust to perturbations smaller than 0.1 before robustness training.", "The curves in Figure 3 plot the values of our measure log(I) between the 25th and 75th percentile for a number of samples.", "This shows that the network is already robust to perturbations of size eps=0.1 for more than about 75% of samples before the training procedure of Kolter and Wong is applied.", "All the same, we agree that the original Figure 3 was confusing in this respect, and have rerun this experiment with a lower minimum threshold for log(I) to make the point clearer in the graph.", "With this lower value of log(P_min), we see the 75 percentile of log(I) over the samples quickly decrease as robustness training proceeds for eps=0.2.", "Notably, however, log(I) is incredibly small before any of this training for eps=0.1, demonstrating how it is important to not only think in terms of whether any violations are present, but also how many: here less the proportion of violating samples is less than 10^-100 at eps=0.1 for most of the datapoints.", "We thank Reviewer 1 for their critical appraisal and helpful suggestions."], "labels": ["multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 392, "sentences": ["Thanks for your efforts in reviewing our paper and the valuable comments.", "We attempt to address your concerns using the following points and hope they can help you better understand our work.", "1. the number of benchmark datasets", "Currently, there are 5 datasets in our experiments.", "Actually, we have evaluated the proposed methods by conducting experiments in many datasets and observed the similar results.", "Due to the space restriction, however, we cannot present them all in the paper.", "We can provide more experiment results in appendix to eliminate this concern.", "2. XGBoost", "We use LightGBM to learn GBDT model in the experiment part.", "LightGBM is proven comparable (even better) with XGBoost in many Kaggle competitions (refer to https://www.kaggle.com/shivamb/data-science-trends-on-kaggle and https://github.com/Microsoft/LightGBM/tree/master/examples).", "Therefore, we think using LightGBM is sufficient for comparison.", "3. Two shortages of tree-based models", "Let us describe these two shortages with more details here.", "1) Hard to be integrated into complex end-to-end frameworks.", "In such framework, there are many modules, each of which may correspond to one sub-task with a global optimization goal.", "The outputs of modules can serve as the inputs of other modules.", "Therefore, to train such a framework in an end-to-end way, the module should be able to propagate the errors from its outputs to its inputs.", "NN can naturally support this, as its learning algorithm is the back-propagation.", "In contrast, tree-based models do not support this as the tree learning process is not differentiable and therefore cannot propagate the errors to its inputs.", "As stated in the Section 2, although there are some works targeting to address this problem, these solutions will lose the automatic feature selection ability and cannot work well on the tabular data.", "2) Hard to learn from streaming data.", "For NN's learning, we can use Stochastic Gradient Descent (SGD) or mini-batch SGD to naturally learn from streaming data, since the NN model could be updated per data sample or per mini-batch of samples.", "However, it is not effective for tree-based model to support this as its learning needs the global statistical information.", "Using the partial statistical information may produce the sub-optimal split points and results in worse models.", "There are some works addressed this problem, like Hoeffding trees, which stores the statistical histograms into leaf nodes.", "However, most of these solutions are designed for the single decision tree.", "Although there are ensemble versions of them, most of them are based on bagging (like Random Forest), which is proven not as good as GBDT.", "In short, NN does not suffer from these two problems due to its mini-batch back-propagation learning process.", "In contrast, tree-based model is hard to solve these two problems due to its learning algorithm is based on global statistical information.", "Therefore, TabNN is a better general solution for tabular data."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 393, "sentences": ["We thank the reviewer very much for reading the paper carefully and providing us with constructive comments.", "We have conducted further experiments applying our Past Decode Regularization (PDR) to the mixture-of-softmax (AWD-LSTM-MoS) model of (Yang et al. 2017).", "We use the same model sizes as in the paper.", "Even with the very limited hyperparameter search in the vicinity of those used in the paper and fixing the PDR loss coefficient to 0.001 (as used in the other models in our paper), we see consistent gains on the Penn Treebank and WikiText-2 datasets.", "The validation/test perplexities are as follows -", "AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)", "Penn Treebank with finetuning -", "56.2/53.8", "||  56.5/54.4", "Penn Treebank with dynamic evaluation -", "48.0/47.3", "||  48.3/47.7", "WikiText-2 with finetuning -", "63.0/60.5", "||  63.9/61.5", "WikiText-2 with dynamic evaluation -", "42.0/40.3", "||  42.4/40.7", "Thus we observe gains of 0.6 and 1.0 points in test perplexity for PTB and WT2.", "With dynamic evaluation, the gains for both datasets is 0.4 points.", "Note again that we did a very limited hyperparameter search and more exhaustive experiments will likely lead to even better gains by using PDR.", "We will update and reorganize the experiments section in the paper accordingly.", "The updated manuscript will be posted shortly.", "Yang et al. 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv:1711.03953."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 394, "sentences": ["Response: We thank the reviewer for their comments.", "We would like to clarify one of the central points of this paper, as the cons presented are built upon a misunderstanding of this point.", "We are not proposing a new transfer learning model -- we are demonstrating the transferrability of the atomic features we have learned.", "We train our structural features on C_r and show that with no re-training they can achieve state-of-the-art results of C_p.", "Applying a classical transfer learning algorithm might improve performance even further, as then we could fine-tune results on C_p.", "Though this is an interesting direction, it is outside the scope of the work we present here, which concerns itself with the learned representations themselves.", "Thus, instead of comparing transfer learning methods, we evaluate the transferrability of both our own structural features as well as those of competitors."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 395, "sentences": ["We thank the review for their positive comments.", "We agree that our main contribution is to cast a graph convolution network as a binary classifier learning to discriminate clean from noisy data and show its excellent results for few-shot learning.", "Q1: In future, I would like to see a joint approach to such training, where the function g(), the nearest neighbour graph loss and the classification loss are all tied in the same objective function and are optimized jointly.", "R1: We fully agree that this is an interesting direction for future research, which should result in a further increase in performance.", "It would be interesting to see if the feature extractor can benefit from the cleaning of noisy images during learning, resulting in more robust feature descriptors."], "labels": ["global_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 396, "sentences": ["We thank the reviewer for the detailed and thoughtful review.", "We address the reviewers main concerns.", "\u201cA more general function is $P(X)_i=Ax_i+\\sum_{j\\in N(x_i,X)} B(x_j,x_i) x_j + c$, where $N(x_n,X)$ is the set of index of neighbors within the set\u2026 Then, can the function family the authors used in the paper approximate this function?", "No.\u201c", ">> We respectfully disagree.", "The function P(X) described by the reviewer can be approximated arbitrarily well using a continuous equivariant function (by using bump functions to approximate indicators functions of neighbors).", "As such it can be approximated with the universal models considered in this paper.", "(e.g. PointNetST, DeepSets etc.)", "Reiterating the main result of this paper: *Every* continuous equivariant function defined solely on a set of feature vectors can be approximated with PointnetST over a compact domain.", "We are happy to include a discussion about this in the revised paper.", "\u201cOn the Knapsack test, the metric of interest is not the accuracy of individual prediction. Rather, whether the network has successfully predicted the optimal solution, or how close the prediction is to the solution.\u201d", ">> Thank you for this comment.", "We will revise the Knapsack graph to show the success metric suggested by the reviewer.", "We remark that the goal here was not to construct a network that solves the Knapsack problem but to demonstrate the difference between universal and non-universal models.", "\u201cSpare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference\u201d", ">> The definition of DeepSets appears in Equations 7 and 8.", "The PointNetSeg model is described in detail in the discussion before the proof of Corollary 1.", "We will make the definitions clearer.", "\u201cLemma 3 is too trivial.\u201d", ">> We agree it is trivial (and indeed the proof is a one-liner).", "If the reviewers feel strongly about this, we can move it to appendix, however we feel it helps to provide a complete picture.", "\u201cP.2 power sum multi-symmetric polynomials. \"For a vector and a multi-index", "...\" I think it was moved out of the next paragraph", "since  the same is defined again as again in the next sentence.\u201d", ">> Thank you. We will make this clear."], "labels": ["global_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 397, "sentences": ["Thank you for the comments, please find our responses to specific points below.", "[Q] \u201cAs far as I can see the most important take home message of the paper can be summarized in \"one should consider non-saturating GAN loss and spectral normalization as default choices [...] Given additional computational budget, we suggest adding the gradient penalty [...] and train the model until convergence.\"", "[A] While we want this study to be approachable by non-experts, some level of formalism is required as our main audience are researchers working on or interested in GANs.", "The summary you provided is indeed correct -- coupled with our open-sourced code, it allows a non-expert to train a GAN with state-of-the-art methods without needing to understand the details.", "On the other hand, for more experienced researchers, we provide more details on which design choices generalize to new settings and identify the biggest obstacles towards fair and unbiased quantitative evaluation of generative models.", "[Q] Limited amount of new insight.", "[A] Our paper presents many useful insights, namely: NS-GAN performs well, spectral norm is a good default normalization technique, gradient penalty should also be considered, even in combination with spectral norm but will cost substantially more in terms of computational resources, popular metrics such as KID and FID result in the same relative ordering of the models so there is no point in computing both, most Resnet tricks do not matter, etc.", "All of these insights are supported by a fair and unbiased rigorous experimental process.", "On top of that, our experiments are reproducible (as already reported by other works), we shared the resulting code and the pre-trained models.", "[Q] Clarification and exposition of plots.", "[A] Say that you had access to a GPU and had to train a model (loss+penalty+architecture).", "How many hyperparameter settings would you need to consider to achieve a certain quality?", "The FID from the plot is the estimate of the min FID computed by bootstrap estimation and the line-plots show this relationship.", "In other words, given a computing budget, which model should you pick?", "We will provide additional details in the caption of the plot.", "[Q] Bayesian optimization and variance.", "[A] We agree and will provide more details.", "When the sequential Bayesian optimization chooses the next set of hyperparameter combinations to test we run the model once (per hyperparameter combination) and report the scores to the optimizer.", "Then, the optimization algorithm takes these scores into account when selecting the next set of hyperparameters.", "The algorithm itself trades-off exploration and exploitation and it can explore hyperparameters \"close\" to the existing ones if they seem promising.", "Hence, the averaging happens implicitly during the search.", "[Q]: Studies and experiments. Stating that lower is better in the plots.", "[A]: Study is a set of experiments (say a study on the impact of the loss).", "Experiment is a concrete run with certain hyperparameters.", "Stating lower is better is a good idea, we will add this to the captions."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 398, "sentences": ["We thank the reviewer for the comments.", "We justify the novelty and significance of the contributions made by this paper as follows.", "1) Novelty of the convergence analysis: The paper by Yuan et al. did not present proof of convergence in the discrete-time setting.", "The authors only provided convergence of the ODE models.", "On the other hand, convergence analysis of momentum methods in non-convex setting is an important but under-explored area  (Yan et al., 2018).", "In the current paper, the convergence results are proved for non-convex objective functions satisfying mild assumptions.", "Appropriate use of some sharp estimates allowed us to obtain concise bounds on convergence of the entire class of PoweredSGD methods for $\\gamma\\in[0,1]$ and the bounds continuously depend on parameters $\\gamma$ and $\\beta$. In the special cases ($\\gamma=0,1$, $\\beta=0$), these bounds matches the best known bounds for GD/SGD/SGDM in the non-convex setting.", "More specifically, we would like to draw the reviewer's attention to the following two papers:", "*", "[Yan18] Yan, Y., T. Yang, Z. Li, Q. Lin, and Y. Yang. \"A unified analysis of stochastic momentum methods for deep learning.\" In IJCAI International Joint Conference on Artificial Intelligence. 2018.", "*  [Bernstein18] Bernstein, Jeremy, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar. \"SIGNSGD: Compressed Optimisation for Non-Convex Problems.\" In International Conference on Machine Learning, pp. 559-568. 2018. (Theorem 3)", "We emphasize that our theoretical analysis leads to significant more concise convergence bounds than those in the above papers.", "Please take a look at Theorems 1 and 2 in [Yan18] and Theorem 3 in [Bernstein18].", "We have highlighted the technical challenges we overcome in order to obtain these results in the updated version (please see Remark 3.3).", "2) Novelty of experiments: The current paper presents substantially more comprehensive experiments for benchmarking the proposed class of optimizers against other popular optimization methods for deep learning tasks.", "In particular, we highlight the experiments on vanishing gradients and learning rate schedules.", "This, in addition to the potential to accelerate initial convergence, makes the proposed PoweredSGD methods useful in many potential applications."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 399, "sentences": ["Thank you for the thoughtful comments. We are glad that you found our problem interesting, and problem formulation/applications of this research well explained.", "Regarding the competing algorithms:  Both algorithms that we compare to, Count-Sketch and Count-Min, are state-of-the-art hashing-based algorithms (see e.g., Cormode & Hadjieleftheriou (2008)).", "Further, they are widely used in practice for processing internet traffic, large databases, query logs, web document repositories, etc.", "To the best of our knowledge, our paper is the first to use machine learning to design better sketches for any streaming problem.", "We tried to cover related work thoroughly in section 2."], "labels": ["multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 400, "sentences": ["1. One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.", "For example, it would be great if the author could demonstrate that without this loss term the distribution of the predicted classes is wrong in the experiments from Section 4.", "A: We actually ran this experiment, where we monitored the KL divergence between the marginal distribution of model predictions and the true marginal distribution of labeled data over the course of training (with and without distribution matching).", "We added the results of this experiment to the appendix of the latest revision.", "2. For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.", "A: For space reasons we provided only a short description of CTAugment, and how it differs from AutoAugment. We will include a longer treatment in the appendix.", "3. On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it\u2019s being reported as 6.24.", "What is the reason for the difference?", "A: The 4.95 error rate in the MixMatch paper is in Table 1 which is the result when using a larger model (26 million parameters).", "Our results are comparable to the WRN-28-2 results (as used in the \"Realistic Evaluation of Semi-Supervised Learning Algorithms\" paper), as seen in Table 5 of the Appendix of the original MixMatch paper.", "4. Another paper, [2], reports very competitive results on CIFAR-10 for 4k labels.", "A: We will include a discussion of this paper in the revised manuscript.", "Similar to the comment on MixMatch above, we only use small models with 1.5 million parameters compared with the 26 million parameters in SWA.", "We chose this experimental setting because it simplifies comparison to existing results, as argued in \"Realistic Evaluation of Semi-Supervised Learning Algorithms\"."], "labels": ["multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 401, "sentences": ["We would like to thank you for reviewing our paper.", "[Unguided Case and Disentanglement] Please refer to our general comment above on why our unguided case performs better now.", "We also updated our \u201cProbabilistic Interpretation\u201d section with analysis on how the contrastive loss helps us to learn a disentangled representation.", "Evidence and comparison to other methods on disentanglement is provided in  Table 9 in Appendix G, where we visualize the correlations between our embedding dimensions.", "[Experiments Section] We have significantly updated qualitative and quantitative results in our \"Experiments\" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 402, "sentences": ["Thank you for the time and effort spent reviewing our paper.", "We are glad you liked the paper.", "We want to emphasize one point that we perhaps did not highlight enough in our paper: there are other existing algorithms that fall into the marginal policy gradients framework.", "Specifically, researchers and practitioners both almost always clip actions for use in robotics control environments (read: MuJoCo tasks).", "Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces.", "Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten existing analyses of variance reduction that can be achieved for clipped actions.", "To respond to your question, yes it is possible (e.g. the example given above), but their is no general procedure that we know of to derive such methods.", "Rather, this would be done on an action space by action space basis"], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 403, "sentences": ["We thank the reviewer for their detailed review and for their suggestions.", "We answer point by point:", "*FW vs BCFW*", "The (primal) proximal problem is created for a mini-batch of samples, and not for the entire data set (details in section 3.2).", "In other words, the primal problem consists of the proximal term which encourages proximity to the current iterate, the linearized regularization, and the average over the mini-batch of the losses applied to the linearized model.", "As a result, we can compute the Frank-Wolfe update for all dual coordinates simultaneously, and we do not need to operate in a block-coordinate fashion.", "We have included this clarification in the new version of the paper.", "*", "Batch-Size*", "We thank the reviewer for this suggestion.", "We have adapted the description of Algorithm 1 accordingly.", "*Convex-Conjugate Loss*", "In order to compare the DFW algorithm to the strongest possible baselines, we choose the baselines to use the CE loss in the CIFAR experiments.", "Indeed we have generally found CE to help the baselines in this setting.", "In addition, the hand-designed learning rate schedule of SGD and the l2 regularization were originally tuned for CE.", "In the case of the SNLI data set, we allow the baseline to use either CE or SVM because using the hinge loss can increase their performance.", "Finally, we choose to always employ the multi-class hinge loss for DFW because it gives an optimal step-size in closed form for the dual, which is a key strength of the formulation.", "*BCFW vs BCD*", "We thank the reviewer for this recommendation.", "It would be interesting indeed to explore how to exploit such updates in the context of the composite minimization framework for deep neural networks.", "In our case, we emphasize that for speed reasons, it is crucial to process the samples within a mini-batch in parallel, and this does not look straightforward with the algorithm in [3, E.3].", "Therefore we believe that for this setting the FW algorithm permits faster updates thanks to an easy parallelization over the mini-batch on GPU.", "*Hyper-parameter*", "Counting a single hyper-parameter for SGD implicitly assumes that SGD can employ a constant step-size.", "Using such a constant step-size for SGD would incur a significant loss of performance (e.g. at least a few percents on the CIFAR data set).", "Therefore in order to obtain good performance, SGD requires a manual schedule of the learning rate, which involves many hyper-parameters to tune in practice."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 404, "sentences": ["Thanks for your insightful comments.", "1. How or why is the benefit.", "This comment is insightful and we also considered about it.", "Intuitively, we would easily fall into the connections between each sentence and image.", "However, it is nearly impossible to pair sentence with images with completely the same meaning all the time.", "According to our investigation, we conclude that the major contribution would be more effective contextualized sentence encoding for better representation from the visual clue combination instead of single image enhancement for encoding each individual sentence or word.", "According to Distributional Hypothesis (Harris et al., 1954) which states that \u201cwords that occur in similar contexts tend to have similar meanings\u201d, we are inspired to extend the concept in multimodal world, \u201cthe sentences with similar meanings would be likely to pair with similar even the same images\u201d, where the consistent images (with similar topic) could play the role of topic or type clues for similar sentence modeling.", "For your example, the topic words are {private, courts, table}, which can be paired with relevant images and other sentences with the same (similar) topics will be paired with the same (similar) group of images.", "This is also very similar to the idea of word embedding by taking each image as a \u201cword\u201d.", "Because we use the average pooled output of ResNet, each image is represented as 2400d vector.", "For all the 29,000 images, we have an embedding layer with size (29000, 2400).", "The \u201ccontent\u201d of the image is just like the embedding initialization.", "It indeed makes effects, but the capacity of the neural network is not up to it.", "In contrast, the mapping from text word to the index in the word embedding is critical.", "Similarly, the mapping of sentence to image in image embedding would be essential, i.e., the similar sentences (with the same topic words) tend to map the similar images.", "To verify the hypothesis, we shuffle the image embeddings but keep the lookup table, to only exchange the features of each image but maintain the sentence-image mapping.", "Unsurprisingly, the BLEU score (EN-RO) is 33.53, which is very close to the reported one (33.78).", "In addition, we randomly initialize the image embedding instead of ResNet, the result is 33.28.", "In comparison, if we randomly retrieve unrelated images to break the lookup, the result is 32.14.", "These results verify the necessity of the lookup table.", "We have added a detailed discussion in the paper (please see Analysis 6.1).", "We believe this finding would be suggestive for the future research since most previous work focused on the content of the image itself.", "As a different research line, we highlight the consistency among the mono-modality to bridge the gap of language and image modeling.", "2. Why stop words are ignored.", "According to the explanation above, we think the spatial relations or grammatical nuances would not be so important in this task if we take the images as topic guidance.", "Ignoring the stopwords can help us get rid of the disturbance of unnecessary high-frequency words (such as function words) being the topic, as the standard practice for TF-IDF topic extraction.", "3. Comparison of different feature extractors.", "Yes. We compared with ResNet101 and ResNet152 on EN-RO.", "The BLEU scores are 33.63 and 33.87.", "It seems deeper ResNet indeed gives better results but the difference is not very significant."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 405, "sentences": ["Thank you for your comments and suggestions.", "We respond to your questions and concerns as follows.", "1. Connection with principal-agent problems.", "Thank you for pointing this out.", "We really appreciate it.", "The problem we address is indeed closely connected to principal-agent problems, or moral hazard problems in economics, which considers whether the agent makes the best choice for what the principal delegates (e.g., a plumber might make more money by suggesting an overhaul rather than a short-term fix).", "In this setting, there are a lot of issues to be modeled, e.g., information asymmetry between principals and agents, how to setup incentive cost, how to infer agents\u2019 types and how to monitor their behaviors, etc.", "Traditional approaches [1] in economics build mathematical models to address these issues separately, leading to complicated models with many tunable parameters.", "In comparison, our paper provides a practical end-to-end computational framework to address this problem in a data-driven way, once the agents\u2019 utility function is written down as a combination of principal\u2019s request and its own preference (Eqn. 1).", "Moreover, this framework is adaptive to changes of agents\u2019 preferences and capabilities, which very few papers in economics have addressed.", "Because of the connection to principal-agent problems and the data-driven nature of the proposed method, there could be a broad number of practical applications.", "We will incorporate a more thorough literature reviews in the next revision.", "[1] The theory of incentives: the principal-agent model, Jean-Jacques Laffont, 2001", "2. More details should be given on the mind tracker module.", "We will explain more implementation details in the appendix in the next revision.", "We will also release the code.", "3. Is it necessary to use deep reinforcement learning for contract generation?", "As stated in the introduction, one of the main points of this work is about incomplete information.", "I.e., we do not know the true agent models and their mental states, and also do not assume that the task dependency is known.", "In real world problems, we indeed can not assume that a manager knows the exact nature of other agents.", "So we want to train a manager that can quickly model worker agents through observations and simultaneously generate optimal contracts.", "In contrast, traditional methods do not consider task dependency, and usually assume agent types are either known or follow a given distribution.", "Also, deep models are flexible enough to handle complicated interactions between agents and changes of settings.", "Thus, deep RL is a more suitable approach than traditional methods under the incomplete information setting."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 406, "sentences": ["We thank the reviewer for the comments.", "We have revised the paper according to the suggestions and would like to clarify several things:", "Q1. Evaluation Time:", "We have added the detailed running time for each component in Table 3 in Appendix A of the revised version.", "Q2. Implementation Details:", "We will share all the source code to make sure it is reproducible.", "Meanwhile, we have included more details as suggested in Appendix A, including a visualization of all layers of the different parts of the network. If 1-2 extra pages are allowed, we can include those details to the paper.", "Q3. Figure 1 is too abstract:", "We have updated the figure to make it more intuitive and contains more details.", "Q4. The top row of Figure 2b is confusing:", "We apologize for the confusion caused.", "Shown at the top row of Figure 2b are not three consecutive frames.", "They are the R, G, B channels of a single frame.", "To avoid confusing, we use different colors for them and explained that in the figure.", "Q5. How the first camera pose is initialized?:", "All the camera pose including the first camera are initialized with identity rotation and zero translation, which are aligned with the coordinate system of the first camera.", "We clarified this at the end of Section 4.3 in the revised version.", "Q6. Evaluation metrics are not clear:", "To facilitate comparisons with other methods, we use the evaluation metrics in previous works in Table 1 and 2, so that we can cite the results of previous methods.", "As we described in the paper, the depth metric are the same as Eigen and Fergus (2015).", "The translation metrics(ATE) are the same as [Wang et al. 2018, Zhou et al. 2017].", "In the revised version, we briefly introduce the definition of these metrics at the beginning of each paragraph in Section 5.2.", "Q7. Attention should be given to the notation in formulas (3) and (4):", "We changed the parameters from \u2018d\u2019 to \u2018d \\cdot p\u2019 which is a 3D point.", "We also removed the redundant subindex \u20181\u2019, because all points \u2018q\u2019 are on the first frame.", "Q8. Terminology consistency through the paper:", "Thanks for the suggestion.", "We consistently use the term \u201cfeature-metric BA\u201d and \u201cbasis depth maps\u201d through the paper now.", "Q9. Typos, Grammar, Format, and Bibliography:", "Thanks for pointing them out. We have revised the paper to fix these problems."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 407, "sentences": ["*All or at least some of these decisions would need to be relaxed to make a convincing paper.", "you are right, even if the focus of the paper is not on getting the best possible score on language modelling, different settings would make this point not only more convincing, but clearer.", "*The reasons for the use of the energy-based formulation are not clear to me.", "Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?", "This formulation is fundamental for the next step of the work in which we are removing the restrictions from the output layer and learning word probability distributions without prior knowledge of the vocabulary size.", "That said, the formulation is just the re-use of the embedding layer transposed.", "It removed an entire set of m x V parameters and got us better results in all our experiments so we decided to use it.", "* It looks like all the results are given on the test set. Did you not do any tuning on the validation data?", "Yes, all the parameters were tuned on the validation data.", "All the models were selected according to their validation data evaluation.", "The early stop criterion is also based on the validation data evaluation.", "We consider the model to converge when it cannot improve further on validation data.", "The models never saw the test set during training or tuning, otherwise we would be cheating and these scores would be irrelevant to compare different settings."], "labels": ["multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 408, "sentences": ["Thank you for the detailed review and constructive remarks.", "Below are answers to the main points that were commented as well as updates on the current work.", "* Sound quality is disappointing and with artifacts:", "We are working on Fast Spectrogram Inversion using Multi-head Convolutional Neural Networks, arXiv:1808.06719, Sercan O. Arik et al. to replace Griffin-Lim inversion ; two possible improvements we expect are much faster (towards real-time) sound rendering and better audio quality.", "We are also working on mini-batch MMD latent regularization (Wasserstein-AE) instead of per-sample KLD regularization (VAE) which may result in improved generalization power and generative quality.", "* Not suited to transfer from audio without label:", "If the audio carries a note information, it can be easily/automatically extracted in the form of pitch tracks as we did for transferring on instrument solos.", "Some audio data do not have note qualities, which are out of the current training setting.", "For that we have been training unconditioned one-to-one models or solely instrument conditional many-to-many models that do not require any note information.", "But we are working on models which incorporate an unconditioned processing option (eg. training while zeroing the one-hot conditioning or adding an entry in the input embedding of FiLM which is the unconditional state) to be trained on a dataset that mixes conditional and non conditional audio (eg. adding instrument solo sections which in parts have a clear pitch track and in others none).", "* A fully convolutional model would process arbitrary length of audio:", "We use the linear layers to set the latent space dimensionality, when processing various length audio sequences, each encoding amounts to about 120ms context and we resynthesize with overlap-ad that mirrors the short-term input analysis ; this process was used when transferring on the instrument solos (a task that was beyond the training setting).", "* Insufficient justification of the 3D latent space:", "At first we validated that our models could perform well in term of training/test spectrogram reconstructions with only 3 latent dimensions, some reasons that we found interesting to enforce this are more related to a possible music/creative application of the model: less synthesis/control parameters for the user (and controls which may then be more expressive), direct visualization of the latent space which is turned into a 3D synthesis space from which users may draw and decode sound paths or create other interaction schemes, a denser latent space that may be better suited for random sampling/interpolations.", "The direct interaction with 3D latent space becomes even more interesting when we pipeline our model with fast-spectrogram inversion.", "* Interesting incremental comparison in one-to-one transfers:", "We keep working on more detailed benchmarks/comparisons that would equally cover one-to-one and many-to-many model variations and that would integrate the new features we are testing.", "* All claims about running time should be corroborated by controlled experiments:", "Indeed we didn\u2019t benchmark yet our models on Nsyth and our approach differs from others such as Mor et al. that report using \u00ab\u00a0eight Tesla V100 GPUs for a total of 6 days", "\u00bb", ".", "From the beginning of our experiment we aim at a much lighter-weight system that could be trained/used more broadly (eg. with a single mid-range GPU).", "The computational cost difference is not rigorously estimated on a same given dataset/task to learn but still we think it is relevent to point that the results we report can be achieved in less that a day on a single Tesla V100 GPU.", "* Why does the MMD version constitute an improvement? Or is it simply more stable to train?", "It is more stable to train, it does not require the extra \u2018cost\u2019 of an auxiliary network training and it can generalize to many-to-many transfer without requiring as many adversarial networks.", "About the significance of score differences, we agree that it needs more details and comparisons, it was also noted by \"AnonReviewer1\" and we should make alternative tests to scale or give a few more references to the benchmark.", "* \"FILM-poi\" .. is this a typo ?", "Thank you for pointing this as well as your other remarks on the writing and use of precise terms/phrases.", "Indeed this is right, we mixed poi/pod but both refer to many-to-many conditioning on pitch+octave+instrument/domain classes.", "We also thank you for pointing more literature to improve our references and discussions to related works."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "other_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 409, "sentences": ["Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:", "4.", "Note that the training data is not constrained with respect to ratios and number of objects addressed by the caption, so the learned behavior should be independent of these aspects.", "Moreover, note that for most ratios there is only one combination of numbers with at most 15 objects in total, but larger images fitting a greater total number of objects would definitely be an option here.", "For the less close-to-balanced ratios 1:2, 2:3, 3:4 where there are multiple possibilities, performance generally is (close-to-)perfect, indicating that there is no increased difficulty of learning multiples in the presence of more close-to-balanced ratios (for instance, 6:9 vs 7:8).", "We hope this clarifies your concern.", "5. We fully agree that it would be very interesting to investigate these models.", "For this paper, we decided to focus on the methodology of investigating such questions in detail (the evaluation for FiLM alone comprises around 100 experiments) as opposed to focusing on the comparison of behavior of different models, and leave the latter to future work.", "We added a few additional sentences to section 3.2 regarding that.", "6. We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer.", "We didn't think about the fact that one may want to control which strategy is learned, which would indeed be interesting, but that's why we considered FiLM as is and didn't experiment with changing architecture details.", "At the same time, considering that understanding \"most\" is only one of many capabilities a VQA model is supposed to learn, these results are unlikely to be an important influencing factor for architecture choice, while at least knowing about the properties of a model is nonetheless interesting.", "7. The evaluation is supposed to show what an architecture is capable of learning under \"ideal\" conditions.", "It's an interesting question whether/how this changes when gradually shifting towards \"less ideal\" setups.", "An advantage of using a controlled setup like ours is that this is possible to investigate, to some degree at least (for instance, add more types of captions to the training data, not just quantifier statements).", "At some point we may be interested in actually investigating the same for real-world data, but we think it's unclear right now what exactly such evaluation data should ideally look like, what problems are most interesting, what details to pay attention to. Artificial data allows us to investigate these questions while avoiding the elaborate and expensive process of obtaining real-world data.", "8. Note that the training data is far less constrained than the evaluation data, including various distracting aspects like additional shapes/colors.", "The evaluation data doesn't contain such distractors, but it would of course be possible (and potentially interesting) to add such.", "We didn't do so since we considered instances with only \"relevant\" attributes to be the most difficult setup, like a minimal pair, where a model is required to focus on all objects and both their shape and color attribute to decide correctly.", "9. We incorporated the changes as you suggested.", "Thanks!", "10. We didn't think about this interpretation -- our intention was to signal that we take the \"The Meaning of 'Most'\" setup and methodology of Pietroski et al. from psychology, and implement a deep learning version for visual question answering models."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "global_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 410, "sentences": ["We thank the reviewer for a careful reading of the paper and the constructive comments.", "Although we proposed Past Decode Regularization (PDR) with language modeling in mind to exploit the symmetry between the input and output vocabulary (and the corresponding word embedding and softmax layer), any model/task that has this symmetry can potentially use a PDR term.", "As suggested by the reviewer, models for tasks like text summarization and neural machine translation (using a byte-pair encoding vocabulary as in Ofir & Wolf 2016) that use an encoder/decoder seq2seq architecture can benefit from PDR and is a topic of future research.", "We will incorporate this discussion in the updated version of the paper.", "We can justify PDR theoretically as an inductive bias on the language model.", "The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform.", "Similarly, the distribution of the first word given the second word will be far from uniform.", "A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one.", "In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the \"true second word\" and decode the distribution of the first word.", "Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram.", "Finally, we have conducted further experiments on larger corpora, specifically the Gigaword corpus.", "We use a 2-layer LSTM with a word embedding dimension of 1024 and hidden dimension of 1024.", "We truncated the vocabulary by keeping approximately 100k words with the highest frequency.", "We compare the performance of the model with and without PDR and using no other regularization.", "We used the same validation and test sets as (Yang et al. 2017).", "We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 in the test perplexity by using PDR.", "We will incorporate these results in the experiments section and post the updated manuscript shortly.", "Press, Ofir, and Lior Wolf. \"Using the output embedding to improve language models.\" arXiv preprint arXiv:1608.05859 (2016).", "Yang, Zhilin, et al. \"Breaking the softmax bottleneck: A high-rank RNN language model.\" arXiv preprint arXiv:1711.03953 (2017)."], "labels": ["global_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 411, "sentences": ["We thank the reviewer for their comments.", "We address their comments individually below.", "> My overall concern is that the experiment result doesn\u2019t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn\u2019t really fit in the \u201ctransfer\u201d learning scenario.", "Response: Our work is indeed not classical transfer learning -- it is in fact an even stricter variant.", "We do not re-train the parameters of the neural network at all using C_p, which is typically done as a \u201cfine-tuning\u201d step in the transfer learning scenario.", "So while we do use C_p^{val} for model selection (i.e., hyperparameter tuning), this is still much less use of the data-poor dataset than in the common transfer learning setting of actually fine-tuning the parameters of the neural network using a subset of the data from the data-poor dataset.", "The use of C_p^{val} for hyperparameter tuning was incidental and not a central point of our paper.", "To really make this clear, we have updated the paper to demonstrate that even if we do not use C_p^{val} for model selection, and instead select from the same class of models we previously generated by using a randomly selected held-out set C_r^{val}, we still obtain state-of-the-art performance (0.892 [0.885 +/- 0.009]).", "In this formulation, C_p is not used at all by our method until test time.", "> Also, the compared methods don\u2019t really use the validation set from the complex data for training at all.", "Thus the experiment comparison is not really fair.", "The competing models do make use of validation set C_p^{val} from the complex data to select amongst the most important hyperparameters of their model -- which is equivalent to what we did in our initial formulation, and favors the competing methods compared to if we use C_r^{val} for hyperparameter search instead.", "> 2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn\u2019t include any significance of the sampling.", "Specifically, the testing dataset is fixed.", "A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.", "Response: The replicates correspond to different training and validation samples of the enriched set -- we have clarified this in the paper.", "While it is true that the hyperparameter validation set was initially fixed, the switch to use C_r^{val} as above resolves this.", "The testing data C_p^{test} is that which has been used in the prior works we compare to (Fout et al. 2017; Sanchez-Garcia et al. 2018).", "Furthermore, use of this subset for performance evaluation is justified as as C_p^{test} corresponds to latest released structures in C_p, leading to a more accurate assessment of how such methods would perform on unreleased structures (as they do no sequence identity pruning).", "Thus our experimental set up", "is rigorous and justified.", "> Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.", "Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can\u2019t capture while SASNet can.", "Response: As we discuss above, we believe our experimental setup and analysis is sufficient to demonstrate that our atomic representation transfers much better across atomic tasks.", "We have also added to our discussion, making clear that our method represents a significant advantage over competing methods when detailed atomic information is available.", "Competitors rely on amino acid-level features that fail to capture specific atomic positions but can be better when the structural is less detailed or accurate.", "> Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.", "Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.", "Response:", "As our paper is primarily about the power and transferrability of our structural features for atomic tasks, we believe a detailed investigation of non-structural features is mostly outside of the scope of this work.", "To show that we can easily include these features, we have included in our appendix some results including non-structural features.", "When adding in the sequence features used by Fout et al. via a simple linear model combining our final hidden layer and the additional sequence features, we are able to achieve a superior performance of 0.921 (0.914 +/- 0.009) versus their performance of 0.896 (0.894 +/- 0.004).", "While BIPSPI (Sanchez-Garcia et al. 2018) does achieve the best combined performance at 0.942, they also use additional sequence correlation features (note their structure-only performance is comparable to that of Fout et al).", "> Some discussion on why the \u201cSASNet ensemble\u201d would yield better performance would be good; could it be overfitting?", "Response: We have removed the SASNet ensemble from the paper, as it was based on C_p^{val} and confuses the point we are making about minimally relying on C_p for training and validation.", "We could definitely investigate further why this mild ensembling yields a small performance increase, but we see this as tangential to the overarching points of the paper."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 412, "sentences": ["Thank you for your review and the overall positive assessment.", "In particular, we are delighted that you see the potential of the stethoscope framework lending itself to much broader applications.", "We agree - particularly for applications regarding the interpretability of deep representations as well as the manipulation of biases contained therein.", "This is a key consideration in our belief that it makes for a valuable contribution to the community.", "We have opted, in this submission, to focus on our primary application domain, which is intuitive physics.", "In particular, we demonstrate the efficacy of neural stethoscopes in interpreting, promoting and suppressing specific information in the context of the complex interplay between visual clues and physical properties in stability prediction.", "Our work is primarily motivated by the question as to what extent neural networks learn about physical principles or whether they merely follow visual clues and how we can guide the learning process.", "We showcase the stethoscope framework here to that effect and, based on its application, provide some interesting and novel [according to Reviewer 1] insights into representations for visual stability prediction.", "As regards the implications of the paper, we would like to address this in the context of the reviewer\u2019s comment that increased performance is not surprising given the additional supervision provided.", "Our submission argues that the manner in which this information is provided really does matter.", "Figure 6 addresses this point in that multi-task learning fails to leverage the potential of the additional training labels (and, indeed, leads to a detrimental effect, Fig 6b) whereas the stethoscope framework allows the specification of whether the information considered should be promoted or suppressed.", "This leads to the performance gains shown in Fig 6a."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 413, "sentences": ["Due to the overlap between reviewer comments, we decided to address all concerns in a single response (please see above)."], "labels": ["other_context"], "confs": [1.0]}
{"abstract_id": 414, "sentences": ["We thank the reviewer for the review and the comments.", "Below we address the main concerns.", "\u201cone may study the simple case of having single input channel, for which the output at index \"i\" of an equivariant polynomial is written as the sum of all powers of input multiplied by a polynomial function of the corresponding power-sum.", "This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.", "Generalizing this to the multi-channel input as the next step could make the proof more accessible \u201d", ">> We will highlight the connection to the case of single input channel and DeepSets permutation invariance universality.", "\u201cThe second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model. ... if there were any other objectives beyond this in the experiments could you please clarify? \u201c", ">> We agree it is trivial (and indeed the proof is a one-liner).", "If the reviewers feel strongly, we can move it to appendix, however we feel it helps to provide a complete picture.", "We included it in the experiments as a naive baseline and to show that adding a single transmission layer indeed provides a significant improvement.", "\u201cFinally, could you give a more accurate citation (chapter-page number) for the single-channel version of Theorem 2.?\u201c", ">> This result is Proposition 2.27 in Golubitsky&Stuart(2002).", "We will update the paper to give a more accurate citation."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 415, "sentences": ["Thank you very much for your comments and your feedback.", "We provide our reply to your questions below:", "(1) In equation (3) we are using scalarization, a well-known technique to solve multi-objective optimization problems (see for example Boyd\u2019s book \u201cConvex optimization\u201d Ch. 4).", "In this case, the maximization problem is a multi-objective optimization problem including both the parameters of the discriminator and of the classifier.", "The parameter alpha controls the importance/priority of each of the objectives.", "The parameter alpha also allows to control the detectability constraints for the attack, which allows us to test the robustness of learning algorithms and defences in different settings, considering more or less aggressive adversaries.", "This is common in most security settings to test system\u2019s robustness and resilience in different attack scenarios.", "(2) In pGAN the discriminator allows to model detectability constraints for the poisoning points.", "In other words, to evade detection or removal of points by algorithms that defend against poisoning attacks, such as the defences we used in our experiment, we want our attack points to be close to the distribution of the genuine data.", "However, please, note that the discriminator\u2019s loss is decoupled from the classifier\u2019s loss.", "In contrast, the generator is the element that competes with both the discriminator and the classifier.", "On the other side, the discriminator does not exclude poisoning data or select any data point but helps to guide the generator to craft poisoning points that are difficult to detect.", "In other words, the discriminator does not filter out the points that are used to train the classifier during the training of pGAN.", "It is not clear to us what the reviewer refers to when mentioning measuring the classification error from the data the discriminator selects, as the discriminator does not \u201cselect\u201d any data point, but just aim to classify genuine from fake data points.", "We would be happy to provide further clarifications on this point if needed.", "(3)-(4) To some extent pGAN can control the specific errors produced in the system, as shown both in Figures 5 and 6.", "But the changes produced in the system may also depend on the characteristics of the dataset and the learning algorithms used.", "pGAN produces poisoning attack points that are close to the decision boundary, \u201cpushing the decision boundary away\u201d from the source class (i.e. the same class as the labels of the poisoning points) towards the samples of the target class.", "Then, we can expect an increase of the false positive rate, which is shown in Figure 6 (centre).", "At some point, when the fraction of poisoning points increases significantly the decision boundary starts to change in a different (and possibly more abrupt way), so that the false negatives also start to increase.", "In Figure 6 (right) this happens when the fraction of poisoning points is larger than 25%.", "In contrast, the label flipping attack is less subtle as it does not consider detectability constraints.", "The attack points are therefore not necessarily close to the decision boundary, and thus, the changes produced in the algorithm are more unpredictable and affect the errors for the two classes.", "If there are points that, in your view, require further clarification, please let us know.", "Thank you very much."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 416, "sentences": ["Thank you for approving our contribution to understanding the transferability of adversarial examples.", "We agree with that the smoothing gradient idea, especially the Gaussian smoothing technique, is not new, since the smoothing strategy could be used in many different scenarios.", "However, we motivate and derive the idea of smoothing the gradient based on our novel understanding on the transferability of adversarial examples between two models.", "To the best of knowledge, we are the first to derive and apply this technique to enhance the transferability of adversarial examples, whose significant improvement is also confirmed by our intensive experiments."], "labels": ["global_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 417, "sentences": ["Thank you for the comments.", "We provide the feedbacks below.", "- Indeed there are a huge number of papers on adversarial examples, but specifically only a small fraction  of them are", "about the  transferability of adversarial examples .", "Understanding why adversarial examples can transfer from one model to another model is a much harder problem, which is a rather unexplored area.", "Indeed we do not provide a perfect explanation in this submission, however the factors we have considered and the well-designed numerical investigations could be helpful for future studies on this topic.", "In addition, the works Fawzi'15 and Athalye'18 did not talk about the issue of adversarial transferability.", "- Could you be more specific on what do you expect for \"larger studies\" and \"general study\u201d?  This will be helpful for improving our work."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 418, "sentences": ["Thanks again, Reviewer #2, for your insightful feedback.", "We respond to your other comments below.", "1.  \u201cWhy investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.\u201d", "See general response #3.", "2.  \u201cFor instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood.\u201c", "We do not believe our models are necessarily underfit.", "In fact, we found that Glow had a tendency to *overfit,* and that one must carefully set Glow\u2019s l2 penalty and choose its scale parametrization (exp vs sigmoid, see Appendix D) in order to prevent it from doing so.", "We thought this overfitting to the training data could be a reason for the phenomenon and therefore we tuned our implementations to have reasonable generalization.", "3.  \u201cIt would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.", "For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting.\u201d", "See general response #1 in regards to data sets and additional results.", "Thank you for the suggestion of looking at data sets with similar statistics.", "We do this, in a way, with our second order analysis and the \u2018gray-ing\u2019 experiment in Figure 5 (b) (formerly Figure 6 (b) in the original draft).", "Gray CIFAR-10 (blue dotted line) nearly overlaps with original SVHN (red solid line) in terms of their log p(x) evaluations.", "Figure 12 (formerly Figure 13) then shows the latent (empirical) distribution of the gray images, and we see that the gray CIFAR-10 latent variables nearly overlap with the SVHN latent variables.", "This is to be expected though, given the overlapping p(x) histograms, since the probability assigned by CV-Glow (in comparison to other inputs) is fully determined by the position in latent space.", "4.  \u201cThe second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.\u201d", "See general response #2."], "labels": ["global_context", "other_context", "multiple_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 419, "sentences": ["Thank you for your comments!", "* We included a table showing accuracy numbers for different values of beta and M (see p. 6, Table 1) for the latent bottleneck sizes K=256 (Figure 2) and K=2 (Figure 3).", "*", "In relation to the figures, we have improved these in the revision.", "We are added a figure tracing the mutual information between representation and output I(Z;Y) vs. the minimality term I(Z;X) for different values of beta (see Figure 2, lower right panel), when training with our loss function.", "This is the usual information bottleneck curve.", "This contrasts with the deficiency bottleneck curve (Figure 2, upper right panel) which traces the corresponding sufficiency term J(Z;Y) (which is just the entropy of the labels minus our loss) vs. I(Z;X) for different values of beta.", "Note that for M=1, J(Z;Y) = I(Z;Y).", "We apologize for the confusion.", "The text now makes this more explicit (see p.7, first paragraph).", "*", "In response to your question about how we estimate the mutual information", ".", "Yes, we minimize an upper bound on both the deficiency and the rate term (see p.3, equation 3 and discussion leading up to the VDB objective in equation 4).", "The estimation of this upper bound is simplified by our choice of the prior and the encoding distribution which are diagonal Gaussians.", "The KL term can be computed and differentiated without estimation.", "We estimate the expected loss term using Monte Carlo sampling.", "We draw samples from the encoder using the reparameterization trick and leverage automatic differentiation (in Tensorflow) to compute the gradients.", "Since the expectation is inside the log, gradient updates may have higher variance for larger values of M.", "Our model is a classifier and our loss term is a tighter bound on the misclassification error (bias) than the usual cross-entropy loss as in the VIB (see p. 12, equation 13).", "Trading bias for variance has been investigated in some recent works (see, e.g., Bamler, Robert, et al. \"Perturbative black box variational inference.\" NIPS 2017).", "See last paragraph in p. 18 for the related discussion in the unsupervised setting.", "* In relation to the connection to IWAE, we have included a detailed discussion in Appendix E.1.", "The method is different from ours, except in the limiting case where M = 1 and beta =1, in which case it coincides with the beta-VAE and also with our method.", "After taking a close look, we make the following observations:", "For M > 1, the IWAE bound does not admit a decomposition like the standard ELBO (see equation 29 and 36) into a reconstruction loss term and a regularization term.", "In particular, this implies we cannot trade-off reconstruction fidelity for learning more meaningful representations by incorporating bottleneck constraints.", "See ensuing discussion in p.18 following equation 36.", "In contrast, our method has a tuning parameter beta.", "The IWAE bound is known to be equivalent to the ELBO in expectation with a more complex approximate posterior qIW (see p.17, equation 34 and 35 and references therein in Appendix E.1).", "For beta values other than 1, a naive trick would be to plant qIW in liue of qphi in equation 37 (p. 18) to get a beta-IWAE of sorts.", "It is not entirely clear however, why we would want to do so when modulating beta already suffices to tune the VAE towards autoencoding (low beta) or autodecoding behavior (high beta) depending on the requirement at hand.", "A similar argument goes in the direction of an \"Importance weighted Variational Information Bottleneck\".", "We have not explored if and how using more expressive posteriors such as the qIW (p. 17, equation 35) can help the supervised bottleneck formulations in VDB or VIB.", "This remains a scope for future study.", "We are now also citing the paper Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016."], "labels": ["global_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 420, "sentences": ["Thank you for your insightful comments to help us improve our paper.", "First of all, we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods, and observe that they are also vulnerable to our proposed attacks.", "Please see our reply to all reviewers.", "Here are our responses to your concerns in \u201cCons\u201d and \u201cMinor comments\u201d.", "Although we were not able to provide theoretical analysis in this paper, our proposed attacks are very effective on state-of-the-art adversarial training methods, and we believe our conclusions", "Currently, there is relatively few theoretical analysis in this field in general, and many analysis makes unpractical assumptions."], "labels": ["global_context", "global_context", "global_context", "multiple_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 421, "sentences": ["1.", "However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.", "As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.", "A: While ReMixMatch comprises many components (some of which are new), we believe our ablation study justifies the reason why each component exists. If there are additional ablation experiments that you think would be helpful for us to run, please let us know.", "2. The objective of the update equation of CTAugment\u2019s learned weights seems contradicted with the purpose of how data augmentation is used", "A: It is true that CTAugment at any point in time will only perform augmentations that the model correctly predicts.", "However, we select augmentations where the probability the model output will change is less than 1.", "As such, the augmentation boundary will grow progressively as the training process converges.", "(We experimentally observe this fact: for example, rotation is initially only invariant up to +/- 13 degrees but throughout training becomes invariant to +/- 30 degrees.)", "We don\u2019t aim to maximize the output variation at any instant, but instead ensure that by the end of training the model is invariant to large perturbations.", "3. The authors should provide ablation study and analysis of their CTAugment.", "A: As also discussed with reviewer 2, for space reasons we provided only a short description of CTAugment, and how it differs from AutoAugment.", "We updated the draft to include a longer treatment in the appendix.", "4. The authors should provide more detail of the setting in the ablation study", "A: We agree with the reviewer the details are sparse.", "We will include more details.", "To answer the reviewer\u2019s specific questions: \u201cNo strong aug.\u201d means that all augmentations were weak (as is done in MixMatch) and \u201cNo weak aug.\u201d means that all augmentations were strong. If there are other questions we will clarify any.", "5. The authors hypothesize that \u201cstronger augmentation can result in disparate predictions, so their average may not be a meaningful target.\u201d", "A: See above, where we found that the experiment diverged in the \u201cNo weak aug.\u201d ablation (using strong augmentations only)."], "labels": ["other_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 422, "sentences": ["We really appreciate your comments.", "The main purpose of this paper is to introduce a new method to solve global optimization problem via replica exchange Langevin diffusion.", "We quantify the acceleration effect from the viewpoint of continuous time process.", "Although this work is inspired from Dupuis's work, their setting is MCMC and they only investigate by large deviation.", "We quantify the acceleration effect by both large deviation and chi^2 divergence.", "Besides, the large deviation rate function in our paper is different with that of Dupuis's since we use an alternative approach.", "We choose such a form of rate function because it is connected to the Dirichlet form, and hence, the convergence of chi^2 divergence.", "We acknowledge that our analysis tools is standard and not fancy in mathematics.", "However, this is not a mathematics conference after all.", "One of our contribution is applying standard mathematical tools to a specific machine learning problem.", "Finally, another contribution is that we propose a discretized algorithm.", "Although Dupuis& et.al's work establishes beautiful and complicated mathematical theory for replica exchange Langevin diffusions, they does not consider the discretization at all.", "In practice, we can only use the discretized one instead of the ideal continuous process to solve problems.", "Our theory quantifies the discretization error and convergence rate and hence, ensures the validity to use the discretized algorithm."], "labels": ["global_context", "global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 423, "sentences": ["Thank you for your detailed review and the constructive comments on our work.", "We note the remarks on the paper writing that we will correct and answer below the main points that were commented.", "* In-depth evaluation of MoVE and comparison of with/without conditioning:", "We agree and this was also pointed by 'AnonReviewer2', we are working on new incremental benchmarks, more detailed on both one-to-one and many-to-many models.", "Moreover, the need of pitch/octave conditioning limits the applicability of our model to transfer only on audio carrying such note features.", "Hence we trained models without conditioning mechanism and, as answered to 'AnonReviewer2', we are planning experiments on models which are conditional but integrating an unconditioned state to be trained in parallel of the note-conditional state.", "**", "*", "Interpretability of the generative scores:", "We agree on this remark, the idea of scaling scores is right and would improve the interpretability of our benchmarks.", "For that purpose, we should define a set of reference scores as you recommended to.", "* Incomplete definition of the metrics:", "We gave references to the papers that introduced such metrics.", "Discussing a set of reference scores should also come with a better explanation of these.", "* Criteria for bolding: we intended to highlight the best scores", "**", "* Pairing generated and real examples by instrument and note to compare:", "In addition to the spectral descriptor distribution plots, we used sample-specific scatter plots to visualize how the transfer maps them individually.", "On the overlap of each instrument tessitura, we can make such pairing.", "We can also transfer and transpose to the target instrument tessitura if needed.", "Remains the question of which metric can be used here to evaluate generation at the sample-level (?), as our model does not aim at reconstructing an hypothetical corresponding sample in the target domain but rather at blending in features from the other domain so that it sounds like the input note (pitch, octave but also some dynamics/style qualities relative to the input instrument) played by the target instrument.", "We later aim at experimenting on mechanisms to control the amount of target feature blending in the process of transfer.", "* Invertible ? Decodable ? Approximate inversion ?", "We agree that the current state of the research should be stated as using approximate spectrogram inversion.", "We plan on replacing the iterative slow spectrogram inversion with Griffin-Lim by faster decoding with Multi-head Convolutional Neural Networks, arXiv:1808.06719, Sercan O. Arik et al.", "*", "**", "Definition of the RBF kernel:", "The summation is on the alpha parameter which can be a list of n values (or a single float value).", "The trainings were done with n=3 and alpha=[1. , 0.1 , 0.05].", "Depending on the kernel and bandwidth definitions, we may link both as", "alpha = 1 / (2 x bandwidth**2).", "* Calculation of reconstruction errors:", "All scores are computed on NSGT magnitude spectrogram slices.", "No evaluation (except listening) is done on the time-domain waveforms.", "The points marked with *** are highlighted as we would gratefully receive further remarks from your review.", "How would you recommend making reference scores to the MMD/kNN evaluations ?", "How would you recommend comparing pairs of generated and ~ corresponding target domain samples ? (at the sample level)", "Is the definition of the RBF kernel correct to you given that clarification (that should be added to the paper) ?", "Thanks again for the interesting feedbacks !"], "labels": ["global_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "global_context", "multiple_context", "multiple_context", "single_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 424, "sentences": ["Thank you for your comments.", "We have added a pseudo-code description of TTS-GAN training algorithm to the updated submission.", "We believe that, together with other architectural details present in the paper, it makes our work reproducible."], "labels": ["global_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 425, "sentences": ["We would like to thank you for reviewing our paper.", "[Experiments Section] We have significantly updated qualitative and quantitative results in our \"Experiments\" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN.", "[InfoGAN] Compared to InfoGAN, our method is novel in two ways: First, we use separate networks to obtain the image embeddings, which enables us to guide some of these networks with simple functions.", "The guidance allows more control over the latent space, even in lack of data.", "Second, we use pairwise similarity/dissimilarity in order to perform disentangling, which is different from InfoGAN's approach of maximizing the label likelihood.", "This point is now addressed in our \"Related Work\" section."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 426, "sentences": ["We thank the reviewer for their helpful feedback on our work.", "Regarding the IWSLT translation result, the key claim we aim to validate is that the theoretical efficiency of K-matrices translates to practical speedups on real models as well.", "We agree that there are other approaches that may offer different model quality vs. inference speed tradeoffs; we simply highlight that K-matrices are one promising method, especially given their important theoretical properties.", "We have added a performance comparison of K-matrices with other structured replacements such as circulant, Fastfood, ACDC, and Toeplitz-like in Appendix B.4.3, showing that K-matrices yield faster inference with similar BLEU score.", "We also point out that our DynamicConv model with K-matrices in the decoder attains a comparable BLEU score with the state-of-the-art from two years ago \u2013 the Transformer model, which continues to enjoy widespread use today \u2013 while having over 60% higher sentence throughput and 30% fewer parameters than this model.", "As mentioned in the shared response, we believe that the speed-quality tradeoff of K-matrices could be further improved with more extensively tuned and optimized implementations.", "Exploring how to continue to improve these structured compression approaches, while retaining the efficiency and theoretical benefits of K-matrices, is an exciting question for future investigation."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 427, "sentences": ["Indeed, GPipe [2] incurs less memory footprint than our pipelining scheme and PipeDream [1] because it only saves the activations at the boundary of each model partition and re-computes the activations of the model during the backward pass.", "However, the re-computation still incurs pipeline bubbles during training.", "Our scheme saves all activations instead of re-computing them to eliminate pipeline bubble, thus achieving better utilization for the accelerators (GPUs).", "Our scheme has less memory footprint than PipeDream because it does not stash weights.", "The main goal of our submission is to experimentally show that our pipelined training, using stale weights without weight stashing [1] or micro-batching [2], is simpler and does converge.", "The paper does achieve this goal, on a number of networks.", "It would be difficult fit a detailed convergence analysis in our paper given the limited space provided.", "Thank you for pointing out paper [3].", "We notice that it is submitted to arXive after the submission deadline of ICLR, thus we were unaware of it at the time of submission.", "Nonetheless, we will cite it and discuss its approach in comparison to ours in the related work section of the final revised version of our paper."], "labels": ["multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 428, "sentences": ["Thank you for your detailed reviews.", "Here are our responses to your questions and concerns.", "1. The authors should provide more details on how the hand-crafted demonstrator agents were made.", "We have added more details, and plan to release the code.", "We indeed implemented search algorithm with simple heuristics for acceleration for all grid-world tasks.", "In Maze Navigation, the state space is extended to the combination of map status and the agent's inventory.", "By this definition of states, an efficiency search can still be achieved.", "2. Scalability?", "We focus on simpler domains to provide proof-of-concept results as the first step on this direction.", "We are definitely interested in studying how our approach can be applied to more complex tasks as future work.", "3. A deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.", "Thanks for the suggestion.", "We have included a more detailed analysis with new visualizations in the updated paper.", "i) We visualize the latent vectors obtained from demonstrations with probing and without probing.", "It indeed shows that with probing, we are able to find new behaviors that correspond to the new latent vectors.", "ii) We also show the correlation between the distance of two consecutive latent vectors m^{t-1} and m^t and, the KL divergence between the two corresponding policies KL(\\pi(a|s^{t+1},m^t) || \\pi(a|s^{t+1},m^{t-1})), i.e., how different the policy would have been if m^t didn\u2019t change.", "The correlation is significant, and thus validates the idea.", "4. 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?", "We focus on improving modeling machine agents, and applying the improved agent models for multi-agent tasks.", "The current form of our approach is not designed for learning from human demonstrations.", "However, there are ways to modify our approach towards that direction: i) learning probing policy with model-based RL; ii) incorporating inductive bias from humans (e.g., the learner knows a specific set of possible goals of the demonstrator and probes the demonstrator to test which goal it has).", "This seems to be a good direction for future work, but we also think that the current research has provided promising results in simpler domains, and hopefully incites more research where human demonstrators are also involved by introducing this problem to the community.", "5. I think this method demonstrates a method for improved collaborative and/or competitive performances given the fact that you already have a single agent with a learned policy.", "Yes, in our experiment, we do assume that the opponent has a learned policy which is unknown to us.", "We think that this is a quite general setting where multiple machine agents are interacting with each other but do not know each other\u2019s true policies and intentions."], "labels": ["global_context", "other_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 429, "sentences": ["Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:", "- We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer, and we do indeed think that a pairing-based strategy is plausible for convolution-based networks.", "When talking in more general terms about \"deep learning models\", we refer to the proposed methodology for \"investigating deep learning models\", and don't want to claim that we actually evaluate a representative number of models.", "We see this methodology, as illustrated by our experiments for one model, as the central part of our contribution.", "The first paragraph of section 3.2 describes this FiLM model and, given the focus on methodology, we considered the description (plus reference to the paper) sufficient here.", "- There are a few points here:", "*", "Since it was shown that humans seem to follow a cardinality-based strategy, the pairing-based one would be not human-like, but nonetheless cognitively plausible.", "We use \"cognitively\" in the sense of \"algorithmically\" plausible, so a procedure that makes sense for solving the problem.", "An example for an implausible method would be to rely on color/shape cues to solve instances involving \"most\", which doesn't make sense for the abstract meaning of \"most\".", "*", "Regarding the question whether comparability to human behavior is indispensable: On the one hand, acceptance of and reliance on systems which follow vastly different principles can be problematic; on the other hand, if the information a system uses to arrive at its conclusion doesn't make any sense to humans (in cases where humans have an intuition what is relevant, like the above example of \"most\" and color/shape cues), we doubt that good performance alone will justify using such a model, as opposed to instead doubting the quality of the underlying benchmark data.", "* The question whether we want a VQA system which returns approximate answers is an interesting one, but we don't intend to claim that this is a desired property, just that it is desirable to know whether (and how exactly) our systems currently solve such tasks approximately.", "A conclusion from our findings may well be that it is worth improving VQA models with respect to their counting capability, as they seem to rely on an approximate as opposed to an exact number system.", "- We added a longer footnote to section 2.4 about the \"one-glance feed-forward-style networks\" for clarification.", "In summary, general precise counting is an inherently recursive ability, and models which don't have an architectural module for recursive computations are not expected to be able to learn this capability, while they may stil learn to subitize or represent numbers approximately (which doesn't require recursion).", "- You're right, this sentence was a bit vague, we rephrased it to: \"these models can indeed learn and utilize more abstract concepts (approximate numbers) than mere superficial pattern matching (\"red square\" etc)\".", "The differences we want to point out is that, on the one hand, (approximate) numbers are a more abstract concept than, for instance, object types like \"cat\", \"chair\", etc as they can be combined with any object type. On the other hand, being able to utilize such representations to answer practical questions like whether \"most\" applies is more interesting than just being able to classify which representation applies.", "- Good point, our reasoning here was mostly influenced by Pietroski et al.'s work and our intuition about the ability of convolutions to learn a local pairing strategy (see addition at the end of section 2.4).", "Presumably, it could be possible to observe the behavior in our paper based on a pairing-based mechanism which works approximately, independent of clustering, as predicted by Weber's Law.", "It's probably impossible to ultimately rule out a pairing-based strategy via experiments evaluating extrinsic behavior only, but we note that there is evidence for Weber's Law in other approximate systems where pairing-based strategies are no alternative, thus suggesting that similar mechanisms are at work here.", "We added a footnote on this to section 2.4.", "- There are a few papers focusing solely on \"most\" in psycholinguistics (like the ones cited) and linguistics in general (e.g., formal semantics), many of which talk about cardinality as a \"core concept\" of human cognition, and many of which contrast a more subconscious concept of cardinality (like the approximate number system) with the conscious algorithmic ability for precise and infinite counting.", "- We consistently use \"interpretation\" in the new version.", "- Fixed.", "- We think that our approach and particularly its inspiration by experiments from psychology are a substantial contribution to evaluation methodology in the context of powerful deep learning models, which are not infrequently described by anthropomorphizing words like \"understand\" and compared to \"human-level\" performance (added a sentence to the introduction).", "The reason for exceeding 8 pages in length is likely due to the more elaborate introduction of the various concepts from Pietroski et al.'s work and experimental methodology in psychology in general, which we assume the audience of this conference is not very familiar with."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 430, "sentences": ["We would like to thank you for reviewing our paper.", "[Unguided Case]", "Please refer to our general comment above on why our unguided case performs better now.", "The main usefulness of our guided approach is to directly capture some of the desired variations in the data.", "This is now clearer on our quantitative and visual results in the \u201cExperiments\u201d section.", "[Heuristic Guidance]", "The main premise behind guiding our siamese networks is to find very simple, yet effective ways to capture some of the variation in the data, through weak supervision.", "For more complex semantics, we discuss the possibility of using a pre-trained network as guidance.", "Please refer to our \u201cDiscussion\u201d section for more details.", "[Differentiable Guidance]", "The transformations need to be differentiable in order to backpropagate the gradients into our generator.", "This is now pointed out and discussed in our \"Discussion\" section.", "Although this limits the function families, we can still use differentiable relaxations of more complicated functions.", "[Gaussian Prior on Latents] In our new experiments, we used uniform distributions to model the generative factors.", "We had experiments with categorical variables, however, we faced training stability issues with them.", "We now point this out in our \"Discussion\" section.", "[Similar Latent Factors] We now use an adaptive margin that depends on the distance between two latent samples.", "So, if samples are close to each other, the margin is smaller, and vice versa.", "[Experiments Section] We now compare our method against Beta-VAE, DIP-VAE, and InfoGAN, both qualitatively and quantitatively.", "Please refer to our updated \"Experiments\" section.", "[Information of Guidance] In Figure 3, we visualize which part of an image was visible to a siamese network.", "In addition, we show how changing the corresponding guided knob affects the generated images.", "[More Than Two Attributes] We now use 32 dimensions for the CelebA dataset and 10 dimensions for the 2D shapes dataset."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 431, "sentences": ["We appreciate the Reviewer's comments, which help us to improve the paper.", "In the final version of the paper we will take them into consideration.", "In the following we reply to the main concerns of the reviewer.", "Q1 - \"... how general this approach would be? ...if rules contain quantifiers, how would this be extended?\"", "The extendibility of the Neural LP framework is a very important and relevant question, which we also mentioned explicitly as a possible future work direction.", "In the rules that we support in our framework all variables are universally quantified.", "While learning rules with existential quantifiers in rule heads is a difficult endeavor in general, even for classical relational learners, the Neural LP framework in principle can be extended to support them as follows: For every relation p, we can create a fresh diagonal Boolean matrix $M_{\\exists p}$, which has 1 at the position (i,i) iff there exists an entity j, such that p(i,j) is in the KG (similar as for classification operators discussed on p. 5).", "Incorporating these matrices into the framework and filtering rules that have the respective relations in the head should allow us to extract the target rules.", "Yet analysing how well such approach performs in practice is still an open problem, which we leave for future work.", "In any case, we will discuss the extendability of the framework in the paper.", "Minor comment 1) - 4.1,", "\"O(n^2/2) -- just put O(n^2) or simply write as n^2/2\".", "This is correct, thank you. We will fix this in the final version.", "Minor comment 2) - \"How are the rules from in Eq (2)? i.e., how is \\beta_i selected for each i? In the extreme case it would be all the permutations.\"", "To avoid exponential enumeration of the predicate orderings sophisticated transformation of the rules has been applied in the Neural LP framework (see [Yang et al. 2017]).", "Minor comment 3) - \"I would suggest a different name other than Neural-LP-N...\"", "Thanks for this suggestion.", "We will certainly consider renaming the approach and fixing this in Table 2."], "labels": ["global_context", "global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 432, "sentences": ["Thank you for the comments and suggestions.", "The technical comments are addressed below:", "Extending result to other target functions:", "We agree that the problem might be significantly more difficult for different target functions, and would like to make the following remarks:", "1.", "Note that in our bias-variance decomposition, only the bias term depends on the target function.", "In other words, our result on the variance (including Theorem 4) would still be valid for other targets, such as two-layer neural network.", "One caveat is that for general target function, the output needs to be properly scaled since our current analysis in Section 5 relies on linearizing the network.", "2. When the target function is a multiple-neuron neural network, deriving the bias term can be challenging.", "However, we note that under the same setup, the bias may be obtained when the teacher is a slightly more general single-index model, i.e. $y=\\psi(\\beta^\\top x)$ with Lipschitz link function $\\psi$, equivalent to a single-neuron network.", "For instance, the bias under vanishing initialization is the same as that of least squares regression on the input, which can be solved under isotropic prior on $\\beta$ via decomposing the activation function similar to Appendix C.5.", "Parameter count:", "To clarify our statement in the discussion section, our current result requires $n,d,h$ to grow at the same rate, and thus $n = O(dh)$ is beyond the regime we consider.", "This is also true for previous works on double-descent in random feature model [Hastie et al. (2019)][Mei and Montanari (2019)].", "When $h \\ll n$, it is not clear if the same analysis still applies (for instance approximating the network with a kernel model), and thus the instability of the inverse may not be the complete explanation of double-descent (if it appears).", "Characterizing the generalization in this regime would be an interesting direction.", "Training both layers:", "Thank you for the suggestion; we have included training both layers simultaneously as a future direction.", "We would like to briefly mention that under certain model parameterization and initialization, gradient flow on both layers may reduce to one of the three models we analyzed (see [Williams et al. (2019)]).", "More generally, our current result may be extended to cases where the dynamics of training both layers can be linearized (for instance initialization in the \"kernel regime\"), for which the learned model can be written down in closed-form."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 433, "sentences": ["Thank you for the thoughtful comments.", "We are glad that you found our topic interesting and appreciated our theoretical analysis and experimental results.", "We address other comments below:", "[Results are only given for the Zipfian distribution]", "Many real-world data naturally follow the Zipf\u2019s Law, as we showed in Figure 5.1 and Figure 5.3 for internet traffic and search query data.", "Thus, our theoretical analysis assumes item frequencies follow the Zipfian distribution.", "While our analysis makes this assumption, our algorithm does not have any assumption on the frequency distribution.", "[Assuming query distribution is the same as data distribution]", "As the reviewer pointed out, the query distribution we use is a natural choice.", "There might be other types of query distributions, such as the one pointed out by the reviewer.", "Intuitively, our overall approach that separates heavy hitters from the rest should still be beneficial to such query distribution.", "[Algorithm design]", "We agree that our algorithms are relatively simple.", "We believe this is a feature not a bug: as we showed in Sec. 4.1, our algorithm does not need to be more complex.", "Specifically, our Learned Count-Min algorithm achieves the same asymptotic error as the \u201cIdeal Count-Min\u201d, which is allowed to optimize the whole hash function for the specific given input (Theorem 7.14 and Theorem 8.4 in Table 4.1).", "The proof of this statement demonstrates that identifying heavy hitters and placing them in unique bins is an (asymptotically) optimal strategy.", "(In fact, our first attempt at solving the problem was a much more complex algorithm which optimized the allocation of elements to the buckets (i.e., the whole hash function h) to minimize the error.", "This turned out to be unnecessary, as per the above argument.)", "[Novelty compared to Mitzenmacher\u2019 18]", "Our paper, as well as the works of Kraska et al \u201918, Mitzenmacher \u201918,  Lykouris &", "Vassilvitskii \u201918, Purohit et al, NIPS\u201918, belong to a growing class of studies that use a machine learning oracle to improve the performance of algorithms.", "All such papers use a learned oracle of some form.", "The key differences are in what the oracle does, how it is used, and what can be proved about it.", "In Kraska\u201918 and Mitzenmacher\u201918, the oracle tries to directly solve the main problem, which is: \u201cis the element in the set?\u201d An analogous approach in our case would be to train an oracle that directly outputs the frequency of each element.", "However, instead of trying to directly solve the main problem (estimate the frequency of each element), our oracle is a subroutine that tries to predict the best resource allocation --i.e., it tries to answer the question of which elements should be given their own buckets and which should share with others.", "There are other differences.", "For example, the main goal of our algorithm is to reduce collisions between heavy items, as such collisions greatly increase errors.", "This motivates our design to split heavy and light items using the learned model, and apply separate algorithms for each type.", "In contrast, in existence indices, all collisions count equally.", "Finally, our theoretical analysis is different from M'18 due to the intrinsic differences between the two problems, as outlined in the previous paragraph.", "[The analysis is relatively straightforward]", "There are three main theorems in our paper: Theorem 8.4, Theorem 7.11 and 7.14.", "Our proofs of Theorem 7.11 and 7.14 are technically involved, even if the techniques are relatively standard.", "On the other hand, the proof of Theorem 8.4 uses entirely different techniques.", "In particular, it provides a characterization of the hash function optimized for a particular input.", "[The machine learned Oracle is assumed to be flawless at identifying the Heavy Hitters]", "Actually, this is not the case.", "The analysis in the paper already takes into account errors in the machine learning oracle.", "Please see the 2nd paragraph of Sec. 4.1 and Lemma 7.15.", "In summary, our results hold even if the learned oracle makes prediction errors with probability O(1/ln(n)).", "We will revise the text to make it clearer."], "labels": ["global_context", "single_context", "other_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 434, "sentences": ["We thank the reviewer for the detailed review.", "Below we address the main concerns.", "\u201cI think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area... For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly\u201d", ">> We discussed the results of the experiments in pages 7-8. We will revise to make this discussion easier to find.", "\u201cWhat would have been nicer would be to perform some sort of ablation study, where the authors studied how the representational capacity of the network changed as a result of them introducing the universal linear transmission layer.\u201d", ">> We refer the reviewer to Figure 2 where one can see that the PointNet model underperforms across various tasks compared to PointNetST that is identical to PointNet except the addition of a single linear transmission layer.", "Furthermore, PointNetQT, PointNetSeg, and DeepSets can be seen as different versions of PointNet variations.", "\u201cA direct theoretical and experimental comparison between PointNet and PointNetST would have been useful for me to understand the impact of the change that the authors introduce..\u201d", ">> In Theorem 1 we state that PointNet is not equivariant universal, but PointNet with a single transmission (PointNetST)  layer is.", "In the experiments section we compare these two models on three different learning tasks.", "\u201cWhere is the conclusion section?\u201d", ">>We felt it is unnecessary but see we were wrong, we will add one."], "labels": ["global_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 435, "sentences": ["We appreciate the reviewer\u2019s positive comments about our work.", "Regarding the convergence and speed of training, we would like to stress that all hyperparameters for training were kept the same as those for training the default model architecture, other than those we explicitly mentioned as being tuned (e.g. learning rate for the speech experiment).", "In particular, for all experiments, the number of epochs is the same for both the baseline approach and the K-matrix approach.", "Additionally, for the speech preprocessing and ShuffleNet experiments, we compare the total wall-clock training time of our K-matrix approach to that of the baseline approach, in both cases finding that the training time required by our approach is at most 20% longer than that of the baseline approach.", "In our updated revision, we also include the training time comparison for the DynamicConv model in Appendix B.4.2 (in this case, the modified model with K-matrices actually trains slightly faster than the baseline).", "We agree with the reviewer that a training plot can help provide a better understanding of how our proposed approach performs, and therefore have included an example plot (for the ShuffleNet experiment) in our updated revision (in Appendix B.2.3).", "Regarding empirical comparisons to dense matrices, in Table 5 (Appendix B.1.2), we compare the use of K-matrices in the raw-features speech model with several other classes of matrices, including dense matrices.", "We find that, while using a trainable dense matrix slightly outperforms just using the fixed FFT (0.3% drop in test phoneme error rate), using a K-matrix instead of a dense matrix yields a further improvement of 0.8% in the phoneme error rate.", "Another empirical comparison of K-matrices and dense matrices is in Section 3.3, in which we replace the linear layers in the decoder of a DynamicConv model with K-matrices; these linear layers are by default dense (fully-connected) matrices.", "Theoretically, in Lemma E.3 we show that arbitrary dense matrices are contained in the BB* hierarchy \u2013 in particular, that any n x n matrix is in (BB*)^{2n-2}, which implies that its K-matrix representation requires at most (4n log n)*(2n-2) = O(n^2 log n) parameters and thus is tight up to a logarithmic factor in n."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 436, "sentences": ["We thank reviewer2 for the kind and constructive review.", "We agree that broader analysis beyond global-local disentanglement is desirable and we hope to perform more experiments in a follow up work."], "labels": ["global_context", "multiple_context"], "confs": [1.0, 1.0]}
{"abstract_id": 437, "sentences": ["The primary goal of the projections is to project all embeddings into the model dimension d so that we can have variable sized embeddings.", "Our goal was not to make the model model expressive.", "Compared to the rest of the model, these projections add very little overhead compared to the rest of the model.", "Doing without them is an interesting future direction though!"], "labels": ["single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 438, "sentences": ["We appreciate the comments of the reviewer.", "Please see our reply below.", "1) - \"... the current proposed method can only deal with one form of numerical predicate, which is numerical comparison.\"", "Apart from simple numerical comparison we are also able to deal with complex classification operators that aggregate numerical attributes using linear functions, where the threshold value is selected in a systematic fashion, (see Classification Operators) as well as negated atoms (see Negated Operators on p. 6).", "We note that such rules are indeed limited to some extent, but they still capture a rather expressive fragment of answer set programs with restricted forms of external computations", "[Eiter et al., 2012].", "Below we present examplar rules learned by our framework, which are not restricted to numerical comparisons.", "2a) - \"The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.\"", "With the rapid development of industrial and scientific knowledge graphs, we believe (and agree with the Reviewer #2) that learning rules that involve multiple modalities is an important and relevant problem.", "Indeed, such rules can not only be used for data cleaning and completion, but they are also themselves extremely valuable assets carrying human-understandable structures that support both symbolic and subsymbolic representations and inference.", "2b) -  \"The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.\"", "To the best of our knowledge Freebase and DBPedia are the only standard KGs with numerical values [Garcia-Duran et al., 2018] used for the evaluation in state-of-the-art works.", "This is the reason why we have selected and used them for our experiments.", "The impact of our approach might appear to be rather modest, since these KGs still have only a limited amount of numerical information.", "Therefore, to demonstrate the power of our approach further, we have also performed evaluation on the synthetic datasets.", "We would be happy to learn about other datasets suitable for our experiments.", "3) - \"The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.", "A good place to start with is to visualize (print out) the learned numerical rules and see if they make any sense.\"", "According to the Reviewer's comment we will extend Section 5 on experimental results by showing more detailed analysis.", "In particular, we will present the following examples of the learned rules from the considered (real-world and synthetic) datasets:", "- FB15K:", "disease_has_risk_factors(X,Z) :- f(X), symptom_of_disease(X,Y), disease_has_risk_factors(Y,Z)", "The rule states that symptoms with certain properties (described by the function f) typically provoke risk factors inherited from diseases which have these symptoms.", "Here, the function f is the sigmoid over a linear combination of numerical properties of X.", "- DBPedia:", "defends(X,Z) :- primeMinister(Z,Y), militaryBranch(Y,X), f(Y)", "This rule states that prime ministers of countries with certain numerical properties (described by the function f), are supported by military branches of the given country.", "The function f is the sigmoid over a linear combination of numerical properties of Y.", "- Numerical1:", "prefer(X,Y) :- isNeighbourTo(X,Y), hasOrder(X,Z1), hasOrder(Y,Z2), Z1>Z2, max{Z2:hasOrder(Y,Z2)}", "This rule with a comparison operator states that a person X prefers neighbours with the maximal order that is less than X's.", "- Numerical2:", "prefer(X,Y) :- isNeignborTo(X,Y), hasBalance(Y,Z1), borrowed(Y,Z2), f(Y)", "This rule states that neighbours with the largest difference between the balance and the borrowed amount are preferred.", "More precisely, here f selects among all X those entities, for which the difference between the balance and the borrowed amount is maximal."], "labels": ["global_context", "other_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 439, "sentences": ["Thank you very much for the comment.", "We have revised the script to reflect the suggestions, and we would like to articulate on the changes we have made.", "Because we are out of space, we will provide the  answers over several comments."], "labels": ["global_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 440, "sentences": ["We really appreciate your comments.", "Replica exchange Langevin diffusion is widely used in classic MCMC over the years.", "Our work also uses this methodology in the setting of nonconvex optimization problem, which arises in many machine learning applications such as training neural networks.", "There are also many interesting questions in this direction, for example, how to choose the best temperature based on the structure of specific problems.", "That is why we still submit it for ICLR.", "As for the comments on math side.", "First, when a->infty, the exchange process should be defined in another way.", "Our current definition, which swapping particles with some rate, is only valid for finite a.", "This extension is not totally trivial and in Dupuis&et. al's work, some results are established.", "In our paper, we only discuss finite swap rate.", "This brings convenience for the discussion of discretization error.", "Otherwise, we need to use a different approach to analyze.", "Moreover, we point out that in discretization, the swapping intensity a should be smaller than the step size.", "This also reflects the nontrivial connection between infinity swapping and discretization.", "Second, the kappa in (3.10) is related to the Poincare inequality and it is also a lower estimate of the spectral gap of Markov process.", "Kappa is can be defined as the solution of a variational problem involved Dirichlet form.", "However, although our result shows that swapping boosts the Dirichlet form, we still cannot obtain an analytical formula of kappa depending on a, since the variational problem makes this relation extremely complicated.", "Even in the field of pure math, it is still very hard to obtain an explicit formula of kappa for a general Markov process.", "However, for this special case, we will keep trying to solve it in the future."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 441, "sentences": ["We thank the reviewer for positive feedback and for championing our paper.", "We are also grateful for your constructive suggestions to improve the paper and would like to report on how we have incorporated your feedback.", "Inspired by your suggestion, we conducted additional experiments on Amazon Reviews, Yelp Reviews, and Semeval (Twitter) datasets, and found that the counterfactually-augmented data resulted in across-the-board gains.", "These experiments are featured in the updated draft."], "labels": ["global_context", "global_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 442, "sentences": ["Due to the overlap between reviewer comments, we decided to address all concerns in a single response (please see above)."], "labels": ["other_context"], "confs": [1.0]}
{"abstract_id": 443, "sentences": ["We thank the reviewer for many positive comments about our paper.", "The typos explicitly mentioned in the review have been corrected, and we did our best to spot other typos not mentioned.", "Besides, all the acronyms have been explained.", "We added the tutorial from Hansen (2016) as the reference for the common choices for setting \\lambda_i in Equation 1, 2.", "We agree with the reviewer that our paper is not theoretically oriented, nor does it address any real world application like robotics or other challenging domain.", "Our point is rather to provide a practical method performing well with respect to the state of the art, which is most often evaluated with the same widely used benchmarks.", "With respect to initialization of hyperparameters, as explicitly mentioned in the \"experimental setup\" section, \"Most of the TD3 and DDPG hyper-parameters were reused from Fujimoto et al. (2018).\" The justification for this choice is to facilitate comparison with previously published work."], "labels": ["global_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 444, "sentences": ["Thank you for the encouraging review.", "[R3: Weakness: It would be good to see some comparison to the state of the art ]", "With regards to your comment on attacking the current state of the art method for smoothed classifiers, we have added new results to the resubmission (Appendix B), in which we attack the adversarially trained smooth classifier [1].", "[1]. Salman et al., \u201cProvably Robust Deep Learning via Adversarially Trained Smoothed Classifiers\u201d, NeurIPS 2019"], "labels": ["global_context", "single_context", "single_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 445, "sentences": ["We are glad you found our work interesting and novel, and thank you for your helpful suggestions for improving the writing. We have taken them on board in the revised paper, making a number of edits.", "1. \"In the introduction, \"the classical approach\" is mentioned but to be the latter is", "insufficiently covered. Some more detail would be welcome.\"", "We have added a reference to what we mean by the classical approach in the related works section.", "2. \"page 2, \"predict the probability\": rather employ \"estimate\" in such context?\"", "We have changed \u201cpredict\u201d to \u201cestimate\u201d.", "3. \"'linear piecewise': 'piecewise linear'?\"", "This was a typo and we have corrected this phrase.", "4. \"What is 'an exact upper bound'?\"", "We mean that it is a true upper bound instead of just being a stochastic estimate of an upper bound (while, on the other hand, Weng et al\u2019s approach is stochastic estimate of a lower bound).", "However, we agree that the \u201cexact\u201d is superfluous and have removed it.", "5. \"I am not an expert but to me 'the density of adversarial examples' calls for further", "explanation.\"", "We think perhaps \u201cthe prevalence of adversarial examples\u201d would be a better phrase and have corrected this.", "We mean that the input model density is integrated over for our metric to calculate the volume of counterexamples in a subset of the input domain, relative the overall volume of that input domain.", "6. \"From page 3 onwards: I was truly confused by the use of [x] throughout the text", "(e.g. in Equation (4))", ".", "x is already present within the indicator, no need to add yet", "another instance of it.\"", "In retrospect, we agree that this was confusing and have removed the [x] notation from the indicator function.", "7. \"In related work, no reference to previous work on \"statistical\" approaches to NN", "verification. Is it actually the case that this angle has never been explored so far?\"", "As far as we are aware this is correct: we have not been able to find any prior work which aims to estimate the statistical prevalence of counterexamples.", "8. \"In page 6, what is meant by 'more perceptually similar to the datapoint'?\"", "We mean that the minimal adversarial distortion for models on CIFAR-10 is known to typically be much smaller than for MNIST.", "The result of this is that an adversarial example on MNIST will often have visual salt-and-pepper noise, whereas an adversarial example for CIFAR-10 typically is indistinguishable to the naked eye from its unperturbed datapoint.", "9. \"In the appendix: the MH acronym should better be introduced, as should the notation", "g(x,|x')", "if not done elsewhere (in which case a cross-reference would be welcome).", "Besides this, writing \"the last samples\" requires disambiguation (using \"respective\"?).\"", "We have added to this description so that it is less terse and more carefully introduces the notation, including changing \u201clast samples\u201d to \u201cfinal samples\u201d and adding in a reference for further reading."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 446, "sentences": ["We thank the reviewer for his comments.", "We discuss the novelty aspects in our general response ( https://openreview.net/forum?id=HkMlGnC9KQ&noteId=S1eid00WaQ ) and will be happy to clarify this in the paper.", "Further comments are addressed below.", "**", "controlling the amount of deformations", "The stability bounds of B+M provide upper bounds on ||Phi(x') - Phi(x)|| (where x' is a deformation of x) based on quantities related to the corresponding diffeomorphism, i.e. the maximum norm and the maximum jacobian norm.", "For simple classes of deformations these can be computed precisely in terms of the parameters of the deformation, e.g. for translations, rotations, scaling or simple parametric warps.", "When bounding these away from zero by a certain constant, ||Phi(x') - Phi(x)|| is then included in a centered ball of the RKHS with a radius growing with this constant.", "This constant then acts as a regularization parameter, just like the size of additive perturbations in the case of adversarial perturbations, and can be tuned by cross-validation.", "** tightness of the lower bounds", "This is something that we verify empirically in our experiments at the end of training by checking the values of spectral norms as a proxy of the upper bound, and looking at the gap with the lower bound.", "In particular, when using the ||f||_M penalty, lower and upper bounds seem to be controlled together in our experiments (Figure 2), making the bound useful, in contrast to PGD, for which spectral norms grow uncontrolled when the lower bound decreases.", "We will further clarify this in the paper.", "eqn (8), (12): thanks for pointing these out, we will fix this in the paper."], "labels": ["global_context", "multiple_context", "other_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 447, "sentences": ["Thank you for the time and effort spent reviewing our paper.", "We mostly agree with your characterization of our work, but we think there are two important points we perhaps did not sufficiently emphasize in our paper and that we would like to mention:", "(1) There are other existing tasks and algorithms that fall into the marginal policy gradients framework.", "For example, researchers and practitioners both almost always clip actions when using policy gradient algorithms for robotics control environments (read: MuJoCo tasks).", "Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces.", "Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten the existing analysis of their algorithm.", "(2) To the best of our knowledge, our work is the first to apply such variance reduction techniques to RL.", "To summarize, our work consists of two components: (a) a new algorithm for directional control and (b) a variance reduction framework that can be applied to directional action space and clipped action spaces.", "While directional action spaces are not very common at this time, clipped action spaces are extremely common.", "We also anticipate that in the future, many additional environments will be available that feature directional actions (many console or PC games, for example).", "For these reasons, we feel that our work is not incremental at all, and is actually quite novel."], "labels": ["global_context", "other_context", "global_context", "global_context", "global_context", "global_context", "single_context", "global_context", "global_context", "other_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 448, "sentences": ["We thank the reviewer for the positive assessment of our work.", "We would like start by stating that we did not mean to claim that the rate of convergence proved in this paper is better that than of Yan et al.", "We have modified the Remarks to clarify the statements.", "In the stochastic gradient setting, the number of gradient evaluation is indeed $T^2$. This is consistent with the result in Bernstein et al. (2018).", "The main point we would like to make is that the bounds are very concise and exactly reduce to that of gradient descent/stochastic gradient descent in the special cases.", "We thank you for pointing out that the bounded variance assumption may also be restrictive and only satisfied on bounded domains.", "It is nonetheless a standard assumption made in the literature.", "We have modified Remark 3.4 (and added Remark 3.5) to make this clear in the updated version.", "Response to other minor points:", "Our convergence analysis is done for non-convex objective functions (similar to that of Yan et al. and Bernstein et al.).", "In the non-convex setting, to the best of our knowledge, there are no theoretical results that show benefits of momentum methods over SGD.", "For experiments, we speculate that the reason is that the batch size used is too small for (Powered)SGDM to gain an advantage over (Powered)SGD.", "We plan to add SGD as a reference algorithm (as suggested by another reviewer).", "Once the experiments are complete, we should be able to see how SGDM compares with SGD in the experiments.", "This may take a while for the ImageNet experiments, but we promise to do so in the final version.", "We have fixed the other issues you raised in your other minor comments. If you have any further comments, please let us know."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 449, "sentences": ["Thanks for acknowledging our novel perspective as well as giving the valuable comments.", "Below, we address the detailed comments.", "Particularly, we clarify some potential misunderstandings and provide new results to show state-of-the-art results.", "Q1: About the main concern on \u201cnovelty, improvement relative to the current state of the art implementations,  and non-linearity of G and D\u201d:", "A1: As for novelty, we first thank you for acknowledging that understanding GANs from the control theory perspective is promising and enjoyable to read.", "Indeed, as agreed by R#2, this is a major novel contribution that provides a unified and promising framework to model the stability of GANs, which includes some recent developments (e.g., Negative Momentum and Reg-GAN, See Sec. 4.1 and Appendix A&C) and also provides us a possibility to explore advanced tools in control theory (e.g., nonlinear control and modern control theory [*3]) to improve both the stability and convergence speed of GANs.", "Then, as some useful examples, in this paper, we particularly showed that the technique of negative feedback can be leveraged to stabilize GANs and developed NF-GAN, which was proven to be effective in our experiments.", "Finally, we added new results in Table 1 in the revision, which shows that the same technique of negative feedback can further improve the state-of-the-art method of SN-GAN [*5].", "Specifically, we apply NF-GAN to SN-GAN [*5] and NF-GAN provides a significant improvement on the state-of-the-art inception score on CIFAR-10 (from 8.22 to 8.45).", "Such results indicate that our technique of NF-GAN can still benefit the state-of-the-art variants of GANs (e.g., SN-GAN).", "Overall, our perspective is novel and it indeed sets new state-of-the-art results as compared to the current variants of GANs.", "As for the statement \u201cthe WGAN analysis does not take into account that G and D are non-linear\u201d, this is a potential misunderstanding.", "In fact, in the WGAN analysis, we do not put any constraints on the G and D networks, which can be any well-defined nonlinear models.", "This confusion may arise from the linearity of the dynamics.", "In Sec. 3.2", ", as we model the dynamics of G and D in the functional space instead of the parameter space, they can both denoted as integral parts (See Eqn. (10)&(11)), thereby both are linear.", "However, this doesn\u2019t influence the nonlinearity of G and D with respect to the weights.", "Technically, we provide an approximate solution to deal with such nonlinearity.", "As discussed in Remark 3 and Section 7 of the revised paper, we also note that the recent analyses of GANs on the functional spaces [*4] can provide a promising solution to solve this approximation and we leave it as our future work.", "Q2: About \u201ccomparison with the baselines\u201d:", "A2: For fairness, we tried our best to fairly compare all methods.", "See details in the Concern 1 of our post for common concerns.", "For the state-of-the-art results, we indeed provided a significant improvement on the inception score of CIFAR-10 (from 8.22 to 8.45) using SN-GAN\u2019s architecture as suggested.", "Q3: How does negative feedback (NF) influence the training of stable dynamics and further evaluation:", "A3: As stated above in our response to Q1, we added the new results of applying negative feedback to SN-GAN, which is a state-of-the-art variant of GANs with empirically stable performance.", "Our results (See Table 1 (bottom) in the revision) indeed show that NF can further improve to reach new state-of-the-art results.", "In general, as NF is essentially a penalty term that regularizes $D$ to the zero-function, we can expect it to be effective for most dynamics [*3].", "We also included the suggested related work (Balduzzi et al. 2018) in Section 5.", "Finally, as for the further evaluation (such as multiple seed runs and 2nd-momentum estimates), we agree it is interesting, but it is very demanding in computational resources, and we leave it for a systematic future investigation.", "In summary, this paper proposes a unified framework to model the dynamics of GANs which is a powerful tool to stabilize and improve GANs.", "The experimental results on SN-GANs demonstrate that NF-GAN can further improve the performance of stable GANs and provide an improvement to the state-of-the-art models.", "Finally, it is a promising direction to follow where further advanced control methods can benefit the training of GANs.", "[*1] Gidel, Gauthier, et al. \"Negative Momentum for Improved Game Dynamics.\" The 22nd International Conference on Artificial Intelligence and Statistics. 2019.", "[*2] Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. \"Which Training Methods for GANs do actually Converge?.\" International Conference on Machine Learning. 2018.", "[*3] Khalil, Hassan K. \"Nonlinear systems.\" Upper Saddle River (2002).", "[*4] Johnson, Rie, and Tong Zhang. \"Composite Functional Gradient Learning of Generative Adversarial Models.\" International Conference on Machine Learning. 2018.", "[*5] Miyato, Takeru, et al. \"Spectral Normalization for Generative Adversarial Networks.\" (2018)."], "labels": ["global_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "global_context", "global_context", "global_context", "global_context", "global_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 450, "sentences": ["We would like to thank you for reviewing our paper.", "[Principled Guidance] The design of guidances is heuristic, but as illustrated in Figure 2 and in Table 2, they are easy to design and are effective.", "Further, we added our unsupervised analyses to show that the method works even without explicit guidance on all tested datasets.", "In this paper, we propose the idea of guidance itself and show that it is imposing a desired semantics on the latent space without having labeled data.", "In our future work, we plan to investigate more principled ways of deciding guidances.", "We now address this point in our \"Discussion\" section.", "[Experiments Section] We have significantly updated qualitative and quantitative results in our \"Experiments\" section and now compare our methods against Beta-VAE, DIP-VAE, and InfoGAN."], "labels": ["global_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 451, "sentences": ["We thank you for your thoughtful review. We are happy to learn that you believe it is an interesting direction that holds potential for high impact.", "Re: simpler methods (like clustering, BoW etc.) might work equally well", ">", "To assess transfer learning potential reliably, we require both the X and y for the target task (i.e supervision).", "Consider the case where the target task is sentiment analysis, and one of the candidate tasks is finding sentence length (SentLen).", "For the sake of the argument, let us assume that the X for both sentiment analysis and sentence length is exactly the same set of movie reviews.", "In such a case, unsupervised metrics like clustering, BoW etc. would indicate maximum transfer potential, whereas the actual transfer potential would be close to zero (assuming the lengths of reviews aren\u2019t correlated with the sentiment).", "This is a fundamental problem of measures that look directly at the input data X without considering the nature of the labels y.", "For the sake of completeness, we will compare our methods with the suggested baselines in the camera ready/future versions of the paper.", "Re: not clear if the classifier weight difference is well defined", "> You are right in noting that the classifier weights might capture dissimilar yet useful features for two similar tasks, and hence the classifier weight difference might under-predict the transfer potential.", "We discuss this issue in the paper (section 4.1), which is why we avoid the set overlap metric.", "Owing to similar concerns, we recommend using CFS information transfer metric over classifier weight difference (which is also supported by results in Table 2 and Figure 3).", "Re: thoughts on how this could be applied outside the context of sentence representations and classification", "> It is easy to adopt our approach to study the information encoded in the encoders for other problems involving structured prediction (say POS Tagging).", "Instead of using a decoder that takes in all the dimensions of the encoded input token, one could iteratively select dimensions that provide the highest gains in decoding the right target sequence (say POS tags).", "Our formulation is very general, and it could potentially also be applied to other modalities like images for tasks like image classification and captioning."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 452, "sentences": ["We appreciate your time and comments on the work."], "labels": ["global_context"], "confs": [1.0]}
{"abstract_id": 453, "sentences": ["We are really thankful for the positive feedback. Here we give detailed answers to the Reviewer's concerns.", "1) - \"... in Table 2, AnyBurl ... yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.\"", "Thanks for pointing this out! We will make the presentation of the results consistent by highlighting the respective number.", "2) - \"... the expressiveness of the learned rules can be somehow limited,...\"", "We remark that our framework supports rules with negation, comparison among numerical attributes and classification operators, where linear functions over attributes can be expressed.", "Such rules capture a fragment of answer set programs, where a limited form of aggregation [Faber et al., 2011] and restricted external computation functions [Eiter et al., 2012] are allowed.", "While these rules might not cover all possible knowledge constructs, they are still valuable and rather expressive for encoding correlations among numerical and relational features.", "Moreover, to the best of our knowledge they have not been directly supported by previous works on rule learning.", "3) - \"Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 ...\"", "Thanks for referring us to this important work! We will certainly add this reference to the paper."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 454, "sentences": ["Thank you for the thoughtful review and positive assessment. We are glad to see that you appreciate the genuine flavor of causality in our paper and support our paper\u2019s acceptance.", "We agree that a formal exposition introducing an NLP/deep learning audience to the basics of interventions and counterfactuals and expressing a toy DAG to explain the spurious associations between the review sentiment and the manifestation in text of other attributes of the review, including but not limited to the genre, actors, budget, etc. We are actively working on preparing this exposition and while it is not yet in the draft we plan to have it prepared in advance of the camera-ready version.", "We thank the reviewer for pointing out that we should have been more thorough in explaining that while genre is a clear example of such a spurious association, it is far from the only one captured in Figure 4.", "Indeed, many other words, including \u201cwill\u201d, \u201cmy\u201d, \u201chas\u201d, \u201cespecially\u201d, \u201clife\u201d, \u201cworks\u201d, \u201cboth\u201d, \u201cit\u201d, \u201cits\u201d, \u201clives\u201d, \u201cgives\u201d, \u201cown\u201d, \u201cjesus\u201d, \u201ccannot\u201d, \u201ceven\u201d, \u201cinstead\u201d, \u201cminutes\u201d, \u201cyour\u201d, \u201ceffort\u201d, \u201cscript\u201d, \u201cseems\u201d, and \u201csomething\u201d, appear to be spuriously associated with sentiment and are captured by the original-only and revised-only classifiers as highly-weighted features.", ", Notably all of these features fall out from the highly-weighted features when our classifier is trained on counterfactually-augmented data.", "Regarding the sensitivity of BERT models, Table 9 shows the ability of a model explicitly trained to differentiate between the original and the revised data.", "This is to shed some insight on how much the two differ (on account of our intervention).", "Because the two indeed are different, we expect that a model should be able to differentiate them to some degree.", "We note that a model class\u2019s ability to differentiate between the original and revised data when explicitly trained to do so may not necessarily be correlated with how susceptible that model is to breaking when evaluated out of sample.", "We\u2019re grateful for your comments on exposition and will continue to address these points as we improve the draft."], "labels": ["global_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 455, "sentences": ["Thank you for your comments.", "With respect to your concern over scalability, the need to input the actions and observations of all agents in the value function (i.e. centralized value function) limits scalability only during training time, and it is a necessary measure to reduce the non-stationarity of multi-agent environments, as discussed in previous work [1].", "We would also like to re-emphasize the fact that our final trained policies are decentralized and do not require any information exchange between agents.", "This trait makes our approach (and other centralized-critic/decentralized-policy approaches) useful in situations where one can train in a simulation where communication is less taxing, but deploy in the real world, where communication may be more challenging.", "We also compared to other methods demonstrating the better scalability of our approach, cf. Table 2.", "Your thinking of \u2018semantically probable\u2019 exchange of information is interesting.", "We note that it is possible to compress each agent\u2019s actions/observations before they are sent to a central critic.", "Our setup naturally allows for this.", "Consider a case with high-dimensional image observations.", "In our approach, each agent needs to embed these observations (along with their actions) before sharing with other agents.", "In a situation where information exchange between agents is expensive, even during training, we can select a sufficiently small embedding space such that performance and efficiency are balanced.", "This notion of compressing embeddings prior to sharing across agents does not fit as naturally into the competing methods.", "Our experiments were especially designed to have two contrasting environments, so that we can illustrate two different aspects of multi-agent RL where we felt like the current approaches have not been able to address at the same time.", "Thus, it is by design that different baselines perform differently on them, as every approach has its own strengths and weaknesses.", "Our experiments demonstrate that our approach handles both environments well, which none of the baselines is able to do.", "Our experiments on Cooperative Treasure Collection demonstrate that the general structure of our attention model (even without considering dynamic attention as in our uniform attention baseline) is able to handle large observation spaces (and relatively larger numbers of agents) better than existing approaches which concatenate observations and actions from all agents together.", "Furthermore, our experiments on Rover-Tower demonstrate that the general model structure alone is not sufficient in all tasks, specifically those with separately coupled rewards for groups of agents, and dynamic attention becomes necessary.", "We have added a new section 6.3  to the supplement that includes visualizations of the attention mechanism both over the course of training and within episodes.", "Our code is available online and a link will be included in the paper once the anonymized review period is over.", "[1] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems, pp. 6382\u20136393, 2017."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 456, "sentences": ["Thank you for the comments and suggestions.", "As you pointed out, our current result in Section 5 does not apply to non-smooth activations -- understanding the generalization of ReLU networks would be interesting future work.", "We have updated the manuscript with a few minor modifications: 1) Figure on the population risk of sigmoid network (first layer optimized) in addition to SoftPlus; 2) additional remarks on the population risk of network in the kernel regime in Section 5.2; 3) corrected typos."], "labels": ["global_context", "multiple_context", "other_context"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 457, "sentences": ["-- \u201cThis work only considers problems for which the optimal input distribution is known, but is motivated by the fact that it could be applied to problems for which the optimal distribution is unknown and thus being able to discover new algorithms. It is hard to support this motivation when no experiments are done in its favor.\u201d", "The long-term agenda / research program is indeed two-fold:", "1. Investigate whether known optimal worst-case algorithms can be reproduced without any domain knowledge (i.e., \u201cCan ML learn Algorithms\u201d).", "This is the case in which the optimal distribution of inputs is also known.", "2. Discover new/better worst-case algorithms for problems with the aid of ML, when neither a good algorithms or input distribution is known.", "#2 is a long-term goal, and not tackled in this paper, but we believe #1 (tackled in this paper) is itself of strong interest (and difficult) -- would ML be able to discover the same \u201cpen-and-paper\u201d algorithms that computer scientists invented?", "The problems we study (ski-rental and Adwords) fall into the first category of problems.", "Note that the algorithms in the two cases are very different in structure.", "Further, please note that even though the optimal distribution of input is known in these two problems, we do not use it at all in training.", "Indeed, this is the main point of this paper -- the previous work of Kong et al. used these distributions to train the algorithm network (and hence that technique still needed the prior theoretical \u201cpen-and-paper\u201d work), while this work starts with ZERO knowledge.", "We follow this approach even in case #1 when the optimal input distribution is known exactly because we have the ultimate goal #2 in mind, that is, we want to design a framework that can eventually also work without knowledge of optimal input distribution (but that goal is outside the scope of this paper).", "-- \u201cNo comparison has been made between their approach and other previous approaches.", "We only know that the proposed approach finds near-optimal solutions with a difference of 0.01 competitive ratio.", "It is thus very hard to know if this new approach brings any improvement to previous work.\u201d", "We believe there is some misunderstanding here as to the contribution.", "As such, there are no previous approaches to \u201clearn algorithms\u201d (besides Kong et al.).", "To be more explicit (in case we didn\u2019t understand the comment)", ", previous work for algorithmic problems could fall into a few buckets:", "(1) The original algorithms papers which found optimal worst case algorithms [Karlin et al. 1986, Mehta et al. 2007].", "These give the analytical benchmarks.", "E.g., [Mehta et al. 2007] proposes the algorithm to solve Adwords, and proves that it achieves the optimal CR of 1-1/e ~ 0.63 (i.e., no matter what the online input sequence is, you get >= 1-1/e of the optimal solution in hindsight if you knew the instance offline).", "Thus the difference of 0.01 CR is a direct comparison to that work.", "(2) One may imagine there could be some kind of optimization (IP / LP) technique or some ML technique to solve specific instances of the problem (a specific instance of Adwords e.g.).", "But this is in fact not a feasible possibility, for two reasons: (a) Our problems are online problems where the full instance itself is not known in advance, and (b) we are looking for worst case competitive algorithms, i.e., a policy that does well no matter how the instance unfolds in the future.", "Thus there can not be previous work to compare in such a bucket.", "(3) Kong et al., 2018 is the closest previous work since it shows how to learn algorithms in the online setting.", "As mentioned above, the critical difference is that our paper learns the algorithms without any prior knowledge of the worst input distribution, but evolves both the distribution and the algorithm jointly (with some parallels to GANs, AlphaZero, self-play, etc. as we have stated).", "Quantitatively, the CR results are equally good; our main objective is to see if the learned algorithm is close in policy to the theoretical algorithm, and whether we are reasonably close to the optimal CR.", "Thank you for your review.", "Please also see our high-level clarification above which we believe can help in better interpretation of our contribution.", "Some specific responses below:", "** Addressing comments on the write-up:", "Thanks, these help improve the presentation greatly (we realize we wrote the exposition more  from a theoretical view and missed important ML details).", "Details on architecture: Agreed, and thanks. We have added some details on the specific network architectures to Appendix C (for ski rental) and Appendix D (for AdWords).", "New suggested structure and related suggestions: These are nice suggestions and explain why the structure was confusing. We\u2019ve worked on these to come close to the suggested structure.", "\u201cMSVV\u201d reference.", "Thanks for pointing out! This is the same algorithm described above in Mehta et al., but we realize that must have been confusing. Fixed.", "** Addressing Technical Comments:", "-- \u201chyperparameter searching\u201d:", "The networks we used in this work are fairly simple: dense layers with standard ReLu activation, and we use standard Adam optimizer.", "Simply choosing commonly recommended values for the parameters turn out to work well for the problems we looked at.", "In general, we agree with the reviewer\u2019s point that hyperparameter searching can be important.", "For this particular work, our focus is to introduce the high-level ideas/framework and offer initial evidence that it can be effective, so we do not dwell much on the technical parts of ML in our discussion due to page limits.", "We also clarify that it is not the case that \u201cwe have no interests in extending ML techniques\u201d in general.", "Indeed, we believe that for the future success of our approach on more open problems in online algorithms, it very much relies on the advances of ML in terms of neural network structure, optimization algorithms and training techniques.", "We also hope our work can motivate the design of new tools/techniques tailored for this direction."], "labels": ["multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "global_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 458, "sentences": ["* the fact that the random projections preserved the inner product (centered at zero) was probably not desirable.", "It might be more fruitful if these linear combinations were learned or sub-senses of words", "(e.g. [1])", ".", "Preserving the inner product means that the distribution of the features is not biased, if we keep adding words to the dictionary, the performance would degrade gracefully with the amount of compression.", "Perhaps a non-orthonormal basis would also work if the network compensates for the different distortions in the inner products.", "You are correct in assuming that other discrete building blocks could be more fruitful, but, we chose language modelling as a setting, not a task (see general response) as such, the building block chosen was the word. We could have chosen sub-words, or characters but the goal here is not the get the best possible language model but to understand a property of the mechanism.", "An interesting idea would be to actually use other information and encode it as random projections (e.g. syntactic dependency patterns).", "The amount of possible patterns is simply too large to be enumerated and as such the random projections would serve as unique \"fingerprints\" for unique \"dependency patterns\" that would be used as inputs.", "1. I do not get the point of bringing up NCE...", "Approximations like NCE (in conjunction with random projections) would allow us to remove the restriction in the output layer.", "We want to imply that our proposal is not incompatible with NCE, but we did not yet explore it so, to make the paper more self-contained it is probably best to leave this out.", "2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).", "we were trying to cram different experiments (with different regularization) in the same figure which is understandably confusing and needs to be corrected.", "3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?", "yes but not necessarily. Alpha can be used to control the expected proportion of non-zero entries, but as long as the probability of a sparse configuration is random uniform, our mechanism guarantees that any sampled index is almost orthogonal to any other sampled index, so it's easier achieve the same while guaranteeing the sparsity in the inputs."], "labels": ["single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 459, "sentences": ["Thanks for your efforts in reviewing our paper and the valuable comments.", "We attempt to address your concerns in the following.", "1. Response to the \"Weaknesses\" part and the comparison with GBDT", "As stated in the response to review 1, our goal is not inventing a model to beat GBDT but developing a model to cover the scenarios not suitable for GBDT such as some applications need online updating.", "\"The next contender\" model in your comment is the GBDT, which indeed works well for tabular data.", "However, GBDT suffers from two shortages, as stated in Section 2 and the responses to reviewer 3.", "These 2 shortages make GBDT very hard to be used in many real-world scenarios.", "For example, in an online recommender system, we need to update the model frequently to achieve the satisfying real-time performance.", "In this case, GBDT will be very inefficient as it needs to be re-trained from scratch.", "In contrast, NN can be learned by mini-batch fashion and therefore can learn from streaming data naturally.", "The proposed TabNN can overcome these shortages and achieve comparable accuracy with GBDT.", "Moreover, compared with previous NN based solutions for tabular data, TabNN outperforms them significantly.", "Therefore, TabNN is a better general solution for tabular data as it can cover more scenarios.", "2. Difference between \"implicit feature combinations\" and \"explicit feature combinations\"", "The main difference lies in whether the feature combination information is explicitly introduced into model structure or not.", "For example, in FCNN, as all features are connected to the neurons in the next layer, there are no feature combination information in the model structure.", "Although the feature combination information are not explicitly provided, one neuron in FCNN can learn a linear combination of its input features.", "Thus, we say there are \"implicit feature combinations\" in FCNN.", "In TabNN, we leverage GBDT to find feature combinations and then construct model structure according to them.", "Thus, we say there are \"explicit feature combinations\" in TabNN.", "\"Implicit feature combinations\" is not efficient as it introduces much more trainable parameters, and has a risk of over-fitting.", "In contrast, \"explicit feature combinations\" let model focus on the more important feature combinations and is more efficient.", "The successful CNN model also uses \"explicit feature combinations\", as it only combines the local pixels.", "3. About \"encourage parameter sharing\".", "Yes, we use parameter sharing in the one cluster of feature groups.", "We will clarify this in the paper.", "4. Benefits brought by the \"Structural Knowledge\"", "We had compared the benefit brought by the 'Structural Knowledge' in the experiment.", "The difference between TabNN (S) and TabNN (R), as shown in Table 3, implies that that the structural knowledge from GBDT yields a large contribution to the performance of TabNN.", "The \"Structural Knowledge\" is in TabNN by default. We will clarify this in the paper."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 460, "sentences": ["Thank you for the valuable and encouraging feedback! Below, please see our replies.", ">> What are the key limitations of AutoLoss? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?", "More discussions on these questions can be very helpful to further understand the proposed method.", "These are indeed good questions.", "We list several limitations we discovered during the development of AutoLoss:", "- Bounded transferability", "We observe AutoLoss has bounded transferability -- while we successfully transfer a controller across different CNNs, we can hardly transfer a controller trained for CNNs to RNNs.", "This is slightly different from some related AutoML works, such as in [1], where auto-learned neural optimizers are able to produce decent results on even different families of neural networks.", "We hypothesize that the optimization behaviors or trajectories of CNNs and RNNs are very different, hence the function mappings from status features to actions are different.", "We leave it as a future work to study where the clear boundary is.", "- Design white-box features to capture optimization status", "Another limitation of AutoLoss is the necessity of designing the feature vector X, which might require some prior knowledge on the task of interest, such as being aware of a rough range of the possible values of validation metrics, etc.", "In fact, We initially experimented with directly feeding blackbox features (e.g. raw vectors of parameters, gradients, momentum, etc.) into controller, but found they empirically contributed little to the prediction, and sometimes hindered transferability (as different models have their parameter or gradient values at different scales).", "- Non-differentiable optimization", "Meta-learning discrete schedules involves non-differentiable optimization, which is by nature difficult.", "Therefore, a lot of techniques in addition to vanilla REINFORCE are required to stabilize the training.", "Please also see our answer to the next question for more details.", "As a potential future work, we will seek for continuous representations of the update schedules and end-to-end training methodologies, as arisen in recent works [2].", "We haved add the above discussion to the latest version as Appendix A.9.", ">> As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.", ">> Any plan for open source?", "We acknowledge the difficulties of training controllers using vanilla REINFORCE.", "During our development of the training algorithm (See Eq.2, the \u201cdiscussion\u201d section in Sec.4, and Appendix A.1), we found the vanilla form of REINFORCE algorithm leads to unstable training.", "We therefore have made many improvements and adaptations by either referring to existing literature, or depending on the specific tasks.", "They include:", "- Substitute from the reward a baseline term, which is a moving average (see section 3, Eq.2)", "- Reward clipping (see section 3, under Eq.2)", "- Use different values of T for different tasks (see \u201cdiscussion\u201d in section 4)", "- Use improved training algorithms (e.g. PPO) for more challenging tasks, and slightly adjust reward generation schemes (see \u201cdiscussion\u201d in section 4, and Appendix A.1).", "We have also revised the submission to disclose more details on how we make these improvements.", "We will make all code and models trained in this paper available for reproducibility.", "[1] Neural optimizer search with reinforcement learning. ICML 2017.", "[2] DARTS: Differentiable Architecture Search. Arxiv 1806.09055."], "labels": ["global_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 461, "sentences": ["We thank Reviewer 4 for stating that \u201cthe proposed method has a good compression ratio while maintaining competitive accuracy\u201d.", "We provide clarification for the two main questions of the Reviewer below.", "Novelty of the paper", "As we state in our introduction, using codebooks to compress networks is not new, as well as using a weighted k-means technique.", "However, as we state in the paper: \u201cThe closest work we are aware of is the one by Choi et al. (2016), but the authors use a different objective (their weighted term is derived from second-order information) along with a different quantization technique (scalar quantization).", "Our method targets a better in-domain reconstruction, as depicted by Figure 1\u201d.", "Note that we already cite two of the suggested references by Reviewer 4, namely \u201cTowards the limit of network quantization\u201d and \u201cThiNet: A filter level pruning method for deep neural network compression\u201d in our work.", "We will further clarify our positioning in an updated version of the paper.", "Compression ratio", "We provide an example of the computation of compression ratio in Section 4.1, paragraph \u201cMetrics\u201d.", "Let us detail it further here.", "The memory footprint of a compressed layer is split between the indexing cost (one index per block indicating the centroid used to encode the block) and the cost of storing the centroids.", "Say we quantize a layer of size 128 \u00d7 128 \u00d7 3 \u00d7 3 with 256 centroids and a block size of 9.", "Then, each block of size 9 is indexed by an integer between 0 and 255: such integer can be stored using 8 bits or 1 byte (as 2^8 = 256).", "Thus, as we have 128 x 128 blocks, the indexing cost is 128 x 128 x 1 byte = 16,384 bytes = 16 kB.", "Finally, we have to store 256 centroids of dimension 9 in fp16, which represents 256 x 9 floats (fp16) = 256 x 9 x 2 = 4,608 bits = 4.5 kB.", "The size of the compressed model is the sum of the sizes of the compressed layers.", "Finally, we deduce the overall compression ratio which is the size of the compressed model divided by the size of the non-compressed model."], "labels": ["single_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 462, "sentences": ["Thanks for your attention to our work.", "1) For the presentation, we apologize for our typos and unclear statement in the paper. And your advice is so helpful. We will modify it.", "2) For the experiment, we will train our experiments longer and modify our network.", "Thanks for your advice."], "labels": ["global_context", "multiple_context", "multiple_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 463, "sentences": ["Thanks for your valuable comments. Below we address the detailed comments.", "Q1: Connection to the Jacobian matrix:", "A1: Thanks for the interesting suggestion.", "Indeed, the proposed regularizer can be interpreted as certain constraints on the Jacobian at the equilibrium point.", "Since at the equilibrium, $D(x)=0$ for all x, indicating that the equilibrium is a global optimal point of the negative feedback regularization $L = \\lambda \\int D^2(x)dx$. Therefore, the Hessian matrix $J = \\frac{\\partial^2 L}{\\partial \\phi^2}$ is positive-semidefinite.", "Otherwise, $\\phi$ is a stationary point instead of the global optimal point of the regularization term.", "Therefore, introducing the $L$ is equivalent to adding a negative-semidefinite matrix to the jacobian matrix of the original dynamics, which do help to stabilize the dynamics.", "We added the related discussion in Appendix E in the revision.", "Q2: The linear assumption on the dynamics:", "A2: Yes. The Laplacian transformation and the following discussions in Sec. 2.2 rely on linear dynamics, but it does not put any restriction on defining the discriminator as a nonlinear neural network.", "In Section 3, we can see that in the function space, the discriminator $D(x)$ and the generated samples $G(z)$ can be considered as integral parts which are also linear dynamics.", "The two non-linear operations are clearly denoted in Fig. 2(right), and we make an approximation to ignore these two non-linear operations which is widely adopted in control theory [*1].", "Q3: Eqn. (7):", "A3: Actually, Eqn. (7) comes from Eqn. (6).", "Letting $e = c - \\theta$, and taking Laplacian transformation on both side of Eqn. (6), we have $s\\mathcal{F}(D(t, x)) = x \\mathcal{F}(c-\\theta)$, which induces Eqn. (7).", "We made this clearer in the revision.", "Q4: The input and output of dynamics:", "A4: For a dynamic, both the input and the output are functions of time $t$. We take $D$ as an example.", "Since the dynamics of $D$ is equivalent to the integral part, the output of $D$ can be formulated as $D(t, x) = \\int_0^{t} g(u, x)du$, where $g$ is also a function of time t. In this setting, we say that the input of the dynamic is $g(t, x)$ and the output is $D(t, x)$, for all $x$. In Eqn. (10), we ignore the $x$ to emphasize that we modeling $D$ in the time-space.", "We added an example for a better presentation in Sec. 3.1 in the revision.", "Q5: The stability of previous methods:", "A5: Indeed, many existing methods can *generate realistic images*, which, however, does not necessarily imply that these methods *are stable*. For example, in Fig. 4 (left), the inception score of SGAN and LS-GAN is high at the beginning but finally diverges after a period.", "The early stopping in GAN's training is widely adopted otherwise the image quality will decrease, which indicates that these methods are not actually stable.", "Besides, NF-GAN can also boost the performance of SN-GAN to achieve new state-of-the-art performance (see details below).", "It indicates that improvement on the stability also benefits the state-of-the-art methods.", "Q6: Experiments:", "A6: For the experiments, we directly used the officially released code of Reg-GAN for fair comparison and it uses the ResNet instead of DCGAN architectures.", "We provided a detailed explanation about the experimental setting and further experimental results of the state-of-the-art performance in our response to \"The Common concerns about experimental setting and results\".", "[*1] Khalil, Hassan K. \"Nonlinear systems.\" Upper Saddle River (2002)."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 464, "sentences": ["[Q] My one ask would have been a survey of how activations might affect performance.", "I sense that everyone has settled upon LeakyReLUs for internal layers, but a survey of that work and experimentation within the authors' framework would have been nice.", "[A] We agree that this is an interesting question in it\u2019s own right and should and will be explored more rigorously in future work.", "At this point, it seems like the number of parameters and whether skip-connections are used is much more impactful.", "[Q] It would be interesting to see what these metrics would reveal when applied to other types of data (e.g. scientific images).", "[A] We are aware of several works in the area of scientific images, such as [1] and [2], where GANs were successfully applied on 2D image snapshots from N-body simulations.", "The main issue for us at this point is having access to such data sets.", "Nevertheless, as these data sets become available for public, we will happily include them within our framework and investigate whether the conclusions extend to data sets beyond natural images.", "[1] https://arxiv.org/abs/1702.00403", "[2] https://arxiv.org/abs/1801.09070", "[Q] I feel the discussion on loss was rushed, and I gained no insight on what the authors thought was a prominent difference between the three losses studied.", "[A] The theoretical differences between these losses were studied in detail in the corresponding publications.", "From the practical side, it\u2019s unclear which statistical divergence to optimize, in particular whether to pick (i) an f-divergence such as Chi-squared implemented by LS-GAN, or (ii) an integral probability metric such as Wasserstein distance, or (iii) a loss function which doesn\u2019t correspond to any statistical divergence, such as NS-GAN.", "Hence, we wanted to provide some insight on how do these perform within different setups, not necessarily the ones used in the original publications.", "To this end we uncover that on the considered data sets it's hard to outperform the non-saturating loss combined with regularization and normalization.", "Apart from this, the empirical evidence doesn\u2019t allow us to say more and we will clarify this in the manuscript.", "[Q] For architectures to be a main pillar of the paper, I feel that this area could have been explored in greater detail.", "[A] We agree with this assessment and we are indeed focusing on regularization and normalization.", "Our main question here was whether swapping Resnet with SNDCGAN leads to the same insights which is indeed the case.", "On the other hand, architectures are such a rich area enabling various design choices that they possibly merit a paper on their own.", "We will clarify the precise goal of the architecture exploration in this work.", "This being said, one major question we wanted to understand is which Resnet tricks from the literature (all 7 of them) are meaningful in practice and we present an ablation in the Section D of the appendix to conclude that the only relevant one is the number of channels which makes sense as it drastically changes the number of trainable parameters.", "[Q] The graphs were difficult to parse.", "I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.", "In addition, different charts presenting only one loss function, with their spectral normalization and gradient penalty variants, would have made the effects of the normalization more obvious on the FID distribution graphs.", "If this can be changed before publication, I would strongly suggest it.", "[A] What we can do is to separate the top from the bottom figure into separate figures and provide more information in the captions.", "Furthermore, in Figure 1, for the FID distribution plots, we can group the methods visually (according to the loss function) by drawing a slightly shaded rectangle around results with the same loss (e.g. https://goo.gl/6YeUL1).", "If you have a specific proposal we would be happy to consider it and update the submission.", "[Q] In the future, the authors should be careful to provide an anonymous repository for review purposes.", "[A] This is a good point and we will address this issue in the future.", "[Q] I would suggest changing the title to be more appropriate and accurate (the researchers are primarily focused on showing the positive and negative effects of normalization across various loss functions and architectures).", "[A] Given the architecture discussion stated above, this is a valid point.", "Our current candidate is:", "\u201cThe GAN Landscape: The effect of Regularization and Normalization across various Losses and Neural Architectures\u201d.", "However, if you have a specific proposal we would be happy to consider it."], "labels": ["single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "other_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 465, "sentences": ["Hello,", "Thank you for reviewing our paper.", "Regarding your comments about the triviality of our paper,", "We undertook an extensive initial phase of experiments where we discovered non-trivial contributions for achieving SOTA performance and fast convergence.", "In particular:", "- To the best of our knowledge, our specific implementation of entity pre-training is novel.", "Our technique can be ported to other neural, multi-task setups with a strong dependence between tasks (e.g. where a model must learn to perform some task before attempting to learn one or more dependent tasks jointly).", "For example, we have already begun porting this scheme to a neural cross-lingual summarization project of ours.", "Entity pre-training accounted for a 0.63% increase in our ablation experiment.", "On the CoNLL04 test set, it accounts for a 1.26% boost in performance, which is large relative to historic improvement on this corpus [1].", "- As opposed to previous models (e.g. [2], [3]) we were able to drop all recurrent architectures, which reduced training times substantially.", "Our work is one of the first architectures for joint NER and RE to do this.", "Because previous papers do not report their training times, we contacted the authors of a comparable method [4] for their training times and found that our method converged between 3-35X times faster for the ACE04, ADE, and CoNLL04 corpora (keeping in mind that we did not train on the same hardware).", "The large range in our estimate is because [4] trained for a wide range (60-200) of epochs.", "Our model serves as a strong baseline for future studies on joint NER and RE architectures and provides guidance on how to best integrate a pre-trained language model into such an architecture.", "For the ADE corpus in particular, we advance SOTA RE performance by >10%, which is substantially larger than improvements have been historically [5].", "It is also complementary to [6] (published in ACL this year), by demonstrating similar performance without the need for templated queries, which, as pointed out in our introduction, may become a limiting factor where domain expertise is required to craft such questions (e.g., for biomedical or clinical corpora).", "Regarding your comments on a System Demonstration submission to ACL,", "Our paper is a methodological advancement, not a system, tool, or demonstration [7] and is not suitable for submission to System Demonstrations at ACL.", "[1] https://paperswithcode.com/sota/relation-extraction-on-conll04", "[2] https://link.springer.com/chapter/10.1007/978-3-030-15712-8_47", "[3] https://arxiv.org/abs/1804.07847", "[4] https://www.sciencedirect.com/science/article/pii/S095741741830455X", "[5] https://paperswithcode.com/sota/relation-extraction-on-ade-corpus", "[6] https://arxiv.org/abs/1905.05529", "[7] https://aclweb.org/portal/content/acl-2020-call-system-demonstrations"], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 466, "sentences": ["Thank you very much for the comment.", "We believe that the core novelty of our work is in introducing a type of regularization based on a functional gradient.", "Because we were able to conduct additional experiment in time, we also conducted feature matching as well and confirmed the superiority of DC-regularization over the method (Table 8)."], "labels": ["global_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 467, "sentences": ["Thank you for the helpful comments and suggestions.", "Regarding the example, the goal of it is to help the reader understand", "the algorithm more easily.", "The wrong assignment with 80% probability", "is used to illustrate the situation where some predictions are wrong.", "\"if we try here $x_2 = 1$\" is based on the original formula, which is", "\"$x_1 \\lor x_2) \\land (\\lnot x_1 \\lor x_2) \\land ( x_1 \\lor \\lnot", "x_2)$\"", ".", "Therefore, the assigned formula is $x_1$.", "We tested PicoSAT, MiniSAT, Dimetheus and CaDiCaL and reported the", "results in the updated paper.", "CNNSAT outperformed all these solvers", "by", "at least two orders of magnitude over the \"Long Range\" dataset."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 468, "sentences": ["We thank a lot for the comments with cares and insights, and appreciate your efforts in reviewing our paper, which is helpful for improving the quality and readability of our writing. We are also glad that you support our paper.", "We agree that it is essential to justify how the reconstruction error works as a measure of privacy in this paper.", "In the revision, we have added the following justification on privacy quantification in Section 2, Section 4 and Section 5.", "We also note that the proposed reconstructive adversarial network (RAN), is not an extension of GAN but only borrows GAN\u2019s thoughts on adversarial training several neural networks, for the data privacy-uniquely problem.", "First, there is no single standard definition of data privacy-preserving problems and corresponding adversary attacks.", "And a fundamental problem in it is the natural tradeoff between privacy and utility, which is affected by different data privacy-preserving methods.", "Our key contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attackers and privacy quantification.", "Second, finding the right measurement for privacy is an open problem in itself.", "To evaluate RAN, one has to pick some quantifications.", "In the present paper, we chose the \u201creconstructive error\u201d as the quantification of privacy because it is the most intuitive one to measure the risk of disclosing sensitive background information in the raw data for the given perturbed data (Encoder output).", "Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a definitely defined application.", "For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN\u2019s Encoder output, and the sensitive patterns founded from the raw data, in the object recognition application."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 469, "sentences": ["We thank the reviewer for their time and response to our paper.", "Phrase similarity results: the tensor component T(v_a,v_b,.) does yield improvement over all other weighted additive methods in 5 out of 6 cases, as shown in Table 3.", "We have also updated that table with additional results, which show that adding in the tensor component improves upon the strong baseline of the SIF embedding method.", "We also added Table 4, which repeats the phrase-similarity task for verb-object pairs, and shows that the tensor component leads to improvement in most cases."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 470, "sentences": ["We have fixed the footnote and capitalization problems.", "Below are replies to other comments.", ">> Comment #1", "We agree vanilla REINFORCE can exhibit high variance.", "However, as we have elaborated in the text below Eq.2, to reduce the variance and stabilize the training, we have made the following adaptations referring to previous works [1,2]:", "- Substitute a moving average B (defined in text) from the reward", "- Clip the final reward to a given range", "We empirically found the two techniques significantly stabilize the controller training.", "Moreover, AutoLoss is not restricted to REINFORCE, but open to any off-the-shelf policy optimization method, e.g. for large-scale tasks such as NMT, we introduce PPO to replace REINFORCE, and adjust the reward generation scheme accordingly (see the paragraph \u201cDiscussion\u201d).", "We\u2019ve also revised Appendix A.1 to cover details of how PPO is incorporated.", "Empirically, with random parameter initialization most experiments manage to converge and give fairly good controllers.", "Almost all main results are averaged over multiple runs as explicitly indicated in the main text and the table or figure captions (e.g. see captions of Table.1 and Fig.2).", "See Fig.2 and Fig.3(R) where vertical bars indicate variances.", "We have also updated Table.1 to show the variance.", "We will release all code and trained models for reproducibility.", ">> Comment #2", "We have provided substantial analysis and visualizations on what AutoLoss has learned in our *initial submission*. Below, we summarize them for your reference:", "- d-ary regression and MLP classification", "*See sec 5.1, the 3rd paragraph in P6 for analysis, and Table.1 for comparisons to handcrafted schedules*: we observe AutoLoss optimizes L1 whenever needed during the optimization.", "By contrast, linear combination objectives optimize both at each step while handcrafted schedules (e.g. S1-S3) optimize L1 strictly following the given schedule, ignoring the optimization status.", "We believe AutoLoss manages to detect the potential risk of overfitting using designed features, and combat it by optimizing L1 only when necessary.", "- GANs", "Per our observation, AutoLoss gives more flexible schedules than manually designed ones.", "It can determine when to optimize G or D by being aware of the current optimization status (e.g. how G and D are balanced) using its parametric controller.", "- NMT", "*See sec 5.1, the 3rd paragraph in P7 and Fig.3(M)*: we have explicitly visualized in Fig.3(M) the softmax output of a learned controller and explain in text: \u201c...the controller meta-learns to up-weight the target NMT objective at later phase\u2026resemble the \u201cfine-tuning the target task\u201d strategy...\u201d.", ">> Comment #3", "We experimented with S>1 and found the improvement marginal.", "However, a large S requires more task model training steps to perform one PG (or PPO) update, meaning longer overall wallclock time for the controller to converge.", "We hence use S=1 as it performs satisfactorily.", "Note that some recent meta-learning literature uses policy gradient with batchsize 1, and report strong empirical results [3].", ">>", "Comment #4", "We\u2019d like to clarify that we have *not* claimed that \u201cAutoLoss can resolve mode collapse in GANs\u201d.", "AutoLoss improves the performance of GANs by enabling an adaptive optimization schedule than a pre-fixed one.", "Our point is better and faster convergence of the model training.", "In the GAN experiments we *qualitatively* observed the generated images are of satisfying quality and exhibit no mode collapse.", "But we never claimed we aim to or can resolve mode collapse.", ">> Comment #5", "We respectfully disagree with this comment.", "The NMT experiments aim to verify that AutoLoss can guide the multi-task optimization toward faster and better convergence on the target task, i.e. our interest is to see how the optimization goes instead of how the MT performs.", "Held-out PPL is the direct indicator of the quality of convergence, while BLEU evaluates the MT performance.", "Hence we believe PPL suffices as a metric to evaluate the performance of AutoLoss.", ">> Comment #6", "We acknowledge that there may exist DCGAN implementations that achieve higher IS on CIFAR-10, but note the following facts:", "- The link verifies in a table that the best official IS (reported in literature) is 6.16 (the number we report).", "- The self-implemented DCGAN 1:1 baseline used in our paper (see Fig.4(c)) achieves an IS=6.7, higher than 6.16.", "- Still, AutoLoss-guided DCGAN achieves IS=7, higher than 6.16 reported in literature, our own implementation, and the result from your link.", "Thanks again for mentioning spectral norm.", "However, these techniques are *completely orthogonal* from the scope of this paper, where we focus on whether AutoLoss can improve the convergence instead of resolving mode collapse.", "[1] Device Placement Optimization with Reinforcement Learning. ICML\u201917", "[2] Neural Optimizer Search with Reinforcement Learning. ICML\u201917", "[3] Efficient Neural Architecture Search via Parameter Sharing. ICML\u201918"], "labels": ["multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 471, "sentences": ["Thanks so much for your constructive feedbacks. Please see our response below.", "1. Influence of the number of images:", "Yes. The reason might be the higher chance of noise.", "It would be very important to provide a group of images that share similar patterns or topics.", "However, too many images for a sentence would have greater chance of noise.", "2. Impact of paired sentence-image dataset:", "Yes. We add the external MS COCO image caption training set and evaluate on the EN-RO task for quick evaluation.", "The BLEU scores are 33.55 and 33.71 respectively for COCO only and Multi30K+COCO.", "In addition, we are also interested in the influence of the number of sentence-image pairs inspired by your suggestion.", "We randomly split the pairs of Multi30K into the proportion in [0.1, 0.3, 0.5, 0.7, 0.9], the corresponding BLEU scores are [33.07, 33.44, 34.01, 34.06, 33.80] respectively.", "These results indicate that a modest number of pairs would be beneficial.", "3. The extra computation:", "The extra computation is negligible.", "The time of obtaining image data for MT sentences for EN-RO dataset, for example, is approximately less than 1 minute by tensor operation in GPU.", "The lookup table is formed as the mapping of token (only topic words) index to image id.", "Then, the retrieval method is applied as the tensor indexing from the sentence token (only topic words) index to image ids, which is the same as the procedure of word embedding.", "The retrieved image ids are then sorted by frequency.", "Learning image representations takes only about 2 minutes for all the 29,000 images in Multi30K using 6G GPU memory for feature extraction and 8 threads of CPU for transforming images.", "The extracted features are formed as the \u201cimage embedding layer\u201d with the size of (29000, 2400) for quick accessing in neural network.", "4. Missing BLEU scores & the number of parameters:", "Because those missing numbers (N/A) are not reported in the corresponding literature."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 472, "sentences": ["Thank you for your reviews and comments.", "We address your questions as follows.", "1. Scalability of the proposed solution", "From our current results, you may see that our approach has a decent scalability -- even though we doubled the subtasks and also introduced additional dependency in Crafting compared to Resource Collection, it does not need much more episodes for converging to optimal policies, where our agent-wise exploration plays an important role.", "Generally speaking, deploying more present workers coupled with our agent-wise exploration should significantly improve the learning efficiency and overcome the challenges introduced from more substasks or a larger dependency graph.", "In addition, the computational complexity is linear in terms of the number of agents, so our approach is also scalable when there are more agents.", "2. What is the reason for using rule-based agents in all the experiments?", "We have actually used RL agents as well (Appendix C.3), and it showed that our approach also works when workers are RL agents.", "In the main results, we focus on rule-based agents because it is computationally demanding to train a large population of RL agents, and our focus was not about the worker policies but rather how the manager assesses the workers\u2019 mental states and encourages an optimal collaboration accordingly.", "In this paper, using a cheap rule-based implementation with randomness has demonstrated the effect of different components of our approach.", "3. Are the authors willing to release the code?", "Yes, we do plan to open source our implementation.", "Specifically, the game environment and the worker agents were implemented in Python and it runs at a speed of more than 300 steps per second.", "We used PyTorch as the framework for implementing all the network modules.", "Typically it took < 10 hours to get a converged result by our approach on a single Nvidia Tesla V100 GPU.", "4. Typos", "Thanks for pointing out these typos. We will fix them in the next revision."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 473, "sentences": ["Thank you for your review.", "Please also see our high-level clarification above which we believe can help in better interpretation of our contribution.", "Some specific responses below:", "Reviewer #1 is absolutely right -- we don\u2019t know yet how to scale this to more difficult combinatorial problems.", "But let\u2019s clarify that statement a bit more:", "The ski-rental problem is often the first problem studied when teaching online algorithms, but it is certainly far from a \u201ctoy problem\u201d when we wish to learn an algorithm from scratch.", "We apologize if we painted an incorrect picture by calling it a \u201csimple example\u201d and a \u201cstaple introductory problem\u201d.", "It is easy to describe in that it has a single hidden parameter (the length of the ski season) and a single revealed parameter (the cost of buying).", "It is a staple introductory problem because it is elegant and illuminates the essential difficulties in designing online algorithms: there is a nearly-trivial factor-2 competitive algorithm (rent until you\u2019ve spent $B, then buy, so even if the ski season ends the next day, you\u2019ve not spent more than twice the least possible amount), but the 1-1/e competitive ratio algorithm is quite creative and subtle, and serves as an introduction to the richness of the field of online algorithms.", "In fact, the Karlin et al. (1986) paper also introduced the notion of competitive analysis of online algorithms, and  is probably the most-cited paper in this field.", "In some sense, this poses us the ideal challenge: can ML approaches discover creative and subtle \u201csolutions\u201d (in our case, an algorithm)?", "On a more technical note, please note that our \u201cmachinery\u201d of solving the two-player game is needed to discover an algorithm for the ski rental problem: if we don\u2019t allow the players to alternate and reach an equilibrium, for any fixed distribution on the ski rental instances (B, K), there is a deterministic algorithm that is optimal (among all online algorithms), and the worst-case performance of that (or any) deterministic algorithm is *provably* limited by a factor of 2 (i.e., there exists some distribution on instances where it will fail badly).", "Also refer to our discussion on this in the high-level clarification at the top.", "The AdWords problem considered is actually a difficult combinatorial problem, and is an archetypal online combinatorial optimization problem that captures the class of problems solvable by one of the most powerful techniques in this area -- primal-dual algorithms, which have led to the state-of-the-art approximation algorithms for numerous hard online (and offline) optimization problems.", "In particular, it generalizes bipartite matching, historically one of the most significant combinatorial optimization problems (led to the development of the classic Hungarian method, see https://en.wikipedia.org/wiki/Hungarian_algorithm).", "We did water down our ambition in a few ways:", "Instead of producing an algorithm that works for inputs of all sizes, we focus on the case of 9x3 (three advertisers, nine slots) -- a fixed finite size!", "This choice was arrived at based on the following criteria: what can we learn in a few hours of computation that\u2019s still *well beyond* what can be achieved through exhaustive search (for an algorithm).", "Think of our task roughly as learning to play a very hard game on a 9x3 board -- we would, of course, love to learn how to play the same game on arbitrary size boards, but the fact is that the game is mighty hard even at this \u201cboard size\u201d (since in each round, one player plays a 0-1 assignment to each cell in the board, and the other player picks a subset of the columns, in fact a weight vector on the columns).", "Instead of producing an algorithm that works for the 0-1 version of the problem, we produce an algorithm that works for the fractional version of the problem.", "This is, once again, motivated by making something work with modest amount of computation.", "Our explorations indicated that producing an algorithm for the 0-1 version needs reinforcement learning, and producing an algorithm that works on all 9x3 instances using this approach would still take several days of computation.", "On calling it Yao\u2019s Principle: as Reviewer #1 correctly noted, this is an application of the classic von Neumann minimax principle to the \u201cgame\u201d between an \u201calgorithm player\u201d and an \u201cinput player\u201d.", "We call it Yao\u2019s principle primarily in accordance with tradition in theoretical CS (see https://en.wikipedia.org/wiki/Yao%27s_principle and also https://blog.computationalcomplexity.org/2006/10/favorite-theorems-yao-principle.html, where it is noted that \u201cYao observed [the result]\u201d and commentators note that it\u2019s called Yao\u2019s principle because this observation has significant consequences for many central problems in TCS).", "We are happy to add text to reflect this."], "labels": ["global_context", "other_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 474, "sentences": ["Thank you for your comments.", "With regard to the structural choices of the attention model, our decision was based on a survey of attention-based methods used across various applications and their suitability for our problem setting.", "Our mechanism was designed such that, given a set of independent embeddings, each item in the set can be used to both extract a weighted sum of the other items as well as contribute to the weighted sums that other items extract.", "When applied to multi-agent value-function approximation, each item can belong to an agent and the separate weighted sums can be used to estimate each agent\u2019s expected return.", "Some other choices of attention mechanisms such as RNN-based ones (widely used in NLP), while interesting, do not naturally extend to our setting as our inputs (ie embeddings from agents) do not form a natural temporal order.", "We have updated our draft to provide more insight into our choices.", "We have included a new section 6.3 in the appendix of our revised draft that visualizes the behavior of our attention mechanism, as well as how it evolves over the course of training.", "While our approach does not significantly outperform the best individual baseline in each environment, it consistently performs near the top in all environments --- other methods falter in at least one of the two settings.", "Our experiments on Cooperative Treasure Collection demonstrate that the general structure of our attention model (even without considering dynamic attention as in our uniform attention baseline) is able to handle large observation spaces (and relatively larger numbers of agents) better than existing approaches which concatenate observations and actions from all agents together.", "Furthermore, our experiments on Rover-Tower demonstrate that the general model structure alone is not sufficient in all tasks, specifically those with separately coupled rewards for groups of agents, and dynamic attention becomes necessary."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 475, "sentences": ["Thank you for the encouraging comments.", "First of all, we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods, and observe that they are also vulnerable to blind-spot attacks.", "Please see our reply to all reviewers.", "We agree that the K-L based method is complicated and computationally extensive.", "Fortunately, we only need to compute it once per dataset.", "To the best of our knowledge, currently, there is no perfect metric to measure the distance between a training set and a test set.", "Ordinary statistical methods (like kernel two-sample tests) do not work well due to the high dimensionality and the complex nature of image data.", "So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem.", "As suggested by the reviewer, we added a new metric based on the mean of \\ell_2 distance on the histogram in Section 4.3.", "The results are shown in Table 1 (under column \u201cAvg. normalized l2 Distance\u201d).", "The results align well with our conclusion: the dataset with significant better attack success rates has noticeably larger distance.", "It further supports the conclusion of our paper and indicates that our conclusion is distance metric agnostic.", "We hope that we have made everything clear, and we again appreciate your comments.", "Let us know if you have any additional questions.", "Thank you!", "Paper 1584 Authors"], "labels": ["global_context", "global_context", "other_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "other_context", "other_context", "global_context", "global_context", "global_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 476, "sentences": ["Thanks for your effort in providing this detailed and useful review!", "We present our clarification in the following:", "Q1: the feasibility of using neural networks to learn cumulative quantities:", "A: In each iteration, only a small subset of information sets are sampled, which may lead to the neural networks forgetting values for those unobserved information sets.", "To avoid such catastrophic forgetting, we used the neural network parameters from previous iterations as initialization, which gives an online learning/adaptation to the update.", "Furthermore, due to the generalization ability of the neural networks, even samples from a small number of information sets are used to update the new neural networks, we find that the newly updated neural networks can produce very good value for the cumulative regret and the strategy mixture.", "(we give related discussion in section 3.1 and add much more experimental results in Figure 5, further details please see the revised paper.)", "Q2: It does not seem necessary to predict cumulative mixture policies (ASN network)?", "A: As you say, any information nodes I_i would be sampled proportionally to \\pi^{\\sigma^t}_i(I_i), which is the same probability as in the definition of the mixture policy (Eq.4).", "Actually, if we have a large enough buffer to save all the sampled nodes, it\u2019s easy to inference the mixture policy accordingly.", "However, in the large game, this large memory is expensive and impossible.", "Another method called reservoir sampling was used in NSFP to address a similar problem.", "We borrow this idea to our method, however, the achieved mixture policy cannot converge to a low exploitability.", "Actually, the third possible solution could employ the checkpoint of each current strategy, and mixture this current strategy accordingly.", "Q3: It would help to have a discussion about how to implement (7), for example do you use a target network to keep the target value R_t+r_t fixed for several steps?", "A: The optimization problem for the double neural networks is different from that in DQN, where the target network is fixed for several steps and only one step of gradient descent is performed.", "In our setting, both RSN and ASN perform several steps of gradient descent with stochastic mini-batch samples.", "Furthermore, in DQN, the Q-value for the greedy action is used in the update, while in our setting, we do not use greedy actions.", "Algorithm E gives further details on how to optimize the objectives in Equation 7 and Equation 8 (Further discussion please see the revised paper.)", "Q4: It is not clear how the initialisation (10) is implemented.", "Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?", "A: Generally, Eq.10 is an idea of behavior cloning algorithm.", "Clone a good initialization, and then continuously update the two neural networks using our method.", "In the large extensive game, the initial strategy is obtained from an abstracted game which has a manageable number of information sets.", "The abstracted game is generated by domain knowledge, such as clustering similar hand strength cards into the  same buckets.", "(refer to section 3.3 in the revised paper.)"], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 477, "sentences": ["Thanks for your effort in providing this detailed and useful review!", "We present our clarification in the following:", "Q: I believe the paper could benefit from more extensive  comparisons in Figure 4A against other IIG methods such as Deep Stack, as well as comparing on much larger IIG settings with many more states to see how the neural CFR methods hold up in the regime where they are most needed.", "A: To address this problem, we add three different kinds of experiments.", "Use small batch size, only a small subset of infosets are sampled in each iteration.", "In this case, we can present the generalization ability of the neural network.", "Use small embedding size and let the number of parameters is much fewer than the number of infosets of the whole game tree.", "In this case, we can present the compression ability of the neural network.", "Use the larger stack to increase the size of the game tree.", "In all these three kinds of experiments, we find the neural CFR can still converge to a good strategy.", "Further details please see Figure 5(A), 5(B) and 5(C) in the revised paper.", "We fixed the typos in the revised paper accordingly."], "labels": ["global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 478, "sentences": ["Thank you for your reviews and comments.", "We respond to your questions as follows.", "1. Scalability?", "While we agree that the tasks in this paper are not real world problems, we think, as a first step towards this direction, the evaluations in this paper have provided some promising proof-of-concept results. Applying the approach to more realistic and more complex tasks could be a good future research direction.", "2. It would be beneficial to provide more information about the baselines", "We have added details of baselines including their reward functions in Appendix E.", "3. For the generated figures, it would be beneficial to include standard deviation and mean over multiple runs", "We show the standard deviation of multiple runs in Figure 7,8,9 in the revision.", "We have done our best to evaluate the robustness given the limited time and will continue to improve the evaluation.", "4. The strong dependence of performance on the dimensionality of the latent, internal state (Fig.4).", "The network architecture design is not the focus of our paper.", "Generally speaking, a higher dimensionality of the latent vector provides a more powerful network to model agents.", "However, as we show in Figure 4, with probing, the network with lower dimensionality can even outperform the baselines trained with latent vectors that have higher dimensions.", "And with the same architecture, probing clearly provides a significant improvement.", "5. Minor issues.", "Thanks for pointing out these issues. We have fixed them in the revision."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 479, "sentences": ["The authors appreciate the reviewer\u2019s time and efforts for reviewing this paper and would like to respond to the questions in the following paragraphs.", "[Comment]", "Compare FICM against simpler exploration baselines such as epsilon-greedy or entropy regularization.", "[Response]", "We would like to thank the reviewer for raising this interesting question, and would like to bring to the reviewer's kind attention that in the original paper of our baseline \"ICM\" [1], the authors had provided a comparison against an \u2018A3C\u2019 baseline (using entropy regularization) with epsilon-greedy exploration method (Section 3 of [1]).", "According to the experimental results presented in Section 4 of [1], it has been demonstrated that ICM is superior to that baseline in a number of environments.", "This is the reason why we omit that baseline in our paper.", "As our primary interest and focus is prediction-based exploration methods using intrinsic reward signals (as discussed in Section 1 of our paper), we only compare our FICM with ICM [1], RND [2] and large-scale [3], concentrating on analyzing the pros and cons between our proposed method and the other prediction-based ones.", "However, we would still be glad to include additional comparisons against the suggested methods in the final version of our paper, if the reviewer considers that is informative for the readers to comprehend the paper.", "[Comment]", "More extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.", "[Response]", "We appreciate the suggestions from the reviewer and would like to share with the reviewer our additional experimental results of ICM using the same hyper-parameter settings described in Section 4.1 in the following figure.", "(figure link: https://imgur.com/5pPl8PV )", "It is observed that ICM is only able to deliver comparable performance to our method in Atari game \"Seaquest\". We would definitely be glad to incorporate these new results in our manuscript in the revised version.", "[Comment]", "Reproducibility.", "[Response]", "Thank you very much for the suggestions.", "We have already uploaded our source codes as well as the demonstration videos to the following sites.", "Our experimental results and statements presented in the manuscript are fully reproducible and verifiable.", "Github: https://github.com/IclrPaperID2276/iclr_paper_2276", "Demo Video: https://youtu.be/JL68QFNj_N8", "We hope that we have adequately responded to your questions, and would be very glad to discuss with you if you have any further comments or suggestions.", "[1] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. In Proc. Int. Conf. Machine Learning (ICML), pp. 2778\u20132787, May 2017.", "[2] Y. Burda, H. Edwards, A. Storkey, and O. Klimov. Exploration by random network distillation. In Proc. Int. Conf. Learning Representations (ICLR), May 2019b.", "[3] Y. Burda, H. Edwards, D. Pathak, A. J. Storkey, T. Darrell, and A. A. Efros. Large-scale study of curiosity-driven learning. In Proc. Int. Conf. Learning Representation (ICLR), May 2019a."], "labels": ["global_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "single_context", "other_context", "single_context", "single_context", "single_context", "other_context", "other_context", "other_context", "global_context", "single_context", "single_context", "global_context", "global_context", "global_context", "global_context", "global_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 480, "sentences": ["Thanks for the detailed and thoughtful review.", "We are glad that you think of this paper as a timely contribution addressing an important problem that must be addressed in order to build more robust NLP systems.", "We agree with your point that it would be great to have a practical takeaway guiding practitioners for what to do in practice.", "We believe that the first step here is to characterize the problem coherently and that having laid this groundwork, one immediate next step is, as you suggest, to develop a more practical solution that requires a less expensive/onerous annotation effort.", "The key contribution of our paper is to provide a clear characterization of a variety of concerns in the language of interventions and to demonstrate that indeed, they can be addressed by acquiring interventional data.", "The knowledge that (i) NLP models trained on counterfactually augmented data suffer less from these problems and (ii) transport better out of sample (see new results in the updated draft, per R3\u2019s suggestion) validates this.", "As you mentioned, our solution requires significant expenditure (both financial and human capital)", "compared to simply labeling data", ".", "As a follow-up, for existing datasets, our next steps include investigating how to make these adjustments in a cost-effective way.", "In preliminary work, we have been investigating how to use humans in the loop more effectively.", "One approach involves using generative models to propose candidate substitutions and relying on humans only accept or reject the revisions (vs having to write them from scratch).", "Our experience with crowdsourcing suggests that this feedback would be significantly cheaper to collect (provided that a reasonable fraction of suggestions were appropriate).", "We additionally note that for some tasks, such as NLI, creating new datasets already requires annotators to synthesize examples de novo and the fractional increase for soliciting counterfactually-augmented data might not be as onerous as compared to tasks where the default is to rely on annotators only for tags.", "We are also appreciative of your constructive suggestions to improve the paper, and have taken several steps to improve the draft.", "These include updating the draft to include (i) a detailed analysis of edits performed on SNLI, (ii) results on various datasets using an ELMo based classifier; (iii) concerning your question about larger Bi-LSTMs, we had tried a large Bi-LSTM but it overfit badly.", "We have updated the draft to include this detail.", "Thanks also for catching several typographic errors. We have addressed them in the new draft."], "labels": ["global_context", "global_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 481, "sentences": ["I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.", "R)We believe as future work our algorithm can be combined with Winograd techniques for optimization.", "For instance winograd is designed to use a batch of images to convolve with a kernel, here an image convolves with a \u201cbatch of kernels\u201d.", "There is no reason why those two techniques can be merged.", "In our implementation we perform a set of convolutions with the input image where FFT can be applied too.", "p2-3, Section 3.1 - I found the equations impossible to read. What", "are the subscripts over?", "In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??", "Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?", "Very good Catch", "it should be (N)x(N) instead of (N+1)x(N+1). (Fixed on the paper)", "Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?", "{k,l} locate the convolving window inside of the input image", "Experimental section: Like depthwise convolutions, you seem to achieve reasonable accuracy at fairly low computational cost.", "It would therefore be much more interesting to compare your networks with ShuffleNet style networks designed for computational efficiency, rather than networks designed mainly to push the benchmark numbers down whatever the cost.", "R)We compared now against: mobileNet, ShuffleNet, HENet, SqueezeNet, we have less number of parameters or better accuracy or both.", "For instance our method has 4X les parameters than shuffleNet and better accuracy (Added to the paper)", "It would be helpful to have the computational cost of the network in FLOPs, and running time compared a regular ConvNet using Winograd/Fourier convolutions.", "R)In this paper we focus on the reduction of parameters, we didn\u2019t focus on the speed, we notice that in our experiment our models were trained using half of the epoch used for the conventional models.", "In terms of the number of operations the LeNet as in the tutorial has 2.29M MAC operations, while our method has 1.23M MAC operations for MNIST.", "(Added to the paper)"], "labels": ["single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 482, "sentences": ["Thank you for your comments!", "* We have enhanced Appendix E with a discussion on some variants of the VAE that generalize the standard evidence lower bound (ELBO) by incorporating different bottleneck constraints to learn better representations.", "In particular, we discuss how our unsupervised objective (p. 18, equation 38) relates to the beta-VAE and the importance weighted autoencoder (Appendix E.1, p. 17,18).", "* Please note that our unsupervised objective (p. 18, equation 38) contains the beta-VAE as a special case when we use only one sample from the encoding distribution (M=1).", "This means that we are naturally comparing with that method.", "* Appendix E.2 (p. 18) now discusses implementation details of the unsupervised objective.", "Finally, we have included some visualizations in Appendix E.3 for the MNIST and FMNIST datasets for different values of M and beta.", "We agree that more comparisons will be beneficial in investigating the properties of the proposed method.", "This is something we are actively working on."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 483, "sentences": ["1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?", "R)It is possible to change all the layers to use Adaptive convolutions, we replaced only one to measure the unitary contribution.", "We chose the first one because it is where the feature extraction is performed, in addition fully connected layers can use this technique.", "One Adaptive layer can replace two traditional layers.", "(Added to the paper)", "2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?", "R) We can use any Activation function.", "We actually tested ReLu with good results, but we chose tanh because it generates weights in the range of (-1,1) avoiding large values given by ReLu.", "(Added to the paper)", "3. Traditional convolutional kernels together with max pooling operations ensures some degree of translation invariance.", "How big is the generalization gap for the tested models when adaptive kernel is used?", "R)we added an experiment to test the generalization", "4. How sensitive are the results to the number of adaptive kernels in the layers.", "R) As in traditional CNNs, the increment of the number of kernels in a layer produces some saturation, with a marginal increment of accuracy.", "In our experiments it was observed that 5 dynamic kernels generates comparable level of abstraction than 30 traditional convolutional kernels.", "(Added to the paper)", "5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?", "yes we have test several layers with adaptive kernels. but we focus on report the results on the first layer to highlight the contribution", "6. On CIFAR10 the results seem to be worse that other methods.", "However, it is important to note that the Adaptive Kernels CNN has way less parameters.", "It would be interesting to see how the performance of adaptive kernels based CNNs scales with the number of parameters.", "R)We added a new experiment where we show how the performance of adaptive kernels improves with the increment of parameters.", "In order to make a fairer comparison we also added another experiment where we compare the accuracy of ResNet18 with 1 adaptive layer against ResNet18, ResNet50 and ResNet101 and it can be seen that the adaptive one performs even better than ResNet50.", "7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.", "R) Added another experiment, where we use DroNet as base to show the benefit of combine Adaptive layers with ResNet, in this experiment we test different configurations to compress the network up to 32X  (Added to the paper)", "8. The authors acknowledge the similarities (and some differences) with Brabandere et al (2016).", "It might be beneficial to include comparison to this approach in the experimental section.", "Moreover, given the similarities, it might be good to discuss the differences in the approaches in the introduction section.", "R)they train a NN to generate a model of another NN, we  have a ACNN that learns how to generate its filters.", "(in the intro)", "Some typos:", "1. the difficult to train the network", "2. table 2: Dynamic -> Adaptive?", "Very Good Catch", "\uf04a", "Overall, the paper presents interesting ideas with some degree of originality.", "I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.", "We compared now against: mobileNet, ShuffleNet, HENet, SqueezeNet, we have less number of parameters or better accuracy or both (Added to the paper)"], "labels": ["single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "other_context", "single_context", "single_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 484, "sentences": ["Hello,", "Thank you for your review of our paper.", "We appreciate the positive assessment of the clarity of our writing.", "Regarding the suggested ablations,", "1. For reasons outlined in our response to the public comment you reference, we do not believe this ablation (as suggested) would be meaningful.", "For convenience, we have copied that response here: In our model, BERT is more than a source of contextual word embeddings as we fine-tune all of its ~110M parameters during training.", "Simply replacing BERT with distributed embeddings and a character-CNN or LSTM wouldn\u2019t allow us to determine the effect of contextualized embeddings because we would simultaneously be removing the majority of our model\u2019s trainable parameters.", "Nevertheless, we performed the suggested ablation by swapping BERT for GloVe embeddings (300 dimensional) and found that NER performance dropped from 89.46% to 40.33% and RE performance fell from 66.83% to 14.44% on the test set of the ConLL04 corpus (note that we had to increase the learning rate by 10X to get the model to converge).", "If you were to somehow control for this drop in model capacity, say by adding in an LSTM network, the ablated model would closely match this paper [1], whom we outperform by ~3% overall on the CoNLL04 corpus.", "This paper is not cited in Table 1 as they report macro-averaged F1 scores, while most other papers (including the current state-of-the-art [2]) report micro-averaged F1 scores, as we did.", "Finally, it is well known that contextual embeddings outperform distributed embeddings on a wide range of NLP tasks, including NER [3].", "The aim of our study wasn\u2019t to compare contextual vs. distributed embeddings but on how to successfully integrate BERT into a state-of-the-art joint NER and RE architecture.", "2. Thank you for this suggestion.", "We are currently performing the ablation, and will comment again once we have the results.", "We will be performing the same ablation as used in [2] (see section 6.2).", "Just note, because our manuscript is already at the page limit, we may have to place the results of this ablation in the appendix.", "Regarding predicted entity label embeddings,", "Before training, all unique entity labels (e.g. B-PER, I-PER, ... etc.) are embedded by assigning them to randomly initialized, continuous vectors of 128 dimensions (this hyperparam is mentioned in Table A.2 of the appendix).", "The embeddings are then updated along with the rest of the models' parameters during training.", "Practically speaking, this is handled for us via the embedding layer in PyTorch [4].", "This is the same method used in the works we compare to ([1], [5], [6]).", "We have updated the text in the manuscript (under section 2) to make this more clear.", "Thank you again for taking the time to review our paper.", "[1] https://link.springer.com/chapter/10.1007/978-3-030-15712-8_47", "[2] https://arxiv.org/abs/1905.05529", "[3] https://arxiv.org/abs/1802.05365", "[4] https://pytorch.org/docs/stable/nn.html#embedding", "[5] https://www.aclweb.org/anthology/P16-1105/", "[6] https://www.sciencedirect.com/science/article/pii/S095741741830455X"], "labels": ["global_context", "global_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 485, "sentences": ["Thanks for your constructive feedback.", "We have modified the paper to clarify some of the terms per your suggestion.", "Please find our detailed response below:", "[R1: In Table 1, for ImageNet, Shadow Attack does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?]", "During attack/crafting, we need to make an adversarial example that gets misclassified even after perturbations drawn from a Gaussian distribution centered at zero with scale sigma.", "During evaluation, while the augmentations are drawn from a similar distribution, the realized random variables are not identical to those used for crafting the adversarial perturbation.", "In ImageNet, where the dimensionality is high (224X224X3) and for larger sigmas, to have a relatively dense and representative sampling, we need to sample a lot more perturbations during adversarial example crafting.", "However, in our experiments, we could only sample up to 400 instances per example (the maximum batch-size that could fit on our machine with 4 GPUs is 400).", "This results in having a sparse sample when the standard deviation is higher.", "One can potentially improve these results by using larger batch-sizes (i.e., sampling more) or a more powerful GPU or even a TPU, however we do not have the resources for such experiments at this time.", "[R1: In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.", "In my opinion, to support the above claim, shouldn\u2019t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?]", "In the original submission, we tried to produce tables that look like the tables in papers that we compare", "to", ".", "The randomized smoothing paper reports certified radii and also accuracy (1-error) under various perturbation bounds.", "However, the CROWN-IBP paper and the improved randomized smoothing paper based on adversarial training of smoothed classifiers (SmoothAdv) only report *error rates* using a fixed distance to the decision boundary.", "This is done because, unlike the Randomized Smoothing method, the radii are not directly calculated in the CROWN-IBP method and cannot be accessed directly;  CROWN-IBP takes a fixed radius chosen by the user, and either produces or fails to produce a certificate for that radius.", "This is in contrast to randomized smoothing, which outputs different radii for different images (a larger radius means a stronger certificate).", "In regards to why we compare the errors on natural images and those of our adversarial images:", "Please see the (updated) last paragraph of Section 5, which explains this comparison in detail.", "In short -  we are comparing the rate at which natural images certify to the rate at which adversarial images certify.", "For the case of large perturbations, we find that our adversarial image produce certificates more often than natural images!", "For small perturbations, our attack still produces certificates reasonably often, although not quite as frequently as natural images.", "This shows that certificates alone cannot be used to reliably discern between natural images, and adversarial images produced by the proposed shadow attack.", "[R1: From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?]", "This happens because, for CIFAR-10, the smoothed classifier is very \u201cconfident\u201d on a subset of the validation images which correspond to that right peak.", "Here, our use of \u201cconfidence\u201d should not be confused with the confidence of a network (output of the softmax layer).", "For the purpose of the certified radii, the \u201cconfidence\u201d we are interested in is related to the prediction of the network on the Gaussian perturbed images (i.e., a very high \u201cconfident\u201d example is an example where all of the perturbed images get the same label).", "[R1: Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.", "A smaller dissimilarity suggests a greater similarity between channels.]", "Good point! We have updated this in the revised document, and we think it enhanced clarity.", "[R1: Lambda sim and lambda s are used interchangeably. Please make it consistent. ]", "Fixed. Thank you.", "[R1: The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.]", "In the revision, we have described what the numbers are representing in more detail."], "labels": ["global_context", "global_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 486, "sentences": ["Pipelined backpropagation is similar to model parallelism but it addresses the resource underutilization issue in model parallelism.", "Our pipelined method might look like async-SGD on surface.", "However, async-SGD (e.g. Dean et al., pointed out by Reviewer 4) utilizes data parallelism (as indicated in Dean el al.) and a parameter server to keep track of model parameters (weights).", "In contrast, our pipelined method does not use any parameter server.", "Furthermore, each accelerator obtains a replica of a full model in asycn-SGD training while each accelerator contains only a part of the model in our pipelined method, on the assumption that the full model does not fit into the memory of a single accelerator.", "The accuracy drops for some models in a pure pipelined training.", "However, hybrid training is able to bring the accuracy of most networks studied in our paper up to a comparable level of the non-pipelined baseline as shown in the evaluation section of our paper.", "Our pipelined method is different from data parallelism in the following way (for a 2-GPU example).", "For data parallelism, a model is duplicated and placed onto 2 GPUs, each GPU containing a full copy of the model.", "On the other hand, for pipelined parallelism, a model is divided into two partitions (on the assumption that it cannot fit in a single device): one is mapped onto GPU 0 while the other is mapped onto GPU 1, each GPU obtaining only a part of the model.", "Communication between these two partitions is necessary to enable activation and gradient transfers.", "Regardless of the parallelization techniques, the maximum speedup of a 2-GPU system is 2X compared to a 1-GPU system.", "To obtain a close to perfect speedup of 2X, the communication overhead must be almost non-existent and the workload needs to be perfectly balanced between the 2 GPUs.", "In our implementation, we obtained a speedup of 1.81X for ResNet-362, which is equivalent to 90% utilization of each GPU.", "Thus, our sentence the reviewer refers to.", "Thank you for pointing out the accuracy of ResNet-20 (similar to Reviewer 1).", "Again, we think the exact inference accuracy of the model is somewhat orthogonal to our study.", "It is the trend of the decline in inference accuracy with pipelining is what we study.", "This trend exists with both our hyperparameters and those at, for example, https://github.com/akamaster/pytorch_resnet_cifar10.", "The use of these set of hyperparameters, obtains an inference accuracy of 91.65% (better than the accuracy stated in the original ResNet paper) for ResNet-20 non-pipelined baseline and 91.21% for pipelined version.", "We are not aware of any reports of an accuracy of ResNet-20 at 92% (perhaps this is approximate).", "Please kindly let us know a pointer.", "It is relatively easy to update our results in the paper with new hyperparameters."], "labels": ["single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 487, "sentences": ["Thank you for the time and effort spent reviewing our paper, and for the detailed suggestions.", "Below we repeat the questions/comments from the review and respond to each in turn.", "\u201cThe paper positions itself generally as dealing with arbitrary transformations T, but really is about angular transformations (e.g. Definition 3.1).", "The generalization is relatively straightforward and was not too surprising given the APG theory.", "The paper would gain in clarity if its scope was narrowed.\u201d", "Our MPG framework not only supports the angular transformation but also covers the recently proposed clipped transformation in CAPG [Fujita and Maeda, 2018].", "The theoretical result is tighter than the one in [Fujita and Maeda, 2018], and it supports general transformations instead of only clipped actions.", "\"I am not convinced that the measure theoretic perspective is always necessary to convey the insights, although I appreciate the desire for technical correctness.\" / \"Generally speaking it seems like a lot of technicalities for a relatively simple result: marginalizing a distribution onto a lower-dimensional surface.\"", "We agree that the measure theoretic approach is not always necessary (indeed for angular actions, it is not needed), but it is necessary for a very common scenario -- clipped actions.", "Researchers and practitioners both almost always clip actions when using policy gradient algorithms for robotics control environments (read: MuJoCo tasks).", "Recently, a reduced variance method was introduced by Fujita and Maeda (2018) for clipped action spaces.", "Their algorithm is also a member of the marginal policy gradients family and our theoretical results for MPG significantly tighten the existing analysis of that algorithm.", "\"It's hard for me to judge of the experimental results of section 5.3, given that there are no other benchmarks or provided reference paper. As a whole, I see APG as providing a minor benefit over PG.\"", "For the results in Section 5.3, the issue is that currently, there are no benchmark environments for directional control.", "We anticipate that in the future this may change (e.g. console and PC games often have directional controls).", "\u201cWhat does E_{pi|s} refer to in Eqn 4.1?\u201d", "The expectation is taken with respect to the policy \\pi conditioned on the current state s (s here is arbitrary, but fixed).", "Stated differently, we are taking the expectation with respect to the distribution $\\pi(\\cdot | s,\\theta)$.", "\u201cCan you clarify what it means for the map T to be a sufficient statistic for theta? (Theorem 4.6)\u201d", "We have now removed this part of the statement because we are no longer absolutely certain of its correctness, and because it is not used anywhere else in the paper.", "\u201cExperiment 5.1: Why would we expect APG with a 2d Gaussian to perform better than a 1d Gaussian on the angle?\u201d", "Because using a 1D Gaussian requires either (1) clipping the angle to [0,2\\pi) before execution in the environment and making updates using the clipped output or (2) using the sampled angle for updates and perform the clipping in the environment.", "In the first case, this approach is asymmetric in that does not place similar probability on $\\mu_{\\theta}(s) - \\epsilon$ and $\\mu_{\\theta}(s) + \\epsilon$ for $\\mu_{\\theta}(s)$ near to $0$ and $2\\pi$. In the second case, this requires approximating a periodic function.", "We include both these reasons at the start of Section 3.", "Lastly, thank you for the concrete suggestions:", "\"Def 4.4: \"a notion of Fisher information\" -- maybe \"variant\" is better than \"notion\", which implies there are different kinds of Fisher information", "Def 3.1 mu is overloaded: parameter or measure?", "4.4, law of total variation -- define \"", "We have addressed these and uploaded a new draft to reflect the changes.", "For the last suggestion, we currently define the law of total variance(variation) in the preliminaries so we did not repeat the definition in Section 4.4.", "We now write \"law of total variance\" instead of \"law of total variation\" to avoid any ambiguity."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 488, "sentences": ["Thank you for the comments and suggestions.", "We agree that characterizing the generalization properties of neural network under different scalings is an important future direction.", "We have updated the manuscript with a few minor modifications: 1) Figure on the population risk of sigmoid network (first layer optimized) in addition to SoftPlus; 2) additional remarks on the population risk of network in the kernel regime in Section 5.2; 3) corrected typos."], "labels": ["global_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 489, "sentences": ["Thank you for your review, below we answer the points that were questioned.", "* Missing implementation steps and optimization details:", "In addition to implementation details, the appendix has a rather detailed table of the architecture parameters.", "Moreover, we will ultimately release codes on Github.", "* Non-matched experiment to practice environment:", "The evaluation of generative models and unsupervised domain translations remains an open question, even less covered in the field of sound.", "We didn't apply our models yet to datasets previously covered in the related works, such as Nsynth, which is planned and would give some more direct comparisons.", "* How to avoid the negative knowledge transfer:", "As we defined our purpose, the resulting generation is a blending of both domains that renders a target timbre while retaining some of the input features.", "It amounts to note class (that is explicitly controlled for the note-conditional model states) together with timbre.", "We plan on experiments on controlling the amount of timbre transfer in between the input and target domains."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 490, "sentences": ["Thank you for your detailed reviews and constructive suggestions.", "We have added the suggested baselines in the revision.", "Here are our responses to your questions and comments:", "1. Equation 1 typo?", "It is not a typo.", "Our reward function is different from existing curiosity reward.", "We are using the change of the real time m^t and m^{t-1} as the reward for inciting behavioral change from the demonstrator.", "We have shown more analysis and visualization to explain why this works in the new revision (Appendix B.2 & B.3).", "Our \u201cself-supervised\u201d baseline is actually using the prediction loss as reward, and it has a worse performance compared to ours.", "2. Baseline missing: Random actions from expert", "Figure 5 shows the results where 10% actions from the demonstrator are purely random.", "With the randomness, our approach is still be able to find meaningful probing policy.", "We have also evaluated the success rate when we use the policy learned from the suboptimal demonstration (10% random actions).", "As reported in the updated Table 1, this policy is comparable to the one learned from optimal demonstrations, and it still outperforms baselines which are all trained from optimal demonstrations.", "3. Baseline missing: Simple RNN policies that communicate hidden states", "We have evaluated this baseline in the revision (i.e., the \u201c2-LSTM\u201d baseline).", "The network architecture is illustrated in Figure 16.", "It indeed performs much worse than our full model.", "4. Ablation study for the importance of fusion", "We have added the result of this baseline (i.e., the \u201cours w/o fusion\u201d baseline), where we concatenate the state feature and the latent vector m^t together.", "The results have validated the importance of using the attention-based fusion layer.", "5. Generalizability argument", "Our main idea is to show as many configurations as possible to the learner by learning a good probing policy.", "Since the probing always starts from a single setting, there is indeed a limit in terms of how different the new settings could be.", "E.g., in Maze Navigation, it is impossible for the learner to change the room layout drastically in the time limit, so the learned policy won\u2019t make sense in a very different room layout (e.g., 8 rooms instead of 4 rooms).", "To obtain a better generalization, we may need to use a better imitation learning approach to replace the current one (behavioral cloning), and possibly using multiple starting configurations.", "But we think that it is somewhat orthogonal to our main contribution.", "The objective of our approach is to discover more diverse settings/configurations and consequently improve whatever imitation learning approach we actually use."], "labels": ["global_context", "multiple_context", "global_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 491, "sentences": ["Thank you for the detailed comments.", "1. We did not do experiments with such generator architecture.", "Although we have considered other architectural choices for generator and ways of conditioning, our early experiments showed that our residual-upsampling scheme is more efficient than parallel wavenet\u2019s full-resolution scheme.", "The correspondence between temporal dimensions of the conditioning and the waveform also seemed important and hence we decided to keep the proposed generator architecture throughout.", "2. Indeed we believe that the use of the ensemble of random window discriminators was the main factor behind the performance we obtained.", "This, however, breaks down to three steps:", "(a) switching from full discriminator to random-window discriminator(s),", "(b) including unconditional random window discriminator(s),", "(c) including several different window sizes in the ensemble.", "As can be seen in Table 1.", ", (a) already brings a huge improvement (from ~1.9 to ~3.4 MOS).", "(b) and (c) also seem to be important; we have considered fixing the window size or using only conditional RWDs, but all of such trials turned out considerably worse.", "Only models combining all of (a) - (c) made it past MOS of 4.1.", "3. Indeed D^c_k and D^u_k should have been clearly defined there; we clarified this notation in the updated version of the submission.", "4. For the training stability, please see our joint response.", "As for the role of the batch size, we fixed it throughout all experiments, but we will include analysis of model stability with smaller batch sizes in the final version of the paper.", "5. Thank you for pointing out this related work. We refer to it in the updated version of the submission."], "labels": ["global_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 492, "sentences": ["We thank a lot for the comments with cares and insights, which are helpful for improving the quality and readability of our writing.", "We have addressed all the comments as follows:", "Response #1: In the revision, we have added the following justification and explanations on privacy quantification in Section 2, 4 and 5.", "First, there is no single standard definition of data privacy-preserving problems and corresponding adversary attacks.", "And a fundamental problem in it is the natural tradeoff between privacy and utility, which is affected by different data privacy-preserving methods.", "Our key contribution in this paper is the RAN framework and the training algorithm, which can accommodate different choices of privacy attackers and privacy quantification.", "Second, finding the right measurement for privacy is an open problem in itself.", "To evaluate RAN, one has to pick some quantifications.", "In the present paper, we chose the \u201creconstructive error\u201d as the quantification of privacy because it is the most intuitive one to measure the risk of disclosing sensitive background information in the raw data for the given perturbed data (Encoder output).", "Third, in the future, we will evaluate RAN using other quantifications of privacy as well in a definitely defined application.", "For example, we could measure the privacy by the hidden failure, i.e., the ratio between the background patterns that were discovered based on RAN\u2019s Encoder output, and the sensitive patterns founded from the raw data, in the object recognition application.", "Response #2: Great help.", "In the revision, we have added more experiments (with more \\lambda settings in RAN) to plot the full Pareto Front of three baselines and RAN, and revised the explanations in Section 3.1.", "And we also noted that the parameter \\lambda can be fine-tuned, e.g., exponentially varied, to read a better tradeoff.", "Response #3: Thanks for pointing out the problems in Eq. 1.", "The utility is evaluated as the accuracy of a Classifier, i.e., the probability Yi=Yi\u2019, which is a commonly used metric .", "And we adopted some randomness, e.g., dropout, in the parametric discriminative models (Encoder and Classifier).", "Response #4: We have added more clarifications on the privacy definition in Section 2.1.", "In particular, the privacy of Max Min |Ii-Ii\u2019|^2 is defined for each data rather than a dataset, which is different from any anonymization based data privacy-preserving techniques.", "Response #5: Although we can plug in any adversary architecture (Decoder) and privacy quantification in RAN, this paper adopts the worst possible Decoder to mirror the Encoder\u2019s architecture.", "That is, we assume a powerful adversary that knows the Encoder in the training.", "\u201cAn exactly reversed model\u201d stands for a layer-by-layer deconvolutional model (Decoder) with known Encoder\u2019s convolution filter number and size, pooling size and each layer\u2019s connection relationship.", "In the revision, we have added above clarification in section 2.3.", "Response #6: Thanks for the comments.", "We call Eq. 3 and Eq.4 adversarial, as explained in out intuition, they need not be opposite all the time.", "And we agree that the resulting model is highly affected by the setting of hyper-parameters n and k. In particular, we have compared the settings of k=1, k=2, k=3, and k=4 for each task and finally select the best overall value k=3.", "As for the number of epoch n, it depends on the usual practices of developers for an acceptable converged result.", "In our experiments, we use n=10,000 for MNIST, UbiSound and Har with batch size=128, and adopt n=20,000 for CIFAR-10 and ImageNet with batch size=256 and batch size=512, respectively.", "In fact, we have already conducted exhaustive micro-benchmark experiments to determine the current design of RAN.", "For example, we adopt different options of model architectures, nine weight updating schemes on when and what order to update Encoder, Decoder and Classifier,  and several settings of the important hyper-parameters (e.g., \u201cn\u201d and \u201ck\u201d) to select the empirically optimized one.", "However, we didn\u2019t present the micro-benchmark results in this paper due to the space limit.", "In the revision, we have added more explanations on the selection of n and k in Section 2.4."], "labels": ["global_context", "global_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "other_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 493, "sentences": ["We are grateful for your time and comment on the work.", "We start by further explaining the contributions in the paper.", "Our main contribution is the combination of MARL with HRL to enable the decentralized learning of controllers that can navigate and seek goals in a robotic humanoid simulation.", "The unique combination of methods allows us to learn these sophisticated controllers with far less data than methods without hierarchy (cite openAI Emergent Tool Use from Multi-Agent Interaction).", "Second, we also consider the environment in the paper another contribution.", "Few multi-agent environments simulate dynamics, and none have articulated humanoid robots that observer their world using egocentric vision.", "We plan to release this environment with the work to allow other researchers to pursue and make progress on important complex tasks.", "Many multi-agent problems have been studies using simple robot models (point-mass, etc), where more complex and realistic models have used the problem because significantly more challenging.", "However, often, an assumption can be made that the robots in the environment share similar morphology.", "We propose a method that uses a form of goal-conditioned RL to learn task agnostic low-level policies that can simplify the share control structure across robots.", "In most HRL methods, the lower level can be viewed as part of the environment, yet this restructuring of the environment enables faster and more capable learning.", "Here we clarify some of the proposed advantages of the method.", "First, the use of HRL enables temporal correlation in action exploration that helps reduce the non-stationarity challenge.", "The advantage of this temporal correlation is shown empirically in Figures 2 and 3 where the PPO policies do not improve on learning the tasks.", "This property can be understood to reduce the variance in the policy gradient.", "Instead of having the policy sample an action every step instead, the low-level policy is triggered for $k$ timesteps with a goal proposal.", "For these $k$ timesteps, no noise is added to the low-level policy outputs.", "Similarly, this $k$-step structured exploration enables learning.", "If we think of the policy as a type of VAE that is learning an encoder (high-level) that is trying to learn a good latent goal (z) that will result in the low-level performing the desired sequence of actions.", "The HRL structure is reducing the dimensionality of the control problem given a low-level designed to perform diverse behaviour wrt to the goal (cite Heess and DIAYN).", "Last, the partial parameter sharing appears to make the learning problem easier.", "We know it is challenging to learn Q functions, which implies that the centralized methods that use Q functions will not scale well.", "We compare our method to MADDPG, a popular centralized method that works by treating the problem as a single agent problem with complete information.", "In our case, this method results in a significant increase in network parameters for the Q function, which leads to poor learning performance, as can be seen in Figures 2 and 3.", "Our particular configuration allows our method to be decentralized, making the individual network for each agent more straightforward.", "We are also interested in generalization to different numbers of agents after training, which is also problematic for centralized methods.", "In short, decentralized learning will allow for more general methods, and HRL enables the learning of sophisticated controllers."], "labels": ["other_context", "global_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 494, "sentences": ["We thank the reviewer for the comments!", "Q: \u201ccomparing directly to Merity et al.'s approach\u201d", "Merity et al.", "share the input and output embeddings via an adaptive softmax where all words have the same embedding size.", "We reimplemented their approach and found that it did not perform very well in our experiments (25.48 PPL; Appendix A, Table 6, last row).", "We found that sharing fixed size input and output embeddings for a flat softmax performs better (22.63 PPL; second to last row of Table 6).", "This is likely because we train all words at every time step, which is not the case for an adaptive softmax with fixed size embeddings.", "Q: \u201cThe discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing\u201d", "We updated the paper and hope that the discussion is clearer now.", "Thank you for the feedback!", "Q: \u201cthoughts as to why full-softmax BPE is worse than adaptive softmax word level\u201d", "Full-softmax BPE is worse because we measure perplexity on the word-level.", "This involves multiplying the probabilities of the individual BPE tokens.", "BPE token-level perplexity itself is actually significantly lower than word-level PPL (around 21.5 for GBW and around 18 for WikiText-103 for the models presented in the paper) but the two are not comparable."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context", "single_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 495, "sentences": ["We thank the reviewer for his/her positive evaluation of our paper and for raising many very useful points which helped us getting to a clearer picture of our contribution.", "A few of these points deserve discussion beyond the changes made in the paper.", "Due to a mistake on page 2, we got the reviewer confused believing we are using importance sampling while we are using importance mixing instead.", "This has been fixed.", "The reviewer mentions it may be possible to construct counter-examples where the gradient updates will prevent convergence.", "This is a very important point.", "There are many RL problems (see e.g. Continuous Mountain Car, Colas et al. at ICML 2018) where at some point the gradient computed by the critic is deceptive, i.e. it drives the policy parameters into a wrong direction.", "In that case, applying that gradient to CEM actors as we do in CEM-RL is counter-productive.", "But the fact that we only apply this gradient to half the population makes it that CEM-RL should nevertheless overcome this issue:  the actors which did not receive a gradient step will be selected and the population will continue improving.", "However, admittedly, in this very specific context, CEM-RL is behaving as a CEM with only half a population, thus it is less efficient than the standard CEM.", "Besides, ERL even better resists than our approach to the same issue: if the actor generated by DDPG does not perform better than the evolutionary population due to a deceptive gradient issue, then this actor is just ignored, and the evolutionary part behaves as usual, without any loss in performance.", "This deceptive gradient issue certainly explains why CEM is the best approach on Swimmer.", "Finally, it may also happen that the RL part does not bring benefit just because the current critic is wrong and provides an inadequate gradient, in a non-deceptive gradient case.", "All the above points have now been made much clear in the new version of the paper, in particular we added an appendix dedicated to the swimmer benchmark.", "The reviewer also raises doubts about the fact that the method of Khadka & Tumer (2018) cannot be extended to use CEM.", "After second thoughts, this is absolutely right.", "As the reviewer says, in both this work and Khadka & Tumer, the RL updates lead to policies that may differ a lot from the search distribution and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.", "But if the RL actor shows good enough performance, this does not prevent from computing a new covariance matrix which includes it.", "The corresponding ellipsoid in the search space may be very large, leading to a widespread next generation, but the process should tend to converge again towards a population of actors where evolutionary and RL actors are closer to each other.", "A result of these second thoughts is that one could definitely build an ERL algorithm where the evolutionary part is replaced by CEM.", "We corrected the paper according to this new insight.", "Unfortunately we did not find enough time to implement and test this algorithm during the rebuttal stage, but we now mention this possibility as an interesting avenue for future work.", "Despite the very interesting points above, the reviewer is wrong when saying that the main distinction between our approach and the ERL approach is that only in ours the information flow is from ES to RL and vice-versa.", "Actually, in ERL, if the RL actor added to the population performs well, it will steer the whole evolutionary population to the right direction just by generating offsprings, so RL and ES also benefit from each other.", "A lot of our effort during the rebuttal stage has been focused on better highlighting the often subtle differences between ERL and our approach.", "For doing so, we replaced Figure 1 with a figure directly contrasting CEM-RL to ERL.", "We also added Figure 6 which better highlights the properties of the algorithms and we performed several additional studies described either in the main text or in appendices.", "The next point of the reviewer is that a good deal of the strong performance of our method and RL may just be due to the fact that we are using multiple actors, thus benefiting from an \"ensemble method\" effect already mentioned in several papers such as Osband et al., 2016 for DQN.", "This point is absolutely valid.", "The reviewer thus suggests a relevant control which would be to keep a population (ensemble) of policies, but only update using RL while sharing experience across all actors.", "This would isolate the ensemble effect from the evolutionary search effect.", "We performed the suggested control.", "The resulting algorithm is a multiple-actor version of TD3.", "Results show that CEM-TD3 actually outperforms this multiple-actor TD3, thus the CEM part actually brings performance improvement.", "About replacing the ReLU non-linearity in DDPG and TD3 prior work with tanh, we spotted that we could get much better results on several environments with the latter.", "This explanation is now clearly mentioned in the paper, and motivates a future work direction which consists in using \"neural architecture search\" for RL problems, the performance of algorithms being a lot dependent on such architecture details.", "Finally, to keep our paper shorter than the hard page limit for ICLR while addressing all the reviewers points, we had to move several studies into appendices, starting with the importance mixing study."], "labels": ["global_context", "global_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 496, "sentences": ["Thanks for your constructive feedbacks! Please see our response below.", "1. About image captioning.", "Yes. Image captioning dataset is absolutely available for creating the lookup table.", "As you suggest, we use MS COCO Image captioning dataset to learn a lookup table and apply it to the EN-RO translation task to do the quick evaluation.", "As a result, the BLEU score is (33.55), which is comparable to the current lookup table (33.78) based on Multi30K, and outperforms the Trans. (base) (32.66).", "Regarding the performance of the standard image captioning system, we train a caption model (Show, Attend, and Tell (Xu et al., 2015b)) with fine-tuned encoder (ResNet101) on the COCO dataset to encode the images.", "The result on EN-RO is 33.58.", "We are a little bit uncertain if we have well understood this request because our task is text to text translation while image captioning is image to text.", "If not, we are glad to address further.", "2. About the minor comments.", "(1)\tThis is typo. It is Q.", "(2)\tYes. We will remove it following your suggestion."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 497, "sentences": ["Thank you for your review and we would be delighted to address your concerns, but do require some clarifications.", "While a learnable lambda could be considered we would argue that the learning of this parameter beyond the grid-search applied in the submission is somewhat tangential to our primary contribution: a unified framework which lends itself to targeted representation analysis and modification.", "1)", "The notion of a map of \\lambdas sounds interesting.", "However, at present, it is not clear to us what this refers to as \\lambda is a weighting on a loss term.", "Clarification would be much appreciated so we can fully engage with this point.", "As far as the existing approach is concerned, Figure 6 illustrates the influence of \\lambda on the accuracy and correlation of global and local stability prediction.", "2)", "The inconsistent correlations between the two tasks are exactly the scenarios where stethoscopes come into their own: testing positive and negative regimes of lambda (corresponding to auxiliary and adversarial training, respectively) reveals the interplay between the two tasks and potentially allows for de-biasing the algorithm as shown in Figure 6a.", "Therefore, in contrast to the design not considering these relationships, it explicitly addresses them.", "Could you please elaborate on the comment \u2019the current design [\u2026] simply sums them up\u2019?", "The stethoscope module has its own trainable parameters and a separate loss function.", "Only the encoder shares weights between main and secondary task."], "labels": ["global_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 498, "sentences": ["We thank Reviewer 2 for their support and questions.", "We answer them below.", "Quantization time", "As we state in our paper, quantizing a ResNet-50 (quantization + finetuning steps) takes about one day on one Volta V100 GPU.", "The time of quantization is around 1 to 2 hours, the rest being dedicated to finetuning.", "Thus, the time dedicated to quantization is relatively short, especially compared with the fine-tuning and even more with the initial network training.", "This is because we optimized our EM implementation in at least two ways as detailed below.", "-\tThe E-step is performed on the GPU (see file src/quantization/distance.py, lines 61-75) with automatic chunking.", "This means that the code chunks the centroids and the weight matrices into blocks, performs the distance computation on those blocks and aggregates the results.", "This falls within the map/reduce paradigm.", "Note that the blocks are automatically calculated to be the largest that fit into the GPU, such that the utilization of the GPU is maximized, so as to minimize the compute time.", "-\tThe M-step involves calculating a solution of a least squares problem (see footnote 2 in our paper).", "The bottleneck for this is to calculate the pseudo-inverse of the activations x. However, we fix x when iterating our EM algorithm, therefore we can factor the computation of the pseudo inverse of x before alternating between the E and the M steps (see file src/quantization/solver.py and in particular the docstring).", "We provided pointers to the files in the code anonymously shared on OpenReview.", "To our knowledge, these implementation strategies are novel in this context and were key in the development of our method to be able to iterate rapidly.", "Both strategies are documented in the code so that they can benefit to the community.", "Incorporating the non-linearity", "As the Reviewer rightfully stated, optimally we should take the non-linearity in Equation (4) into account.", "One could hope for a higher compression ratio.", "Indeed, the approximation constraint on the positive outputs would stay the same (they have to be close to the original outputs).", "On the other hand, the only constraint lying on the negative outputs is that they should remain negative (with a possible margin), but not necessarily close to the original negative outputs.", "However, our early experiments with this method resulted in a rather unstable EM algorithm.", "This direction may deserve further investigation."], "labels": ["global_context", "other_context", "other_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 499, "sentences": ["We appreciate your valuable comments.", "As you have said, these methods are popular in practice and achieve good performance.", "However, most of them are done in the setting of MCMC, and people rarely use them in nonconvex optimization.", "One contribution of this paper is to apply these techniques to optimization problems.", "Our paper also tries to understand the acceleration effect of replica exchange.", "We quantify it in both LDP and the convergence of chi^2 divergence.", "Although in Dupuis\u2019s work, he also quantifies the acceleration effect via LDP, the LDP theory we use in this paper is different from that.", "Specifically, his approach is based on the LDP variational theory, and ours is based on the theory of Donsker-Varadhan.", "As a result, our rate function has different form from his.", "In our paper, we also analyze the acceleration in the convergence of chi^2 divergence.", "It is a new perspective and not discussed by Dupuis.", "We emphasize that LDP and chi^2 divergence are two different approached to quantify convergence.", "They have different meanings.", "The first one characterizes the decay rate of the probability that the empirical measures deviate from the stationary measure and the second one characterizes the decay rate of the discrepancy between the transit distributions and limiting distribution.", "Although the theory of LDP and convergence of chi^2 divergence for a general Markov process are well established and standard, to the best of our knowledge, our paper is the first to apply these tools in this specific problem.", "In our paper, one contribution is that we demonstrate the acceleration effect of replica exchange mathematically.", "We first show that the LDP rate function is boosted by replica exchange.", "Dupuis\u2019s work includes similar results but in a different form.", "We also show that the derivative of chi^2 divergence is boosted.", "Specifically, we demonstrate that a strict positive term caused by the replica exchange is added, if the density ratio between current distribution and limiting distribution is not symmetric.", "We say that a function is symmetric if we swap the positions of variables, the function value does not change.", "In this case, the derivative of chi^2 divergence is strictly boosted, and hence, the convergence is accelerated strictly.", "It reflects the benefits of replica exchange.", "To the best of our knowledge, this phenomenon has never been observed by previous literature, including Dupuis\u2019s paper.", "We think it is interesting and useful.", "Another contribution of our paper is the discretization algorithm.", "In practice, it is impossible to simulate the continuous process directly, and discretization is necessary.", "To the best of our knowledge, no one has discussed the discretization of replica exchange Langevin diffusion before.", "Our paper is the first one to analyze the discretization theoretically.", "In this paper, we establish the linear convergence rate for the discretization error, which is highly trivial since the process has state-dependent jumps.", "This result, combined with the acceleration effect, justifies the empirical success of the replica exchange Langevin diffusion in practice."], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 500, "sentences": ["We would like to thank the reviewer for the positive feedback.", "We reply to the the two questions below.", "Q1: The learning procedure is confusing. It is highly recommended to provide the pseudocode of the proposed method.", "R1: We will provide the pseudocode in the future versions of the paper:", "Training:", "X_L : clean set", "C_L : class set", "X_Z : noisy set", "# For each class name", "For c in C_L:", "#Take the clean examples belonging to this class", "X_L^c : subset of X_L with label c", "#Only consider noisy examples with the class name in the text", "X_Z^c = filter_by_text(X_Z)", "# Build the graph for this class, and learn the GCN for cleaning", "A^c = build_graph(X_Z^c)", "M^c = GCN_model(X_L^c, X_Z^c, A^c)", "#Clean examples always get weight 1", "for i in X_L^c:", "r_i = 1.0", "#Noisy examples get the learned weight", "for i in X_Z^c:", "r_i = assign_relevance(M^c(X_Z^c(i))", "#Add the noisy examples to the list of training images for this class", "X_L^c = concatenate(X_L^c, X_z^c)", "#Learn a classifier jointly for all classes.", "Use the relevance weights for noisy examples when learning the classifier", "W = train_classifier(X_L^c, r)", "Testing", "Given test image Q", "v = extract_feature(Q)", "scores = W^T v", "prediction = argmax(scores)", "Q2: Since there are many tasks and each task has a large-scale data, I'm afraid that the running time will explode. How to deal with this issue?", "R2: The complexity is linear in the number of classes, since classes are processed independently.", "Furthermore, text filtering is applied before cleaning, which reduces the number of images to be considered for a given class.", "Please also see the response R1 to reviewer1."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 501, "sentences": ["Thank you for your comments!", "* In our method, \"``more informative\" means \"``less deficient\".", "We have added a figure tracing the mutual information between representation and output I(Z;Y) vs. the minimality term I(Z;X) for different values of beta (see Figure 2, lower right panel), when training with our loss function.", "This is the usual information bottleneck curve.", "The deficiency bottleneck curve (Figure 2, upper right panel) traces the corresponding sufficiency term J(Z;Y) (which is just the entropy of the labels minus our loss) vs. I(Z;X) for different values of beta.", "The text now makes this more explicit (see p.7, first paragraph).", "Note that for M=1, J(Z;Y) = I(Z;Y).", "We can see that when training with our loss, we achieve approximately the same level of sufficiency (measured in terms of I(Z;Y)) while consistently achieving more compression (note the log ordinate for I(Z;X) in the lower left panel in Fig. 2) for a wide range of beta values.", "* We included two new figures plotting the representation for MNIST (p. 19, Figure 7) and Fashion-MNIST (p. 19, Figure 8) in Appendix E.3 for an unsupervised version of the VDB objective (p. 18, equation 38)."], "labels": ["global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "other_context", "other_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 502, "sentences": ["Thank you for the thoughtful comments.", "We are glad that you found our algorithmic approach original, and our experiments promising.", "Regarding the notation, given that the topic of our paper is inherently interdisciplinary -- spanning machine learning and algorithm theory -- we need to use notions and notation from both communities.", "This can lead to misunderstandings, but there is no easy way around it.", "In the paper we tried to follow the notation used in heavy-hitter analysis in algorithm theory to make it easy to compare the analysis to past work.", "But since there is no standard notation across both fields, it is difficult to find a notation that is easily accessible to both communities.", "In addition, there are indeed a few places in the paper where our phrasing could have been better, thank you for pointing this out.", "We discuss this in more detail below, and hope this should clarify any misunderstandings.", "Regarding our proofs, they are all self-contained.", "- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.", "Starting with S and i: I guess S and i are both simply varying-length sequences in U.", "To clarify, the input S is a sequence *of elements* from some universe U.", "To give an example, we could have U={0...65535}, in which case the sequence S would consist of integers in the range 0...65535.", "For example, S = 10101, 21222, 10222, 1, 10, 1, 52233, 62223 is an example sequence of length 8 whose items belong to U.", "The remainder of the problem definition is as described in the introduction: a frequency estimation algorithm reads the sequence S in one pass, and after that, for any element i from U, reports an estimate of  f_i,  the number of times element i occurs in S. In the above example, we have, e.g., f_1=2.", "- In general the intro should focus more on an intuitive (and/or formal) explanation of the problem setting, with some equations that explain the problem you want to work on. Right now it is too heavy on 'related work' (this is just my opinion).", "Thanks for the suggestions.", "We will include more explanation in the introduction and condense related work while keeping it thorough.", "- In describing Eqn 3 there are some weird remarks, e.g. \"N is the sum of all frequencies\". Do you mean that N is the total number of available frequencies? i.e.", "should it be |D|? It's not clear to me that the sum of frequencies would be bounded if D is not discrete.", "N is the sum of all frequencies; i.e., N = \\sum_{ i \\in U }", "f_i.", "- Your F and \\tilde{f} are introduced as infinite series.", "Maybe they should be {f1, f2,..., fN}, i.e. N queries, each of which you are trying to be estimate.", "The series are indeed finite, we skipped the last index for simplicity.", "Formally, it should be F = {f_1, \u2026, f_|U|} and ~F = {~f_1, \u2026, ~f_|U|}", "- In general, you have to introduce the notation much more carefully.", "Your audience should not be expected to be experts in hashing for this venue!! 'C[1,...,B]' is informal abusive notation.", "You should clearly state using both mathematical notation AND using sentences what each symbol means.", "As stated, C[1...B] is a one-dimensional array.", "Equivalently, it is a B-dimensional vector.", "We refer to C as an \u201carray\u201d as opposed to \u201cvector\u201d for the sake of consistency with prior work on frequency estimation, and to avoid nested subscripts.", "C[b] indeed denotes the b-th element/bin of C. Regarding the notation h: U -> [B] : we use [B] to denote the set {1...B}. We define it in Section 7, but we should have defined it earlier.", "The formula h: U->[B] indeed denotes a function h that maps elements of U to {1...B}.", "- Still it is unclear where 'fj' comes from. You need to state in words eg \"C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \\in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.\"", "We hope that after the earlier clarifications, the equation C[b] = sum_{j:h(j)=b} f_j  is more clear now.", "- What I don't understand is how fj is dependent on h. When you say \"at the end of the stream\", you mean that given S, we are analyzing the frequency of a series of sequences {i_1,...,i_N}?", "f_j does not depend on h, only on the input sequence S. Since an element j can occur anywhere in S, the equation C[b] = sum_{j:h(j)=b} f_j  holds only after the algorithm scans the whole sequence S.", "- The term \"sketch\" is used in Algorithm1, like 10, before 'sketch' is defined!!", "As explained in the description, items not stored in unique buckets \u201care fed to the remaining B \u2212 Br buckets using a conventional frequency estimation algorithm SketchAlg\u201d.", "The word \u201csketch\u201d in Algorithm 1 refers to the storage used by SketchAlg.", "To avoid confusion, we will shorten line 10 to \u201cfeed i to SketchAlg\u201d."], "labels": ["global_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 503, "sentences": ["Thank you for the appreciation on the novelty of our paper.", "- We have carefully proofread the manuscript and fixed the typos in the revised version. Could reviewer be more specific about the odd formulations, so that we can improve them?"], "labels": ["global_context", "multiple_context"], "confs": [1.0, 1.0]}
{"abstract_id": 504, "sentences": ["Thanks again, Reviewer #3, for your thought-provoking critique.", "We respond to your other comments below.", "1.  \u201cIn particular, Section 4 is a series of empirical analyses, based on one dataset pair\u2026.However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.\u201d", "See general responses #1 and #3.", "2.  \u201cIt is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper\u2026", ".Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.\u201d", "See general response #2.", "We emphasize that we are not trying to approximate the density function, only approximate the difference and characterize its sign.", "Moreover, the special structure of CV-Glow makes these derivative-based approximations better behaved and more tractable than an expansion of a generic deep neural network.", "3.  \u201cSome parts of the paper feel long-winded and aimless\u2026.In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.", "Section 2 background takes too much space.", "Section 3 too much redundancy", "-- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.\u201d", "We will attempt to make the writing more concise.", "But we believe that most, if not all, of Section 2 is necessary in order to make the paper self-contained and accessible to someone who has never before seen invertible generative models.", "While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.", "4.  \u201cI don't think Glow necessarily is encouraged to increase sensitivity to perturbations.", "The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.\u201d", "We are not saying that the model will totally disregard the latent density and attempt to scale the input to very large or infinite values.", "Our point is made in the context of volume term which is only one of the terms in the change-of-variable objective.", "The log volume term in the change-of-variable objective is maximizing the very quantity (the Jacobian\u2019s diagonal terms) that the cited work on derivative-based regularization penalties has sought to minimize.", "The maximization of the derivatives in the objective directly implies increased sensitivity to perturbations.", "5.  \u201cFigure 6(a) [Figure 5(a) in revised draft] clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.\u201d", "We are not sure how you are drawing this conclusion; perhaps from the scale of the x-axis?", "The histogram in Figure 6 (a) (original draft) has an x-axis covering the interval [0.4, 0.55], meaning the maximal difference between a mean in *any pair of dimensions* is 0.15.", "Scaling back to pixel units, 0.15 * 255 = 38.25, meaning that 38.25 pixels is the maximum difference in means.", "While this is not a difference of zero, we don\u2019t see how you could say this \u201cclearly suggests\u201d that the means are \u201cvery different.\u201d  In the latest draft, this figure---now Fig 5 (a)---has an x-axis that spans from 0-255.", "Hopefully the overlap in the means in now conspicuous.", "6.", "\u201cHowever, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite: V\u00edt \u0160kv\u00e1ra et al. Are generative deep models for novelty detection truly better? at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.\u201d", "Thank you for pointing us to this work.", "We cite it in the revised draft.", "It looks like they test on UCI data sets of dimensionality less than 200, and therefore their results speak to a much different data regime than the one we are studying.", "7.  \u201cA part of the paper's contribution (section 5 conclusion) seem to overlap with others' work.", "The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).\u201d", "While we do also analyze constant images, we believe that our results for multiple data set pairs (FashionMNIST-MNIST, CIFAR10-SVHN, CelebA-SVHN, ImageNet-CIFAR10/CIFAR100/SVHN) and for multiple deep generative models (flow-based models, VAE, PixelCNN) is novel.", "Our conclusions are arrived at through focused experimentation and a novel analytical expression applied to CV-Glow."], "labels": ["global_context", "other_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "other_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "single_context", "single_context", "single_context", "single_context", "single_context", "global_context", "multiple_context", "multiple_context", "multiple_context", "multiple_context", "single_context", "multiple_context", "multiple_context", "multiple_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 505, "sentences": ["Thanks for your attention to our work.", "1) For the motivation, Because the traditional algorithms deal with GANs via a Markov chain:", "$(f_0,g_0)\\rightarrow (f_1, g_0)\\rightarrow (f_1,g_1) \\rightarrow \\cdots\\rightarrow (f_{n},g_{n-1})\\rightarrow (f_n,g_n)$. It is like a kind of reinforcement learning--- but the environment (Here it is $f$) is changing. And we want to view it from the angle of game theory. And then we try to minimize the new loss.", "2) For the presentation: we will try to modify it.", "And we apologize that there are some typos about the $f^*$ and $g^*$ in the Eq.(22).", "The $f^*$ and $g^*$ means the dependent variable of duality gap.", "And the $\\Leftrightarrow$ definition of $\\mathcal{F}$ means equivalence.", "It can also be written as $f\\in \\mathcal{F}\\rightarrow 1-f\\in mathcal{F}$. And we will modify other improper presentation.", "3)For the experiment: we will spend some time to train GANs with more iteration and modify it.", "Thanks"], "labels": ["global_context", "single_context", "single_context", "single_context", "single_context", "multiple_context", "single_context", "single_context", "multiple_context", "global_context"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}