{"abstract_id": 0, "sentences": ["Thanks for your feedback.", "We discuss each comment in the following:", "- The experiments are not large scale", "We respectfully disagree with the reviewer's main comment that the experiments are not large scale.", "One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4).", "Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches).", "Sure, this is not the scale of 80 million tiny images; but one wouldn\u2019t ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances.", "Representation learning, the topic of this conference, has many facets.", "Learning representations from \u201cbig data\u201d (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side.", "Both are valuable in different circumstances.", "- No substantiate insight with respect to NP-hard problems", "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.", "To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives.", "These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard.", "Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees.", "Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML.", "Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem.", "So powerful non-convex solvers might be of a significant advantage over convex relaxations.", "Our paper simply shows ONE example for this.", "- It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly?", "It would not be possible to set the input dimension the same as the embedding dimension.", "Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error.", "The size of the embedding dimension can be too low to achieve this.", "One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation.", "However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding.", "- Methods, where items have no representation, are questionable", "Items having no representation is a caveat of the data available rather than that of the method.", "The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework.", "- How to generalize to unseen items", "First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage.", "We believe that in our case, generalization is realizable.", "One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items.", "The network can be trained with extra batches of triplets which involves the new items.", "- The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.", "We don\u2019t really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this."]}
