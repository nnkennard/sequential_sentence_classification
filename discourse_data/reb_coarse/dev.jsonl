{"abstract_id": 251, "sentences": ["Question 3:", "We did our best to write the paper such that it includes all details needed to fully understand the proposed method and its theoretical background.", "The referee indicates (in sharp contrast to referee 3) that the paper is not nicely written, nor easy to follow.", "We invite the referee to be specific about the sections of the original manuscript that need more clarification, allowing us to revise these sections.", "Question 4:", "The optimization algorithm used is the ADAM optimizer (Kingma & Ba, 2014).", "We refer to section 4 of Kingma & Ba (2014) for a proof of convergence for convex functions.", "It is known that that loss surfaces of deep neural networks are typically non-convex, however the gap between global and local minima is believed to be small for (see our answer to the referee\u2019s last question for more detail on this statement).", "Question 5:", "We kindly ask the reviewer to elaborate on the given statement.", "Could the reviewer indicate which sections are found to be imprecise and unpolished, and which parts of the manuscript need a better presentation?", "=====================================", "Second part of the review", "=====================================", "Question 1:", "We specify the Gumbel-max trick in the paragraph below equation 4.", "To make the paper more self-contained, we will extend this paragraph to further clarify the Gumbel-max trick.", "We also refer to our answer to the first question of this referee, in which we elaborated more on the Gumbel-max trick as well.", "Question 2:", "All training parameters were tuned empirically.", "However, we agree it is worth elaborating on our insights regarding the influence on performance of some of them.", "We experienced that performance was most sensitive to the learning rates for the sampling and task models, and the temperature parameter tau of the softmax relaxation.", "We augmented the discussion of our revised manuscript to share these insights.", ".", "Question 3:", "We know experience replay as a reinforcement learning technique for storing previous state/action pairs.", "However, our method does not make use of reinforcement learning, so could the reviewer please elaborate how experience replay would relate to our method?", "Question 4:", "In Section 4.1 (MNIST classification) we already compared our proposed sampling method to used Gumbel top-K sampling for data subsampling.", "We are currently also running experiments that allow for extensive comparison with the recently proposed LOUPE method by Bahadir et al. (2019).", "Question 5:", "A large part of the experiments in this work are focusing on compressive/partial Fourier measurements.", "This adequately reflects the measurement setup in many real-world problems, such as k-space measurement in magnetic resonance imaging (Lustig et al.), Xampling for ultrasound imaging (Eldar et al.), and non-uniform step frequency radar (Huang, 2014).", "In addition, we cover direct pixel sampling, related to real-world applications such as compressive cameras.", "We would like to emphasize that the proposed approach is measurement-domain agnostic, and therefore can be applied across a vast amount of real-world problem.", "In addition, our ongoing research already shows promising results for real-world applications such as magnetic resonance imaging and ultrasound imaging.", "This is part of future work.", "Question 6:", "The trends towards using deep learning for data-driven compressed sensing indeed has the downside of not having guarantees on finding a global minimum, as the loss surface of a NN is highly non-linear and non-convex.", "Still, these data-driven results have shown to be very promising (Gregor et al., 2010; Jin,2019; Bahadir et al., 2019; Mousavi, 2019)", "However due to the weight space symmetry problem (Goodfellow et al., 2016) the loss surface contains a vast amount of local minima with the same error value.", "The size of the gap between local and the global minima remains an open field of research.", "However, citing from Goodfellow et al. (2016):", "\u201cThe problem remains an active area of research, but experts now suspect that,", "for su\ufb03ciently large neural networks, most local minima have a low cost function", "value, and that it is not important to \ufb01nd a true global minimum rather than to", "\ufb01nd a point in parameter space that has low but not minimal cost (Saxe et al.,", "2013; Dauphin et al., 2014; Goodfellow et al., 2015; Choromanska et al., 2014)\u201d", "As such, we leverage the empirically-shown ability of stochastic gradient descent to optimize this non-convex function in our NN for finding local minima.", "Indeed there is no guarantee on finding a global optimum.", "We thank the reviewer for the feedback.", "The reviewer states in his/her summary: The parameterization is used to simplify the subsampling distribution.", "We would like to comment on this, by stating that the reparametrization is not used for simplifying the subsampling distribution; on the contrary it actually enables sampling from this trained distribution.", "In fact, without this reparametrization, our generative sampling model (DPS) would not be differentiable.", "Below we elaborate upon the questions and raised concerns by the reviewer:", "Question 1:", "We respectfully disagree with the referee\u2019s conclusions, and will elaborate on the above statements in the following.", "While we disagree, we did however undertake a significant effort to further clarify and provide additional evidence in the revised manuscript, taking into account these comments.", "Regarding the theoretical correctness of deep probabilistic subsampling, in section 3.2 we explain how we incorporate a well-known reparametrization trick, termed the Gumbel-max trick (Gumbel,1954), to sample from a categorical probability distribution.", "Note that this shares similarities with the reparameterization trick used for sampling from trained gaussian distributions in a vanilla variational autoencoder.", "The Gumbel-max reparametrization perturbs the logits of the categorical distribution with Gumbel noise after which, by means of the argmax, the highest value is selected.", "Gumbel (1954) showed that this reparametrization allows sampling from the original categorical distribution.", "Recent state-of-the art work on a relaxation of this trick, termed Gumbel-softmax sampling (Jang et al., 2017) or the concrete distribution (Maddison et al., 2016), allows us to apply this relaxed reparametrization inside a neural network as it enables gradient calculation, which is needed for error backpropagation in the training procedure of the network.", "We would like to ask the reviewer what is believed to be missing from this explanation on the subsampling part of our proposed method.", "Regarding the theoretical basis used for the design of the task network; we took a theoretically principled approach by exploiting a model-driven network architecture for the CIFAR10 reconstruction problem.", "To that end, we unfold the iterations of a proximal gradient scheme (Mardani et al., NeurIPS, 2018), allowing for explicit embedding of the acquisition model (and therewith the learned sampling) in the reconstruction network.", "Regarding the referee\u2019s conclusion that the manuscript lacks comparison to the approaches of (Xie & Ermon (2019); Kool et al. (2019); Pl\u00f6tz & Roth (2018): We would like to point out that these three references all together put forward the Gumbel top-k method.", "Note that the use of the Gumbel top-k method for compressive sampling is also new, and in fact constitutes a specific case (constrained version with shared weights across distributions) of the proposed deep probabilistic subsampling (DPS) framework.", "In the MNIST experiments we already included Gumbel top-k sampling, but we will also add this for the other experiments in the revised manuscript.", "In addition, we added a thorough comparison of the DPS to LOUPE (Bahadir et al, 2019), a recently proposed data-driven method for subsampling.", "Question 2:", "We would first like to refer the referee to third paragraph of the introduction, where we explicitly formulate the main shortcoming of compressed sensing:", "\u201cThese [compressed sensing] methods, however, are lacking in the sense that they do not fully exploit both the underlying data distribution and information to solve the downstream task of interest.\u201d", "Then, in the list of main contributions, we write:", "\u201cDPS: A new regime for task-adaptive subsampling using a novel probabilistic deep learning framework for jointly learning a sub-Nyquist sampling scheme with a predictive model for downstream tasks\u201d", "Subquestion 2:", "We are of course willing to further specify any details that the referee misses in the current paper. We would therefore like to kindly invite the referee to be specific about the details that he/she would like to be added to the manuscript.", "We respectfully disagree with the referee\u2019s conclusion that the method does not support a significant contribution.", "We propose a fully-probabilistic generative model for trainable sampling, that exploits both the underlying data distribution and information to solve the downstream task of interest.", "Our generative model builds upon recent advances on Gumbel max and top-k reparameterizations and their relaxations, showing for the first time how discrete sample selection can be done in a data-driven and task-adaptive fashion.", "This opens up a vast array of new opportunities in compressed sensing."], "labels": ["nonarg", "dispute", "dispute", "dispute", "nonarg", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "dispute", "dispute", "dispute", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 252, "sentences": ["We want to thank reviewer#4 for your review. Your summary correctly reflects the content of our paper.", "We want to comment on the suggestions for new experiments:", "Comparing with exhaustive search: This is a good idea.", "However, one concern is that since the search space is combinatorial, even a shallow network (e.g., 5) with a smaller number of precisions (e.g., 32, 8, 1) can have a large search space of (e.g., 3^5 = 243 architectures) for which exhaustive search is intractable.", "Comparing with DARTS and ENAS: ENAS is not open-sourced, so a direct comparison is difficult.", "A more detailed analysis comparing DNAS with DARTS is discussed in the reply to reviewer#1, minor concern #2: https://openreview.net/forum?id=BJGVX3CqYm&noteId=S1lyG-h7A7", "We plan to perform the suggested experiments of comparing with exhaustive search and DARTS.", "The results will be hopefully updated before the revision deadline and the camera-ready if the paper is accepted."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 253, "sentences": ["Thank you for the review.", "While formulating a proximal function for model compression might be an interesting idea (if search space is highly limited) as the reviewer suggested, we believe that our proposed method is fundamentally different from proximal gradient descent approaches due to the following reasons:", "1) Proximal gradient descent is meant to solve a convex optimization problem while our aim is to solve a non-convex problem in which each local minimum exhibits vastly different test accuracy after compression.", "Jumping to another local minimum from a certain minimum would not be easily achieved by convex optimization methods.", "2) Finding a particular flat minimum is the key to obtaining good model compression (and good generalization as well).", "Such an exploration, however, cannot be obtained by a proximal function since we need to investigate lots of different local minima with different amount of flatness in loss surface.", "3) While proximal gradient descent can be useful to find a certain local minimum close to the starting point given a convex constraint, wide exploration (associated with possibly transient accuracy loss in the initial training as shown in Figure 2.(b)) is necessary to escape from a point with sharp loss surface.", "Investigating many different local minima would be only available with large learning rate (as we have chosen for our experiments) and/or large amount of weight distortion.", "4) Our effort to introduce optimal distortion step size and learning rate for a given compression problems is connected to exploration, not exploitation (which potentially supported by proximal functions where convergence matters).", "Even though proximal gradient descent selects step size only considering convergence, Figure 1 can lead to the results such as Figure 2(b) which cannot be obtained if only local exploitation is employed.", "Finding a flat minimum has been known to be a difficult work as shown in the paper \u201cOn large-batch training for deep learning: generalization gap and sharp minima\u201d, ICLR 2016.", "We firmly believe that our search space exploration method based on optimal distortion step size and amount of weight distortion enable us to produce better local minima well-suited to various model compression techniques.", "In short, unfortunately, we have failed to understand how you could connect our technique to proximal functions and proximal gradient descent.", "We strongly hope that you reconsider your decision."], "labels": ["nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 254, "sentences": ["We would like to thank you for the review and comments.", "We revised the manuscript to address your concerns.", "Below we summarized your concerns/questions with our answers.", "Q1: Some arguments that are presented could deserve a bit more precision.", "A1: We acknowledge that quantization is a very active research area in model compression and there are numerous quantization techniques with unique and distinct characteristics.", "We could not introduce and discuss lots of exciting quantization techniques such as vector quantization due to the limited space.", "We feel that introducing other quantization techniques in details would make the paper distracted since those techniques cannot be compared with compression ratio only (i.e., inference architecture, computation methods, and storage design would be different).", "Instead, we added more thorough introduction to binary codes in Section 1 to explain unique computational advantages of using binary codes.", "We introduced \"Hardware-aware Automated Quantization with Mixed Precision\" in Section 1 since fractional quantization on average is available as you pointed out, while FleXOR can also employ different quantization bits for each layer (i.e., we believe HAQ method can be applied on top of FleXOR).", "Q2: More extensive and thorough experiments could improve the impact of the paper.", "A2: We agree that including extensive quantization methods and model architectures would greatly improve the impact of the paper.", "Unfortunately, as we discussed above, our goal in this paper is to improve quantization schemes based on binary codes.", "Including quantization methods of different assumptions may require much lengthy discussions that make comparisons a lot complicated.", "For example, \"Hardware-aware Automated Quantization\" could be additionally applied to binary codes, and FleXOR is not conflicted with such an architectural techniques to improve compression ratio.", "Deep compression, TTQ, and TWN involve weight pruning that deserves large space for discussions (nonetheless, we compared TWN, TTQ, and BinaryRelax using ternary quantization scheme in Table 5 of Appendix).", "Deep compression also includes CSR format and Huffman coding which would make comparisons more complicated.", "We chose a few representative quantization methods mainly based on binary codes to facilitate fair and focused comparisons, and correspondingly, ResNet models on CIFAR-10 and ImageNet are selected for our experiments since most previous works (of binary codes) commonly include those models.", "For example, we could not include HAQ in the paper for experimental results, because HAQ chooses MobileNet and ResNet-50 only as model architectures while comparisons are made with only PACT and Deep Compression methods.", "Q3: Providing some code and numbers for inference time would be great.", "A3: Due to the internal policy of our organization, we cannot open our codes publicly at this moment. Hence, we provide a link to anonymous code to the reviewers only until we get an approval for public release.", "Please refer to our message available to the reviewers only.", "Overhead of weight decryption on-the-fly is extremely small even with CPUs or GPUs, since decryption involves only a binary matrix multiplication over GF(2), which can be easily supported by existing SIMD or vector operations.", "Since a binary matrix is too small (e.g., 10x8), computational overhead is just ignorable compared with other computations."], "labels": ["nonarg", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "dispute", "dispute", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 255, "sentences": ["[Q] The paper presents a novel hierarchical clustering method over an embedding space.", "In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learned.", "The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods.", "[A1] Dear Reviewer 1, thank you for the thoughtful review.", "The reviewer mentioned our key point correctly.", "Many works on flat-clustered representation learning except for VAE-nCRP, has been limited to capture flat-level data structure.", "[Q] The paper address a relevant problem, which is of great interest for extracting knowledge from data.", "[A1] There are a lot of high-dimensional data around us, and it obviously contains complex structures inside. What we would like to argue through this study is that we can analyze the complex structure of data in the embedding space learned by a deep neural network.", "[Q] In general, the quality of the paper is high.", "The presented approach is based on a sound formalization of hierarchical clustering and deep generative models.", "The paper is easy to follow in spite of the technical difficulty.", "The experimental evaluation is really extensive.", "It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view.", "[A1] Thank you for the comment.", "As we assume a rather complex prior to embedding for flexibility, the technical depth of formalization has deepened.", "We concerned that it would be confused for the readers including the reviewers, to understand.", "Therefore, we carefully presented the figures, especially in Figure 3(a) showing an example of the variable values.", "In the case of experiments, we have devised various quantitative and qualitative experiments to assert why we need this hierarchically clustered representation learning.", "We empirically observed the performance improvement of both density estimation and hierarchical clustering, which motivates the joint optimization.", "Additionally, we qualitatively showed embedding plot, image generation, and result hierarchy with various datasets.", "[Q] The only issue with this paper is its degree of novelty, which is narrow.", "The proposed method adapt a previously presented hierarchical clustering method in the \"standard space\" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.", "The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.", "[A1] As the reviewer pointed out, we adopted the partial components with the previously proposed techniques or methodologies.", "The theoretical contribution of our study can be considered in conjunction with the unified model based on the fully Bayesian approach of the probabilistic graphical model and the neural network.", "Additionally, we tuned the several detailed heuristic algorithms for operations such as GROW, PRUNE, and MERGE.", "Also, if we take a naive pipelined approach of iterative training between the hierarchical Gaussian mixture model and representation learning, then this work would be an obviously incremental work.", "[A2] VAE imposes a single Gaussian prior on embeddings, which leads to 1) the over-regularization, and 2) poor representations [1,2,5].", "[1] Chen, Xi, et al. \"Infogan: Interpretable representation learning by information maximizing generative adversarial nets.\" NIPS. 2016.", "[2] Hoffman, Matthew D., and Matthew J. Johnson. \"Elbo surgery: yet another way to carve up the variational evidence lower bound.\" Workshop in Advances in Approximate Bayesian Inference, NIPS. 2016.", "Therefore, the recently published researches can be divided into two branches: 1) designing of an objective function by introducing the additional regularized terms, or 2) constructing of a more flexible prior.", "Our work attempts to the latter approach, which proposes a new prior called a hierarchical-versioned Gaussian mixture distribution prior to the first trial of hierarchical density estimation in the embedding space.", "Another work of the latter approach is:", "- Variational Deep Embedding (VaDE) [3]: VAE+GMM", "- VAE-nCRP [4]: VAE+(nCRP+GMM)", "- VAE with a VampPrior [5]: VAE+ a variational mixture of posteriors prior.", "The contribution of these studies lies on 1) the formalization as a unified model based on the newly proposed prior, though not the original technique proposed by the authors, and 2) demonstrating the superiority of the prior.", "[3] Jiang, Zhuxi, et al. \"Variational deep embedding: an unsupervised and generative approach to clustering.\" IJCAI, 2017.", "[4] Goyal, Prasoon, et al. \"Nonparametric Variational Auto-Encoders for Hierarchical Representation Learning.\" ICCV. 2017.", "[5] Tomczak, Jakub, and Max Welling. \"VAE with a VampPrior.\" AISTATS. 2018.", "Best regards,"], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "dispute", "dispute", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 256, "sentences": ["> This suggests that PointGoal navigation with dense GPS signal is might be a poor choice to benchmark RL algorithms and we should proceed to harder tasks.", "Agreed.", "We hope that our algorithm, DD-PPO, and our pretrained models will help accelerate progress on harder tasks like PointGoal Navigation without GPS+Compass, ObjectGoal/RoomGoal Navigation, Instruction Following, etc."], "labels": ["nonarg", "concur", "concur"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 257, "sentences": ["We want to thank the reviewer#1 for your feedback.", "Your summary correctly reflects the content of our paper.", "We hope this rebuttal can address your concerns.", "Major concern: Trained sampling vs random sampling", "We sample architectures every a few epochs, mainly because in our experiments, we want to analyze the behavior of the architecture distribution at different super net training epochs.", "This analysis is illustrated in figure 3 of our paper.", "We can see that at epoch-0, where the architecture distribution is trained for only one epoch (close to random sampling), the sampled architectures have much lower compression rate.", "Similarly, for epoch-9, architectures also have relatively low compression rate.", "In comparison, at epoch-79 and epoch-89, architectures have higher compression rates and accuracy.", "The difference between epoch-79 vs. epoch-89 is small since the distribution has converged.", "As the reviewer#2 suggests, we can train the super net until the last epoch, then sample and train architectures from this distribution.", "Figure 3 shows that the five architectures sampled at epoch-89 are much better than the five architectures at epoch-0, which are essentially drawn from random sampling.", "Also, note that for CIFAR10-ResNet-110 experiments, the search space contains 7^54 = 4x10^45 possible architectures, 45 sampled architectures are tiny compared with the search space.", "Reviewer #2 suggests comparing with a \u201ccost-aware\u201d random sampling policy.", "We tried a simple baseline that at each layer, we sample a conv operator with b-bit precision with probability", "prob(precision=b) ~", "1/(1 + b)", "The performance of this policy is much worse since for a conv operator with precision-0 (in our notation, bit-0 denotes we skip the layer), the sampling probability is 33x higher than full-precision convolution, 2x higher than 1-bit, 3x higher than 2-bit, and so on.", "Architectures sampled from this distribution are extremely small but with much worse accuracy.", "We understand this might not be the best \u201ccost-aware\u201d sampling policy.", "If reviewer#1 has better suggestions, we are happy to try.", "Minor concern #1: Value of the Gumbel Softmax function", "Yes. We agree with the comments that the advantages of the Gumbel Softmax technique are two-fold:", "1. It makes the loss function differentiable with respect", "to the architecture parameter \\theta", ".", "2. Compared with other gradient estimation techniques such as Reinforce, Gumbel Softmax balances the variance/bias of the gradient estimation with respects to weights.", "Minor concern #2: Comparison with non-stochastic method such as DARTS", "DARTS [1] does not really sample candidate operators during the forward pass.", "Outputs of candidate operators are multiplied with some coefficients and summed together.", "For the problem of mixed precision quantization, this can be problematic.", "Let's consider a simplified scenario", "y = alpha_1 * y_1 + alpha_2 * y_2", "Let's assume both y_1 and y_2 are in binary and are in {0, 1}. Assuming alpha_1=0.5 and alpha_2=0.25, then the possible values of y are {0, 0.25, 0.5, 0.75}, which essentially extend the effective bit-width to 2 bit.", "This is good for the super net's accuracy, but the performance of the super net cannot transfer to the searched architectures in which we have to pick only one operator per layer.", "Using our method, however, the sampling ensures that the super net only picks one operator at a time and the behavior can transfer to the searched architectures.", "Minor concern #3: Warmup training", "We use warmup training since in our ImageNet experiments.", "We observe that at the beginning of the super net training, the operators are not sufficiently trained, and their contributions to the overall accuracy are not clear, but their cost differences are always significant.", "As a result, the search always picks low-cost operators.", "To prevent this, we use warmup training to ensure all the candidate operators are sufficiently trained before we optimize architecture parameters.", "In our ImageNet experiments, we found that ten warmup epochs are good enough.", "In CIFAR-10 experiments, warmup training is not needed."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 258, "sentences": ["We thank for your valuable comments and suggestions.", "=== Regarding to the assumptions, specifically, twice differentiable/smooth Hessian =", "=", "=", "Twice differentiable/smooth Hessian are only used for analyzing the process of escaping saddle points.", "So we conjecture that one can relax the assumptions and introduce the notions like ``locally twice differentiable'' and ``locally smooth Hessian'', meaning that the assumptions only need to hold in the region of the saddle points.", "Since the gradient norm in the region of the saddle points is small, it implies that the Hessian should not change too much and ``locally smooth Hessian'' should make sense.", "However, we are not aware of any related works of escaping saddle points introducing any measures of ``locally smooth Hessian''.", "You might actually point out a good research direction.", "=== Regarding to the empirical results/experiments ===", "We appreciate your acknowledgment of our contributions and pointing out that the properties may only need to be satisfied at some critical points during training deep neural nets.", "We will keep updating the paper and conducting more thorough experiments.", "=== Regarding to the small step size ===", "We think that it is a gap, for which people in the community haven't have any good remedies yet.", "Almost all of the theoretical works in nonconvex optimization and deep learning require a small step size (e.g. works of natural tangent kernel, works of showing the global convergence for a two layer neural net).", "Nevertheless, we want to note that the step size $\\eta = O(\\epsilon^5)$ in our paper is of the same order as the closely related work (Daneshmand et al. 2018) of escaping saddle points."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "concur", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 259, "sentences": ["Thank you for your constructive comments! We would like to address your concerns as follows:", "1. Clarification on Fig. 4.", "We rewrote the caption for Fig. 4.", "Specifically, for Wave-U-Net, the green curve indicates the fitting result compared against the noisy target, and the red curve is the result evaluated against the clean signal.", "Therefore, Wave-U-Net fits the noisy target fast but does not produce the clean version of the signal during fitting.", "For Convolution and Dilated Convolution networks, they do fit faster but saturates with low-quality output.", "Harmonic Convolution produces much better results, which is ~3.5 dB higher.", "We highly recommend listening to examples at https://anyms-sbms.github.io to feel the difference.", "2. Dilated convolution in paper\u2019s notation.", "We have added a section in the appendix to include dilated convolution in the paper's formulation.", "3. Clarification on Fig. 2.", "Since the plots in Fig. 2 are log-scale, one would expect nearly linear fall-off of energy from low-frequency components to high-frequency components, which is the case of (a).", "But (c)(e) exhibit drastically different fall-offs of energies compared with (a).", "We have modified the caption of Fig. 2 to be more specific.", "We compared our model with unsupervised/supervised NMF for sound source separation, a common unsupervised baseline for this task.", "The evaluations are reported as follows:", "----unsupervised----", "guitar:          SDR: 2.17   SIR: 2.78   SAR: 14.19", "congas:        SDR: -0.20  SIR: 0.23   SAR: 14.76", "xylophone:  SDR: 2.04   SIR: 3.61   SAR: 12.13", "----supervised----", "guitar:          SDR: 5.97   SIR: 7.56   SAR: 12.81", "congas:        SDR: 1.77  SIR: 2.76   SAR: 11.97", "xylophone:  SDR: 8.08   SIR: 12.33   SAR: 11.72", "Please let us know for any questions.", "Thanks again for your suggestions, which have made this submission stronger.", "Thanks,", "Authors"], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 260, "sentences": ["We thank AnonReviewer1 for the careful reading and encouraging comments.", "Indeed, the idea of a non-linear or adaptive gain normalization is novel to our knowledge, and is the main reason for deciding to submit this work to ICLR.", "We based our theoretical insight on an extensive reading and experience on neurophysiological data which we tried as much as possible to reconcile with the latest literature in ML/DL.", "In particular, we think that this problem is resolved in most DL approaches using heuristics such as dropout ( http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf ) or batch normalization ( https://arxiv.org/abs/1502.03167 ).", "We acknowledge that the objective we use (equiprobability) may seem arbitrary, but we think that 1) it best fits constraints in biological populations of neurons 2) it can be adapted to other priors on the desired probability of nodes in the network.", "In our current revision", ",  while keeping the same theoretical framework and simulation results, we have highlighted our main contributions: 1/ to show that $\\ell_2$ normalization leads to non-homogeneous data 2/ provide with an exact rule 3/ propose a simplfied heurstics and show its effectiveness.", "Also, we have fixed the typos and minor issues  (in \"Misc\") in the revision that is being uploaded to the openreview preprint server.", "Thanks again for your careful reading."], "labels": ["nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 261, "sentences": ["We thank you for acknowledging the novelty our findings and your appreciation for the elementary nature of our theory.", "----------------", "- Q: How do Section 2 & 3 fit together?", "Although it is true that our paper can roughly be divided into two section, we want to stress that these sections are inextricably linked due to the nature of their topics, since we see invariance as a limit case of inverse stability.", "We therefore think it is natural to study both of them.", "However, the analysis of the limit case, invariance, admits more powerful tools (see e.g. Theorem 4), since one is only interested in whether a singular value is zero or not.", "Hence, the invariance is qualitative, whereas for stability we need to quantify singular values.", "-----------------", "-Q: Combinatorial problem to check Theorem 4:", "While there are indeed a combinatorial number of possible tuples that the Theorem 4 describes, we can use the following trick in the design of the Algorithm 1 (Appendix A3) to circumvent these computations: The set of tuples (A, b) that form omnidirectional tuples is a null-set in all tuples of same form, we therefore ignore this case in our numerical analysis.", "Hence, we only have to check whether we have a compact or unbounded preimage.", "This can be done by simply checking whether A is omnidirectional or not.", "----------------", "-Q: Upper bounds and inverse stability:", "The smallest singular values are directly linked to inverse stability for points from the same input polytope (where the linearization is exact).", "The upper bounds (Lemma 9) and the correlation effect are interesting, as they show how a well-conditioned matrix (subset of rows almost orthogonal) may become instable due to the removal of rows via ReLU.", "If the correlation of some rows is arbitrarily small (but non-zero) between remaining and removed rows, the upper bounds can be arbitrarily small.", "Thus, this Lemma provides an intuition how hard it is to globally control inverse stability with a vanilla architecture (linear mapping followed by ReLU).", "However, when considering an epsilon ball around activations, two main questions arise: 1) Are all points in the ball reachable from the considered input polytope?", "2) Do points from other input polytopes map to the epsilon ball?", "If the second case holds, one would need to consider different linearizations of the network and thus extend the analysis to movements between the polytopes.", "-> Added a comment in the newly written \u201cScope\u201d section in the revision", "-------------------", "-Q: Actionable consequences from paper:", "One consequence of our paper is that it is close to impossible (each layer need at least to double the number of neuron) to enforce invertibility and it is similarly hard to enforce compactness in ReLU layers.", "This leads to the conclusion that if one wants invertibility or even just compactness reliably over the whole space, vanilla architectures using ReLU are not a good tool for the task.", "Hence, our analysis can be seen as an argument for additional structure like dimension splitting in reversible networks (see e.g. Jacobsen et al. (2018)).", "These structures allow for guarantees as they are by design bijective, while vanilla architectures show a breadth of possible effects as shown in our analysis.", "-> Added a comment to \u201cPractical Implications\u201d in the revision", "- Q: Illustrative experiments:", "We currently thinking about an experiment to better illustrate the intuition of our theory and would appreciate any suggestions.", "We thank the reviewer for the helpful comments and we would appreciate further suggestions.", "We added an illustrative example in the introduction to give an intuitive understanding of invariance, stability and their relationship.", "We would appreciate further suggestions."], "labels": ["concur", "nonarg", "nonarg", "dispute", "dispute", "dispute", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 262, "sentences": ["We thank Reviewer 1 for their detailed comments and feedback.", "Reviewer 1\u2019s main concerns are 1) that the related works section does not sufficiently frame our work with previous literature, 2) that the proofs of theoretical guarantees are not sufficiently rigorous, and 3) that the experiments section is not comprehensive enough.", "We have posted a significantly updated new draft to address these concerns.", "-------------------------------------", "Experiments", "Reviewer 1 claims that we do not sufficiently compare with enough other methods, and specifically asks for comparisons with Feudal Networks (FuN) and Roderick et al., 2017.", "We already comprehensively compare with the prior non-demonstration state-of-the-art, which use a comparable amount of prior knowledge, in each game.", "Since we already compare with the prior state-of-the-art approaches, and other approaches perform significantly worse than the prior state-of-the-art approaches, we do not compare with the many other deep RL approaches.", "In particular, FuN and Roderick et al., 2017 both report results on Montezuma\u2019s Revenge.", "The prior state-of-the-art approach we compare against, SmartHash, outperforms these approaches by 1.75x and 4x respectively, at the number of frames they report (200M and 50M respectively).", "Our approach further outperforms SmartHash by over 2x.", "Reviewer 1 further asks for evaluation on more games.", "We believe that we have already demonstrated a significant improvement over the prior state-of-the-art, and additional experiments could be prohibitively expensive.", "In particular, we follow Aytar et al., 2018, and evaluate on 3 of the hardest exploration games from the Arcade Learning Environment.", "We do not evaluate on many of the simpler other games (e.g., Breakout), because they do not require sophisticated exploration and can already be solved with current state-of-the-art methods.", "We use the same set of minimally tuned hyperparameters (tuned only on Montezuma\u2019s Revenge) and obtain new state-of-the-art results by over 2x, suggesting that our approach can generalize to new tasks.", "Our results are not cherry-picked as R1 suggests: following many recent deep RL works, e.g., Ostrovski et al., 2017, Tang et al., 2017, we run 4 seeds on each task, and obtain statistically significant results.", "Even our *worst seed* outperforms or is competitive with the prior state-of-the-art *best seed*.", "We note that running 10 seeds would approximately cost $30,000 per additional game in compute.", "Renting the appropriate equipment (e.g., via Google Cloud) to run a single seed to completion costs ~$1,500.", "To run 20 seeds (10 for our approach, 10 for the prior state-of-the-art) would cost 20 x $1,500 = $30,000 or roughly the median US annual salary.", "---------------------------------------", "Related Works", "We\u2019ve updated the related works section in our recently posted draft to more carefully compare  Please see Sections 1 and 7 for updated related work.", "The main critical difference between our work and other HRL works is that we build an abstract MDP, which enables us to plan for targeted exploration; other works also learn skills and operate in latent abstract state spaces, but not necessarily in a way that satisfies the property of an MDP, which can make effectively using the learned skills difficult.", "--------------------------------------", "Theory", "In the updated draft of our paper, we have updated the rigor of the theory section: please see Section 5 and Appendix C for updated theory.", "To summarize: we\u2019re interested in the sample complexity of RL algorithms, i.e., the number of samples required for the learned policy to become near-optimal (achieve reward at most epsilon less than the optimal policy).", "Standard results (e.g., MBIE-EB, R-MAX) can guarantee a near-optimal policy, but they require so many samples (polynomial in the size of the state space) in deep RL settings, that the guarantees are effectively vacuous.", "In contrast, for a subclass of MDPs, our approach provably learns a near-optimal policy in a number of samples polynomial in the size of the *abstract* MDP.", "Responding to R1's additional feedback:", "R1 asks if our method applies to continuous spaces.", "Our method applies to continuous spaces with no changes, we can just discretize the abstract state (not the concrete state).", "In particular, our method may be well-suited for many robotics tasks, which often have the full state (e.g., joint angles and object positions) available.", "For example, in a task like stacking blocks with a robotic arm, a good state abstraction function would be the position of the end effector and blocks, which are directly available in the state (e.g., in Stacker from DM Control Suite).", "R1 says that the randomized exploration used by the discoverer is underwhelming.", "We view the simplicity of the discoverer as advantageous.", "Fundamentally, exploration requires some degree of randomness, and we were already able to achieve state-of-the-art results without overcomplicating the discoverer.", "We note that this random exploration is only for locally discovering nearby abstract states.", "Globally, we drive exploration by incrementally growing the safe set (renamed known set in the updated draft).", "R1 asks for experiments that do not use RAM state information.", "We clarify that we use the RAM state information for the state abstraction function, which is a fundamental component of our work, so it is not possible to run experiments without this RAM information.", "However, we explore the robustness of our method to the exact chosen abstraction in section 7.4 and find that our method achieves state-of-the-art results over a wide range of state abstraction functions, suggesting that alternate state abstraction functions could be used.", "We also note that our experiments compare with state-of-the-art approaches, which also use prior knowledge comparable to our usage of RAM state information."], "labels": ["nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "dispute", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "dispute", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 263, "sentences": ["Thanks for your comments.", "We missed this previous work.", "Jenatton et al. proposed an approach to predict tree structures by using Bayesian optimization to combine independent Gaussian Processes with a linear model that encodes a tree-based structure.", "We have cited and discussed the work in the revision.", "The focus of our work is to propose a new tree prediction problem (layout completion) and introduce Transformer-based approaches for addressing the problem.", "It would be future work to investigate other tree-based models for this problem."], "labels": ["nonarg", "concur", "nonarg", "concur", "nonarg", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 264, "sentences": ["Thank you for your thoughtful review.", "We have updated the paper based on your comments to improve clarity and reproducibility.", "We list a summary of our main changes below:", "- In order to make it easier for readers to understand the differences between different models and how they are related to InfoNCE, we have added a summary in Table 1.", "- We have improved notations by adding explicit definitions before they are used in Section 2 and Section 4, and added a short description of Deep InfoMax in Section 4.", "- We have included model and training hyperparameter details in Section 5.1 and Appendix B.", "- We added a motivation for mixing two different terms in the objective function.", "Our DIM is primarily designed to improve sentence and span representations.", "We combine it with MLM which is designed for learning (contextual) word representations, since our overall goal is to create better representations for both the sentence and each word in the sentence.", "We also note that Deep InfoMax for learning image representations mixes multiple terms in their objective function.", "We only take one of the terms from the full objective function and mix it with MLM.", "Regarding equation I_{DIM}, it is supposed to contain two g_{\\omega} and no g_{\\psi} as we use one network for encoding both the sentence and n-grams.", "This is not a typo."], "labels": ["nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 265, "sentences": ["Thank you for your comments and questions.", "We have updated the manuscript to clarify these questions.", "1) The VAE is hypothesized to produce blurry images when the inference/generative models are not sufficiently expressive for the data modeling task, and in particular due to the typical choice of MSE loss (i.e. Gaussian error model), thus blurring sharp edges in complex natural image data [1,2,3].", "In the case of cryo-EM, the high noise in the images is typically assumed to be Gaussian and therefore using the MSE loss has a denoising effect.", "In our experiments, we were able to achieve resolutions up to the ground truth resolution or matching published structures with our architecture and training settings, though we agree with the reviewer that exploring alternative generative models is a promising future direction.", "[1] https://arxiv.org/abs/1611.02731", "[2]  https://openreview.net/pdf?id=B1ElR4cgg", "[3] https://arxiv.org/pdf/1702.08658.pdf", "2) We observed accurate reconstructions as long as the dimension exceeded the dimension of the underlying data manifold and faster training with higher dimensional latent variables.", "We have added these results to the appendix in the revised manuscript.", "3) We varied the number of classes for comparison against SOTA discrete multiclass reconstruction and selected 3 classes which had the lowest error for our comparison in Table 2.", "We have added these results to the appendix in the revised manuscript.", "4) Our coordinate-based neural network model for volumes provides a general framework for modeling extrinsic orientational changes in a differentiable manner.", "This work could be applied in other domains of scientific imaging such as reconstruction of tomograms or CT scans."], "labels": ["nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 266, "sentences": ["Dear AnonReviewer2,", "thank you for your constructive feedback. Below we address your concerns and questions.", "\u201cJudging from Table 1, the proposed method does not seem to provide a large contribution.", "For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.\u201c", "\u2192 The authors of NASNet only provide results for two regimes of parameters (3.3M and  27M) as they do not perform multi-objective optimization but rather just vary two parameters for building NASNet models (number of cells stacked, number of filters).", "Their method might be optimized to yield good results in these regimes and, admittedly, LEMONADE does not outperform NASNet for models with ~4M parameters.", "However, from Figure 3 and Table 2 one can see that only varying these two parameters for NASNet models is not necessarily sufficient to generate good models across all parameter regimes.", "E.g., LEMONADE clearly outperforms NASNet for very small models (50k params, 200k params - Table 2).", "We also refer to Appendix 3 (\u201cLEMONADE with 5 objectives\u201d), Figure 6, in the updated version of our paper, where one can see that while NASNet has quite strong performance in terms of error, number of parameters and number of multiply-add operations, it performs poorly in terms of inference time.", "Hence, there is a benefit in doing multi-objective optimization if one is actually interested in multiple objectives and diverse models rather than a single model.", "This is the main contribution of our paper and different to, e.g., the NASNet paper.", "The same likely also applies for ENAS (as they use the same search space and conduct very similar experiments).", "We also would like to highlight two things: 1) NASNet requires 40x computational resources than LEMONADE, so even if NASNet performs better for ~4M parameter models, LEMONADE achieves competitive performance in significantly less time.", "2) Table 1 shows results for models trained with different training pipelines and hyperparameters, and hence it is hard to say architecture X performs better than architecture Y since differences could simply be due to e.g. different learning rates, batch sizes, etc.", "In contrast, all other results in the paper (e.g., Figure 3 and Table 2) provide comparisons with exactly the same training pipeline and hyperparameters.", ".", "\u201cIt would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix. \u201c", "-> Thanks, we agree; we re-organized our paper accordingly.", "\u201c- In the case of the search space II, how many GPU days does the proposed method require?", "-> We also ran this experiments for 7*8 GPU days, however the method converged after roughly 3*8 GPU days (meaning that there were no significant differences afterwards).", "\u201cAbout line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.\u201d", "-> The population is updated to be all non-dominated points from the current population and the generated children, i.e. the Pareto frontier based on all current models.", "We clarified this in Algorithm 1.", "Thanks for pointing us towards this.", "We hope this clarifies your questions. Thanks again for the review!"], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "dispute", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "nonarg", "concur", "concur", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 267, "sentences": ["Thank you for your review.", "We are a bit surprised since the paper provides answers to the exact questions you raised as missing. We are sorry you missed it, and we have cleaned up the presentation so it is hopefully now clear that we do answer these questions and more.", "The answers, as you pointed out, were much desired and not known before.", "Below are answers resultant from eq. 5 to the specific questions the referee raised, with some added definitions to make them concrete.", "1. \u201chow deep should a model be for a classification or regression task? \u201c", "We show in section 6.1 that the dependency of the classification error on the number of layers is also well approximated by eq. 5 (recall $m$ scales linearly with depth).", "So, if we consider some target error $\\epsilon_{target}$, we can solve eq. 5 for m or n given the other or for both, attaining the m,n contour for $\\hat{\\epsilon}(m,n) = \\epsilon_{target}$.", "2. \u201cWhat is the minimum/maximum layers of a deep model? \u201c", "For a fixed dataset size, model scaling eventually contributes marginally to error reduction and becomes negligible when $bm^{-\\beta} \\ll n_{lim}^{-\\alpha}$ (Eq. 5).", "Define the relative contribution threshold $T$ as satisfying $ T = \\frac{n^{-\\alpha} }{ bm^{-\\beta}}$. (For example, $T=10$.) Then the maximal useful model size meeting threshold $T$ is:", "$$     m_{max}(T) = \\left(bT\\right)^{1/\\beta} n_{lim}^{\\alpha/\\beta}  $$", "As for minimal depth, here too let\u2019s consider a definition as a working example: what is the minimum depth that could meet a certain error level $\\epsilon_{target}$ (if data is not a limit).", "For example, when the target error is small relative to the \u201crandom guess error\u201d $\\epsilon_0$ (equivalently when $ n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$), by solving eq. 5 for $m$ we have:", "$$ m_{min} = \\left(\\frac{b}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\beta} $$", "3. \u201cHow much data is sufficient for a model to learn? What is the minimum/maximum size of the data set?\u201d", "Similarly to the above:", "Minimum data needed for target error (if model size is not a limit):", "$$ n_{min} = \\left(\\frac{1}{\\frac{\\epsilon_{target}}{\\epsilon_0}\\eta-c_\\infty}\\right)^{1/\\alpha} $$", "4. Maximum useful data (in the marginal sense $T$ for a limited size model, as above):", "$$n_{max}(T) = \\left(1/bT\\right)^{1/\\alpha} m_{lim}^{\\beta/\\alpha} $$", "In particular, note that there is also a minimal amount of data and model size needed for better-than-random-guess error level, characterized by the location of the pole $\\eta$: $n^{-\\alpha}+bm^{-\\beta}< \\eta$", "5. \u201cDo we really need a large data set or just a subset that covers the data distribution?\u201d", "Via careful dataset sub-sampling (as noted by reviewer 3) we show that indeed more data *is* needed to improve performance (reduce error) while holding the class distribution fixed (in expectation), for a given architecture and scaling policy.", "For directly viewing the error manifolds decoupling the dependency on model and data size, see figure 1 and in appendix C.", "6. \u201cWhat's the relation between the size of a model and that of a data set? \u201c", "The joint form in Eq. 5 captures the relation between data-size and model-size (and error) completely.", "7. \u201cBy increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?\u201d", "For example, from Eq. 5, it is clear that a sweet-spot in terms of balancing the effect of the data/model sizes on limiting the error is $n^{-\\alpha} \\approx bm^{-\\beta}$ .", "When considering this sweet spot for example, increasing depth/width/both such that the model size $m$ is increased by a factor $f$ to a new size is $m\u2019 = mf$, the corresponding increase in data maintaining the sweet-spot is $n\u2019 = nf^{\\beta/\\alpha}$", "8. How about the gain of the task performance?\u201d", "The effect on the performance is given by evaluating Eq.5 for the initial and scaled $m,n$.", "For example, in the powerlaw region ($c_\\infty \\ll n^{-\\alpha} + bm^{-\\beta} \\ll \\eta$):", "The effect on the performance is $\\epsilon\u2019 = \\epsilon f^{-\\beta}$"], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 268, "sentences": ["Thank you for your review.", "We agree that assumption (H2) is very restrictive and have added some results relaxing it in Section 3.4 in the latest version of the paper.", "Please see the comment above entitled: \u201cRelaxing Assumption (H2)\u201d for more details.", "However, it it worth pointing that even under Assumption (H2), learning does not necessarily converge.", "As shown in Fig 2. Left and Section 3.3, any initialization in the top left red region will fail to solve the problem.", "In that case, the confidence on the corresponding class will be 0.5 after a finite number of updates.", "As far as assumption (H1) is concerned, it is very classic in deep learning theory (see for instance [1,2,3]) and we have not been able to relax it.", "[1] T. Laurent and J. von Brecht. Deep linear networks with arbitrary loss: All local minima are global.", "ICML 2018", "[2] Z. Liao and R. Couillet. The dynamics of learning: A random matrix approach. ICML 2018.", "[3] S. Arora et al. On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization. ICML 2018."], "labels": ["nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 269, "sentences": ["Thank you for your review.", "Below we attempt to answer your concerns.", "We also want to point out that we have added some insights/results relaxing one of our main assumptions in Section 3.4 of the latest version of the paper.", "For more details, please see the comment above entitled: \u201cRelaxing Assumption (H2)\u201d.", "The path we attempted to draw through the paper aims at the evolution of a nonlinear neural network\u2019s classification performance throughout its training and at the factors that influence it: from the norm of the input to the type of loss used for learning or the frequency of features present in the training data.", "Our framework is able to establish properties on the behavior/convergence of certain classifiers during their training on separable data.", "Those insights match some observations made by machine learning practitioners, in particular about the sigmoidal shape of learning metrics or the efficiency of the hinge loss on certain tasks.", "We have added an explanation of what we mean by \u201clearning dynamics of deep learning\u201d in the last paragraph of the first page.", "It usually refers to the evolution of weights and outputs of neural networks throughout training.", "For instance, the work by Saxe et al in 2013 is entitled \u201cExact solutions to the nonlinear dynamics of learning in deep linear neural networks\u201d.", "We based our title on that paper since it extends some of its results to nonlinear neural networks.", "We understand your concern and have made the title more specific.", "Tentatively, we chose: \u201cConvergence Properties of Deep Neural Networks on Separable Data\u201d.", "Let us assume for simplicity that in Corollary 3.3, p = 0.5 (ie that the classes are balanced) and that ||x_1|| = 1, ||x_2|| = 0.5.", "Then the confidence of the network on those classes corresponds to the red and dashed purple curves of Fig. 2.", "Right.", "In particular, we see that reaching any level of confidence takes approximately twice as much time on class 2 (red curve) than on class 1 (dashed purple curve).", "That is effectively what the corollary is expressing.", "We have edited the corresponding sentence to make it less assertive.", "We have added the missing labels in the latest version of the paper. Thank you for pointing it out."], "labels": ["nonarg", "nonarg", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 270, "sentences": ["Thank you for your comments, we will be responding with specific comments to AnonReviewer3 here, and more general comments to the reviewer above.", "R3: \u201cIt would be good to understand what benefit does the stochasticity of RBMs provide.\u201d", "The stochasticity of the RBM provides a number of benefits over more deterministic methods.", "Firstly, the stochasticity allows for full sampling from the RBMs distribution, and has the ability to identify all possible modes in a multimodal distribution if sampled for long enough.", "As there are many possible solutions to a Boolean logic query (and integers can have many different factors), we note that the statistics that this method provides can give us a variety of answers to the queries, allowing the user to evaluate each individual solution based on its individual merit.", "R3: How do deterministic neural networks perform on the addition and factoring tasks? The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across as arbitrary.", "To the best of our knowledge, deterministic neural networks have not been well studied for the integer factorization problem.", "In [4] deterministic neural networks are used, but are able to factor smaller integers, on a more restricted problem, and are fully trained on the subset of all integers.", "We have cited this work in our related works section, and mentioned its impact.", "R3: That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.", "We agree that this method of composing simple functions to compute more complex ones is intuitive, and may not be very surprising, but we think that this helps data and model efficiency in a different manner than presented in previous papers.", "As far as scaling up the tasks and problem sizes, we are showing a method of combination here, and are scaling up the problem sizes continuously.", "We believe this combination method could be used for other things, and have presented it here as a proof of concept rather than a definitive survey with all possible uses."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 271, "sentences": ["Thank you for your review and comments."], "labels": ["nonarg"], "confs": [1.0]}
{"abstract_id": 272, "sentences": ["We thank Reviewer 3 for their comments.", "Reviewer 3 points out the strong state-of-the-art performance of our approach as a strength and mentions prior knowledge (our use of RAM state information) as a minor weakness.", "To clarify, in our experiments, we outperform previous non-demonstration state-of-the-art approaches that use a comparable amount of prior knowledge.", "We discuss our usage of prior knowledge in greater detail in the section titled \u201cPrior Knowledge\u201d in our response to Reviewer 2."], "labels": ["nonarg", "nonarg", "dispute", "concur"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 273, "sentences": ["Thank you for your thorough and helpful review.", "We also believe that the criteria we identified will be useful for others in narrowing the search for functions that approximate the generalization error of NNs in realistic settings with no access to the true data distribution.", "Concerns regarding overfitting and uncertainty estimation: Given your suggestion, we performed 10-fold cross validation in all tasks and found high quality results and cross-fold consistency.", "We now report updated cross-val for all results in section 6 including figures 3,4 and in the newly-added figure 5.", "We believe that this addresses both the overfitting concern and the uncertainty estimation concern.", "Additional evidence that there is no overfitting is the good extrapolation results (section 7), as acknowledged by the reviewer.", "Regarding the envelope function (equation 5): This form of function is a simple case of the (complex) rational function family (simple pole at $\\eta$, simple zero at the origin in this case).", "This family arises naturally in transitory systems in control theory and electrical engineering, e.g., when considering the frequency response of systems.", "It captures naturally powerlaw transitions.", "With that said, as we stress in the end of section 5, the particular choice of envelope is merely a convenience one and there may be other such functions / refinements.", "We leave further exploration of this aspect to future work.", "We have fixed the misspelling in \u201cdifferentiable\u201d.", "Thanks for pointing this out."], "labels": ["nonarg", "nonarg", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 274, "sentences": ["Thanks for your comments! First, we have to clarify some misunderstandings.", ">>> it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1)", "BadGAN has already theoretically proved that complement data are helpful for semi-supervised learning.", "In this paper, we demonstrate", "that,  using our unseen data, the proofs in badGAN still can be satisfied but in a more concise way.", "Therefore, compared to badGAN that requires extra PixelCNN, DSGAN saves more computational memory and is time-efficienct.", ">>> It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.", "In Novelty detection, we use the reconstruction error as a criterion to determine whether an image comes from seen class or unseen class.", "It is expected that images from the seen classes should be reconstructed better than those reconstructed from unseen classes.", "However, VAE cannot force the unseen classes with high reconstructed error.", "So, we combine DSGAN with VAE to deal with this issue.", "Due to the above reason, it is expected that \"our sampled reconstruction results are not good as VAE\".", "Note that the seen class, car, still can be reconstructed well by our method in Fig 8 (at the last row).", "The quantitative results in Table 3 further validate our approach.", ">>> I would also expect more ablation studies about how to pick p_{\\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.", "In fact, how to design $p_{\\hat{d}}$ depends on applications  instead of datasets, as described in Sec. 4.1 and Sec. 4.2.", "Please note that, in Section 5.2.1, we used the same $p_{\\bar{d}}$ for ALL datasets.", "We also want to clarify the datasets used in our experiments.", "In semi-supervised learning, we follow our competitors to conduct experiments on MNIST, SVHN and CIFAR10.", "In novelty detection, our method is evaluated on CIFAR10, which is also common in this application.", "Furthermore, we also add additional experiments about generating complement data in CelebA, which is a more complex dataset.", "We can see from Fig. 10 (Appendix G) that DSGAN can create complement data for complicate images well."], "labels": ["nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 275, "sentences": ["We thank the reviewer for the comments and appreciation, and would like to answer the reviewer\u2019s questions as follows:", "Q1:The authors claim that the LM optimization in BA-Net is memory inefficient and may lead to non-optimal solutions.", "It\u2019s not clear to me that the proposed method can guarantee optimality any better.", "It\u2019s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.", "A1: Thanks for pointing this out and sorry for the confusion!", "Here we don\u2019t mean that our method can fix the optimality problem in any way.", "We wish to provide some of our analysis of the limitation of BA-Net, and hope our method could provide complementary perspectives to rethink the problem and mitigate the non-optimal issue in terms of performance with more ML component.", "In terms of number of iterations, our method does not have a restriction, since our iteration happens outside the neural network and acts as an incremental improvement.", "In contrast, BA_Net\u2019s iteration is part of the LM optimization and it is inside the network.", "Thus if it unrolls more iteration steps, the memory cost will increase linearly.", "We have updated the paper for this.", "Q2: Show the test time behavior of the network when it is run with more iterations than it is trained with (say 10 or 20)", "A2: Thanks for the suggestion! We added Table 4 in Appendix C that shows performance of the network with more iterations(from 2 to 20).", "Q3:It\u2019s not made entirely clear whether the training back propagates through the update/construction of the pose and depth cost volumes.", "A3: Gradients can back-propagate through cost volumes, and cost-volume construction does not affect any trainable parameters.", "We updated this point in the revised version.", "Q4: In equation 5, \u201cx\u201d should be \u201ci\u201d.", "A4: Thanks for pointing out that! We have fixed the typo."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "concur", "nonarg", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 276, "sentences": ["We thank the reviewer for valuable feedback.", "\u201cthe method they propose offers very little that is new when compared to e.g. Vaswani\u201d", "While the final representation of Time2Vec resembles that of positional encoding, the motivation behind Time2Vec is completely different than that of positional encoding.", "The new things offered by Time2Vec compared to positional encoding and other previous work include:", "- Instead of using time as a scalar feature similar to other features (which as the reviewer also pointed out is a naive way of handling time), we identify the characteristics that differentiate \u201ctime\u201d from other features and propose a representation that enables exploiting those characteristics.", "Note that using time as a scalar feature similar to other features is currently the prevalent choice (see the references in the last paragraph of section 2).", "- Obviating the need for hand-crafting functions of time by instead enabling these functions to be learned from data, and backing up the representation theoretically as, according to Fourier sine series, it can approximate any function in a given interval (see the last paragraph of our response to reviewer 5).", "- Providing a comprehensive set of experiments showing the merit of Time2Vec for time-series prediction problems where time is an important feature.", "This includes, among other things, results for modeling periodic behaviors of signals which is not a goal in positional encoding.", "Although our representation resembles positional encoding on the surface, it has not been clear in the time-series community if/how/why positional encoding can be used to replace hand-crafted functions of time, and there has been no empirical evidence to show its merit.", "\u201cWeak baseline\u201d", "Our goal is to propose a representation of time that can be used instead of merely a float notion of time (as opposed to beating the state-of-the-art on a particular dataset)", ".", "Therefore, all our comparisons are head-to-head comparisons between a model with and without Time2Vec.", "This includes LSTM+T vs LSTM+Time2Vec, TimeLSTM1 vs TimeLSTM1 + Time2Vec, and TimeLSTM3 vs TimeLSTM3 + Time2Vec.", "It would not be sensible to, e.g., compare LSTM+Time2Vec to TimeLSTM3 (or some other model) because the results of such an experiment do not provide evidence towards Time2Vec being useful or useless.", "Specific comments:", "We will clarify the sentence in section 2.", "Except for the hand-crafted experiment, we did not use Time2Vec in non-recurrent architectures.", "In Section 5.2, a \\tau is indeed missing; we\u2019ll fix this.", "If the paper gets accepted, we will expand the experiments with fixed frequencies in the final version.", "In both LSTM+T and LSTM+Time2Vec, for Event-MNIST \\tau is a feature between [0, 783], for NT-DIGITS and SOF \\tau corresponds to the Unix timestamps when events occurred, and in LastFM and CiteULike \\tau corresponds to the delta between the current and previous event."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "dispute", "nonarg", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 277, "sentences": ["Thank you for the insightful comments.", "1. It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done.", "Is there any hope this could be applied to problems like 3D imaging data or videos?", "---", "Regarding the concerns on method scalability.", "The current bottleneck of our approach is processing of all feature maps pixel-wise.", "We see potential of scaling our approach by operating on superpixels, or NxN patches, instead of processing all pixels individually."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 278, "sentences": ["1. We first want to point out the main contributions of the paper.", "First, we address the catastrophic forgetting problem in continual learning.", "Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations.", "Hereby we extend the idea of [2] to generative networks.", "We highlight the differences to DGR [3] in the Sec. 2 of our work.", "2.", "Equation (5) and (6) are taken from [2] one to one.", "Equations (3) and (4) are adopted from [2]: equation (3) describes the annealing of the parameter s, we anneal it globally over the course of epochs, whereas [2] anneal it for each epoch over the number of batches; equation (4) is a simplified version of the one used by [2].", "3. To avoid confusion of the proposed method to utilize techniques of DGR[3] in order to prevent forgetting in the G, we kindly ask the reviewer to refer to our response (2) to the Reviewer 1.", "In the proposed work we adopt the generative replay not in order to avoid storing previous samples, but in order to prevent forgetting in the discriminator (which is used as a final classification model).", "Data synthesized by the generator is replayed for to the discriminator during the training of the subsequent tasks.", "There is no replay applied to the generator network.", "In order to avoid storing previous data, we utilize parameter level attention mechanism similar to HAT [2].", "Concerning the time comparison, there is no reason why our approach should be less time efficient then DGR based approaches [1, 3] as our method does not require retraining the generator from scratch at each time step.", "4. Why our method does not outperform joint training on SVHN?", "Using generated samples accommodates for better performance then joint training is the case of tasks of relatively low complexity such as MNIST.", "Indeed, such a result has been shown in other works, e.g. [1].", "As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with a steady quality of the generated samples.", "Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples.", "Thus, this effect can not be observed neither in the SVHN not the CIFAR10 benchmarks.", "5. Grammar mistakes and typos.", "This will be fixed in the updated version of the paper.", "6. No guarantee to work for any task or scenario.", "As pointed out by the reviewer and is true for many machine learning method, there is no guarantee that the proposed method will work for any task or scenario.", "[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.", "[2] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.", "URL http://arxiv.org/abs/1801.01423.", "[3] H. Shin, J. K. Lee, J. Kim, and J. Kim.", "Continual learning with deep generative replay.", "In", "Advances in Neural Information Processing Systems, pages 2990\u20132999, 2017."], "labels": ["nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "dispute", "dispute", "dispute", "dispute", "dispute", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 279, "sentences": ["Thank you for your comments and questions; we have incorporated these in the revision and respond to your questions below.", "Q1: Why is the color distribution generated using a subset of pixels?", "A1: We use a random subset of pixels simply for computational feasibility.", "Each distribution is compiled over 1000  images, and drawing a distribution over all pixels per image increases that number by 256^2, causing the computation to be very slow.", "In contrast for the remaining operations, we measure one statistic per image based on the bounding box, which is fast.", "Q2: What are the classes in the bottom right of the transformation limitation / data variability plots?", "A2: The classes in the bottom right corner of the plots are wooden spoon (shift x), cleaver (shift y), and computer keyboard (zoom).", "These classes are more difficult for BigGAN to model accurately, and they deform easily or become unrecognizable under alpha transformations, which may prevent the object detector from reliably detecting them.", "Q3: What are the results of manipulations in Stylegan z latent space?", "A3: We experimented with manipulations in the Stylegan z space \u2014 in general the effects of these transformations are weaker, and may entangle other transformations along with the target transformation.", "For example, when recoloring a car using a walk in z, it may inadvertently also rotate or zoom the car.", "On the other hand, in the w latent space we are better able to change the desired attribute without other side effects.", "We have added a new qualitative figure (Fig. 28) in the appendix illustrating these differences.", "Minor Flaws: Thank you for your careful review in catching these mistakes.", "We have updated the typos in the revision.", "In Fig 2 we optimize for a z which approximates a shifted version of the original image x.", "Hence, the G(z+\\alpha w) image does not exactly match the original image G(z) or the shifted edit(G(z), \\alpha), but is intended to approximate the shifted image."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 280, "sentences": ["Thank you for your thoughtful review.", "We have updated notations in Equations 1 and 2.", "The expectations are now taken over random variables (A and B) and the function takes particular values (a and b) of these random variables.", "Regarding your comment about increasing bias and reducing variance, we did observe that the quality of the InfoWord representations is relatively stable across different runs in our experiments (as evaluated by performance on downstream tasks). Could you please clarify a bit more whether this is what you are asking?"], "labels": ["nonarg", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 281, "sentences": ["We thank you for acknowledging our findings to be useful to shed more light on the inner workings of ReLU-networks.", "We respond to your raised points below:", "---------", "- Q: Algorithm applied layer-by-layer:", "As correctly observed, the application of our algorithm to classify the preimage of one data point of one ReLU layer does not easily translate to more than one layer.", "On the one hand, as pointed out, as soon as the preimage is no longer only a point itself it is no longer applicable.", "On the other hand it is a first step towards a multilayer analysis and allows a localized layer-by-layer analysis for the first time.", "-> For more on this we refer to the newly added Section \u201cScope\u201d in the revision.", "--------", "- Q: Applicable to CNNs:", "It is true that our analysis is quite general considering MLPs and not specifically CNNs and indeed we find it very likely that there are stronger results possible for CNNs than the ones we presented.", "-> Added a discussion on CNNs in the new \u201cScope\u201d Section in the revision", "------------", "- Q: Relation to Carlsson et al. (2017):", "While the work of Carlsson et al. (2017) rather focus on a general analysis on the shape of preimages of activities at arbitrary levels and gives a first geometrical view as a piecewise linear manifold, we present in our work an in-depth understanding for preimages and the inverse mapping of ReLU networks:", "1) We perform a qualitative analysis for the preimages and give computable conditions when the inverse image of an output is finite, infinite or a single point by performing an intuitive mathematical derivation.", "2) We analyze the stability of the inverse mapping by investigating the singular values of the linearization of the network and confirm our theoretical results by numerical experiments.", "---------", "We therefore think that our work can be seen as a significantly different approach to the one presented by Carlsson et al. (2017).", "We thank the reviewer for the helpful comments and would appreciate further discussions."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "dispute", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 282, "sentences": ["1. We believe that the transferability of SECRET is due to two major aspects: 1) that we keep representations for the credit assignment separate from those for the RL task and 2) that we use a self-attentional architecture, which was shown to transfer in settings other than RL.", "Better credit assignment is desirable and should arguably lead to better transfer results in the case of SECRET.", "Nevertheless, it is not necessarily true for other credit assignment methods available because they are designed for the online setting and intricately coupled with an RL agent.", "The focus of the paper being on transfer, we proposed a transfer method relying on credit assignment.", "In our opinion, comparing its credit assignment capabilities to other existing methods is outside of the scope of the paper.", "2. We included the results of varying the window size in the new Appendix B.1.", "Briefly, with bigger windows, there is less partial observability, and the attention no longer matches the trigger.", "Please see the new appendix for more details.", "3. Relational Deep RL ([1]) uses spatial self-attention to infer and leverage relations between \"objects\" (pixel representations).", "Crucially, it does not make use of the sequential aspect of the RL task.", "Instead, SECRET relies on temporal credit assignment, which could be presented as a form of temporal relations (as dictated by the reward function).", "Those are very different approaches to handling relations (if SECRET can be deemed as relational).", "We think it would indeed be an interesting research direction to combine both spatial and temporal aspects for credit assignment or relational reasoning.", "4. There are two different aspects here: 1) the reward model could be trained on very few trajectories in the source domain, or 2) it could be applied on very few trajectories to build the potential function in the target domain.", "For 1), in practice, we only redistribute the nonzero rewards that were successfully predicted by the reward model, so insufficient prediction capabilities are not a problem.", "We added a sentence in the main text to mention the fact that we consider correctly predicted nonzero rewards.", "If the model does not manage to predict nonzero rewards, then SECRET falls back to the Vanilla RL case.", "In the worst case scenario, SECRET could predict a small proportion of the nonzero rewards and assign wrong credit, which could lead to a slowed down procedure.", "For 2), the potential function used in SECRET relies on trajectories with nonzero rewards.", "In the worst case scenario, the potential function could not reflect accurately the structure of the MDP and lead to a slowed down learning procedure.", "We now include two additional experiments in Appendix B.3 that explore both scenarios.", "We show that with a small number of trajectories, either in the source or the target domain, the performance of the agent does not drop too much.", "5. The samples generated in the target domain are not included in the number of episodes reported in the paper.", "While debatable, our motivation to do so is that we use the same fixed policy we used in the source domain to generate those trajectories.", "Note that there is no learning procedure involved during the collection of the target samples.", "6. Maybe a follow-up to consider for the coffee test is to adapt from using a coffee-brewing machine to making it from scratch :)", "[1] Zambaldi V., Raposo D., Santoro A., Bapst V., Li Y., Babuschkin I., Tuyls K., Reichert D., Lillicrap T., Lockhart E., Shanahan M., Langston V., Pascanu R., Botvinick M., Vinyals O., Battaglia P. - Deep Reinforcement Learning with Relational Inductive Biases. ICLR 2019."], "labels": ["concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 283, "sentences": ["We thank you for your thorough review, which has undoubtedly helped improve the paper.", "First, we agree that assumption (H2) is restrictive and have added some insights/results relaxing it in Section 3.4 in the latest version of the paper.", "For more details, please see the comment above entitled: \u201cRelaxing Assumption (H2)\u201d.", "Nevertheless, we wish to emphasize that even under Assumption (H2), learning can still fail.", "Fig 2. Left and Section 3.3 show that any initialization in the top left red region will lead (after a finite number of updates) to a confidence of 0.5 on the corresponding class.", "The network does not provide correct classification at the end of training even though it does at the beginning.", "Here are responses to your other concerns:", "- Indeed, our intent in the statement of Theorem 3.2 was to describe the scaling of the solution with respect to those two quantities, but it can be misinterpreted. We have clarified it in the new version of the paper.", "- We have run that experiment and included it in Fig 3. Right among our other recent findings.", "- Corrected in the new version.", "- We have added a line in the last paragraph of Section 4 stating that for the Hinge loss, u(t) grows exponentially in t.", "- We agree that the observed phenomenon can appear in other machine learning methods and is not specific to gradient descent.", "However, in the case of deep neural networks, it is the prevalence of certain gradient directions that determine the final classifier.", "Our results suggests that models converge to solutions that privilege the \u201csimplest\u201d explanation, in an Occam\u2019s razor fashion, which provides an explanation to the \u201cimplicit generalization\u201d of deep nets characterized by Zhang et al.", "Our Kaggle experiment\u2019s aim is to emphasize potential failure modes of current architectures/algorithms (one can think of a self-driving car trained on a road with clear lane markings and operating on a road without such markings).", "The ability to transfer knowledge to test sets coming from a different distribution is key to building more intelligent and robust systems."], "labels": ["nonarg", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "dispute", "dispute", "concur", "concur", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 284, "sentences": ["Thank you for your comments, we will be responding with specific comments to AnonReviewer4 here, and more general comments to the reviewer above.", "R4: \u201cWhat\u2019s less clear to me is what kinds of combinatorial optimization problems can be mapped onto the RBM *training* problem\u201d", "The combination method we propose here can be applied to RBMs that are calculated by directly setting weights and by training individual sub units.", "We acknowledge there are pros and cons to both approaches; directly calculating the weights gives guarantees on probabilities and mixing rates, while training can produce a more compact, data, and computationally efficient model.", "Some algorithms will be more amenable to training, while others more amenable to directly calculating and setting weights, so we believe that this should be addressed on an algorithm by algorithm basis.", "We try to present one possible algorithm and a possible combination mechanism that we believe could work for others.", "R4: \u201cThe paper states that the problem of training \"large modules\" is \"equivalent to solving the optimization problem\", but does not explain how.\u201d", "Training a full module to solve an optimization problem in the context presented here involves supplying samples from a large portion of the subspace that we are trying to model.", "Based on the results we have seen, we only achieve good performance once we have samples from >30% of the subspace (depending on the problem).", "In addition, the RBMs perform significantly better when trained on the full space we are trying to model.", "We view this as the RBM creating an associative memory where it \u201cmemorizes\u201d examples and recalls them afterward, and do not view this as a data and computationally efficient method of solving these problems.", "R4: An example is presented in Figure 3 but is not expanded upon in the main text.", "I\u2019d like the authors to validate my understanding:", "An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder\u2019s inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin].", "After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling.\u201d", "Yes, your understanding is correct.", "We train on the joint density over inputs and outputs, and solving a problem amounts to clamping (conditioning) a subset of the units and sampling the remaining units via Gibbs Sampling.", "We have made an effort in the revision to make sure that this is more clear.", "In the case of solving factorization problem, we clamp some of the visible units to the integer we are trying to factor, and use gibbs sampling to get statistics for the remaining units conditioned on the output number.", "R4: I\u2019m also confused by the presentation of the results.", "For instance, I don\u2019t know what \"log\", \"FA1\", \"FA2\", etc. refer to in Figure 6.", "Also, Figure 6 is referenced in the text in the context of binary multiplication (\"[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6\"), but presents results for addition and factorization only.", "We have presented results for addition and factorization in the main body of the paper, but refer to readers of the paper to the appendix where we have included a larger set of results.", "The results were omitted from the main body of the paper for the sake of brevity.", "The naming of units as \u201clog\u201d \u201cFA1\u201d, \u201cFA2\u201d, etc. are meant to represent the size of the base unit that was merged to create this larger unit, \u201clog\u201d referring to logical units (AND, XOR, etc.), \u201cFA1\u201d being 1 bit full adder, \u201cFA2\u201d being a 2 bit full adder, etc. we have made this clear in the figure caption.", "R4: The way I see it, implementing reversible boolean logic circuits using RBMs is an artificial problem, and the key idea of the paper -- which I find interesting -- is that in some cases it appears to be possible to combine RBMs trained for sub-problems into larger RBMs without needing to fine-tune the model.", "We also agree that there may be other applications to this type of merging of RBMs without further training, and we are working to look at those in greater detail.", "Invertible Boolean Logic provides a good test bed for this idea, and as explained above, we do believe it has a very intimate relationship with Boolean Satisfiability problems and other combinatorial optimization problems."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "dispute", "concur", "nonarg", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 285, "sentences": ["It's true that there are many activation functions that the result doesn't apply to, and in fact isn't true for.", "The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general."], "labels": ["concur", "dispute"], "confs": [1.0, 1.0]}
{"abstract_id": 286, "sentences": ["We thank the reviewer and its careful reading of our paper.", "Concerning point 6: Indeed, we acknowledge that this type of paper may be unconventional for the audience at ICLR.", "But we strongly", "believe that scientific knowledge on biological vision is essential to work out the models that will shape DL in the future.", "Thus, we fully understand the rating given by the reviewer and would like to suggest that our revision addresses the main comment and show that it is relevant for a presentation at ICLR.", "First, we have extended the results by using the useful suggestions of AnonReviewer3 (point 3):", "As suggested by the reviewer we have tested how the convergence was modified by changing the number of neurons.", "By comparing different numbers of neurons we could re-draw the same figures for the convergence of the algorithm as in our original figures.", "In addition, we have also checked that this result will hold on a range of sparsity levels.", "In particular, we found that in general, increasing the l0_sparseness parameter, the convergence took progressively longer.", "Importantly, we could see that in both cases, this did not depend on the kind of homeostasis heuristic chosen, proving the generality of our results.", "This is shown in the supplementary material that we have added to our revision (section \"Testing different number of neurons and sparsity\") .", "This useful extension proves the originality of our work as highlighted in point 4, and the generality of these results compared to the parameters of the network.", "Second, the comment made in point 5 is essential: figures 1 and 3  in our first revision where not showing appropriately the qualitative improvement which is achieved in the resulting filters.", "Indeed, we were showing 18 atoms chosen at random from the 441 filters from the dictionary.", "We initially thought that this \"blind\" shuffling would be a fair representation of the data, but as revealed by point 5, this was not true.", "We have now changed the strategy by now showing  \"the upper and lower row respectively show the least and most probably selected atoms.\"  (see captions of figures 1 and 3).", "This now shows clearly the qualitative improvement in using a proper homeostasis and in particular that using the $\\ell_2$ normalization leads to the emergence of filters which are aberrant (too or not enough selective).", "In particular, we now show quantitatively the probability of choice of each atom - showing that most active filters are used twice more as least active ones.", "Finally, we have made an extensive pass on the manuscript to take into account the different points  and make sure that this approach derived from biological vision is relevant for the audience at ICLR.", "As such, we believe this major change in the way we present the work, both in the quality of the resulting filters and in the generality of the results, have significantly changed the scope of our work to justify its acceptance to ICLR.", "We thank again the reviewer for these very useful contributions to our work."], "labels": ["nonarg", "concur", "dispute", "dispute", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 287, "sentences": ["Thank you for your constructive feedback!", "Main comment 1:", "Absolutely, this is a difficult issue: there is no perfect middle ground where it is possible to study the contributions in their simplest instantiations while at the same time verifying their practical effectiveness.", "We have opted to place the bulk of our emphasis on a realistic scenario (Atari with a Rainbow-like learning agent) that practitioners of Deep RL would find relevant.", "To isolate effects, our experimental section includes many variants and ablations, allowing us to state with confidence that modulating behaviour using the bandit improves performance compared to uniform (no bandit) or untuned (fixed modulation) baselines.", "And this is separately validated across multiple classes of modulations.", "But indeed, as you point out, we cannot guarantee that the improvements we see are purely due to exploration.", "At the same time, it\u2019s worth recognising that, by design, the method proposed will try to cater to the underlying learning algorithm and would ideally generate samples that would benefit the underlying learning procedure.", "We will highlight this ambiguity in the revised paper.", "Main comment 2:", "Sorry, this was not very clear: The performance outcome for each variant is measured on multiple independent runs (seeds).", "All outcomes are then jointly ranked, and the corresponding ranks are averaged across seeds.", "Finally, these averaged ranks are normalized to fall between 0 and 1.", "A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.", "Figure 4 then further aggregates these normalized ranks across 15 Atari games.", "Note that these joining rankings are done separately per subplot (ie modulation class).", "Thus the reason that no fixed arm is always good does not depend on the inter-seed variability as much as on the fact that the best arm differs in different games.", "We will clarify this in the caption too.", "The bandit does not generally do better than the best fixed arm in hindsight -- in general, this would still need to be identified --", "but it is not far off, and it handily outperforms untuned arms, allowing us to remove some of the hyper-parameter tuning burden.", "Additional question 1:", "We acknowledge that our presentation focused maybe more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).", "The updated paper will change the emphasis, and clarify that a closer, more faithful, learning progress proxy remains future work.", "We will also clarify that the little phrase \u201cAfter initial experimentation, we opted for the simple proxy\u2026\u201d implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.", "Additional question 2:", "Of course, even an ideal metric LP(z) would remain a local quantity, and pursuing it would not guarantee the maximal final performance -- but it is valuable if local optima are not the prime concern.", "Performance plateaus are a nuisance in general, and within the simple space of modulations we consider, there is no magic bullet to escape them.", "However, our approach does the next best thing: when performance becomes an uninformative (ie on a plateau), it encourages maximal diversity of behaviour (tending toward uniform probabilities over z), with the hope that some modulation gets lucky -- and then as soon as that happens, very quickly focusing on that modulation to repeat the lucky episode until learning is progressing again.", "Additional question 3:", "Indeed, thank you. We have updated the text to place more emphasis on this contribution.", "Additional question 4:", "The way we would summarize these results is that the bandit is more or less on par with the *best* fixed exploration policy, and so the added complexity is justified by reducing the need to tune exploration. Is this what you meant?", "We think we could address all your concerns, but please let us know if you have further questions, the discussion period lasts until the end of the week!"], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 288, "sentences": ["[Q] The paper proposes using the nested CRP as a clustering model rather than a topic model.", "The clustering is on the latent vector input into a neural network for generating the observation.", "A variational approach is derived.", "The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.", "[A1] Dear Reviewer 2, thank you for the thoughtful review.", "As the reviewer mentioned, we exploited the nested CRP prior to the path selection process.", "For performing a hierarchical density estimation task in embedding space, we additionally designed a hierarchical-versioned Gaussian mixture model prior with the nested CRP prior.", "[Q] A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.", "From the generative model, it seems every data point has its own Dirichlet vector on levels.", "For topic models, this makes sense since that vector is then drawn from multiple times (once per word) from a Discrete, so there's a distribution to actually learn.", "My understanding is that this isn't being done here.", "[A1] Thank you for the very constructive comments.", "In fact, we intended to model the level proportion as shown in the third part of our generative process on page 4.", "Often, for grouped-data, the level proportion (or topic proportion) is modeled as a group-specific variable.", "Under our non-grouped data setting, for example, two following approaches are possible: 1) as the reviewer mentioned, globally define a level proportion once, take multiple level samplings for each data, and 2) as our modeling, locally define the data-specific level proportion, followed by sampling the level (this is actually auxiliary variable for specifying the Gaussian distribution).", "The reason we chose the latter approach is for modeling more flexible prior.", "The Gaussian mixture distributions exist separately for each level, and we assume the generative process that the mixing coefficient for the level would be different for each data.", "Please consider that the data-instance we handled is a high-dimensional data of a document/an image rather than a word/a pixel.", "The hierarchically Gaussian mixture distributions are learned for different levels, and here assuming a common level proportion for all data forcefully limits the expressive power of the model.", "Also, for preventing the overfitting, we placed the common prior, Dirichlet(\\alpha), on the data-specific level proportion, which can be considered as one of the regularization terms.", "[A2] Also, I would like to explain the reviewer\u2019s comment as the formulae.", "The prior we suggested is this: \\sum_{\\zeta, l} nCRP(\\zeta_n) * \\eta_{nl} * Normal(-) (\uf0e0 please refer to the Figure 3(a).).", "Moreover, the point that the reviewer pointed out is on \\eta_{nl}, i.e., \u2018the reason for designing \\eta as \\eta_{nl}, why \\eta is data-specific variable?", "\u2019", ".", "There are similar works, which previously published [1-3].", "They designed data-specific mixing coefficients of Gaussian mixture models, for improving more flexibility like ours.", "[1] Ban, Zhihua, Jianguo Liu, and Li Cao. \"Superpixel Segmentation Using Gaussian Mixture Model.\" IEEE Transactions on Image Processing 27.8 (2018): 4105-4117.", "[2] Zhang, Hui, et al. \"Automatic Visual Detection System of Railway Surface Defects With Curvature Filter and Improved Gaussian Mixture Model.\" IEEE Transactions on Instrumentation and Measurement 67.7 (2018): 1593-1608.", "[3] Ji, Zexuan, et al. \"A spatially constrained generative asymmetric Gaussian mixture model for image segmentation.\" Journal of Visual Communication and Image Representation 40 (2016): 611-626.", "Under the newly proposed Gaussian mixture models from the above papers, the cluster assignment of data is sampled once from the data-specific mixing coefficient, where there is no theoretical problem as a fully Bayesian formalization.", "[A3] We were very impressed with the mathematical detail of the reviewer\u2019s comment and thanked you to the reviewer again. If the reviewer agrees with our argument, we will reflect the argument in our paper.", "Best regards,"], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 289, "sentences": ["Thanks for your comments!", ">>> Experimental settings are clear, however, what makes me confused is that the construction for $p_{\\bar{d}}$ is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images.", "In responding to this comment and the comment of Reviewer #1, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.", "In this experiment, we generate the color images of size 64 $\\times$ 64.", "Similar to 1/7 experiments on the MNIST dataset, we let $p_{\\bar{d}}$ be the distribution of face images with glasses and without glasses, and let $p_{d}$ be images without glasses.", "We sample 10000 images with glasses and 10000 images without glasses from CelebA, and we set $\\alpha$ to 0.5.", "In order to verify the generated image quality of DSGAN, we also train a GAN for comparison.", "GAN is trained with the same amount of training images (but only using face images with glasses since GAN is to learn the distribution of training data).", "In other words, we assume GAN can use complement data as training data directly.", "On the contrary, DSGAN only uses complement data indirectly (the difference between $p_{\\bar{d}}$ and $p_d$).", "Figure 10 in Appendix G shows generated images and FID for both methods.", "We can see that our DSGAN can generate images with glasses from the given $p_d$ and $p_{\\bar{d}}$, and the FID of DSGAN are comparable to that of GAN.", "The experiment validates that DSGAN still works well to create complement data for complicate images.", ">>> The model seems to be sensitive to the hyper-parameter $\\alpha$, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?", "Since the optimal $\\alpha$ of generating \"unseen\" data in DSGAN depends on the degree of overlap between $p_{\\bar{d}}$ and $p_d$, it might need to be fine-tuned for different datasets.", "However, in our experiments, we set $\\alpha$ to $0.8$ in most cases.", "Theorem 1 illustrates $\\alpha$ should be expected to be as large as possible if both network G and D have infinite capacity.", "Though the networks never have the infinite capacity in real applications, a general rule is to pick a large $\\alpha$ and force the complement data to be far from p_d, which is similar to the ablation studies in Sec. 5.1.", "According to our empirical observations, $\\alpha = 0.8$ is the good choice for all datasets.", "Table 11 in Sec. F of Appendix shows the experimental results of how $\\alpha$ affects the performances.", "We use different $\\alpha$ values in the MNIST, SVHN and CIFAR10 dataset, respectively.", "One can see that we achieve the best performances at $\\alpha = 0.8$."], "labels": ["nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 290, "sentences": ["Thank you for the valuable remarks.", "We have tested most of the concerns in points 1. - 4. during our experiments.", "We however could not provide full-extent analysis due to the limited length of the paper.", "Let us respond to each of the points separately below.", "1. How does varying the number of nearest neighbors change the network behavior?", "---", "We observe that using small k (~5) doesn't always provide enough information to perform the denoising and the network is therefore less robust against adversarial examples.", "On the other hand, having k too high (~20) yields too much regularization and the network original performance decreases more significantly.", "In our experiments, we have found k=10 to be a reasonable compromise.", "2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?", "---", "We refer the reviewer to the Section 3 and Section 4.1. of our paper where this is addressed in detail.", "3. Does just simple filtering of the feature map, say, by local averaging, perform equally well?", "---", "It does not.", "We have tried simple smoothing of the feature maps and it not only does not make the network robust against adversarial attacks, but also regularizes the original network too much which results in significant loss in classification accuracy.", "Moreover, local averaging uses the information from the corrupted image itself to filter the feature map, which could even further amplify the noise.", "4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?", "---", "This is of-course true.", "Obviously, selecting very poor nearest neighbors will definitely break the method as the newly created feature map will not express the original information anymore.", "In our paper, we even reason that the adversary often tries to fool the KNN algorithm directly, as we mention at the end of Section 4.3.2.", "Moreover, we believe that our results show when do things start to \"break down\".", "We explicitly mention that an unbounded attack will always fool the network.", "Also, our figures in the main text and tables in the supplementary material show that with increasing magnitude of the perturbation, things start to \"break down\".", "5. Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training.", "It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.", "---", "We provide an evaluation below as well as add an additional section with the results in the supplementary material.", "We have compared our approach to adversarial training method using the code provided by Madry etal.", "https://github.com/MadryLab/cifar10_challenge", ".", "The ResNet-32 baseline model provided in Tensorflow repository (the same we use as CNN baseline in our paper) was trained using the script provided in the cifar10_challenge repository above.", "We have used two training configurations producing two baseline models1 - the default one provided by the repository (ResNet-32 CNN A) and then the same one as in our paper (ResNet-32 CNN B).", "PeerNet was trained traditionally without adversarial training.", "The attack was left as defined by the repository by Madry etal.", "ResNet-32 CNN A: original_acc = 78.86% | adversarial_acc = 45.47%", "ResNet-32 CNN B: original_acc = 75.59% | adversarial_acc = 42.53%", "PeerNet:         original_acc = 77.44% | adversarial_acc = 64.76%", "Results show superiority of PeerNet on this benchmark.", "PeerNet was trained without considering any specific attacks and still outperforms ResNet-32 CNN, which was adversarially trained using this specific attack, by margin of 20%."], "labels": ["nonarg", "concur", "dispute", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "other", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 291, "sentences": ["Thank you for your comments, we will be responding with specific comments to AnonReviewer3 here, and more general comments to the reviewer above.", "R2: For instance, in the introduced approach, only an example of combination is provided in Figure 1.", "It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.", "As far as explaining the method of combination, and the associated mathematical properties, we have tried to do this in greater detail in section 3 (Approach).", "We used Figure 1 and the combination matrices to show what exactly is happening when we combine the models, and how the models mathematically combine.", "In our revision we have made an effort to outline this in greater detail.", "R2: Overall, the paper seems to be a report consisting of a few interesting observations rather than introducing a solid and novel contribution with theoretical guarantees.", "In regards to the lack of theoretical guarantees, we have shown that the equilibrium distribution is what we expect it to be, and mathematically have shown that the final distribution of interest has the mode we expect it to.", "It has been shown in many texts that Gibbs Sampling converges to this equilibrium distribution at a geometric rate in Markov Random Fields.", "Finding the exact convergence rate involves calculation of the eigenstructure of the markov chain transition matrix, which is in general computationally intractable for RBMs of moderate size [1].", "Given this, we have added an extra theorem to show how the upper bounds on convergence rate changes as we merge RBMs, this can be seen in Section 3.1 on \u201cConvergence Rate and MCMC\u201d.", "We show that the rate of convergence of the RBM is geometric in the number of sampling steps, and that the combined RBM will have a convergence rate bounded by the sum of the convergence rates of the individual RBMs.", "If we want to have further theoretical guarantees, we have the ability to exactly set model parameters, as mentioned in section 3.2 to get the exact distribution of interest, and to combine those RBMs with directly calculated parameters.", "As mentioned in that section, this is not a data efficient, or computationally efficient method which is why we chose to not pursue it.", "[1] Pierre. Bremaud. Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues, volume 1.Springer New York, 1999. ISBN 9781441931313."], "labels": ["nonarg", "nonarg", "nonarg", "dispute", "concur", "concur", "nonarg", "dispute", "dispute", "dispute", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 292, "sentences": ["Thank you for the review.", "First, we want to mention that DeepTwist is proposed not only for weight pruning, but also for other compression techniques, such as quantization and low-rank approximation, as we discussed in Section 4.2 and 4.3", "After weight pruning is performed and zero weights are removed, we usually obtain a sparse matrix to represent non-zero weights.", "There are lots of existing sparse matrix computation libraries to support SpMV (sparse matrix-vector multiplication) and so on.", "If a matrix is highly sparse, then we would reduce memory footprint and amount of computations (for example, we can skip zero weights during computation) significantly.", "There have been extensive studies of efficient hardware implementation after weight pruning, and we want you to refer to the paper \u201cEIE: efficient inference engine on compressed deep neural network\u201d or \u201cDeep compression: compressing deep neural networks with pruning, trained quatization and Huffman coding.\u201d", "In this paper, we have not discussed particular sparse matrix implementation methods which are not our focus in this paper.", "We would greatly appreciate if you can reconsider your decision based on our comments and other methods we also discussed (i.e., quantization and low-rank approximation)."], "labels": ["nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 293, "sentences": ["Dear AnonReviewer3,", "thank you for your positive review and constructive feedback!", "We agree that the structure of the paper was not optimal and reorganized it along the lines you suggested (thanks for the suggestion!).", "Below we address specific questions.", "\u201cI am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?\u201d", "-> The latter: we compared with the models with the closest match in # of parameters.", "\u201cWhy is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?\u201d", "-> We stated that defining a trade-off between objectives is not necessary (in case you are referring to this statement), which would, e.g., be necessary when one would scalarize objectives by using a weighted sum.", "Rescaling an objective, however, is different as it is independent from other objectives: it only depends on that specific objective and which scale is important to the user and the application.", "For the number of parameters, the log scale is natural to cover a large range of sizes: think of a plot of size vs. performance; in order to see anything for small sizes one would typically put the size on a log scale (and we indeed did, see, e.g., Figures 3 and 4).", "Therefore, it is most natural to also put the number of parameters on a log scale for LEMONADE.", "\u201cIt seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectives-1 dimensional surface with the population of parents. How could scaling be handled?\u201d", "-> We think having 4-6 objectives is a realistic dimensionality for NAS applications, and scaling to significantly more objectives (which would indeed be problematic for our method, but also for multi-objective optimization in general) is typically not necessary.", "In response to this question, to demonstrate this, wee conducted a new experiment with 5 objectives (performance on Cifar 10, performance on Cifar 100, number of parameters, number of multiply-add operations, inference time) to show that LEMONADE can handle these realistic scenarios natively.", "We refer to the updated version of our paper for the results (Appendix 3,\u201cLEMONADE with 5 objectives\u201d), but in a nutshell the results are very positive and qualitatively resemble those for two objectives.", "While we put this experiment into the appendix for now to not change the main paper too much compared to the submitted version, if the reviewers agree we would also be very happy to include this experiment in the main paper.", "We hope this clarifies your questions. Thanks again for the review!"], "labels": ["nonarg", "nonarg", "concur", "nonarg", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 294, "sentences": ["Thanks for your comments!", ">>> Only the 1/7 examples of MNIST dataset are provided in case studies.", "I am wondering for more complicated images, how is the performance?", "In responding to this comment and the first comment of Reviewer #2, we perform one more experiment on CelebA to demonstrate that DSGAN can work well even for complicated images.", "In this experiment, we generate the color images of size 64 $\\times$ 64.", "Similar to 1/7 experiments on the MNIST dataset, we let $p_{\\bar{d}}$ be the distribution of face images with glasses and without glasses, and let $p_{d}$ be images without glasses.", "We sample 10000 images with glasses and 10000 images without glasses from CelebA, and we set $\\alpha$ to 0.5.", "In order to verify the generated image quality of DSGAN, we also train a GAN for comparison.", "GAN is trained with the same amount of training images (but only using face images with glasses since GAN is to learn the distribution of training data).", "In other words, we assume GAN can use complement data as training data directly.", "On the contrary, DSGAN only uses complement data indirectly (the difference between $p_{\\bar{d}}$ and $p_d$).", "Figure 10 in Appendix G shows generated images and FID for both methods.", "We can see that our DSGAN can generate images with glasses from the given $p_d$ and $p_{\\bar{d}}$, and the FID of DSGAN are comparable to that of GAN.", "The experiment validates that DSGAN still works well to create complement data for complicate images."], "labels": ["nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 295, "sentences": ["Thank you for reviewing our paper.", "We would like to make a quick clarification right away, which we hope will change your assessment.", "All works you cite use non-linear BoF encodings on top of pretrained VGG (or AlexNet) features; the effective patch size of individual features is thus large and will generally encompass the whole object of interest.", "In contrast, our BagNets are constrained to very small image patches (much smaller than the typical object size in ImageNet), use no region proposals (all patches are treated equally) and employ a very simple and transparent average pooling of local features (no non-linear dependence between features and regions).", "That\u2019s why BagNets (1) substantially increase interpretability of the decision making process (see e.g. heatmaps), (2) highlight what features and length-scales are necessary for object recognition and (3) shed light on the classification strategy followed by modern high performance CNNs.", "None of the cited papers addresses any of these contributions.", "PS: We do cite similar approaches in our paper, see first paragraph of related literature. We will add your references there.", "Maybe the following perspective also helps: the works you cite use BoF over larger image regions, but the embeddings for each region are still based on conventional, non-interpretable DNNs (like VGG).", "Our work \"opens this blackbox\" (to use a very stressed term) and provides a way to compute similar region embeddings in a much more interpretable way as a linear BoF over small patches.", "In other words, if the works you cite would use BagNets instead of VGG, they would basically use a \"stacked BoF\" approach: first, small and local patches are combined to yield region embeddings (BagNet), and these region embeddings are used by a second BoF to infer image-level object labels and bounding boxes."], "labels": ["nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "concur", "dispute", "dispute", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 296, "sentences": ["> All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.", "Yes, we experiment only with myopic variants of exploration, but (A) our approach is not limited to this initial set of behaviour modulations, and could be extended to trade off between intrinsic and extrinsic motivation, or between model-free and model-based mechanisms; and (B) the variations we consider may not be ideal, but they are the ones most commonly used in domains like Atari.", "> The proposed proxy is simply the empirical episodic return.", "It is not well explained in the paper how this proxy correlates with the Learning progress criteria.", "The proxy seems to encourage selecting modulations that lead to generate most rewarding trajectories.", "How this proxy incentives the agent to explore poorly-understood regions?", "In other terms, how this proxy help to tradeoff between exploration and exploitation ?", "Thank you for this suggestion, we have now clarified this connection in Section 3.", "We acknowledge that f departs from LP in a number of ways.", "First, it does not contain learner-subjective information, but this is partly mitigated through the joint use of with prioritised replay that over-samples high error experience.", "Another potential mechanism by which the episodic return can be indicative of future learning is because an improved policy tends to be preceded by some higher-return episodes -- in general, there is a lag between best-seen performance and reliably reproducing it.", "Second, the fitness is based on absolute returns not differences in returns as suggested by Equation 1; this makes no difference to the relative orderings of z (and the resulting probabilities induced by the bandit), but it has the benefit that the non-stationarity takes a different form: a difference-based metric will appear stationary if the policy performance keeps increasing at a steady rate, but such a policy must be changing significantly to achieve that progress, and therefore the selection mechanism should keep revisiting other modulations.", "In contrast, our absolute fitness naturally has this effect when paired with a non-stationary bandit.", "We have also updated the paper to highlight that our proposed proxy is to be understood as an initial, simple, working instance, with a lot of remaining future work that could extend and refine it.", ">", "The modulation adaptation problem is framed into non-stationary multi-armed bandit problem but the authors present a heuristic to solve it instead of using provably efficient bandit algorithm such as [...]", "Thank you for the suggestion!", "We had experimented with a few of these variants before designing the proposed adaptation method.", "We have now included such a plot in the paper, comparing our method to UBC and Thompson sampling (Appendix E.3 and Figure 16).", "As you can see from this comparison, the performance of these well-known bandits depends on the game, and it is subject to tuning, which is what we wanted to avoid in the first place.", "In most games our bandit performs significantly better.", "> The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me", "[...]", "The distribution of f(z) does change as a function of the parameter change and thus as a function of time.", "This is precisely the kind of non-stationarity that our adaptive mechanism has to deal", "with", ".", "This is also the reason behind the adaptive window used in this work.", "In a sense, one can see the size of the window as a proxy for the effective time horizon at which things can be seen as stationary in the learning.", "The window over which we integrate evidence is chosen to make the best recommendation; thus every time we deviate too much from the sample distribution captured within it, we consider this as a sign of non-stationarity and shrink the window.", "This is by no means optimal, nor do we claim it is, but it seems to be a reliable enough proxy to outperform candidates that do assume stationarity (as portrayed by the comparison in Figure 16).", "> I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.", "Is it a baseline with the best hyperprameters in hindsight?", "The \u201cfixed reference\u201d is described in Appendix C, and corresponds to the most commonly used settings in the literature.", "We made this clear in the main body of the text.", ">", "From the plots of learning curves in appendix, the proposed methods doesn\u2019t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?", "Yes, we show this in aggregate in Figure 6 (old Figure 5-right): it shows how the bandit is roughly on par with uniform when the modulation set is curated, but the bandit significantly outperforms uniform in the untuned (\u201cextended\u201d) setting.", "We clarified the caption for this too."], "labels": ["nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 297, "sentences": ["We are grateful for your feedback.", "We hope that the above discussion assuaged the reviewer\u2019s concerns regarding novelty and some unclear details.", "We briefly address the two questions regarding the setup:", "During testing, in the setting with known GT boxes (Sec 4.2), we assume that the 2D instance boxes are given.", "In the detection setting, the 2D instance boxes are the result of the learned detector.", "Given the (detected or known) instance boxes, the union boxes and binary masks can be easily computed - the union box is just the larger box containing both instance boxes, and the mask highlights these instance boxes in the union box.", "Training and Testing  Inference Time on a single GPU (Maxwell Titan X)", "1. Train time: 65 hrs", "2. Test time: 0.55s per image"], "labels": ["nonarg", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 298, "sentences": ["-----------------------------------------------------------------------", "Q\uff1aA lot of clarity is required on the choice of evaluation metric; for example, choice of distance measure?", "What is the L1 norm applied on?", "A: Answer about Evaluation metrics:", "(1) We want to evaluate if the generated graphs are scale-free graphs in the direct evaluation for dataset scale-free graphs.", "If the degree distribution of generated graphs is the same to the degree distribution of real target graphs, the generated graphs are good.", "(2) There are many classical evaluation metrics focusing on measuring the similarities or distance of two distributions.", "The four metrics in this paper are among the most authoritative and commonly used ones in existing works, e.g., [2][3][4][5].", "Answer about L1 norm:", "(1) L1 norm is applied to the weight adjacent matrix of the graph.", "Our methodology is achieved by a trade-off between L1 loss and adversarial loss (GAN-D).", "Specifically, L1 makes generated graphs share the same rough outline of sparsity pattern like generated graphs, while under this outline, adversarial loss allows them to vary to some degree.", "(2) L1 norm is commonly used in GAN in relevant domains, e.g., in image-translation domain, for example, reference [1] (with 600+ citations) and reference [6] (with 1300+ citations).", "They have done extensive experiments to show the advantage of such a strategy.", "(3) The experiment demonstrates its effectiveness.", "Specifically, the proposed GT-GAN that uses L1 norm outperformed all the other comparison methods shown in Table 2,3 and 4.", "-------[2", "]", "Schieber, T. A., Carpi, L., D\u00edaz-Guilera, A., Pardalos, P. M., Masoller, C., & Ravetti, M. G. (2017).", "Quantification of network structural dissimilarities.", "Nature Communications, 8, 13928.", "-------[3] Bauckhage, C., Kersting, K., & Hadiji, F. (2015, July). Parameterizing the Distance Distribution of Undirected Networks. In UAI (pp. 121-130).", "-------[4] Chiang, S., Cassese, A., Guindani, M., Vannucci, M., Yeh, H. J., Haneef, Z., & Stern, J. M. (2016).", "Time-dependence of graph theory metrics in functional connectivity analysis. NeuroImage, 125, 601-615.", "-------[5] You, J., Ying, R., Ren, X., Hamilton, W. L., & Leskovec, J. (2018). GraphRNN: A Deep Generative Model for Graphs. arXiv preprint arXiv:1802.08773.", "-------[6", "] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.", "-----------------------------------------------------------------------", "Q\uff1aI did not completely follow the arguments towards directed graph deconvolution operators.", "There is lack of clarity and the explanation seems lacking in parts in this particular section; especially since this is the key contribution of this work.", "A: (1) Our decoder is symmetric to the encoder in their architectures.", "The encoder does n-hop edge information aggregation from the input graphs and learns the latent representation of nodes.", "Then, we first decode the node embedding to get the n-hop aggregated information on edges by node-to-edge layer and then we further decode the n-hop aggregated information layer by layer by n-layers back to get the output adjacency matrix.", "(2) Different from image deconvolution, for each hidden channel, we have two filters vertical to each other, i.e., one is a column vector while the other is a row vector.", "To get the nth hop information of edge <i,j>, row filter decodes all the (n+1)-th hop information of outgoing edges of node i and column filter decodes all the (n+1)-th hop information of incoming edges of node j.", "(3) To make our description clearer, we have updated our paper in Section 3.2.2, e.g, by adding \u201cTo get the nth hop information Aij, row filter decodes all the (n+1)-th hop information of outgoing edges of Vi and column filter decodes all the (n+1)-th hop information of incoming edges of Vj.\u201d", "-----------------------------------------------------------------------", "Q: Typo:. The \u201cInf\u201d in Tabel 1", "A: As explained in Section 4.2.4 \u201cResults on Scale-Free Graphs\u201d, the \u201cInf\u201d in Tabel 1 represents the distance more than 1000.", "We really hope that we have explained every confused point clearly and please let us know if there are any other points.", "Thank you once again for your reviews.", "Dear Reviewer:", "Thank you very much for your comments and suggestions.", "We would like to answer your questions in detail as follows:", "-----------------------------------------------------------------------", "Q: The authors claim that their method is applicable for large graphs.", "However, it seems the experiments do not seem to support this.", "A: (1) We did not mention that we handle \u201clarge graph\u201d, but instead we only mention that we handle \u201clarger\u201d graph.", "In the domain of graph generation, currently, the proposed graph generative models can typically only deal with graphs with dozens of nodes or less (except GraphRNN which can scale to 300).", "Compared to them, our model handles relatively \u201clarger graph\u201d (6-10 times larger than most existing methods).", "(2) Translation in graphs is a new topic and we have not found many datasets in very large scale, so we do not test on much larger nodes. But the scalability experiments can still show the superiority of our model compared to others.", "(3) We typically test small-size graphs because most of the comparison methods can only handle small-size graphs.", "-----------------------------------------------------------------------", "Q: It is not clear how the noise is introduced in the graphs. I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.", "A: Thanks for the review comment.", "(1) The noise is introduced by the dropout function in each convolution layer.", "Dropout functions by randomly ignore 50% of neuron\u2019s output of a network in our mode by a uniform distribution.", "(2) The way we add noise", "is well-recognized and commonly-used in generative deep learning models[1].", "The noises added in GANs aim to enable the diversities in the generated graphs to avoid the problem that GANs tend to favor producing same output rather than spreading it evenly over the domain.", "(3) We have shown the analysis of the translation quality against noise in Figures 4 and 5.", "In Figure 5 (see in the supplementary material), each logarithm plot in each column show the power-law trend of each randomly generated graph, which will look linear in such a logarithm plot.", "It can be seen that the generated graphs show the similar randomness pattern as the real graphs.", "Moreover, the larger the graph is (see the graph size of 150), the smaller the randomness is, and the clearer the power-law trend is, which verifies that the translation quality of our method.", "------[1]", "Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.", "-----------------------------------------------------------------------", "Q: It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.", "Do we know how does the connectedness of the input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?", "Towards this, how does the computational complexity scale wrt to the connectedness?", "A: (1) Similar to all the existing graph deep generative learning methods for generic graphs, we do not have additional assumptions on the graphs.", "The domain of graph deep generative learning methods typically do not require to distinguish or preprocess specific topological types of graphs before applying it, no matter it is strongly- or weakly- connected graph, complete graph, planar graph, scale-free graph, or graphs that have other specific patterns.", "This is actually one of the core advantages of deep learning based models where the graph patterns are not extracted or pre-identified manually by the human but automatically discovered by the end-to-end deep models.", "(2) This paper has given the time complexity in the worst case: O(n^2) as shown in 3.4.", "The worst case happens when the graph is a complete graph.", "The time complexity of a strongly-connected graph will not be worse than that.", "Dear Reviewer,", "Thank you very much for your new and previous comments.", "We have revised our paper again in order to address all of them in the paper.", "The modifications are listed as followings:", "1. For graph deconvolution, we have modified and reorganized the content.", "The Section 3.2.2 on \u201cGraph Deconvolution\u201d has been reorganized to two subsections \u201cnode-to-edge deconvolution\u201d and \u201cedge-to-edge deconvolution\u201d.", "We also extended them to make the description on deconvolution operations clearer and more comprehensive.", "2.", "For graph deconvolution, we have also added a new figure and refined the equations\u2019 descriptions.", "Figure 3 is added to describe the mechanism of our proposed deconvolution operators as well as their correlation to the convolution operations.", "Equation 6, Equation 7, and their descriptions have also been revised to make them clearer and concrete.", "Specifically, Figure 3 describes how the node representation and edge representation are respectively decoded by our deconvolution layers, while Equation 6 and Equation 7 describe how to aggregate the decoded information into the final weighted adjacent matrix.", "3. We have referred to all the figures in the body of text.", "4. We have added statements to describe how to introduce random noises in the whole architecture, see in the 2nd paragraph of Section 3.1 in Page 4.", "5. We have added statements of describing the reason to use L1 loss and how L1 loss is applied, please see in the paragraph before Equation 2 in Page 4.", "Additionally, we also added the statements of how L1 norm and GAN loss function jointly, see in the paragraph after Equation 2.", "6. We have added the statements why the metrics are chosen to evaluate the scale-free dataset, please see in the 2nd Paragraph of Section 4.2.2.", "Additionally, to improve the reproducibility of the proposed methodologies and experiments, we have already released our code in https://github.com/anonymous1025/Deep-Graph-Translation-.", "More architecture parameters are also provided in Appendix E.", "Thank you very much again for the comments and please let us know if there are any other issues."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 299, "sentences": ["We thank the reviewer for the useful comments, below our replies.", "1. \"The clarity is below average. In Section 2 the main method is introduced. However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.", "It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way.\"", "We fully agree with this criticism. Following your suggestion (and also that of reviewer # 2) we have moved some material from the appendix to the main text.", "2. \"I also struggled a little to understand what is the difference between forward interpolate and filtering\"", "In this work we refer by filtering to the process of inferring the optimal latent state z_t  at time t, using observations x_{1:t} from the trial up to time t, not including observations to the future of t. By forward interpolation we refer to the process of smoothing, (inferring optimal z_t from observations of the complete trial x_{1:T}, including points to the future of t), and then evolving the inferred z_t with the learned VIND dynamics.", "After evolving for k steps, the Generative Model is used to generate data which is subsequently compared with the observations at time t+k.", "We do not refer to this procedure as \u201cprediction\u201d since the initial state z_t for the forward interpolation was obtained by making use of the full data.", "We have added clarifying comments at the beginning of section 4.", "3. \"Given the existing body of literature, I found the technical novelty of this paper rather weak\"", "We would like to reiterate that the novelty of the paper is <i>twofold.</i>", "First and foremost, we propose the use of a novel variational approximate posterior that shares the nonlinear dynamics with the generative model.", "This feature is powerful because it uses known information about the true posterior in the design of the approximate one.", "Naively, the feature also seems to be a curse because the variational approximation is rendered intractable for the case of nonlinear dynamics.", "This is the reason why such approximate posteriors have not been proposed before.", "We have added a sentence in the introduction emphasizing this crucial point.", "The second novelty is a method to deal with this intractability, via the Laplace approximation and the fixed-point iteration method.", "We showed that the resulting algorithm, which intercalates a gradient step and a FPI step yields very good results in well-known, difficult tasks such as dimensionality expansion in the single cell data or the WFOM task.", "4.- \"abstract: uncover nonlinear observation? -> maybe change \"observation\" to \"latent dynamics\"?\"", "The term \u2018nonlinear observation\u2019 in the line \u201c\u2026Variational Inference for Nonlinear Dynamics (VIND), that is able to uncover nonlinear observation and transition functions from sequential data \u2026\u201c, found in the abstract refers to the observation map in the Generative Model.", "That is, VIND uncovers both a nonlinear \u201cobservation\u201d model, that maps nonlinearly a latent state to the data, and nonlinear latent dynamics mapping the latent state at time t to the state at time t+1, which we refer to as the \u201cnonlinear transition functions\u201d.", "On the other hand, we agree that \u201cnonlinear latent dynamics\u201d is a better fit than \u201ctransition functions\u201d for the abstract and we have performed this replacement."], "labels": ["nonarg", "nonarg", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 300, "sentences": ["Thank you for your comments.", "- LSTM", "LSTMs are indeed a strong model for tree prediction on previous tasks.", "To allow the model to access ancestry nodes during decoding, one way is to concatenate the parent node latent representation with the input of each step for decoding children, and then feed the concatenated vector to LSTM (e.g., Dong & Lapata ACL 2016).", "However, since the ancestry has a variable-number of nodes (as decoding proceeds)", ", to directly access these nodes during decoding, attentional mechanisms would be an efficient way, which is one of our motivations to use Transformer models that are attention-based.", "Of course, LSTM equipped with Attention would achieve the same benefit.", "In addition, positional encoding in Transformer also allows us to easily model spatial locations of UI elements.", "Our early experiments with LSTM did not yield good results on this spatial layout problem.", "That said, we agree it is worth investigating the performance of LSTM on this problem further.", "Since this is the first paper on this topic, we chose to focus on introducing the problem and providing Transformer-based approaches as a baseline for future work.", "- Eval metrics", "We agree the IR-based metrics have limitations.", "This is why we provided multiple eval metrics including edit distances and next-N accuracy.", "The Edit Distance metric was designed by taking into account human factors in interaction tasks based on the key-stroke level GOMS models.", "We can clarify this further in the revision."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 301, "sentences": ["Thank you for the review.", "While the weight formats after model compression follow well known ones, our model compression method is significantly different from the existing ones.", "Let us discuss some parts of reasons.", "- Training models after compression in order to recover accuracy is as important (if not more) as compressing weights.", "We have found that occasional distortions (not compressing weights for every mini-batch like previous techniques), relatively large learning rate, and training batches in full-precision (unlike previous ones which store compressed weights during entire training) would be the key to recovering or even increasing the accuracy.", "- Exploring large search space in much wider area is suggested in this paper through large distortion step and large learning rate (note that many compression-aware techniques perform compression at every batch has distortion step of \u201c1\u201d while much smaller learning rate for retraining that normal training is chosen).", "As we discussed in the paper, investigating various local minima is crucial for good model compression.", "- Our pruning method is fundamentally different from the previous ones because we do not incorporate a masking layer.", "While previous pruning ideas keep zero weights during training, we do not have any zero weights at any moment except at the weight distortion step.", "- Our low-rank approximation is also unique one since 1) we do not alter the structure for training even after performing SVD, 2) very high learning rate associated with transient accuracy loss is allowed for DeepTwist, and 3) we change SV spectrum continuously while the previous ones perform SVD only once (in practice, retraining low-rank approximated model has been considered to be very difficult, if not impossible).", "- Even though our pruning method is even simpler compared to the previous ones, compression rate is significantly better or very close to the one based on sophisticated Bayesian inference model.", "- Low-rank approximation results on PTB (Figure 2) shows even higher compression rate compared with weight pruning (Table 3), which is surprising to us because pruning has been known to show much higher compression ratio compared with SVD (fine-grain vs. coarse-grain or structured).", "- Quantization is performed also in a very different way.", "Unlike previous ones, we do not consider quatization during", "training. \u201cDo not perform quantization at every batch, but instead recover accuracy through full-precision training, high learning rate, and occasional quantization\u201d is the key message.", "- Overall, our occasional compression is a significant one since we can greatly reduce amount of computation overhead from compression.", "If our technique is a simple extension from the previous ones, we could not obtain such impressive results with high compression rate and improved accuracy.", "We believe that our paper suggests a wide view on how model compression should be performed."], "labels": ["nonarg", "dispute", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 302, "sentences": ["Thank you for your comments. We would like to address your concerns as follow.", "1.", "We do not use the gradient penalty in WGAN-GP (1-GP) to improve the original GAN.", "Our 0-GP, although has a similar form as 1-GP,  is motivated from a very different perspective and produces very different effects.", "We assume that you find our 0-GP similar to 1-GP because of the use of the straight line from a fake to a real sample.", "In the response to reviewer 1, we propose a more sophisticated way to find a path from a fake to a real datapoint.", "The new method highlights the difference between our method and 1-GP.", "2. The 0-GP is not the only contribution of our paper.", "We start by analyzing the generalization of GANs, showing the problem of the original GAN loss.", "Although generalizability is one of the most desirable properties of generative models, it has not been studied carefully in GAN literature.", "Based on our analysis, we propose 0-GP to improve the generalization of GANs.", "On the 8 Gaussian dataset, GAN-0-GP can generate plausible unseen datapoints on the circle, implying better generalization.", "We show that the original GAN loss makes GAN focuses on generating datapoints in the training dataset.", "0-GP-sample proposed in [4] encourages the generator to remember the training samples.", "That result in the mode jumping behavior: when we perform interpolation between $z_1$ and $z_2$, the output does not smoothly transform from $x_1 = G(z_1)$ to $x_2 = G(z_2)$ but suddenly jump from $x_1$ to $x_2$. The behavior can be seen in figure 8 of BigGAN paper (https://arxiv.org/abs/1809.11096).", "3. We will include WGAN-GP to the baselines for the sake of completeness.", "However, as discussed in the previous paragraphs and in our paper, WGAN-GP and their 1-GP does not address the same problem as our 0-GP.", "As discussed in our paper, 1-GP does not help improving generalization in GANs.", "[4] even showed that 1-GP does not help WGAN (and the original GAN as well) to converge to an equilibrium.", "The phenomenon can be seen in our MNIST experiment where GAN-1-GP fails to produce any realistic samples after 10,000 iterations.", "It has been observed that WGAN-1-GP does not converge to an equilibrium, the generator continues to map the same noise to different modes as the training continues.", "In our synthetic experiment, WGAN-1-GP is less robust to change in hyper-parameters than GAN-0-GP.", "Detailed results will be included in our revision.", "Please refer to [4] for more in-depth discussion about the non-convergence of WGAN-GP.", "When $p_g$ is the same as $p_r$, the gradient of the optimal discriminator in GAN and the optimal critic in WGAN must be 0.", "Any non-zero centered GP will not help GANs to converge to the optimal equilibrium.", "Our 0-GP helps to improve both generalization and convergence of GANs.", "Our 0-GP can be applied to WGAN as well.", "Similar to the original GAN, WGAN and WGAN-GP can overfit to the dataset: the distance output by the critic can be larger than the Wasserstein distance between the two distributions.", "However, overfitting in WGAN and WGAN-GP is not as severe as in GAN.", "This is partly because the gradient in WGAN and WGAN-GP does not explode so mode collapse is much harder to observe.", "4. We will include more related works to our paper.", "The vast body of work on GANs makes it difficult to find all related works.", "We only focus on some key papers on the topic.", "Discussion about VEEGAN and Lucas et al. will be added to our next revision.", "However, we want to emphasize that our work is about improving the generalization of GANs.", "Reducing mode collapse is related to but is not exactly the same as generalization.", "As in the 8 Gaussian dataset, a GAN without mode collapse is the one that can generate all 8 modes.", "A GAN with good generalization should be able to generate unseen datapoints on the circle and to perform smooth interpolation between modes.", "5. We will add more details about the experiments to the appendix.", "The code for all experiments will be released after the review process.", "For the imagenet experiment, we used the code from [4] which is available on github.", "We note that [4] is a state-of-the-art method which is able to help GAN to scale to massive datasets and it is used in BigGAN paper.", "6.Thank you for your suggestion about the paper layout.", "Adding a table that summarizes referred gradient penalties is a good idea.", "Thank you again for your suggestions.", "We have revised our paper to address your concerns as follows:", "1. A background section is added with basic information about GANs and a definition of generalization.", "A table summarizing the referred gradient penalties is also added.", "2. We extended the Related works section to include papers which address the mode collapse problem.", "The writing of this part and the whole paper was revised.", "3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting.", "Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update.", "4. WGAN-GP is included to our ImageNet experiment.", "Our GAN-0-GP outperforms WGAN-GP by a large margin.", "5. Implementation details are added to the appendix.", "The code for all experiments will be released after the review process.", "6. We added the analysis for the 'mode jumping' problem to Section 6.2.", "We showed that GAN-0-GP-sample suffers from the problem.", "On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.", "7. A new algorithm for finding a better path between a pair of samples is added to our paper."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "dispute", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "dispute", "dispute", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 303, "sentences": ["Thank you for your comments and questions; we have incorporated these in the revision and respond to your questions below.", "Main Argument:", "Q1: It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.", "A1: We performed some additional experiments using the progressive gan generator [1] on CelebA-HQ dataset.", "One interesting property of the progressive gan interpolations is that they take much longer to train to have a visual effect -- for example for color, we could obtain drastic color changes in Stylegan W latent space using as few as 2k samples, but with progressive gan, we used 60k samples and still did not obtain as strong of an effect.", "This points to the Stylegan w latent space being more \u201cflexible\u201d and generalizable for transformation, compared to the latent space of progressive GAN.", "Moreover, we qualitatively observe some entanglement in the progressive gan transformations -- for example, changing the level of zoom also changes the lighting.", "We did not observe large effects for the shift transformations, although perhaps more hyperparameter tuning may improve these results.", "We have added a section B.6 in the appendix and figures illustrating these results.", "Q2: Does training the generator and interpolation jointly improve the quality of the generator in general?", "A2: This is an interesting question.", "We are in the process of investigating this hypothesis to see if sampling from both the latent space and transformation alpha can help improve sample diversity and potentially FID.", "We did not yet observe an improvement in preliminary experiments on Cifar10, but the experiments are ongoing and we will add complete results on this question to the final version of the paper.", "Minor Comments:", "Q1: In appendix A.2 the authors explain how the range of is set for the different experiments. However it's not clear how is this range used in practice ? Do you sample uniformly in this range to train the linear interpolation ?", "Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?", "A1: We pick the ranges using two criteria: qualitatively acceptable and quantitatively under a fixed threshold for FID score.", "We pick alpha steps uniformly within the ranges (shifts and rotations are integer steps).", "For training, we have between 20k and 40k samples for all models, and beyond these numbers we don\u2019t see much improvement.", "Q2: There is a typo in equation 6", "A2: Thank you for your careful review in catching these mistakes. We have updated the typo in the revision.", "Q3: In figure 6: What does the right figure represent ? especially what are the different colours ?", "A3: The right side of the figure has three rows: the top row shows the plot of per-class zoom variability; there are two black datapoints we chose as examples to show instances of low and high variability classes.", "The middle row shows the distribution of the low variability datapoint (\u201crobin\u201d class), and the bottom row shows the distribution of the high variability datapoint (\u201claptop\u201d class).", "On the left of these plots we show qualitative results.", "In each of these two plots, we show dataset, -\\alpha^*, and +\\alpha^* distributions with black, green, red, respectively.", "We have updated the revision to clarify these in the figure caption.", "[1] Karras, Tero, et al. \"Progressive growing of gans for improved quality, stability, and variation.\" ICLR (2018)."], "labels": ["nonarg", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 304, "sentences": ["We thank the reviewer for the comments and appreciation.", "We have revised the paper according to the suggestions and would like to clarify as follows:", "Q1: In Sec. 3 the Authors write \"We then sample the solution space for depth and pose respectively around their initialization\".", "However in Sec 3.2 they write \"we uniformly sample a set of L virtual planes {dl} Ll=1 in the inverse-depth space\".", "In what way are the planes \"around their initialization\"? If the initial depth map spans over multiple orders of magnitude, will the planes be uniformly sampled between the minimum and maximum disparity of the initial map?", "If yes, it seems that the initial depth map is not really needed, just its minimum and maximum value is needed, but then how come the method can be applied iteratively with respect to depth?", "A1. Thank you for pointing this out. \"We then sample the solution space for depth and pose respectively around their initialization\" is a writing mistake and we have corrected it in our new version.", "Only the solution space for pose is sampled around initialization.", "We uniformly sample planes in the inverse-depth(disparity) space between a fixed minimum and maximum range.", "The initial depth is used for maintaining geometric consistency.", "The depth, under such a situation, could still be improved through iterations.", "Since the pose is improved over the iteration, the depth cost-volume would be updated accordingly, and better depth can be inferred from the more accurate cost-volume.", "Q2: The Authors mention that depth maps are warped onto the virtual planes using differentiable bilinear interpolation.", "Is there a mechanism to protect from interpolating across discontinuities? If no, were bleeding edge artifacts observed?", "A2.", "We thank the reviewer for pointing out the potential problem of our warping method on the depth maps.", "Since depth maps often have discontinuities, we agree with Review #3 that differentiable bilinear interpolation may do damage to the geometry consistency and smooth the edges.", "We also updated our experiment results with nearest neighbor instead of bilinear interpolation for depth warping, and revised the corresponding results (Tab. 1-3) and figures in the paper.", "Notably, our results can get slightly improved by the updated nearest neighbour method inspired by the question asked by Reviewer#3.", "To verify this, we added an experiment in Appendix C, which runs nearest neighbor sampling instead of bilinear interpolation.", "With nearest neighbor warping method, the performance of our model on DeMoN MVS dataset gains a slight boost with retraining.", "Here are the comparisons:", "MVS dataset", "L1-inv     sc-inv     L1-rel     Rot     Trans", "Ours (bilinear)", "0.023", "0.134", "0.079", "2.867    9.910", "Nearest neighbor(retained)", "0.021", "0.129", "0.076", "2.824    9.881", "This shows that nearest neighbor sampling is indeed more geometrically meaningful for depth.", "We updated the method to use nearest sampling and update the result accordingly.", "We also discussed the strengths and weaknesses briefly of each interpolation method in Appendix C.", "Q3: In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.", "A3.", "Empirically, learning based method may outperforms traditional feature matching methods on these situations since it relies on image priors.", "In addition, our method has geometry consistency between multiview depth maps as the input, which encourages local smoothness and consistency to some extent.", "In some textureless, reflective or transparent cases that feature matching methods does not work, our method gains extra information from the initial depth maps of other views by the depth consistency part of the cost volume.", "In Appendix D, Figure 8, some qualitative comparisons with COLMAP[1] are provided as an argument.", "We have updated our paper and show more visual examples in Appendix D, Figure 9.", "Q4: the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.", "A4. Thanks for your suggestions, we will release code upon the acceptance.", "Furthermore, we have put more details about model architecture as in Appendix A Figure 4 and Figure 6.", "[1] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4104\u20134113, 2016."], "labels": ["nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 305, "sentences": ["We thank the reviewer for having taken the time to judge our paper and to have detailed his judgement on their two points.", "We would like to point out that AnonReviewer4's final quantitative score as well as the confidence given will be crucial for the fact that this paper will or will not be presented at ICLR.", "We would like to respectfully detail how we completely disagree with the comments given in the two points, but acknowledge that this was mainly due to the way we presented the motivation for the paper.", "We hope the revised version of the paper now meets the standards for ICLR and justifies to update the \"red flag\" (clear rejection) to a green light.", "First, the goal is not faster computation on a CPU.", "Our (github-shared) code runs in a few dozens of seconds per learning on a standard laptop - but the goal is mainly to be able to test all parameters.", "We have not used SPAMS in this work as we could use the similar methods which are used in the sklearn library.", "However, SPAMS is a great inspiration for our framework.", "(For information, the complete simulations for this paper take approximately 12 hours --which are easily distributed on a cluster as we multiplied the number of independent learning runs using different classes of parameters, cross-validations and types of sparse coding algorithms - in total approx 500 experiments.", "It takes a dozens of minutes on a 100 nodes cluster.", ").", "Our motivation is mainly to understand biological vision and hope this would percolate to ML.", "Yes, we obtain faster convergence, but as an epiphenomenon of the better efficiency of our adaptive homeostatic algorithm.", "However, we agree that this was not clear in this first revision: atoms which were displayed looked qualitatively similar.", "We have solved this issue thanks to the comments of the anonymous reviewers by now displaying the most and least active atoms.", "This shows a clear distinction between different methods and an important result: when $\\ell_2$ normalizing atoms, dictionary learning may converge to a result for which the ratio of activity between the most activated and the least activated is of the order 2.", "This result is often overlooked in dictionary learning and is a first novel result of the paper.", "This being said, Figures 1 and 3 now show the clear qualitative advantage of using homeostasis in unsupervised learning.", "This now certainly allow to understand *why* convergence speed is a good indicator ---not for an advantage on the running speed on a classical CPU--- but rather in showing that this allows a more efficient dictionary learning overall.", "Concerning the point \" It is not even clear that the final compression of the baselines would not be better.", "Even if they did show these convincingly, it is not obvious to me that it is valuable.\", we have performed the same experiments on more iterations such that we clearly see that baseline stay separate.", "Finally on the same point, we have not used at this point any application, such as supervised learning,  as it is out of the scope of this paper. But we thank the reviewer for suggesting it.", "Second, we had already done the comparison \"against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties\" but we had initially omitted to include this supplementary data (that takes the form of a single jupyter notebook which allows to reproduce all results).", "We have now included it in an anonymized format.", "This supplementary material contains code to replicate all figures but also additional experiments to test the effect of the different parameters.", "In short, we verified that the results we present are valid over a various number of parameters of the network, like the learning rates (figure 2) but also sparsity and the size of the dictionary (see Response To AnonReviewer3 @ https://openreview.net/forum?id=SyMras0cFQ&noteId=BylQtQPHRX ).", "As in Sandin, 2017 paper we have shown similar results in OMP.", "We are in the process of extending this framework to other sparse coding algorithms (LARS and lasso_lars) as plugged in from sklearn without any modification (in theory) to these algorithms.", "Indeed, we should remind that our adaptive homeostasis allows to be implemented by modifying the norm of each atom of the dictionary (as was done in the original work by Olshausen).", "We also show in the paper the application to a one-layer convolution network and our preliminary results show that we can extend this to a hierarchical network.", "I hope that with these clarifications on the form we gave to the paper (without changing the theory behind it), the statement that \" I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.\"  could be re-assessed to allow us to share this work inspired by biology to the ICLR community."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "concur", "nonarg", "dispute", "dispute", "dispute", "dispute", "concur", "concur", "concur", "dispute", "dispute", "dispute", "nonarg", "concur", "dispute", "dispute", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 306, "sentences": ["Thank you for your thoughtful review.", "We have updated Equation 1 and the paragraph above so that I(...) is consistently a function of two variables."], "labels": ["nonarg", "concur"], "confs": [1.0, 1.0]}
{"abstract_id": 307, "sentences": ["We thank you for interest in our work and your thorough review.", "We found your review particularly helpful in our efforts to create a more structured and formally sound version of the paper.", "------------", "- On Notation:", "+ We now have an introductory sentence to our notation section to improve the flow of reading and in order to not open the section with bullet points right away.", "+ As you suggested we changed our inequality notation to curly brackets to make it visually clearer that we are dealing with vectors.", "+ In fact we did introduce our subscript notation of the kind \"b |_{y>0}\".", "It is defined in our section on notation.", "Nevertheless your troubles compelled us to restate the meaning of this notation at the time of its first usage.", "------------", "- On Omnidirectionality:", "We are glad that you seem to find the concept of omnidirectionality intriguing. In the new version of our paper we therefore tried hard to make the definition as intelligible and intuitive as possible.", "We now use the following, equivalent, formulation as our definition: The matrix A of the form m x n is omnidirectional if for every given x in R^n \\ {0} there exists a row A_i of A such that <A_i, x> > 0.", "Or in less formal terms: There is no open linear half-space in R^n that does not contain an A_i.", "This geometric formulation of the definition is not only the origin of the naming, but it is also a mathematically sound formulation similar to the one you suggested \u201cA is full rank and there does not exist any X such that Ax < 0\u201d.", "The problem with your formulation (from the point of view of our notation) lies in the usage of the inequality sign since we defined it in the notation section to be element-wise.", "Your formulation would therefore require every entry of Ax to be negative, while for omnidirectionality one entry would be sufficient as long as the others are non-positive.", "This miscommunication also encouraged us to change our signs to the curly version as suggested by you.", "The original reason we used the previous definition was that we thought it would show more clearly what the core property is, namely that omnidirectionality can be used to nail down one precise solution. But we are now convinced that the best introductory formulation is the geometric one, as it offers an intuition of omnidirectionality.", "------------", "- On Orderliness in General:", "+ As you suggested we carefully restructured our paragraphs and removed the appearances of \u201c\\\\\u201d.", "+ We corrected as many typos as we could find, we would be very thankful for pointing out any further typos!", "+ We tried to improve the readability by increasing structure of longer segments of text e.g. by introducing informal titles.", "+ We added a clear and formal definition of the \u201cbinary\u201d diagonal matrices representing the application of ReLU.", "(Section 3.1)", "+ We rewrote potentially ambiguous statements in order to remove any inaccuracies.", "We hope we addressed your main concerns and our changes based on your review led to a paper that conforms with your standards of exposition.", "We want to thank you again for your thoughtful review and would welcome further advice."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "dispute", "dispute", "concur", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 308, "sentences": ["-- We will add further clarification regarding what C, Z represent.", "-- As rightly mentioned by the reviewer, our method can handle very high dimensional control variates.", "-- Lemma 1: Yes, your assumption is correct in general for variational posterior.", "-- Improving disentangled representation learning over beta-VAE: Beta-VAE obtains disentangled representations by explicitly posing a trade-off between the \u2018quality of disentanglement\u2019 (factorisation of the posterior) vs. the image reconstruction quality.", "Our method removes this trade-off\u2014-we decouple \u2018disentanglement of the latents\u2019 from \u2018generation quality\u2019, specifically by having a two-stage training process.", "This allows us to potentially have much higher disentanglement, while still maintaining image quality, unlike beta-VAE where the quality of generation would necessarily be compromised.", "We would like to emphasize that this is possible only because of the two-stage training process (please see comments to Reviewer 2 regarding d-separation)."], "labels": ["concur", "concur", "concur", "dispute", "dispute", "dispute", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 309, "sentences": ["Thanks a lot for appreciating our contribution!", "> Comparison with attention models is necessary to compare the important patches obtained from conventional networks.", "In the paper  (section 4.3) we quantitatively show that the patches important to BagNets are also important for standard CNNs.", "Is that the direction you were thinking about? If you have a different experiment in mind we would like to kindly ask you for more details."], "labels": ["nonarg", "nonarg", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 310, "sentences": ["1. Thank you for your comments and thank you in particular for pointing us to a reference we missed, which we have added to the manuscript.", "Ullrich et al. introduce some of the same foundational building blocks for applying differentiable models to the cryoEM reconstruction task.", "In particular, they propose a differentiable voxel-based representation for the volume and introduce a variational inference algorithm for learning the volume through gradient-based optimization.", "Due to their voxel-based representation, they introduce a method to differentiate through the 2D projection operator.", "In contrast, we parametrically learn a continuous function for volume via a coordinate-based MLP, which seamlessly allows differentiation through the slicing and rotation operators without having to deal with discretization.", "Their method is able to learn a homogeneous volume with given poses, whereas we perform fully unsupervised reconstruction of heterogeneous volumes.", "They show empirical experiments that highlight many of the challenges for variational inference of these models.", "In particular, inference of the unknown pose is challenging with gradient-based optimization and contains many local minima (their Fig 6), which we address with a branch and bound algorithm.", "We report a Fourier Shell Correlation (FSC) metric, which is a commonly used resolution metric in the cryoEM field.", "Voxel-wise MSE is not typically used in the cryoEM literature as it is sensitive to background subtraction and data normalization.", "We have added training times for these methods to the SI.", "2. The normalization constant in Eq. 3 is the partition function over all possible values of the latent pose and volume.", "Instead of computing this (intractable) constant, coordinate ascent on the dataset log likelihood is used to refine estimates of pose and volume in traditional algorithms.", "3.", "The extent of the 3D space is determined by the dataset\u2019s image size and resolution.", "We define a lengthscale such that image coordinates are modeled on a fixed lattice spanning [-0.5, 0.5]^2 with grid resolution determined by the image size.", "The absolute spatial extent is thus determined by the Angstrom/pixel ratio for each dataset.", "Similarly, final volumes for a given value of the latent are generated by evaluating a 3D lattice with extent [-0.5,0.5]^3 with grid resolution determined by the dataset image size.", "We have added the absolute spatial extent to the description of each dataset in the revised manuscript.", "4. We have included additional architectural details in the revised manuscript, and we will be releasing the source code which will hopefully further clarify the architecture."], "labels": ["concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 311, "sentences": ["4.", "Our approach has 2 important hyperparameters: scaling parameter s used for calculating binary mask from the embedding matrix as well as  \u03bb_RU, that controls the size accuracy trade-off (see Sec. 4.1 \u201cjoint training\u201d).", "We add a table analyzing the sensitivity of the parameter \u03bb_RU observing the expected behavior: higher values of \u03bb_RU lead to a smaller model size, however, reduced G size is positively correlated with the final classification performance of D (smaller G -> lower accuracy of D).", "+---------+---------+-------+", "| \u03bb_RU  | Acc.5 | Size |", "+---------+---------+-------+", "| 2E-06 | 98.16 | 660", "|", "+---------+--------+--------+", "| 0.002 | 98.22 | 638", "|", "+---------+--------+--------+", "| 0.2     | 98.02 | 598", "|", "+---------+--------+--------+", "| 0.75   | 97.36 | 577", "|", "+---------+--------+--------+", "| 2        | 86.82 | 522", "|", "+---------+--------+--------+", "5. We use the baseline presented by [1], that tackles identical scenario.", "To our knowledge [1] provides the state of the art performance in \"strict\" class incremental setup without using real samples.", "We consider a joint training (JT, classical training) of the discriminator as the upper performance bound.", "Joint training features a setup in which the discriminator is trained on ALL real samples of the previous tasks.", "The reviewer proposes to simulate information loss and use a random subset of real samples to train the upper bound model.", "However, this would certainly give a worse performance than when using all real samples.", "We, therefore, think that used JT upper bound is appropriate.", "Furthermore, using generated samples accommodates for better performance than simply storing instances only in case of tasks of relatively low complexity such as MNIST.", "Indeed, such a result has been shown in other works, e.g. [1].", "As explained in Sec. 5.2, this can be attributed to a potentially higher diversity with steady quality of the generated samples.", "Clearly, the performance of the classifier trained on the generated samples highly depends on the complexity of the task and quality of the generated samples.", "Thus, this effect can be observed neither in the SVHN nor the CIFAR10 benchmarks.", "6. The CIFAR results will be provided in the Tab. 1 alongside with other datasets in the next version.", "To ensure a fair comparison with the benchmark methods that do not use any network expansion strategy for the generator (e.g. [1,6]), we initialize our G to be approximately 50% of the size of the G used in the benchmarks.", "Also a study on network growth dynamics is provided in Fig. 5 (Sec. 5.3), showcasing a lower network capacity than the worst case scenario.", "Growing the generator is an essential part of our method that addresses the scalability problem in continual learning, e.g. with always growing amount of data model\u2019s capacity will be exhausted at a certain point.", "Noteworthy, the discriminator is not affected by the proposed dynamic network expansion mechanism and features the same architecture as in the benchmark methods.", "We believe the comparison to the joint training is fair because DGM only grows the capacity of the generator.", "In the discriminator, only the last classification layer is expanded with the growing model\u2019s output space as new classes are added.", "Thus, for k-th task we compare the accuracy of a discriminator with identical architecture trained on real samples of all k tasks (JT) with one trained on DGM-synthesized samples of k-1 tasks+reals of k-th tasks.", "Thus DGM\u2019s discriminator has no advantages over the joint training generator.", "8. Finally, we will address typos, writing and presentation issues in the updated version of the paper.", "[1] C. Wu, L. Herranz, X. Liu, Y. Wang, J. van de Weijer, and B. Raducanu. Memory Replay GANs: learning to generate images from new categories without forgetting. In Advances In Neural Information Processing Systems, 2018.", "[2] J. Serr\u00e0, D. Sur\u00eds, M. Miron, and A. Karatzoglou. Overcoming catastrophic forgetting with hard attention to the task. CoRR, abs/1801.01423, 2018.", "URL http://arxiv.org/abs/1801.01423.", "[3] H. Shin, J. K. Lee, J. Kim, and J. Kim.", "Continual learning with deep generative replay.", "In", "Advances in Neural Information Processing Systems, pages 2990\u20132999, 2017.", "[4] S. Rebuffi, A. Kolesnikov, and C. H. Lampert. icarl: Incremental classifier and representation", "learning.CoRR, abs/1611.07725, 2016. URL http://arxiv.org/abs/1611.07725.", "[5] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. S. Torr. Riemannian walk for incremental learning: Understanding forgetting and intransigence. CoRR, abs/1801.10112, 2018. URL http://arxiv.org/abs/1801.10112.", "[6] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, pp. 2990\u20132999, 2017.", "We thank the reviewer for their constructive comments.", "We address them as follows.", "1. We first would like to point out the contributions of our work.", "First, we address the catastrophic forgetting problem in continual learning.", "Thereby we introduce Dynamic Generative Memory (DGM) - an adversarially trainable generative network endowed with neuronal plasticity through efficient learning of sparse attention mask for layer activations.", "Hereby we extend the idea of HAT[2] to generative networks.", "Secondly, we address the scalability problem in continual learning.", "To ensure sufficient model capacity to accommodate for new tasks, we propose an adaptive network expansion mechanism in which newly added capacity is derived from the learnable neuron masks.", "2. We further we would like to clarify a possible confusion of the proposed method to be a combination of Deep Generative Replay (DGR)[6] and HAT[2].", "As pointed out in the Sec. 2 of our work, Deep Generative Replay (DGR) tries to prevent forgetting in the generator by retraining it from scratch every time a new data chunk becomes available.", "Thus, in DGR the generator would lose information at each replay step since the quality of generated samples highly depends on the quality of samples generated by the prior generator causing \"semantic drift\".", "This contrasts our method, which effectively retains the knowledge in the generator using HAT like neuron masking and only loses information through \u201cnatural\u201d forgetting.", "This allows us to use \u201ccomplete\u201d learned representation during learning and inference of the subsequent tasks as well as speed up the training (no replay of G is involved).", "3. We are not simply shifting the forgetting problem into G.", "Our work tackles the problem of class incremental learning.", "As opposed to task-incremental setup and shown in previous work, e.g. [3,4,5], models in class incremental setup (with single-head architecture) require a replay of previously seen categories when learning new ones.", "The reason for using G", "is not having access to samples of previous classes in the \u201cstrict\u201d incremental setup and using generated samples instead.", "As pointed out in our work, restricting storage of real samples represents a more realistic setup, since in real-world applications such an \u201cepisodic memory\u201d with real samples is often impossible due to memory and privacy restrictions."], "labels": ["concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "concur", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 312, "sentences": ["Thank you for your review.", "We will revise our paper according to your suggestion.", "We would like to quickly address your question about the experiment here.", "For MNIST and ImageNet experiment, the whole dataset was used.", "For the ImageNet experiment, we used the code from [4].", "Details about all experiments will be added to the appendix.", "We thank you for pointing the typo in Figure 3.", "We will also add an in-depth discussion about our method and other related works to our next revision as suggested by other reviewers.", "Thank you for your constructive review.", "We have updated our paper to address your concerns.", "The changes are summarized as follow:", "1. A background section is added with basic information about GANs and a definition of generalization.", "A table summarizing the referred gradient penalties is also added.", "2. We extended the Related works section to include papers which address the mode collapse problem.", "The writing of this part and the whole paper was revised.", "3. Another MNIST experiment is added to Section 6.1 to further demonstrate the effectiveness of our method in preventing overfitting.", "Specifically, our GAN-0-GP is the only GAN that could learn to generate realistic samples when the discriminator is updated 5 times per generator update.", "4. WGAN-GP is included to our ImageNet experiment.", "Our GAN-0-GP outperforms WGAN-GP by a large margin.", "5. Implementation details are added to the appendix.", "The code for all experiments will be released after the review process.", "6. We added the analysis for the 'mode jumping' problem to Section 6.2.", "We showed that GAN-0-GP-sample suffers from the problem.", "On the other hand, our GAN-0-GP is robust to the problem and is able to produce better interpolation between modes.", "7. A new algorithm for finding a better path between a pair of samples is added to our paper."], "labels": ["nonarg", "concur", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 313, "sentences": ["Thank you for your comments and questions.", "Classical cryo-EM reconstruction algorithms (e.g. cryoSPARC) are described in Section 2.2 at a high level and we refer the reader to its reference (Punjani et al. 2017) for more details on their implementation.", "To clarify the relationship between the cryoSPARC and cryoDRGN heterogeneous reconstruction in Figure 4, CryoSPARC imposes a discrete model for heterogeneity, specifically a mixture model of K volumes.", "The cryoSPARC results in Figure 4 are the volumes and the distribution of images over the 3 clusters from their unsupervised reconstruction.", "In contrast, the continuous latent variable from cryoDRGN unsupervised reconstruction is able to reconstruct the continuous motion of the ground truth volume.", "We have clarified the text to reduce any confusion and added training times for these methods to the appendix.", "Thank you for the recommendations!"], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 314, "sentences": ["Thank you for the valuable feedback!", "We uploaded a revised version of the paper based on the comments.", "- The reason behind using V-GMM is that V-GMM is much faster than KDE in inference and has a better generalization ability compared to GMM.", "We use V-GMM as a proof of concept for the idea \u201cCuriosity-Driven Experience Prioritization via Density Estimation\u201d.", "Other density estimation methods can also be applied.", "We now clarify these reasons in Section \u201c2.3 Density Estimation Methods\u201d of the revised paper.", "- We concatenate the goals and estimate the trajectory density instead of state density because HER needs to sample a future state in the trajectory as a virtual goal for training.", "- For episodes of different length, we can pad or truncate the trajectories into same lengths and apply V-GMM.", "Another method is to use PCA or auto-encoder to reduce the dimension into a fixed size and then apply CDP.", "- Similarly, to handle scaling issues, for very high dimension vectors, we can first apply dimension reduction methods, such as PCA and auto-encoder, and then use CDP.", "- The reference for \"It is known that PER can become very expensive in computational time\u201d is actually the \u201cPrioritized Experience Replay\u201d paper itself.", "On page three of the PER paper, it writes \u201cImplementation: To scale to large memory sizes N , we use a binary heap data structure for the priority queue, for which", "finding the maximum priority transition when sampling is O(1) and updating priorities (with the new TD-error after a learning step) is O(log N). See Appendix B.2.1 for more details. \u201c", "In their Atari case, the memory size N is of 1e4 transitions.", "In our hand manipulation environment cases, the memory size N is of 1e6 trajectories, and each trajectory has 100 transitions.", "Thus, the memory size is 1e4 (theirs) vs 1e8 (ours).", "The complexity of updating priorities is O(log N).", "Therefore, PER is very expensive in computational time, at least in our case.", "The memory buffer size N can be found in OpenAI Baselines link: https://github.com/openai/baselines"], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 315, "sentences": ["Comment: The proposed method seems to be specifically designed for the", "generation of contrastive explanations, i.e.  why the model predicted class", "A and not class B. While the generation of this type of explanations", "is", "somewhat novel, from the text it seems that the proposed method may not", "be able to indicate what part of the image content drove the model to", "predict class A. Is this indeed the case?", "Response: The goal of this paper is not to answer \"why A?\" but rather", "\"why A and not B?\"  The visual answer to the two questions may be similar,", "but it may not.", "We seek to highlight what in the image would need to", "change to make it a B and not an A, as a way of explaining this contrast.", "There are other papers that seek to answer the question of \"why A?\" but", "that is not our focus.", "Comment: Although the idea of generating contrastive explanations is", "quite interesting, it is not that novel. See Kim et al., NIPS\u201916,", "Dhurandhar et al., arXiv:1802.07623.", "Response: Dhurandhar et al. does use the term constrastive explanation.", "However, they look at the question of \"why A?\"  Contrastive in their case", "refers to whether something is or is not present that drives the", "classification of \"A.\"  This is a different constrast than ours", "that", "contrasts \"A\" to \"B\" (rather than \"present for A\" to \"absent for A\").", "We think this is also an interesting form of explanation, but a different", "one.", "Kim et al. also has a different form of model criticism; they look at the", "dataset as a whole for examples that help explain.", "We look at a different", "problem: for a given example (perhaps not even from the training set),", "why is it not class B?", "Comment: The work from Samek et al., TNNLS\u201917 and Oramas et al.,", "arXiv:1712.06302 seem to display similar properties in their explanations", "without the need of explicit constractive pair-wise training/testing.", "The", "manuscript would benefit from positioning the proposed method w.r.t. these", "works.", "Response: The work from Samek et al. is similar to PDA in its essence. We", "will add the comparison with this method to our work for sake of", "completeness.", "In the experiment section of Oramas et al., they proposed", "a synthetic flowers dataset that can be used for our purpose.", "Since it", "is synthetic and fine-grained, we can compare the method qualitatively", "and quantitatively.  We sent a request to the authors for accessing the", "dataset.", "If we granted this access we will add quantitative comparisons", "to our paper.", "Comment: In the evaluation section (Sec.4.1) the proposed method is", "compared against other methods in the literature.", "Three of these methods,", "i.e. Lime, GradCam, PDA, are not designed for producing contrastive", "explanations", ", so I am not sure to what extend this comparison is", "appropriate.", "Response: The only method we found before submitting the paper which was", "able to answer the contrastive explanation was xGems.", "However, other", "methods could be shoe-horned into trying to answer the question of \"why A", "and not B?\" and so we figured we should demonstrate that they were not", "sufficient and that a new method (like ours) was necessary.", "Comment: The reported results are mostly qualitative. I find the set of", "provided qualitative examples quite reduced.", "In this regard, I encourage", "the authors to update the supplementary material in order to show extended", "qualitative results of the explanations produced by their method.", "Response: We have added a supplementary section, adding more qualitative", "results. Thank you for your suggestion."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 316, "sentences": ["We thank the reviewer for their feedback."], "labels": ["nonarg"], "confs": [1.0]}
{"abstract_id": 317, "sentences": ["Response to AnonReviewer2", "We would like to thank you for your positive review and comments.", "We would be happy if you have any other questions."], "labels": ["nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 318, "sentences": ["We would like to start by clarifying the difference between the final implementation (what the reviewer referred to as engineering contribution) of our method with its scientific contribution.", "1. Scientific Contribution: Most recent work on disentangling generative modelling tries to obtain an independent/factorised posterior over the latent generative factors without directly addressing the problem of d-separation, which theoretically prohibits factorisation of the posterior in models such as beta-VAE, conditional GAN or stack GAN.", "To further elaborate, due to d-separation, models from prior work that have the same underlying plate notation either fail to disentangle the representations (since $p(c,z|x) \\neq p(c|x)p(z|x)$ ) ) or do so at the cost of lower generative quality\u2014-because their training relies on having an additive information-theoretic penalty term.", "Our method, on the other hand, decouples the problem of learning disentangled latent representations and high fidelity generation into two separate problems by introducing a hierarchical structure (sub-graph c-y) that is trained separate from the rest of the model.", "This allows obtaining a posterior $p(c|y)p(z|x,y)p(y|x)$, which in fact guarantees the disentanglement of the factors c from z while preserving the generative strength of the model.", "2. Supervised Setting: We would argue that the setting where labelled data (C) is available is more natural than the unsupervised setting as we aim to learn physical simulators (such as graphics engines) that have a well-defined control variate structure.", "This setup appears in many previous works, e.g. conditional GAN and its derivatives.", "Testing such models on synthetic datasets (i.e. outputs of graphics engines) where one can control the generative variables is a standard practice in the field and allows for better testing.", "3. Unsupervised results: For the unsupervised setting, in addition to our face dataset and CelebA, we also present the results on the chairs and cars in the Appendix (See Figure 5, Figure 7, Figure 12, Figure 13).", "4. Experiments: We would argue that our qualitative plots and quantitative metrics are in line with the evaluation used in current SOTA work.", "In fact, we provide a very thorough mix of quantitative and qualitative experiments for both supervised and unsupervised settings.", "We would like to point out that there are no accepted measures in the field for the quality of learned disentangled representation (see Locatello et. al. [https://arxiv.org/pdf/1811.12359.pdf](https://arxiv.org/pdf/1811.12359.pdf)) and most previous papers in the field include a similar mix of quantitative and qualitative results in their experiments section.", "Also, we provide all the code so that it can be verified that the reported results are not cherry-picked."], "labels": ["nonarg", "concur", "concur", "dispute", "dispute", "dispute", "dispute", "dispute", "concur", "dispute", "dispute", "dispute", "dispute"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 319, "sentences": ["We would like to thank the reviewer for constructive feedback.", "Results: We would like to clarify that all the results reported in the paper are on test sets (this includes Figures 1, 2, 3, and 5 as well as those in the supplementary).", "We decided to report the test set performance for all epochs instead of just the last epoch to show that: 1- in many cases, LSTM+Time2Vec consistently outperforms LSTM+T, 2- replacing the notion of time with Time2Vec does not deteriorate the performance, and 3- adding Time2Vec makes the model reach its best performance faster.", "Sorry about the confusion, we will clarify this in the paper.", "\u201cIf adding your representation to the SOTA model improved test set performance (or at least sped up training without hurting test set performance), then that would be better evidence.\u201d ->  This is indeed what we did.", "We showed that adding Time2Vec to LSTM+T (the model used in several recent works - see the last paragraph of related works section) and to two variants of TimeLSTM (a recent architecture with remarkable results on asynchronous sequential datasets) improves test set performance.", "\u201ctest accuracy/recall@K with/without your representation for more than one other state of the art algorithm for these datasets\u201d: Upon the reviewer\u2019s request, we are looking to extend one more architecture with Time2Vec. If we managed to obtain results until the end of the rebuttal period, we will post them here.", "Dataset that exhibits seasonality: The hand-crafted dataset has been created to serve that purpose (we could change the frequency from weekly to monthly or quarterly).", "The reason for using a hand-crafted dataset was because we could control the underlying dynamics and verify if the model can learn the correct dynamics.", "Optimization of sine functions: The results we have reported in the paper demonstrate mean and standard deviation across multiple runs.", "In each run, we initialize the parameters randomly.", "The standard deviations provide evidence that the performance of LSTM+Time2Vec doesn't depend on the initialization values more than a standard LSTM+T model.", "Moreover, from Fig 1(b) and 1(c), it can be observed that the standard deviation of LSTM+Time2Vec is even smaller than that of LSTM+T.", "Theory: According to Fourier sine-cosine series, any real-valued function f(t) that is integrable on an interval of length P can be approximated as f(t) = a_0 + sum_{n=1}^{N/2} (a_n cos(2nt\\pi/P) + b_n sin(2nt\\pi/P)) by choosing appropriate weights a_n and b_n.", "Since cos(x)=sin(x+\\pi/2), the cos functions can be replaced with sine functions so f(t) can be approximated with N sine functions.", "By concatenating Time2Vec to the input, as explained in the second paragraph of Section 4, we allow the sequence model to learn a function (or multiple functions) of time based on the data by taking a weighted sum (the weights correspond to a_n and b_n in the formula above) of the sinusoids.", "Learning a function of time from data rather than fixing it to a hand-crafted function can potentially lead to better generalization.", "We will state the theory behind Time2Vec more explicitly."], "labels": ["nonarg", "dispute", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 320, "sentences": ["Thank you for your helpful suggestions and we would like to address your concerns as follows:", "1. Better explanatory texts for natural statistics comparison.", "We have modified the caption for Fig. 2 and text in Sec 2.4 to be more clear about the natural statistics analysis.", "This analysis is intended to contrast the natural statistical differences among the representations, to indicate that different modeling approaches are needed for each of them.", "Models that capture image priors well might not transfer to spectrograms or raw waveforms.", "2. Equation 1 typo fixed.", "3. Complex Coefficient vs Spectrograms.", "Thanks for the suggestion.", "We intentionally use the spectrogram notation as we do not use complex-valued kernels with complex-valued convolution.", "Yet in order to generate the audio signal, we simply generate the real and imaginary parts of the STFT coefficients such that we can convert them to waveform using inverse STFT.", "We have modified the text in the implementation details in Sec. 3 and the setup paragraphs in Sec. 4.2, 4.3, and 4.4 to make this point.", "4. Details in the experiments to clear up the settings.", "We have modified the text in Sec. 4.2, 4.3, and 4.4 to make the details more clear.", "For experiments in Sec. 4.2 and 4.3, the network\u2019s output is the complex STFT coefficient, the raw waveform is then recovered by inverse STFT using the overlap-and-add method.", "For experiments in Sec 4.4, the output of the network is the ratio mask, and the separated audio is generated by an Inverse STFT operated on the input STFT coefficients multiplied by the predicted ratio mask.", "The L1 loss is calculated between the predicted ratio mask and the ground truth ratio mask.", "Please let us know for any questions.", "Thanks again for your suggestions, which have made this submission stronger.", "Thanks,", "Authors"], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 321, "sentences": ["Thank you for your review.", "Your summary correctly and comprehensively reflects the gist of our paper.", "One minor correction we would like to make is that our experiments are not only conducted on the Cifar-10 dataset.", "On ImageNet dataset, we were able to compress ResNet models with no or little accuracy loss, but reduce the model size by up to 21.1x and computational cost by up to 103.5x, better than previous baselines.", "Please let us know if you have further questions or concerns that we can help clarify."], "labels": ["nonarg", "nonarg", "dispute", "dispute", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 322, "sentences": ["Thank you for your comments.", "We have revised the paper to address the issues you brought up.", "- Contribution", "We developed our approach based on Transformer models.", "We agree with the reviewer that the model novelty is relatively incremental.", "However, the focus of the paper is to contribute a new prediction problem and adapts and applies the Transformer model for this problem to establish a benchmark for future exploration, which we believe has values.", "- Benchmark & Reproducible", "The data that our experiments used is an open dataset:", "https://storage.cloud.google.com/crowdstf-rico-uiuc-4540/rico_dataset_v0.1/semantic_annotations.zip", "We will release our data preprocessing, and model code, including all the eval metrics to ensure the work is reproducible.", "- Technical details", "Thanks for pointing out the issues with our presentations.", "We agree much detail on embeddings can be condensed or moved to Appendix.", "We included embedding details in the paper because a reviewer from the venue we previously submitted to requested these details to be in the paper.", "We revised the notations in the paper to make formulation clearer.", "In addition, we added more details about the data as you suggested.", "Given a partial tree, there can be more than one way to complete the layout.", "Given a 10%, 50% and 80% BFS partial layout, the mean number of completions of the layout is 2.97, 1.23 and 1.17 respectively.", "Given a 10%, 50% and 80% DFS partial layout, the mean number of completions is 3.63, 1.24, and 1.17 respectively."], "labels": ["nonarg", "concur", "nonarg", "concur", "concur", "dispute", "nonarg", "nonarg", "nonarg", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 323, "sentences": ["Thank you for the valuable feedback!", "We uploaded a revised version of the paper based on the comments.", "- To improve the clarity, we clarify why we chose to use V-GMM, among the three basic density estimation methods, including KDE, GMM, and V-GMM.", "(in the revised version of the paper Section \u201c2.3 Density Estimation Methods\u201d)", "The reasons are the following:", "1. GMM can be trained reasonably fast for RL agents.", "GMM is also much faster in inference compared to Kernel Density Estimate (KDE) (Rosenblatt, 1956).", "2. Compared to GMM,  V-GMM has a natural tendency to set some mixing coefficients close to zero and generalizes better.", "3. We only use a basic density estimation method, such as V-GMM, in our framework as a proof of concept for the idea \u201cCuriosity-Driven Experience Prioritization via Density Estimation\u201d.", "Other destiny estimation methods can also be applied in this framework.", "- We move the exact setup (Section \u201c2.1 Environments\u201d in the new version) in early sections to improve the clarity of the paper.", "- We are glad that you like the idea of the paper. Yes, indeed the curiosity mechanism in our context is related to surprise.", "The idea of our method is also related to neuroscience (Gruber et al., 2014).", "- Yes, the entire trajectories need to stored in the replay buffer and the memory size increases as the trajectory length increases.", "However, this is a general issue with off-policy RL methods which uses experience replay, such as DQN and DDPG.", "Our method CDP only uses the trajectories that are already in the memory, so CDP does not introduce additional memory usage."], "labels": ["nonarg", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 324, "sentences": ["Comment: One of the problems highlighted in the paper regarding existing", "explanation modalities is the use of another black-box to explain the", "decisions of an existing deep network (also somewhat of a black-box) which", "the authors claim their model does not suffer from.", "Response: While the GAN is certainly another black box, it is a function", "just of the data (or the data domain), and not a function of the", "discriminator from which we want to extract explanations.", "Therefore,", "training different models, switching prediction tasks on the same", "domain, or any other similar changes would not require changing the GAN.", "Comment: Learning such a model of the input space is an overhead in itself.", "Response: Overhead calculations of some form are almost impossible to", "avoid.", "Whether this is an overly large computational burdon will depend", "on the problem, although we believe the GAN or VAE need only be trained once", "per domain.", "Comment: The paper does not provide any quantitatively convincing results", "to suggest the generator in use is a good one.", "Response: Measuring the reconstruction accuracy (of goodness of the", "generator)", "is difficult, as each measure has its own flaws.", "For instance,", "Norm measures are sensitive to translations in the image.", "Comment: Experiments demonstrating comparisons between GANs and VAEs as the", "reference generative model for explanations would have made the paper", "stronger (as the proposed approach relies explicitly on how good the", "generative model is)", "Response: This is a good suggestion. We have added experiments using", "with variational autoencoders (VAEs) instead of GANs", ".", "We believe these", "address this concern and some of the comments above.", "Thank you.", "Comment: The paper proposes an interesting experiment to show that the", "proposed approach is somewhat capable of capturing slightly adversarial", "biases in the input domain (adding square to the top-left of images of", "class 8). While I like this experiment, I feel this has not been explored", "to completion in the sense of experimenting with robustness with respect", "to structured as well as unstructured perturbations.", "Response: While we could certainly perform more experiments in this vein,", "we are uncertain what type of unstructured perturbations would be useful", "(and how then to measure whether the technique captured the correct", "explanation).", "Comment: typographical errors...", "Response:  Thank you.", "We have fixed the typos.", "Comment: Section 7 in Gradcam (https://arxiv.org/pdf/1610.02391.pdf)", "provides a procedure to generate counter-factual explanations using", "Gradcam. Is there a particular reason the authors did not choose to adopt", "the above technique as a baseline?", "Response: The the proposed counter-factual experiment for GCAM produces", "*any* counter-factual explanation, not a targeted explanation.", "It answers", "\"why A?\" and not \"why A and not B?\" as we do in this paper."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "other", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 325, "sentences": ["1. vs StackGAN: Our method introduces a learning method that allows for training generative models with disentangled latents without compromising on the generative quality (unlike all SOTA mutual information (MI) based disentangled representation learning methods such as beta-VAE or info-GAN).", "More fundamentally, our method provably avoids issues posed by d-separation that theoretically prohibit disentanglement in the current SOTA methods.", "This is completely different from the motivation of StackGAN which aims to use iterative refinement (like several other generative models) to learn a generative map from image captions and does not care about disentanglement.", "2. Unsupervised control variable discovery: Beta VAE (or other MI-based methods) disentangle the latents by compromising the generative quality.", "The more the model forces disentanglement, by giving more weight to a certain information-theoretic regularizer, the worse the generated images become.", "By decoupling the training into two steps, our method allows for far better disentanglement than beta-VAE like methods without compromising the generative quality.", "Novelty: Our method aims to solve the fundamental issue of d-separation in disentangled representation learning.", "It allows for a theoretically consistent way of obtaining factorisation in the posterior without any information-theoretic penalties.", "It is true, that one can describe the method as a (non-trivial) combination of beta-vae + GAN.", "But this description mischaracterizes the fundamental problem that we have identified and proposed a solution for.", "(Please refer to comments for Reviewer 2 under \u2018Scientific Contribution\u2019)"], "labels": ["concur", "dispute", "dispute", "nonarg", "concur", "dispute", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 326, "sentences": ["Dear Reviewer:", "Thanks very much for your comments and questions. We would like to explain them in detail and modify our paper accordingly.", "----------------------------------------------------------------------------", "Q: First, the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 \"graph translator\".", "A: General architecture: The whole framework includes a translator and a discriminator.", "(1) Translator.", "Translator consists of an encoder, a decoder, and a skip network, which first learn the representation of the graph and then decode it back to the target graph.", "See details in the third part of the answer.", "(2) Discriminator.", "Our discriminator aims to classify the generated graphs and the real target graphs given the input graph.", "(3) The translator and discriminator are trained together, and the final goal is that the discriminator cannot distinguish the generated graphs and real target graphs.", "After training such a model, the translator will be used in the test phase.", "The logic behind edge-to-edge convolution:", "(1) Generally speaking, the purpose of edge-to-edge convolution layers is to aggregate the neighborhood information of nodes.", "Specifically, the n-th edge-to-edge convolution layer aggregates the n-th hop connection information of nodes related to each edge.", "(2) Different from image convolution, for each hidden channel, we have two filters, one is a column vector while the other is a row vector.", "To learn the nth hop information of edge <i,j>, row filter aggregates all the (n-1)-th hop information of outgoing edges of node i and column filter aggregates all the (n-1)-th hop information of incoming edges of node j.", "(3)  Edge-to-edge layers are important to extract some higher-level graph features, e.g., the n-hop reachability from a node to another; n-hop in-degree and out-degree, and many other higher-order patterns.", "Different blocks in the graph translator:", "Translator consists of an encoder, a decoder, and a skip network.", "(1) Encoder.", "The encoder does n-hop edge information aggregation from the input graphs using edge-to-edge layers and then uses the edge-to-node layer to learn the latent representation of nodes.", "(2) Decoder.", "Reversely, the graph decoder first uses node-to-edge layers to decode the node representations to aggregated edge information and then further decode that into adjacency matrix, which is the final generated graphs.", "(3) Skip-network.", "Over the encoder-decoder framework, we also added skip-network (the black line of Fig.1) which can directly map the edge aggregation information in every hop from the input graph to the output graph so that can preserve the local information in every resolution (i.e., every hop).", "----------------------------------------------------------------------------", "Q: how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies?", "Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.", "A: (1) L1 norm is applied to the weight matrix.", "Our methodology is still general enough which is achieved by a trade-off between L1 loss and adversarial loss (GAN-D), which jointly enforces Gy and T(Gx) to follow a similar topological pattern but may not necessarily the same.", "Specifically, L1 makes T(Gx) share the same rough outline of sparsity pattern like Gy, while under this outline, adversarial loss allows the T(Gx) to vary to some degree.", "(2) Combining L1 loss and adversarial loss is well-recognized and validated.", "Works on image-translation have proposed and utilized L1 loss and adversarial loss jointly in GAN, for example, reference [1] (with 600+ citations) and reference [2] (with 1300+ citations).", "They have done extensive experiments to show the advantage of such a strategy.", "Furthermore, in our experiments, we found the performance when using L1 loss and adversarial loss jointly is better than using either of them.", "------[1] Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., & Efros, A. A. (2016). Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2536-2544).", "------[2]", "Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. arXiv preprint.", "-----------------------------------------------------------------", "Q: Third, and slightly related to the previous point, why do you need a conditional GAN discriminator, if you already model similarity by L1?", "Typically one would use a GAN-D() to model \"proximity\" to the source-distribution, and then a similarity loss (L1 in your case) to model \"proximity\" to the actual input sample, in the case of traditional domains.", "Instead, here you seem to suggest using L1 and GAN to do basically the same thing, or with significant overlap anyways.", "This is confusing to me.", "Please explain the logic for this architectural choice.", "A:(1) The logic of using both of them has been explained in the answer to the last question.", "(2) The logic has been well-utilized and verified in the image-translation domain. Again please see the details in the answer to the last question.", "(3)", "Our ablation experiment also demonstrates the similar advantage of using both losses for graph translation than only using L1 loss.", "Specifically, the proposed GT-GAN that uses both loses outperformed the S-Generator that only uses L1 loss on all three datasets by 10% in accuracy on average as shown in Table 2,3 and 4.", "-----------------------------------------------------------------", "Q: Four, could you please explain the setting for the \u201cgold standard\u201d experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behavior, and label accordingly? That said I am not 100% sure of this problem setting.", "A: Yes, \u201cgold standard\u201d method is directly trained based on real target graphs instead of generated ones.", "Specifically, as you know, all the comparison methods in our paper are generative models which generate graphs, and our experiment is to evaluate how real the generated graphs are.", "One way to evaluate this is by \u201cindirect evaluation\u201d, where we use the graphs generated by different comparison methods as training data to train a classifier based on KCNN (see reference (Nikolentzos, et al.,2017) in the paper), and then compare which model generates \u201cmore-real graphs\u201d by testing their corresponding trained classifier on test set which consists of real graphs.", "In \u201cgold standard\u201d method, it directly uses the real graphs to train the classifier (still based on KCNN), so it is expected to get the best performance.", "Therefore, \u201cgold standard\u201d method acts as the \u201cbest-possible-performer\u201d, and is used as a benchmark to evaluate all the different generative models on how \u201creal\u201d the graphs they can generate: the closer (and better) their performance is to the \u201cgold standard\u201d one, the \u201cmore real\u201d their generated graphs are.", "We hope we were able to answer everything to your satisfaction, please let us know if there are any more open points.", "Thank you once again!"], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 327, "sentences": ["We thank the reviewer for insight into our paper.", "The reviewer found some points, where we were not clear enough. It is now the time to respond to them.", "1. The reviewer noticed,", "that  \u201cin the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed\u201d.", "According to the best knowledge of the authors, the Cramer-Wold kernel (which defines the Cramer-Wold metric), except for the classical RBF kernel, is the only known characteristic kernel which has closed form for radial gaussians, and we believe the respective computations in other cases (like the inverse quadratic kernel used in WAE-MMD), would be highly nontrivial.", "2.", "The reviewer also points out, that the evidence lower bound ELBO, when used with a notiGaussian prior results in case of VAE in a generally analytic formula.", "It was never the intention of the authors to sneak in that VAE cannot do it.", "Our primary goal was to define a method for training the Gaussian prior generative model using a different closed form formula for the distribution distance.", "At the same time VAE requires encoder to be Gaussian non-deterministic, and random decoder, which is not the case in CWAE (as well as in a WAE model, see Tolstikhin https://arxiv.org/pdf/1711.01558.pdf).", "The kernel used in the derivation is not a Gaussian kernel but has a closed form formula for a product of two Gaussians (see last equation in the current paper), itself not being Gaussian.", "The Gaussian kernel itself is not well suited,", "because it has an exponential rate of decay, and loses much information on the outliers (see also Bi\u0144kowski et al.,  https://arxiv.org/pdf/1801.01401.pdf, section 2.1).", "Our objective was to add a method alternative to the WAE method, but simpler in use (e.g. less parameters to be found).", "We have extended the contribution part (in the introduction) and added Sections A and B to the Appendix, to make things clearer.", "Thank you again for your comments and suggestions. Have our responses and the changes we made to the manuscript addressed all of your concerns?"], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "dispute", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 328, "sentences": ["POINTS 1 AND 2 OF THE REVIEW", "The reviewer has noticed that the cw-distance resembles that of a U-statistic MMD estimate, and thus the proposed model very much resembles MMD itself.", "We fully agree with the reviewer, that CWAE is a model based on the kernel as the divergence measure for distributions, and consequently can be seen as a modified variant of WAE-MMD (we have added the respective comments in the paper, see the extended introduction, and added a section B in the appendix, which discusses the comparison in more details).", "However, there are some important, in our opinion, differences between those two models, which also result in an improved training speed and stability of CWAE compared to WAE-MMD (see refined experiments in section 5, as well as figures in the appendix showing comparisons between proposed CWAE and WAE and SWAE models in the Appendix).", "The differences are:", "Due to the properties of the constructed Cramer-Wold kernel, we are able to substitute in the distance the sample estimation d(X,Y) of d(X,N(0,I)) given by its exact formula.", "Consequently, the CWAE has, while being trained, potentially less stochastic perturbation then WAE-MMD.", "CWAE, as compared to WAE-MMD, has no parameters (while WAE-MMD has two).", "We observed that in many cases (like log-likelihood), the logarithm of the probability function works better, since it increases the role of examples with low-probability.", "Thus, instead of using an additional weighting parameter lambda (as in WAE-MMD) whose aim is to balance the MSE and divergence terms, we decided to automatically (independently of dimension) balance the two terms of the loss function, by taking the logarithm of the divergence.", "Moreover, since our kernel is naturally introduced with the sliced approach and kernel smoothing, the choice of regularization parameter is given by the Silverman's rule of thumb, and depends on the sample size", "(contrary to WAE-MMD, where the parameters are chosen by hand, and in general do not depend on the sample size)", ".", "The appropriate clarifications are given in the appendix B.", "Summarizing, in the proposed CWAE model, contrary to WAE-MMD, we do not have to choose parameters.", "Additionally, since we do not have the noise in the learning process given by the random choice of the sample from normal density", ",  CWAE in generally learns faster than WAE-MMD, and has smaller dispersion of the cost-function during the learning process (see Figures 7 and 8, Appendix F).", "POINT 3", "The reviewer notices that the WAE-MMD does not need to sample when used with Gaussian prior and a Gaussian RBF kernel.", "We fully agree that the gaussian kernel has the close formula for the product of two gaussians.", "However, the problem (see Tolstikhin et al\u2019s paper Wasserstein auto-encoders, https://arxiv.org/pdf/1711.01558.pdf, Section 4, also Bi\u0144kowski et al,  https://arxiv.org/pdf/1801.01401.pdf) that Gaussian kernel does not work well with the model, as its derivatives decrease too fast, and the model with Gaussian kernel is unable to learn to modify points which lie far from the center.", "We have added a respective comment in Appendix A. As to the best knowledge of the authors, the introduced Cramer-Wold kernel is the unique characteristic kernel which has the closed form for spherical gaussians, and does not have exponential decrease of derivative (as the case of RBF kernel).", "POINTS 4 AND 5", "As the reviewer accurately and carefully noticed, we have not formally proved that cw-distance is a true distance, and that the definition is introduced partially: first for two clouds of points, then a distribution and a cloud.", "This is true.", "We have added a respective proof in Appendix, Section A, where also the precise mathematical construction of the general form of Cramer-Wold metric is presented.", "We have also added the comment at the beginning of Section 3 of the paper.", "We hope clarifies our unintentionally imprecise original approach.", "POINT", "6", "The reviewer asked \u201cwhat is image(X) in Remark 4.1?\u201d", "By image(X) we understand the set of all possible values the random vector X can attain (we have included the footnote in Remark 4.1 explaining the notation)."], "labels": ["nonarg", "nonarg", "concur", "nonarg", "nonarg", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "dispute", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "dispute", "dispute", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 329, "sentences": ["Thank you for your comments!", "We agree with R3 that it would be ideal to have a one-size-fits-all metric.", "Unfortunately, the complex landscape of the problem doesn\u2019t permit a single recommendation.", "We did our best to conduct a detailed and honest study.", "We believe our experiments to be some of the most extensive in this area, and we hope they will contribute to researchers\u2019 understanding of the problem.", "It\u2019s important to note, though, that BERTScore is an improvement over the commonly used Bleu across the board.", "Our recommendation to use F1, while potentially not optimal in specific cases, generally performs very well and much better than Bleu.", "There are largely two sets of options, (1) Among P, R, F; and  (2) What model to use.", "For (1), as we specify, F-BERT is a reliable metric for MT.", "For (2), Roberta-Large performs consistently well for to-English language pairs.", "The results are less conclusive for from-English language pairs.", "BERTScore computed with Multilingual-BERT is better than most existing metrics except on few low-resource languages.", "We have updated the paper with these recommendations in Section 7.", "We are using word pieces in all experiments, and we compute IDF using word pieces.", "We updated the paper to make this clear in Section 3, under Importance Weighting.", "Regarding unknown words handling, we computed the IDF on the reference sentences in the test set.", "This ensures that the IDF is the same for all MT systems that are tested.", "The candidate sentences generated by MT systems may contain words that never appear in the test set.", "We apply plus-one smoothing to handle such words.", "Following your suggestion, we further studied idf scoring.", "We computed idf scores on the monolingual English corpus released by WMT18 and experimented with BERTScore computed with the Roberta-large model.", "We have found that this leads to worse performance, likely because of the domain shift."], "labels": ["nonarg", "concur", "concur", "concur", "concur", "dispute", "dispute", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 330, "sentences": ["Thank you for the valuable feedback!", "We uploaded a revised version of the paper based on the comments.", "- We added a mathematical justification paragraph in Section 3.3 \u201cAn Importance Sampling Perspective\u201d.", "We argue that to estimate the integral of the loss function L(\u03c4) of the RL agent efficiently, we need to draw samples \u03c4 from the buffer in regions which have a high probability, p(\u03c4), but also where L|(\u03c4)| is large.", "Since, p(\u03c4) is a uniform distribution, i.e., the agent replays trajectories at random, we only need to draw samples which has large errors L|(\u03c4)|.", "The result can be highly efficient, meaning the agent needs less samples than sampling from the uniform distribution p(\u03c4).", "The CDP framework finds the samples that have large errors based on the \u2018surprise\u2019 of the trajectory.", "Any density estimation method that can approximate the trajectory density can provide a more efficient proposal distribution q(\u03c4) than the uniform distribution p(\u03c4).", "The sampling mechanism should have a property of oversampling trajectories with larger errors/\u2018surprise\u2019.", "- To mitigate the influence of very unusual stochastic transitions, we use the ranking instead of the density directly.", "The reason is that the rank-based variant is more robust because it is not affected by outliers nor by density magnitudes.", "Furthermore, its heavy-tail property also guarantees that samples will be diverse", "(Schaul et al., 2015b).", "- Yes, the experiments are mostly in deterministic domains.", "- In the FetchSlide environment, the best-learned policy of CDP outperforms the baselines and PER, as shown in Table 1.", "Yes, we did not use the last set of parameters but used the best one encountered during training, as described in Section 4 \u201cExperiments\u201d: \u201cAfter training, we use the best-learned policy as the final policy and test it in the environment.", "The testing results are the final mean success rates.\u201c", "- Our implementation is based on \u201cOpenAI Baselines\u201d, which provides HER. We combined HER with PER in \u201cOpenAI Baselines\u201d.", "OpenAI Baselines link: https://github.com/openai/baselines", "- To improve the clarity of the paper, we move the exact set-up into the earlier section, Section 2.1 \u201cEnvironments\u201d.", "In this section, we also redefine the \u201cstate\u201d based on your suggestions.", "We delete the \u201ctroublesome\u201d sentence and also clarify what the goal actually is in Section 2.1.", "For more detail, please read the revised paper, Section 2.1."], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 331, "sentences": ["Thank you very much for your thoughtful review.", "We would like to point out that our experiments include multiple architectures (WRN and ResNet for image classification, LSTM and transformers for language modeling) and optimizers (SGD for image classification, SGD and Adam for language modeling).", "These were chosen according to standard implementations in the literature.", "However, we agree that it is important to demonstrate the results on a greater variety of architectures and optimizers and in particular in a manner that allows to assess the stability with respect to changing them for a specified task.", "Following your suggestion, we have therefore added experiments with both VGG and DenseNet, each trained with both SGD and Adam, on CIFAR100.", "The results conform with good agreement to the functional form defined in Eq. 5, with fit quality quantitatively very similar across all the architectures/optimizers settings in these experiments, and in particular reaching small divergences.", "We added a new section (6.2) and figure (Fig. 5) for these experiments.", "We do believe that the variety of architectures/optimizers examined over a variety of tasks (extending to large datasets over both vision and language processing) in this study, augmented with the explicit additions following your valuable feedback, experimentally cover a meaningful chunk of settings, which supports our conclusions.", "We hope you will reevaluate the paper in light of these additions, and welcome any additional feedback."], "labels": ["nonarg", "nonarg", "nonarg", "concur", "concur", "nonarg", "concur", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 332, "sentences": ["We would like to thank you for the review and comments.", "We revised the manuscript to address your concerns.", "Below we summarized your concerns/questions with our answers.", "Q1: It is not justified to go over the 8 page soft limit.", "A1: We removed some redundant information in the paper, moved a few paragraphs and figures to Appendix, and added discussions according to the review comments in the revised manuscript.", "Now, the paper has full 8 pages that is a soft limit.", "Q2: There likely no computational savings when compared to lookup tables.", "A2: As we added in Section 1, if weights are quantized in binary codes, then the number of multiplications is significantly reduced (even though scaling factors have full precision) or most computations can be replaced with bit-wise operations, which have been introduced and discussed as unique advantages of using binary codes in previous works.", "Since we do not suggest new computation methods using binary codes, computational savings using quantized weights become the same as those of previous binary-codes-based quantization techniques.", "FleXOR, however, saves on- and off-chip memory requirements significantly if $N_{in}$ is smaller than $N_{out}$, and reducing memory bandwidth/footprint is crucial to designing energy-efficient inference systems.", "We included this discussion in the evaluation parts.", "Q3: The evaluation section lacks experiments that evaluate the computational savings.", "A3: Since binary codes and lookup table would be associated with vastly different inference architecture, computation methods, and storage design, it is difficult to analyze detailed comparisons on FleXOR and lookup-table methods.", "We chose quantization schemes using binary codes in the experimental results because 1) binary codes are being widely studied and 2) we can focus on the practical issues on binary codes.", "Since all of quantization techniques in Table 1 and Table 2 follow the form of binary codes with the same q bits, comparisons have been made under the same computational savings (thus, model accuracy is emphasized).", "FleXOR, however, provides not only higher model accuracy but also additional storage savings due to the proposed encryption algorithm/architecture using XOR logic.", "We added discussions on the same computational savings and additional storage savings of FleXOR in Section 4 and 5."], "labels": ["nonarg", "concur", "nonarg", "nonarg", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 333, "sentences": ["Thank you for your constructive feedback!", "> Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.", "More explanations are needed.", "Thank you for this suggestion (shared by other reviewers too): the updated version of the paper will clarify these things, and relegate less of the information to the appendix too.", "Specifically for Figure 4, the performance outcome for each variant is measured on multiple independent runs (seeds).", "All the outcomes are then jointly ranked, and the ranks are averaged across seeds.", "Finally, these averaged ranks are normalized to fall between 0 and 1.", "A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.", "Figure 4 then further aggregates these normalized ranks across 15 Atari games.", "Note that these joining rankings are done separately per subplot (ie modulation class).", "We think we could address all your concerns, but please let us know if you have further questions, the discussion period lasts until the end of the week!"], "labels": ["nonarg", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 334, "sentences": ["Thank you for your constructive feedback!", "Comment 1:", "We acknowledge that our presentation focused more than necessary on ideal scenarios that use learning progress LP(z) while the practical version used a (maybe disappointingly) simplistic choice of proxy f(z).", "The updated paper will change the emphasis, and clarify that a proper learning progress proxy remains future work.", "We will also clarify that the little phrase \u201cAfter initial experimentation, we opted for the simple proxy\u2026\u201d implies quite extensive experimentation with other plausible proxies that looked promising in individual environments but were not consistently effective across the suite of Atari games.", "Comment 2:", "Sorry, our presentation of Figure 4 was not very clear: The performance outcome for each variant is measured on multiple independent runs (seeds).", "All the outcomes are then jointly ranked, and the ranks are averaged across seeds.", "Finally, these averaged ranks are normalized to fall between 0 and 1.", "A normalized rank of 1 corresponds to all the N outcomes (seeds) of a variant being ranked at the top N positions in the joint ranking.", "Figure 4 then further aggregates these normalized ranks across 15 Atari games.", "Note that these joining rankings are done separately per subplot (ie modulation class).", "The bandit is not guaranteed to reproduce the performance of the best arm for a couple of reasons: (a) the signal f(z) it obtains is noisy, (b) if is myopic in that it reflects only current performance not future learning, and (c) the dynamics are non-stationary, so the best arm changes over time.", "For all these reasons, the bandit we use is a conservative one that tends to spread the probability mass among decent-looking arms, while suppressing obviously sub-optimal arms.", "The experiment you suggest (picking the best hyper-parameter after the first X episodes) is exactly what we investigated in Figure 5 (left subplot).", "The empirical result is that it works well for some games but not others, and better for some modulation classes than others, but overall it\u2019s not reliable.", "The updated paper will split Figure 5 into two to increase clarity.", "Comment 3:", "Thank you for that suggestion: we will update the organization of the paper to make the main body more self-contained.", "Comment 4:", "The updated paper will discuss related work in more depth, including the suggested [A] and [B].", "We think we could address all your concerns, but please let us know if you have further questions, the discussion period lasts until the end of the week!"], "labels": ["nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "nonarg", "concur", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 335, "sentences": ["We thank the reviewer for the positive and constructive feedback.", "Below we answer the questions and concerns:", "Question 1:", "We agree with the referee and will therefore include a visualization of the trained distributions using Gumbel top-k sampling and a realization of the sampling pattern.", "We are currently running experiments to obtain Gumbel top-k results for the \u2018lines and circles\u2019 and CIFAR10 experiments as well.", "Since we did not sufficiently emphasize that leveraging Gumbel top-k sampling for learning signal subsampling matrices is part of the novelty of the present work, we clarified this in the revised manuscript.", "In fact, using Gumbel top-k sampling in this context can be seen as a constrained version of DPS, with shared weights across the M distributions.", "To also include previously-published baselines, we are currently running experiments with the recently proposed LOUPE method by Bahadir et al. (2019).", "Question 2:", "Indeed, the notion of compressed sensing has spurred vast work, ranging from sensing strategies to signal recovery algorithms.", "On the sensing side, sampling strategies are typically designed to satisfy the Restricted Isometry Property (RIP); describing isometry of the sensing matrix given K-sparse vectors, and thereby providing signal recovery guarantees, given an appropriate algorithm.", "On the algorithm side, sparsity in some basis transform is typically exploited, leveraging a wide variety of optimization algorithms spanning from proximal gradient methods to projection-over-convex-set and greedy algorithms.", "More recently, deep learning methods have been proposed for fast signal recovery from CS measurements, yielding state-of-the-art results.", "In this context, DPS adopts current practices in data-driven CS recovery, but extends this to incorporate subsampling (the sensing) in an end-to-end pipeline.", "Such an end-to-end (sampling-to-any-task) learning strategy opens up opportunities for data-driven optimization of sensing strategies beyond theoretically-established results.", "As pointed out by the referee, the shortcomings of disjoint optimization in classical CS are perhaps most evident when high-level tasks such as classification are part of the pipeline.", "As such, we are currently running additional experiments to better illustrate this.", "Question 3:", "We agree with the reviewer that such a comparison might be of interest.", "As such, we are currently running additional experiments to include a comparison to Gumbel top-k (as we did for the MNIST classification case) as well as the method proposed by Bahadir et al. (2019).", "Notably, and unlike our method, the latter approach does not permit setting a specific subsampling rate, with this rate is only being indirectly controlled via hyperparameter settings.", "As a follow-up on our answer regarding the second question, we would like to mention that we added a case in the MNIST classification experiment (DPS-topk), in which we jointly train a reconstruction network with a subsampling pattern.", "We subsequently train the classifier network on the reconstructed images.", "It shows that learning a task-adaptive (classification in this case) sampling pattern outperforms disjoint learning of sampling and the task."], "labels": ["nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 336, "sentences": ["Thank you for your comments! We appreciate the very detailed review.", "We have included the missing citations and fixed the typos in the revised version.", "Similar to your hypothesis, we suspect that multilingual BERT cannot produce high-quality representations for Turkish and Finnish.", "This can lead to worse performance of BERTScore.", "Based on [1] and [2], YiSi-1 trains word2vec embeddings on the monolingual data provided as part of the WMT translation task, which may explain its comparably higher performance on these languages.", "We believe it is an important future direction to improve the performance of multilingual BERT on low-resource language, but this requires a broader study of training BERT in low-resource regimes.", "We have studied computing the idf on a larger corpus.", "We computed idf scores using the monolingual English corpus released by WMT18, a much larger amount of data then we used before.", "The importance-weighted version of BERTScore using these idf scores performs worse than the original importance-weighted version.", "We hypothesize this is due to the domain shift between the corpora.", "Beyond paraphrase detection, we didn\u2019t try using BERTScore for text similarity tasks.", "The results on paraphrase are definitely promising.", "Given the number of experiments we conducted, we decided to consider this an important direction for future work.", "Indeed, several groups are already following the direction of using BERTScore for other tasks, including [3] and one of the papers R1 points to (\"Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization\").", "We discuss these follow up works in Section 7.", "[1] Chi-kiu Lo. 2017. Meant 2.0: Accurate semantic mt evaluation for any output language. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Tasks Papers, Copenhagen, Denmark, September. Association for Computational Linguistics.", "[2] Chi-kiu Lo. 2018. The NRC metric submission to the WMT18 metric and parallel corpus filtering shared task. Proceedings of the Third Conference on Machine Translation: Shared Task Papers, Belgium, Brussels, October. Association for Computational Linguistics.", "[3] Qin, L., Bosselut, A., Holtzman, A., Bhagavatula, C., Clark, E., & Choi, Y. (2019). Counterfactual Story Reasoning and Generation. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Hong Kong, China, November. Association for Computational Linguistics."], "labels": ["nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "nonarg"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 337, "sentences": ["Thank you very much for your comments, which is very helpful for clarifying our contribution and improving the presentation of the paper.", "Please see the inline responses.", "Q1: The paper is easy to follow but the authors are expected to clarify the rationality in integration of the loss function. How the parameter of \\lambda_r, \\lambda_t, and \\lambda_r influence the performance.", "It would be better if the authors could present some analysis.", "A1: There are in general two rules to follow when choosing the lambda for optimization: 1) the loss term provides gradient in similar numerical range, such that no single loss should dominate the training since accuracies in depth and camera pose are both important to reach a good consensus.", "2) we found in practice the camera rotation has higher impact on the accuracy of the depth probably but not the opposite.", "This is presumably because that pose cost volume accumulate depth differences of all the pixels such that is more tolerant to the depth error.", "To encourage better performance of pose, we set a relatively large \\lambda_r.", "Note that all the losses are necessary to achieve good performance.", "On the validation data, some preliminary experiments by grid search values of each lambda, show that the performance of our model is not very sensitive to various values of lambda.", "Therefore we provide a combination of lambda that produces the model for our experiment, and presumably there could be other settings that may potentially further improve the performance.", "We have added some insight to Section 3.5 about the loss function.", "Q2: The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.", "A2: We thank the reviewer for the suggestion.", "We add more analysis with the state of the art in Section 4.2, especially about the case that other methods outperforms our method.", "Q3:The experiments in section 4.3 are also expected to be improved.", "It is difficult to draw a conclusion that the method is better than other ones based on such limited experiments.", "A3: Thanks for this point.", "However, the main experiments and the conclusions are in Sec. 4.2; and  thus Section 4.2 included  much more insights and discussion of our model Vs. the other baselines in the revised version.", "In contrast,  Section 4.3 lists the ablation study, where the purpose of experiments is to verify the necessity and sufficiency of some system design options of our model and demonstrate the behavior under controlled experiments, instead of comparing with other methods.", "Specifically, we show the performance of our method with different number of iterations, with and without pose cost volume, and different numbers of the input view.", "At the same time, we found that our method also outperforms other methods in some aspects.", "In Figure 2, the curves are going down, which means that our method can effectively reduce depth and pose error from DeMon.", "The solid curves are consistently lower than dashed curves, which means our pose cost volume outperforms Steinbr\u00fccker et al. (2011) in pose estimation and further benefits depth estimation.", "In Figure 3, the blue curve is significant lower than orange curve, which means that our method is more robust in the situation with fewer views than COLMAP.", "Even though, the main purpose is not to compare to others but provide some analysis on important model components."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "nonarg", "nonarg", "concur", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "concur", "concur", "concur", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 338, "sentences": ["The reviewer observed that \u201cauthors need to highlight at least one practical advance introduced by the CW distance\u201d and suggested the following potential options:", "1) Faster training times.", "2) Stabler training.", "3) Usefulness of the CW distance outside of the autoencoder context.", "Ad. 1) (faster training) The experiments show, that CWAE model approaches best generalization, measured with the FID score, much more rapidly, than it is the case with WAE or SWAE models.", "E.g., when trained on the CelebA problem, the FID-score in case of CWAE drops below 100 after only about 75 batches, while for the WAE model only near 400 batches, it does so.", "The same applies to SWAE.", "Needless to say, the FID score for CWAE is near a common best value (after about 500 epochs) of about 95 (these are results are for a DeConv encoder-decoder architecture, see. Appendix E for details; for a direct comparison with Tolstikhin at al\u2019s paper results for an identical architecture to theirs are given in Table 1 in the paper) after a much shorter processing time.", "This is both thanks to the quicker convergence, but also due to faster batch processing (as it was shown in the paper).", "The MMD-like cloud-to-cloud formula for CW-distance (see equation (3) in the paper) is much more cumbersome than the actual one cloud-to-distribution used in the experiments derived in the paper and shown in equation at page 5 of the paper.", "The proposed Cramer-Wold kernel behaves correctly.", "We have added graphs describing this to the paper, exchanging those on page 8 (as the new are much more clearer).", "Graphs comparing CWAE, WAE and SWAE learning, on both CelebA and CIFAR10 datasets, shall be added to the Appendix.", "Ad. 2) (stable training) We have run repeated experiments with different initializations for all the generative models, as the reviewer has suggested.", "All experiments show that CWAE learning process is stable and repetitive: the standard deviations, for most of the coefficients computed during training are smaller than those of WAE or SWAE models (in particular CWAE minimizes WAE distance faster then WAE-MMD).", "We have added appropriate graphs to the paper.", "Ad. 3) (CW usefulness) We have verified how the Cramer-Wold metric works as a Gaussian goodness of fit,", "however, the results were not satisfactory.", "The tests based on Cramer-Wold metric were, in general, in the middle of compared tests (Mardia, Henze-Zirkler and Royston tests).", "We doubt it can be efficiently applied in this direction.", "However, since Cramer-Wold metric is defined by characteristic kernel, it can be applied in the large field of kernel-based methods in machine learning (where its particular advantage lies in the fact that it can be efficiently computed for the mixture of radial Gaussians).", "The reviewer noted that \u201cbesides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.", "In our opinion sliced approach works well for neural networks, as the neural networks see/process data by applying similar one dimensional projections.", "Also the success of neural networks based on the classical activation functions, as compared to RBF networks, supports this.", "Concerning the closed-form, Cramer-Wold kernel is the only known to the authors, which is given by the sliced approach and has a closed form for radial gaussians.", "The reviewer also noted, that \u201cSilverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?\u201d.", "In our opinion the model works well due to the fact that we compare it to the Gaussian N(0,I), where the Silverman\u2019s kernel is optimal.", "However, if the prior in general would not be standard Gaussian, the situation could possibly be different."], "labels": ["nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur", "nonarg", "nonarg", "nonarg", "nonarg", "nonarg", "concur", "concur", "concur", "nonarg", "concur", "concur"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}