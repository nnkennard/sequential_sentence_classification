{"abstract_id": 0, "sentences": ["Thanks for your feedback.", "We discuss each comment in the following:", "- The experiments are not large scale", "We respectfully disagree with the reviewer's main comment that the experiments are not large scale.", "One needs to see the background of existing work: Existing ordinal embedding methods are known to be notoriously slow and embedding more than 10,000 points is not practical - as reflected in our experiments (see Figure 4).", "Our new approach manages to get one order of magnitude higher (100000 many points and about 4 million triplets), without diverting to heuristics such subsampling or adding extra information such as invoking active oracles (as needed in landmark approaches).", "Sure, this is not the scale of 80 million tiny images; but one wouldn\u2019t ask an author of an improved SAT-solving algorithm, say, to scale to 80 million instances.", "Representation learning, the topic of this conference, has many facets.", "Learning representations from \u201cbig data\u201d (as in 80 million images with RGB representations) is one side, but learning representations when little data is available (no explicit representation, just binary-valued triplet comparisons) is the other side.", "Both are valuable in different circumstances.", "- No substantiate insight with respect to NP-hard problems", "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems.", "To elaborate, consider optimization problems that arise in unsupervised learning - for instance, ordinal embedding objectives, clustering objectives or dimensionality reduction objectives.", "These optimization problems are typically not solved directly since there are non-convex, discrete, NP-hard.", "Instead, we resort to convex relaxations and many convex relaxations do not come with any guarantees.", "Consider, however, if we could use a non-convex optimization toolbox to directly tackle the original optimization problem - which is currently NOT the standard practice in ML.", "Then the value of the true objective already informs us of how close we are to the optimal solution of the optimization problem.", "So powerful non-convex solvers might be of a significant advantage over convex relaxations.", "Our paper simply shows ONE example for this.", "- It is not clear why the log n representation for items is chosen -- why not just map to embeddings directly?", "It would not be possible to set the input dimension the same as the embedding dimension.", "Our experiments demonstrate that we need input representations of size at least Omega (log n) to sufficiently reduce the triplet error.", "The size of the embedding dimension can be too low to achieve this.", "One could argue that instead of using a small network like ours, a heavily over-parameterized neural network could potentially accomplish the same with smaller input representation.", "However, the computational complexity of the method is significantly affected by this and this is in conflict with the main goal of the paper: scaling ordinal embedding.", "- Methods, where items have no representation, are questionable", "Items having no representation is a caveat of the data available rather than that of the method.", "The representationless framework of triplets is relevant to many applications (e.g. crowdsourcing), and the whole field of comparison-based learning works in this framework.", "- How to generalize to unseen items", "First, it is not standard practice to discuss the generalization to unseen instances in unsupervised machine learning problems, for example in the literature on clustering. But of course, if generalization exists, it is of advantage.", "We believe that in our case, generalization is realizable.", "One possible approach would be to reserve some extra bits in the binary representation of inputs, and then utilize it to represent new items.", "The network can be trained with extra batches of triplets which involves the new items.", "- The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.", "We don\u2019t really see a link to matrix factorization or relational learning. If the reviewer has some idea of such connections, we would be happy to learn of this."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 1, "sentences": ["Overall:", "We thank you for your time and appreciating the strengths of our work.", "Based on your guidance and suggestions, we have tried to do additional experiments to improve the quality of our work.", "Concern 1: Comparison to Meta-RevGrad", "Meta-RevGrad tries to achieve feature invariance at the embedding level.", "Achieving such feature invariance for high-dimensional feature mapping can be a very weak constraint [1], causing limitation in performance.", "Recently, generative approaches, following image-to-image translation have been shown achieve better domain adaptation, as this constrains the feature embedding to generate the data in a new domain.", "Being a non-GAN based approach, concepts such idt (encouraging the styling network to behave as an identity when given a target domain instance as input) and revMap (constructing source instance back from generated target instance) are not applicable in this scenario, as no instances or images are being generated from a feature embedding.", "Concern 1, 2, 3: Experiments", "Thank you for these suggestions.", "Taking your comments and comments from other reviewers into account, we have made improvements to the experimental section.", "Specifically:", "\u201cDomain Adaptation Baselines\u201d,", "We have now added additional domain adaptation baselines, designed in the setting suggested by Reviewer 2.", "\u201cSimple baseline \u2013 combining a subset of a new domain as training set\u201d", "We see the merit of this baseline, but there are several challenges in executing this.", "Designing it in a fair way is tricky.", "Using some labelled data in target domain maybe unfair, as we are not allowed to see meta-test data.", "Moreover, this is likely to not work, as the meta-train data would be too large, and would dominate, and we do not have a clear way to set the weights.", "\u201cDramatic Domain Shift, Omniglot to Fashion-MNIST\u201d", "This could be an interesting setting, but we don\u2019t think this will work very well, as the tasks are themselves completely different. We would not expect a character recognition model to transfer to a object recognition task, as the visual features are very different.", "Minor:", "Thanks for this; we have updated the draft to make the presentation clearer.", "Unlabelled data refers to only the domain of the meta-test data, but the meta-test data is never used in meta-training.", "L_da is essentially the sum of L_gan and L_cycle.", "[1] Shu, R., Bui, H.H., Narui, H. and Ermon, S. A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR 2018"], "labels": ["rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 2, "sentences": ["We hope that the reviewer will change his opinion once we clarify the goal of our paper and explain how it relates to prior work, as we believe we are fundamentally on the same page.", "We are well aware of SIFT, HOG, the results of Olshausen and Field on learning image filters from a few example images (some of us are sufficiently old to have implemented all such methods from scratch as grad students!) and no annotations, as well as Mallat\u2019s Scattering nets [1].", "In fact, we discuss and evaluate Oyallon\u2019s 2017 implementation [2] of this at page 5 and table 2 in the paper.", "However, the existence of these methods does not detract from the message of this paper.", "Our goal is to provide \u201ccritical analysis\u201d of current self-supervision methods because these *specific* tools are now very heavily researched.", "Our paper sends a cautionary message: current self-supervised learning techniques cannot improve on what can be obtained from a single image plus transformations for early layers in a network, and only improves in a limited manner for deeper layers, despite ingesting millions of images (which is touted as their key advantage).", "In particular, the claims are not limited to the first few layers as we show that one image recovers two thirds of the performance of deeper layers as well.", "This message, which is a partially negative result, stands on its own, regardless of whether good low-level features can be obtained in some other ways (e.g. manually) and, we hope the reviewer will agree, should be known by the community.", "Nevertheless, we also agree with the reviewer that it is interesting to put these findings in a broader context, so we are happy to expand the discussion of prior feature learning/design work further.", "However, please note that none of this literature makes our specific findings on the limits of self-supervision obvious.", "Furthermore, although this is a little besides the point, in the paper we do show in Table 2 that scattering transforms works as well as conv1, but that from conv2 onwards self-supervision on a single image does better, so even the claim that handcrafted features are equivalent to the first few layers in deep networks is not proven.", "Also, the fact that Olshausens\u2019s filters resemble conv1 does not mean that they are equivalent to conv1 in recognition performance.", "\u2014", "[1] J. Bruna and S. Mallat. \"Invariant scattering convolution networks.\" TPAMI 2013", "[2] E. Oyallon, et al. \"Scaling the scattering transform: Deep hybrid networks.\" ICCV 2017"], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 3, "sentences": ["Thank you for your supportive review.", "We answer the specific queries below and have also added them to the revised version of the paper.", "1.         We found that the entropy regularizer produces an ME score that stays constant across training, at the cost of the model being less confident about predictions made for seen classes.", "We added details regarding this condition to the manuscript.", "2.         The base rate is the probability of observing a new word in the target at that particular point in training.", "We go through the remaining sentences in the corpus from the target compute the probability of sampling a sentence with at least one new word.", "Thus, the base rate at time t in training is defined as:", "$$P(\\text{new in target at t}) = \\frac{ \\text{# of unseen sentences in target with new words}} {\\text{# of unseen sentences}}$$", "3.", "In Section 4.2, we use \u201cnew\u201d to refer to the set of all the unseen classes at a particular timepoint t.", "For the classifier, P(N|t) is calculated by adding the probabilities the model assigns to all the \u201cnew\u201d classes when iterating through the remaining corpus (similar to Equation 1 in our paper).", "For the dataset, we compute P(N|t) by sampling all unseen images in the corpus and compute the proportion from \u201cnew\u201d classes given their ground truth labels.", "4.         We will release our code and data with the publication of the paper.", "Most of our experiments are easy to replicate as they use standard datasets, models, loss functions and optimizers.", "We sincerely hope that our challenge and these resources will stimulate progress in this area.", "Please also see above where we write a general response to all reviews."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 4, "sentences": ["Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "Here we respond to your specific comments.", "\"(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.\"", ">>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation.", "The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples.", "While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A).", "This leads to better generalization ability for test tasks.", "In Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as \"Label Propagation\").", "\"", "(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.", "Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.", "For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.", "This is a major concern.\"", ">>> At first, we want to clarify the few-shot network architecture setting.", "Currently, there are two common network architectures: 4-layer ConvNets (e.g., [1][2][3]) and 12-layer ResNet (e.g., [4][5][6][7]).", "Our method belongs to the first one, which contains much fewer layers than the ResNet setting.", "Thus, it is more reasonable to compare TADAM with ResNet version of our method.", "To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:", "Method", "1-shot    5-shot", "SNAIL [4]", "55.71     68.88", "adaResNet [5]                        56.88     71.94", "Discriminative k-shot [6]", "56.30     73.90", "TADAM [7]", "58.50     76.70", "--------------------------------------------------------", "Ours", "59.46     75.65", "--------------------------------------------------------", "It can be seen that we beat TADAM for 1-shot setting.", "For 5-shot, we outperform all other recent high-performance methods except for TADAM.", ">>> We want to clarify that \"Label Propagation\" in Table 1 and Table 2 is a strong baseline.", "It combines label propagation method [8] with episodic meta-learning.", "The usage of transductive inference makes this baseline outperform most published state-of-the-art methods.", "Moreover, the performance of TPN over label propagation is not very small.", "For example, in miniImagenet, TPN outperforms label propagation with 1.44% and 1.25% for 1-shot and 5-shot respectively, but this advantage grows to 3.20% and 1.68% with \"Higher Shot\" training.", "The improvements are even larger for tieredImagenet with 4.68% and 2.87%.", "We believe in few-shot learning, this is a large improvement.", "[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML. 2017.", "[2] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.", "[3] Yang, Flood Sung Yongxin et al. \"Learning to compare: Relation network for few-shot learning.\" CVPR. 2018.", "[4] Mishra, Nikhil et al. \"A simple neural attentive meta-learner.\" ICLR. 2018.", "[5] Munkhdalai, Tsendsuren et al. \"Rapid adaptation with conditionally shifted neurons.\" ICML. 2018.", "[6] Bauer, Matthias et al. \"Discriminative k-shot learning using probabilistic models.\" arXiv. 2017.", "[7] Oreshkin, B.N., Lacoste, A. and Rodriguez, P., 2018. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS. 2018.", "[8] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 5, "sentences": ["Thank you very much for the positive comments.", "We added the more experimental data of runtime analysis to address the Reviewer's main concern.", "Q1. My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.", "Compressability is evaluated, but that was already present in the previous work.", "Therefore the novel contribution of this paper over [1] is not clearly outlined.", "We conducted additional simulations to evaluate the runtime benefit of the proposed method compared to that of the method in [1].", "We generated random 512-by-512 matrices with pruning rate ranging from 70 % to 95 % and simulated the number of parameters fed to PEs in 10000 cycles.", "The assumptions used for the simulation and analysis data have been updated in Figure 6c of the revised manuscript.", "We could observe that proposed parallel weight decoding based on the second Viterbi decompressor allowed 10 % to 40 % more parameters to be fed to PEs than the previous design [1].", "The proposed method outperformed both baseline method and [1] in all simulation results.", "Please note that the data described in Figure 6c has been updated from Figure 7, and our method shows better performance in new data compared to the data shown in the original manuscript.", "While preparing for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.", "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.", "Therefore, we updated Figure 7 in original manuscript to Figure 6c in the updated manuscript according to the new data.", "Q2. It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.", "In the revision, we added the more precise mathematical description of the input and output of each block in Figure 1 and showed the change of the exact weight representation at each process.", "We first prune weights in a neural network with the Viterbi-based pruning scheme [1], then we quantize the pruned weights with the alternating quantization method [2].", "Our main contribution is the third process, which includes encoding each weight with the Viterbi algorithm, and retraining for the recovery of accuracy.", "With our proposed method, the sparse and encoded weights are reconstructed to a dense matrix as described in Figure 2.", "Figure 2 illustrates the purpose of our proposed scheme, which is the parallelization of the whole sparse-to-dense conversion process with the VDs while maintaining the high compression rate.", "Q3. Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)", "Thanks very much for the suggestions.", "We tried to fix grammatical mistakes as much as possible in the revision.", "Reference", "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "[2] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 6, "sentences": ["We thank Reviewer 2 for the constructive feedback.", "Here is our point-to-point response to the comments and questions raised in the review:", "1. \u201cIt is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.\u201d", "We do not claim that our method is more efficient than Miyato et al.\u2019s method, which uses the spectral norm of the convolution kernel matrix to approximate the spectral norm of the convolution operation.", "In fact, our proposed method is computationally more expensive than their approximate scheme because each power iteration in our method requires a conv/deconv operation rather than a simple division used by Miyato et al.\u2019s.", "We introduce our new spectral normalization scheme for convolutional layers because there exist examples where the true spectral norm of a convolution operation can be arbitrarily larger than Miyato et al.\u2019s approximation.", "Therefore, Miyato et al.\u2019s normalization scheme is not guaranteed to control the spectral norm of convolutional layers which is critical for controlling a DNN\u2019s generalization performance (please see our generalization bounds in Section 3).", "To further support our argument, we performed additional experiments demonstrating how our proposed method better controls the spectral norm of convolution layers, resulting in better generalization and test performance.", "The results are presented in Appendix A.1.", "Furthermore, we run several experiments to show that our method is not significantly slower than Miyato et al.\u2019s method, and we report the results in Appendix A.1, Table 3.", "2. \u201cFig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing\u201d", "We relabel the axes and add a more thorough explanation in the caption.", "We note that the text explaining Figure 3 mentions how the margin normalization is performed (paragraph 3 in section 5.1): the margin normalization factor is exactly the capacity norm \\Phi described in Theorems 1-4.", "We clarify that we divide the obtained margins by the values of \\Phi estimated on the dataset.", "3. \u201cThe epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\u201d", "Yes, the epsilons are chosen to be different depending on whether we are looking at norm_inf attacks or norm_2 attacks.", "This is because the two norms can behave very differently in adversarial attack experiments.", "For example, a norm_inf attack of 0.5 implies that all pixels can be changed by 0.5.", "On the other hand, a norm_2 attack of 0.5 means the overall Euclidean norm of perturbation across all pixels is bounded by 0.5, resulting in a much less powerful attack.", "Based on this comment, we update the plots with the same attack-norm to have the same scale.", "4. \"Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.", "However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.\"", "We redo the visualization in Figure 6 to make the gains provided by SN clearer.", "We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.", "5. \"The baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).", "It is thus unclear whether the advantage can be maintained after applying these standard regularisers.\"", "We did not originally discuss weight decay, dropout, and batch normalization as none of these methods were motivated by the theory we introduced in section 3.", "However, due to the reviewers\u2019 concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay, dropout, or batch norm in Appendix A.2.", "In our experiments, the SN-regularized network still performs better in terms of test accuracy."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 7, "sentences": ["Thank you to Reviewer 3 for your thoughtful critique and we are happy that you share our enthusiasm for the motivation behind our approach.", "We share your curiosity on the qualitative behavior of such systems, and as documented in this response we have augmented the paper to address that and other of your suggestions.", "Re: \"- no qualitative analysis on how modulation is actually use by the systems.", "E.g., when is modulation strong and when is it not used \"", "Following the reviewer\u2019s suggestion, we have added a figure that shows the dynamics of neuromodulation in the cue-response task (Figure 3, in the Appendix).", "This figure shows that while neuromodulation clearly reacts to reward, this reaction is complex and varies both within each episode and between runs.", "Re: \"- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\".", "Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.", "Furthermore PTB is not a \"challenging\" LM benchmark.\"", "We agree that, while the differences are statistically significant, they are minor.", "We were using that word technically, but do not want to give the wrong impression.", "We have thus modified the text to make it clear that we mean \u201cstatistically significant\u201d only.", "We also removed the adjective \u201cchallenging\u201d as regards PTB.", "We agree that, ideally, a comparison with SOTA architectures would be desirable.", "As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.", "We will keep trying to investigate such massive architectures in the future.", "Importantly, our purpose in this task is to show that, **all other things being equal**, a neuromodulated plastic LSTM can outperform a standard LSTM in realistic settings.", "We believe that outperforming standard LSTMs (again, all else being equal) on their \u201cworkhorse\u201d task domain (language processing) is worthy of notice, especially given the ease of implementation of our method which requires only adding a few lines of codes (<10) to a standard LSTM implementation and can then be used as a drop-in replacement to standard LSTM."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 8, "sentences": ["First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers,  and apologize for the late response due to additional experiments and modifications of the paper.", "Remark 1. Expression and detail", "A : We apologize to the reviewer for the lack of clarity in the manuscript. We have modified our expression.", "Remark 2.", "What is \"Selection Network\"?", "A : It is a module that estimates the confidence of the softmax output according to the inputs of the classification network.", "The selection network is trained with sigmoid and binary cross-entropy in a supervised manner.", "And the threshold is not 0.5 but high because selection network is learned with many \u20191\u2019 labels with close to 100 % training accuracy.", "The selection network has advantages in out of class unlabeled data.", "Since softmax output is a relative value, the softmax output can be high for some out of class unlabeled data.", "In our original paper (in table 10), there already exist results of softmax output for in or out of class unlabeled data with 0.9999 thresholds.", "Further, we experimented with the same threshold in table 4 of the new version and the results have shown that out of class unlabeled data are added even with an extremely small threshold such as 0.99999 (epsilon = 10^-5).", "Remark 3. \"As the base classifier is different for various baselines, it is hard to compare the methods.\"", "A : SST has a network structure similar to other papers.", "The difference of structure was that the selection network is added and Gaussian noise and the mean only batch norm are not used.", "As mentioned in the paper (4. Experiments), our supervised learning performs slightly better than conventional SSL algorithms because of different settings such as learning rate and Gaussian noise on the input layer.", "(When SST uses Gaussian noise, ours are also degraded.)", "Remark 4.", "Experiments Detail ( data setting, threshold, number of iterations, animal vs nonanimal)", "A :", "==> Data setting", "The purpose of experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.", "Therefore, we experimented with the popular setting.", "We have added a detailed description on the data setting to Section 6.3 of the supplementary material.", "==> Iterations & Threshold", "We have missed out on a detailed description of how to set up some hyper-parameters.", "We set parameters as follows.", "The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.", "In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.", "While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.", "Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added.", "The growth rate of epsilon is determined according to when the validation accuracy saturates.", "The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.", "If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.", "If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.", "We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.", "(In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.", "However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.", "In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.", "In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.", "Other details are the same as those of the first experiment.", "(In previous versions, the training iterations of fixed mode had been fixed.", "Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)", "=", "=> Animal vs non-animal", "The citation of that part is obscure and has been modified.", "We experimented similar to the [1] and they categorized according to the animal.", "Our approach is similar but not identical.", "Their unlabeled data came from only in 4 classes, however, we selected unlabeled data in all classes.", "[1] Odena, Augustus, et al. \"Realistic Evaluation of Semi-Supervised Learning Algorithms.\" (2018)", "Some Questions and comments", "Remark 5. \"The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?\"", "A : To the best of our knowledge, the main purpose of transfer learning is to improve the performance on the target domain by effectively utilizing the knowledge of the source domain.", "However, in our case, there is no separated source and target domains.", "We focus on the single classification task.", "We think that the goal of our method and that of transfer learning are quite different.", "Remark 6. \"What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?\"", "A \" We have modified that expression and we wanted to address that \"if one class dominates the dataset, the performances are degraded by the imbalanced distribution.", "(Analysing the classification of imbalanced data-sets with multiple classes: Binarization techniques and ad-hoc approaches, 2013)\"", "Remark 7. \"Figure 3: wouldn\u2019t the plot of accuracy vs amount of data be more suitable here?\"", "A : I agree that your suggestion is more suitable for the figure.", "However, it is difficult to show the figure you want because the number of selected samples is different every time.", "Remark 8. \"Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?\"", "A : The performance depends on the initial points, therefore sometimes the performance is not good. Since the inputs are the x and y coordinate values, it can be very easy to add to the training set.", "(ex.. class 1 : (-1, 0), (1, 0) , class 2 : (0.5, -0.5), (1.5, -0.5) , then decision boundary could be (:, -0.25) then class 2 unlabeled data (0, 0.5) is classified as class 1 and can have a very high selection score.)", "Remark 9. Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?", "A : In fixed mode, we ensemble the selection scores, which makes the prediction more consistent.", "Also, for a more reliable selection score, we do not add unlabeled data to the new training set and train with labeled data only for 5 iterations.", "Remark 10. \"How was it possible to improve the performance in the experiment of section 4.2 with 100% of irrelevant classes?\"", "A : We suspect that this performance improvement is due to re-initializing learning rate.", "After constructing a new training dataset, we retrain our model with the learning rate of the initial value.", "In decay mode (Figure 2, Figure 3 (a) and (b) of the original manuscript), the accuracy is slightly increased and gets saturated while unlabeled data is not being added.", "However, the accuracy begins to increase or decrease relatively more after adding selected data to the new training dataset.", "In fixed mode (Figure 3 (c) and (d) of the original manuscript), the improvement with the 100% of irrelevant classes seems to be due to re-initializing learning rate.", "However, SST algorithm with other ratios of out-of-class samples results in performance improvement compared to the 100% because out-of-class samples are not selected."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 9, "sentences": ["Thanks for the insightful comments.", "We\u2019ve tried to improve our paper based on your feedback.", "Most significantly, we\u2019ve performed additional ablation studies to confirm that our modeling choices improve performance, and we provide further empirical insight on what the coreference operations do.", "We\u2019ve also updated the model description and the notation in Section 4 to clarify modeling mechanisms and choices.", "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "Below we address your concerns point-by-point.", "The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.", "This is especially the case in a few places involving coreference:", "1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.", "2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.", "While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.", "======", "Based on your comments, we\u2019ve performed additional ablations to measure the impact of the co-reference mechanisms.", "We find that removing any of them leads to a decrease in performance (Rows 2, 3, 4 of Table 5).", "To provide more than just this quantitative insight, we\u2019ll expand here on how KG-MRC handles coreference to better motivate the modeling choices:", "The construction of graph G_t from G_{t-1} uses co-reference disambiguation of nodes to prevent node duplication and to enforce temporal dependencies.", "We perform coreference disambiguation between location nodes of G_t and G_{t-1} via Eq. 1 (call this inter-graph coreference) and between the location nodes in the same graph Gt (call this intra-graph coreference) via Eq. 2.", "The inter-graph coreference yields new, intermediate representations for the nodes in G_t.", "These are further updated via the intra-graph coreference step.", "Inter-graph Co-ref: One way to think about this is that we construct a new graph G_t at every time step.", "Now the graph G_{t-1} might contain some location nodes which are predicted again at time step \u2018t\u2019 (e.g., in Figure 2, leaf node already existed in G_{t-1}).", "Instead of replacing an old node with an entirely new node at \u2018t\u2019, we take a recurrent approach and do a gated update that preserves some information stored in the node in previous time steps while adding new information unique to time step \u2018t\u2019.", "Intra-graph Co-ref: Inter-graph co-ref isn\u2019t enough since the MRC module makes its span predictions independently.", "This means that, at time step t, the model could predict the same span/location for multiple entities and add all these duplicates to the graph.", "Moreover, a single location might have the same surface form but be from different parts of the paragraph (e.g. \u201cleaf\u201d in the 1st and the 5th sentence of the para in figure 2).", "The operations in Eq. 2 resolve this by performing self-attention (i.e., the predicted locations of all entities are compared to each other).", "=====", "Response continued from above.", "Why does the graph update require coreference pooling again?", "Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?", "=====", "We agree that the coreference pooling in the graph update seems repetitive at first glance.", "We have further clarified the explanation given in the text and included another ablation experiment  (row 4 of Table 5) to confirm its usefulness.", "This step does indeed repeat Eq. 2.", "In a nutshell, this is necessary because, after the recurrent and residual graph updates (Eqs 3.1 - 3.3) that propagate information across edges, we may end up with different representations for location nodes corresponding to the same location.", "We don\u2019t want these representations to diverge from each other because of information propagation.", "To give you more detail:", "The graph update step ensures information propagation between entities and location representations.", "Specifically if the current location of entity \u201ce_t\u201d is predicted as \u201c\\lambda_t\u201d, the graph update steps ensures that both the entity and location representation gets the same update (via eq 3.2 and 3.3).", "This would have been sufficient if every entity had a unique location.", "But, multiple entities can actually exist in the same location.", "Let\u2019s consider this small graph below", "Water - -> leaf", "CO_2 --> leaf", "Here both water and CO_2 exist in the same location, leaf.", "But let\u2019s say that the MRC model picked the \u201cleaf\u201d span from sentence 1 (of the text in Fig 2) for \u201cWater\u201d and from sentence 4 for CO_2.", "In reality, they refer to the same location entity \u201cleaf\u201d.", "Now, due to eq. 3.3, the two embeddings of leaf will get two different residual updates (one would be corresponding to Water and other would be because of CO_2).", "Because of the different updates, the two representations of the same entity might diverge.", "To remedy this, we re-use the coreference matrix \u201cU\u201d we create in eq. (2), which should already have a high attention score corresponding to the two leaf locations.", "Thus we perform a similar operation to the intra-graph update.", "====", "Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?", "====", "The \u201cprefixes\u201d that our model reads at each time step comprise all sentences up to and including the current sentence s_t.", "The motivation for this modeling choice was empirical.", "In our preliminary experiments we evaluated alternative strategies, such as (a) only considering the current sentence s_t, and (b) considering the entire paragraph at every time step.", "We found that operating on prefixes performed best.", "This is in line with the findings of Dalvi et al., 2018, where the Pro-Global model (which uses prefixes) performs better than the Pro-Local model (which operates on single sentences)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 10, "sentences": ["We agree with most of the comments."], "labels": ["rebuttal_concede-criticism"], "confs": [1.0]}
{"abstract_id": 11, "sentences": ["Thank you Reviewer 2 for your positive appraisal of our results and presentation.", "As documented below, we do our best to address your questions, which have helped us improve the paper.", "Re: \"The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.\"", "We have added \u201csimilar to\u201d in order to emphasize that we adapted and re-ran their architecture (we still use some of their code, which we believe might warrant citation; we are happy to drop it altogether if it is found confusing).", "Re: \"One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017.\"", "We agree that, ideally, a comparison with SOTA architectures would be desirable.", "As explained in the response to Reviewer 1, despite all our efforts, we found the technical challenges insurmountable given our computational and engineering resources.", "We will keep trying to investigate such massive architectures in the future.", "Re: \"Why were zero-sequences necessary in Experiment 1? [...] Perhaps the authors could clarify on what a confounding \"time-locked scheduling strategy\" would look like in this task?\"", "The random zero-inputs make the timing of the cues unpredictable, forcing the network to be driven specifically by the stimuli - as opposed to learning a pre-programmed strategy at each given time step.", "This is merely a convenient choice to make the task more challenging.", "Re: \"Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?\"", "Again, this simply makes the task more challenging.", "Non-target cues operate as distractors and having pairs of stimuli shown before each response increases the uncertainty in reward credit assignment (i.e. when receiving a reward, the network must still find out which of the two stimuli is the target).", "To better describe the task, we have added a schema of an episode to Figure 1.", "We hope this may facilitate understanding.", "Re: \"Why is non-plastic rnn left out of Figure 2b?\"", "As documented in Miconi et al 2018, non-plastic networks are terrible at this task.", "We are happy to run this experiment and include it if the reviewer finds it useful.", "Typos: \"However, in Nature,\" -- no caps", "in appendix: \"(see Figure A.4)\" -- the figure is labeled \"Figure 3\"\"", "We thank the reviewer for noticing these typos and have fixed them in the text."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 12, "sentences": ["Thanks a lot for your review and for pointing us to the reference, we will add and discuss this work in our paper.", "The referenced paper mimics the Choquet integral to fuse different neural networks such as CaffeNet, GoogLeNet, and ResNet50 that have been pre-trained for classification problems and can be viewed as ensemble method for multiple noisy classifiers.", "Contrary, we are interested in regression problems that have inherent non-additive effect such as automatic summarization.", "Furthermore, the referenced paper is much closer to the Choquet as we intent to be.", "As we describe in the paper, the proposed architectures are only inspired by the Choquet integral.", "This idea can be found in both of our architectures.", "In Figure 1c, u_i and in Figure 1d g_i * u_i model these meaningful intermediate values.", "We do not claim that we obtain any theoretical guarantees or properties of the Choquet integral.", "\"How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral?\"", "As described above, the proposed approaches are inspired by the way Choquet integrals handle non-additive utility aggregations.", "We do not claim that we obtain any theoretical guarantees or properties of the Choquet integral.", "Furthermore, the main idea of this work is to not learn a representation.", "Instead, we propose to predict many meaningful intermediate values that can simply be summed to obtain a set utility.", "\"What is your loss or your algorithm?\"", "We describe in Section 3.3 that we use mean squared error (MSE) and mean absolute error (MAE) in our experiments.", "We use MSE because it is usually used in regression problems.", "We were also interested in the mean absolute error because minimizing this loss might be more appropriate in a task such as automatic summarization, in which we don't want to punish a model strong if it makes a few severe mistakes compared to making many small mistakes.", "We also describe in Section 3.3. that we use Adam as optimizer.", "\"According to the illustration, it seems that you first obtain \u201cfeatures/representations\u201d. Then the representations are fed to the four architectures you listed in figure one.\"", "This is correct.", "\"RNN-based approaches are with better \u201ccomplexity\u201d comparing to your sum baseline and \u201cDeepset\u201d approach.\"", "We also compare against an RNN-based approach (abbreviated with \"RNN\" in the paper).", "The RCN approach is the smallest modification one can make to implement our idea into a standard RNN.", "Hence, we think that the comparison is fair and meaningful.", "Furthermore, we demonstrate in the extrapolation experiments that standard RNNs tend to overfit.", "The simple sum baselines and deepsets perform better in this experiments.", "Hence, a \"better\" complexity turns out to be prone to overfitting, which shows that larger models are not necessarily better."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 13, "sentences": ["We thank the reviewer for the comments and feedback. We will certainly clarify them in the final paper.", "1. Title of the paper", "- We agree that the main highest-level task that we show is VQA, even though our method is more general.", "Our title aimed to convey that we showcase PMN on a host of increasingly complex visual reasoning tasks such as relationship detection, counting, and captioning, as well as VQA.", "Our focus is on VQA as it happens to be one of the most complex visual reasoning tasks that can leverage each of the (relatively) simpler tasks.", "2. Description of variables", "- Thanks for the feedback.", "Epsilon means the environment, some of the definitions are written in Section 3, but we agree that it can be somewhat challenging to interpret as there are many variables.", "We edited the text to address variables more gently and to explain the arrow sign.", "3. Query for the relationship module", "- The relationship module is fed an N-dimensional (corresponding to N image regions) one-hot vector as input during training.", "When it is called by other task modules (such as counting), an N-dimensional probability vector is computed using softmax on image regions (see A.4, point 3) and not using the importance scores.", "This acts as a soft version of the one-hot sampled vector so that we can backpropagate gradients.", "4. CIDEr score of captioning", "- That may be true to some extent.", "However, we think that explicit label information might still be useful since the visual features (environment) are from Faster RCNN and contain diverse information such as edges, background, color, and size.", "5 and 6.", "Comparison with SOTA models for counting and relationship detection", "- To the best of our knowledge, Zhang et al. (2018) is the SOTA method on counting in the context of visual question answering.", "Our counting module leverages that but achieves higher performance on the number questions - 54.39% with ensembling and 52.12% without vs. 51.62% of Zhang et al. (2018).", "Note that 51.62% of Zhang et al. (2018) is from a single highly regularized model that provides small gains from ensembling.", "This shows that additional modules help.", "Kim et al. (2018) which is concurrent to our work shows similar performance.", "For the relationship detection task, other works such as Lu et al. (2016) unfortunately have a different setup which makes direct comparison difficult.", "7. Table 4, accuracies are from Zhang et al. 2018", "- Yes, the numbers are from their paper.", "One possible explanation for this could be their use of high regularization for a single model instead of ensembling.", "Thus, the performance improvement from training on the train set (evaluating on validation) to training on train+val (evaluating on test-dev) is smaller.", "(Zhang et al. 2018) Learning to Count Objects in Natural Images for Visual Question Answering", "(Kim et al. 2018) Bilinear Attention Networks", "(Lu et al. 2016) Visual Relationship Detection with Language Priors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 14, "sentences": ["Thank you for your insightful comments. We have incorporated your suggestions into the revised version of the paper.", "Q: Relationship to optimizers with adaptive learning rates, and comparison between Adam and Adam-APO.", "While Adam and Adagrad are often described as having \u201cadaptive learning rates,\u201d they still have a global learning rate that is just as critical to tune as for SGD.", "In our experiments, we consider tuning the learning rate for RMSprop, which also maintains adaptive learning rates for each parameter, and is closely related to Adam/Adagrad.", "Adam is essentially RMSprop with momentum; APO can be applied to Adam by applying momentum on top of the updates computed by APO.", "To address your question about Adam, we added experiments for tuning the global learning rate of Adam with APO in appendix Section G, Figure 14, where Adam-APO achieves better performance than Adam with a fixed global learning rate, and achieves comparable performance as Adam with a manual schedule.", "Q: Comparison with population-based training (PBT)", "We have added a comparison between APO and PBT in appendix Section H, Figure 15.", "For population-based training, one must carefully select many hyperparameters, including the size of the population, the perturbation strategy (e.g., randomly perturb the learning rate by multiplying it by 1.2 or 0.8), the exploration interval (e.g., the number of training iterations to run before exploiting other members of the population).", "We used PBT and APO to tune the learning rate of RMSprop while training a ResNet34 model on CIFAR-10.", "For PBT, we used a population of size 4, and chose to exploit/explore after each epoch of training.", "We tried multiple exploration strategies, and found that it was critical to set the probability of resampling a learning rate from an underlying distribution to be 0; otherwise, the learning rates could jump from small to large values, and yield unstable training.", "In contrast, APO only requires a simple grid search over lambda, and all other hyperparameters can be kept at their default settings.", "We found that  APO substantially outperformed PBT, achieving a lower final training loss and equal test accuracy in much less wall-clock time; this shows the advantage of gradient-based methods for tuning learning rates, such as APO, compared to evolutionary methods based on random perturbations such as PBT.", "Q: The convergence results appear to rely on strong convexity of the loss.", "How is this a reasonable assumption?", "Note that we assume strong convexity of the loss as a function of the output units, not as a function of the weights.", "Hence, our assumption is fairly realistic in the neural net setting.", "The loss function on top of the network output is usually defined as a simple convex function; for instance, in regression, a common choice of loss function is the quadratic loss (i.e, the squared distance between the network output and the true label), which is strongly convex.", "In fact, even without assuming that the loss function is strongly convex and that the output manifold is dense, we are still able to show a fast convergence rate.", "In the updated version of the paper, we show that our algorithm with an oracle converges to stationary point globally with a fast rate, which provides insight into why APO works well.", "Q: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?", "APO is robust to the initial learning rate of the base optimizer, using the default meta learning rate suggested in our updated paper.", "We have added a section to the appendix in which we include RMSprop-APO experiments on Rosenbrock, MNIST, and CIFAR-10 to show that the training loss, test accuracy, and learning rate trajectories are nearly identical when starting with initial learning rates {1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7}, spanning 5 orders of magnitude.", "Note that 1e-2 is quite a large initial learning rate for RMSprop."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 15, "sentences": ["We thank the reviewer for the valuable feedback!", "The suggestion comments were very helpful and led to a clear improvement of our manuscript.", "We reply to the answers and comments in the order they were raised:", "(1) While indeed we need more samples of weight matrices than e.g. for applying VI for BNNs for due to the input dependency, we do not believe this makes our method unscalable to real world scenarios.", "Note, that input dependent samples are also needed in the variational training of VAEs (where the number of hidden variables is of course much smaller than the number of weight parameters in our setting).", "While we present the training algorithm naively in an online version for clearness in Algorithm 1, in practice mini-batching can be done efficiently, due to the availability of batched linear algebra operations, at least in the framework we use (PyTorch), e.g. torch.bmm, broadcasting semantics, etc.", "For convolution layers, we can simply use a different type of mixing distribution, e.g. a fully-factorized multivariate normal instead of matrix-variate normal.", "(2) Thank you very much for the pointer to VIB!", "We have added a section in the updated manuscript to compare the objective of CDNs with that used in VIB and VI for Bayesian neural networks (see new Section 4).", "Furthermore, while we  always used 1 sample during training in the original submission (which indeed makes the CDN an instance of VIB) we now added experiments using 10 samples (see Section 6.4) in an experimental analysis of the different objectives.", "The results show that the CDN objective produces superior results compared to VI and VIB.", "(3) Of course! We have moved the test accuracy (which previously was only given in the Appendix and thus hard to find) to the legends of the plots to make it more easily accessible.", "CDNs give better uncertainty estimates while still having similar predictive power compared to the baselines.", "(4) Thank you for the great suggestion.", "We performed the following 2 experiments for the revised version: First, we picked a weight of a CDN trained on a toy regression experiment (with heteroscedastic  noise) at random and visualized its conditional distributions given different values of x. We found that the means and variances vary for different x.  Furthermore, we picked a weight of a CDN trained on a toy classification dataset (created by sampling x ~ 1/2*N(-3, 1) + 1/2*N(3, 1), and assign y=0 if x comes from the first Gaussian and y=1, otherwise) at random and visualized its marginal distributions.", "We found that CDNs indeed capable of learning multimodal weight distribution and to learn input specific mixing distributions.. We detail this in Appendix G.", "(5) We found that the regularization term has a significant impact on the quality of the prediction and the uncertainty estimate (we found that the uncertainty estimates are worse with small \\lambda).", "It makes sure that the variance of \\theta is not shrinking too much, i.e. encouraging the mixing distribution to be close to the prior implies it should have similar variance to the prior (which was chosen to be large).", "Naturally, the coefficient \\lambda controls this behavior: as \\lambda increases the validation accuracy is decreasing while the uncertainty is increasing (and vice versa).", "This gives rise to the selection heuristic for \\lambda we applied: pick the highest \\lambda that still gives high accuracy on the validation set (e.g. > 0.97 in MNIST).", "We found that this works very well in the experiments we did (on OOD and adversarial examples).", "Furthermore, indeed CDNs are rather designed to capture the (heteroscedastic) aleatoric uncertainty.", "We have revised the toy experiments to better account for that.", "However, curiously, CDNs also work well in tasks that are usually shown as prime examples of epistemic uncertainty, e.g. OOD classification and adversarial attack.", "(6) Thank you for this feedback.", "You are right! We have revised the baseline experiments with Bayesian models so that they either use \\lambda = 1 or the settings that the original authors recommended, i.e. we only tune \\tau in KFLA and set \\tau = 0.01 in noisy-KFAC as these are the settings suggested in their respective publications.", "Note, that the conclusions keep unchanged.", "References:", "[1] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016.", "[2] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Variational dropout and the local reparameterization trick.\" Advances in Neural Information Processing Systems. 2015"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 16, "sentences": ["Thanks for your comments.", "We are sorry to say that we miss some previous works, especially the ECCV one. And we will provide more literature review in our updated paper.", "We admit that the computation process of F-pooling and the ECCV method is the same.", "However, we defend the novelty of our work.", "The values of our work are not how the output of F-pooling is computed.", "Instead, the values are the strict definition of shift-equivalence and the theoretical properties of F-pooling.", "In previous works, they even don\u2019t give an operable definition shift-equivalence when down sampling involved.", "Please refer to our general response for more of F-pooling\u2019s values.", "Moreover, we discuss some practical problems of F-pooling.", "Such as how to deal with the imaginary part and the zero-padding of convolutions.", "With suitable settings, the shift consistency of F-pooling is much better."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 17, "sentences": ["Thank you for your thoughtful review. We will address your concerns in turn.", "Q1: The degradation function F is challenging to obtain in real world scenarios.", "A1: Many state-of-the-art approach, including SRCNN and SRGAN, has their own implicitly defined degradation function.", "They use their function F to generate training samples during their training process, while we use our explicitly defined function F during the inference process.", "If the assumed degradation function F is not exactly the function in real scenarios, both these state-of-the-art approach and our method will suffer.", "So it is unfair to criticize our motivation just because we explicitly write out the degradation function F.", "Q2: TV can also be applied for different restoration tasks, and it is easier to be optimized.", "A2: We think you underestimate the difficulty of those restoration problems.", "Please check the degraded images in Table 3.", "These images are damaged so badly that TV cannot recover any meaningful thing.", "As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 18, "sentences": ["We thank the reviewer for the detailed review.", "Below we address the main concerns.", "--------------------------------------------------------------------------------------------------------------------------------", "Q:\u201dso", "framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail\u201d", "A: We agree with the reviewer and will change our wording accordingly.", "--------------------------------------------------------------------------------------------------------------------------------", "Q:\u201dI would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from\u201d.", "\u201cthe two results coincide for the exchangeable case\u201d", "A: We agree with the reviewer that such a discussion will be helpful to the reader. We will add such a discussion (in addition to the short discussion at the end of Appendix 1).", "--------------------------------------------------------------------------------------------------------------------------------", "Q: Comparison to popular graph convolution methods (GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.).", "A: As discussed in our response to Reviewer 2, We will add a theoretical result that shows that our model is at least as powerful in terms of universality as [Kipf & Welling ICLR 2017]."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_none", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 19, "sentences": ["Thank you for your comments!"], "labels": ["rebuttal_social"], "confs": [1.0]}
{"abstract_id": 20, "sentences": ["Dear Reviewer 3, thank you for taking time to read and review our paper and for your useful comments.", "Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.", "> i) \u201cThe proposed architecture is mainly adopted from the graph attention networks (Veli\u010dkovi\u0107  et al. 2017) and the relational graph networks (Schlichtkrull et al. 2018).", "Such a simple combination is a good attempt to incorporate both node features and edge features but the novelty is quite limited.\u201d", "We concede that the modifications to the existing models is a minor contribution.", "We would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.", "We plan to make our implementation public to aid research in the area.", "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs (Zhang et al. 2018) for non-relational graphs.", "We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance the paper\u2019s contribution.", "> ii) \u201cIn table 2, I don\u2019t really see any promising results compared to baselines. There are little improvements over the baselines or even significantly worse.\u201d", "We agree that the results do not improve on those in Schlichtkrull et al. (2017) or Wu et al. (2017) in a significant way.", "This statement especially in light of the more thorough experiments conducted since - we have performed the same level of hyperparameter optimisation to an RGCN model and have found that the there is no gap between RGCN and sum-style attention RGAT.", "These results will be included in the new manuscript.", "We also feel that some of the results being \u201csignificantly worse\u201d is one of the main contributions of our paper.", "Specifically, the results on MUTAG for RGAT with sum attention are quite a lot lower than its RGCN counterpart.", "This was an unexpected result for us as GAT-style attention was anticipated to handle the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].", "The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.", "On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.", "> iii) \u201cCould you explain why your MUTAG is now a single graph and is cast as node classification problem?\u201d", "The MUTAG dataset in its standard form from e.g. TUD Benchmark Data Sets for Graph Kernels you are absolutely correct in your description.", "There is an alternative dataset version of mutag distributed with dl-learner [1] that is leveraged in the approach of RDF2Vec from Ristoski and Paulheim 2016 [2].", "In this scenario, MUTAG is in presented in the RDF, where each of the 340 presented complex molecules is described by the links in an RDF with edges (triples) of the form", "d1 -> hasAtom -> d1_1", "d1 -> hasBond -> bond1", "d1- > hasStructure -> ring_size_6-1", "where in this case, d1 is the complex molecule, d1_1 is the first atom in complex molecule d1, and bond1 is the first bond in complex molecule d1, and so on.", "There are many more types than this, and are viewable in the .owl located at [2].", "Nodes correspond to entities from the point of view of RDF.", "Each complex molecule is given a label according to whether it is mutagenic or not (determined from an isMutagenic property in the RDF - this is removed from the dataset before training/evaluation).", "This corresponds to 340 labels in total and constitutes a semi-supervised node-classification task (nodes corresponding to anything other than the complex molecule itself are unlabelled).", "Each molecule can be viewed as a separate graph, or the RDF can be viewed as a single graph comprising of a description of all molecules.", "If we train on batches of separate graphs and can fit all of the molecule graphs in memory, then these two are the same from the point of view of the training objective.", "[1] https://github.com/SmartDataAnalytics/DL-Learner/tree/develop/examples/mutagenesis", "[2] https://www.semanticscholar.org/paper/RDF2Vec%3A-RDF-Graph-Embeddings-for-Data-Mining-Ristoski-Paulheim/844d502387a3996f167b04e2e83117c30c22e752"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 21, "sentences": ["1- The idea of using concepts to represent a problem is simple, but using it along with neural network based embedding gives us the opportunity to gain concept continuity as discussed on the last paragraph on page 7 and table 2, which is an active field of research in education.", "The focus of this work is on problem embedding and its application in a recommendation system that uses problem embedding to project students\u2019 performance for the problems they solved onto the problems that they have not solved yet.", "Using the evaluation on unseen problems, a problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time we cover all the concepts necessary for them to learn.", "In the meanwhile, we got the interesting idea of negative pre-training on training with imbalanced training data and tested our hypothesis and included in the paper.", "Due to space limit, we did not include the literature review and comparison of other methods in terms of memory use and training complexity, but you can find them in the response of a previous comment below titled \u201cResponse to Question on Negative Pre-Training\u201d on this page to see the comparison.", "We can include the literature review for training on imbalanced data sets as well as comparison of other methods with negative pre-training in terms of memory use and training complexity in the final version.", "In summary, a) oversampling extremely suffers from over-fitting, b) SMOTE method that generates synthetic data sample is not feasible in word space, so the generated synthetic data (that are mathematical problems) are not of use for our training purpose, c) borderline-SMOTE both suffers from the same issue as SMOTE and its high complexity for finding the pairwise distance between all data samples, which is a burden in high dimensional data, and d) hybrid methods need m >> 1 weak learners in contrast to negative pre-training that uses a single learner.", "Memory use and training time is an issue for hybrid method when the weak learners are deep neural networks with too many parameters.", "We are currently running a broader experiment for negative pre-training on other data sets to gain more insight on it, but for the purpose of the task proposed in this work, it outperforms one-shot learning, which cannot be said that is the state-of-the art, but is a common practice. There is no notion of state-of-the-art in training on imbalanced data sets since due to our best knowledge, there is no method that outperforms all the other ones, and the performance of different methods depends more on the nature of the data set.", "2- The data set being small", "is the nature of the application since creating mathematical problems is a creative process, so it is hard to have a very big data set.", "The Prob2Vec method is performing well on this not relatively big data set, which is our goal, but if we have a bigger data set (as we have right now with more than 2400 problems), Prob2Vec may even have a better performance since with more data we can have a more precise concept and problem embedding.", "3- Thanks for your suggestion.", "4- It is difficult for humans to determine a similarity score consistent across a large enough training set, so it is not feasible to simply apply supervised methods to learn a similarity score for problems.", "Even if problem-problem similarity annotation is feasible, a lot of effort should go into the annotation, which is not scalable."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 22, "sentences": ["We thank the reviewer for the comments on our paper.", "- We have included the result concerning a noisy oracle for the F_p moment estimation problem in the paper.", "- We like the question of minimizing the number of oracle calls.", "This is an interesting open problem and we intend to explore it in future work."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 23, "sentences": ["Thanks so much for your valuable review comments!", "Following your suggestion, we evaluated the Byzantine settings Multi-Krum (Blanchard et al 2017) and Bulyan (El Mhamdi et al 2018 ICML).", "For both DBA and centralized attack we use the aggregation rule that can tolerate f Byzantine workers among n workers", "(Blanchard et al 2017)", ".", "For centralized attack there is 1 attacker and n-1 non-Byzantine workers.", "For DBA there are f distributed attackers and n-f non-Byzantine workers.", "The total number of poisoned pixel amounts are kept the same.", "1. Multi-Krum", "- To meet the assumption that 2f + 2 < n, we set  (n=10, f=3) for loan and (n=12, f=4) for image datasets.", "The Multi-Krum parameter m is set to m=n-f.", "For Tiny-imagenet we decrease the poison ratio to 5/64 for both attacks.", "Other parameters are the same as described in the paper.", "- For CIFAR and Tiny-imagenet, we find that DBA is more effective.", "- For LOAN and MNIST, both attacks don\u2019t behave well.", "We believe the reason can be explained by the fact that Loan and MNIST are simpler tasks and benign clients quickly agree on the correct gradient direction, so malicious updates are more difficult to succeed.", "2. Bulyan", "- We use Bulyan", "based on the Byzantine\u2013resilient aggregation rule Krum", ".", "To meet the assumption that 4f + 3 <= n, we set  (n=15, f=3) for loan and (n=20, f=4) for image datasets.", "- For CIFAR, DBA is more effective.", "- For other datasets, both attacks fail.", "However, we note that our distributed and centralized backdoor attacks are not optimized for Byzantine setting.", "We believe it\u2019s worthwhile to explore the distributed version of other new attack algorithms, e.g. A Little Is Enough (Baruch et al 2019) that manipulates its update to mitigate Krum and Bulyan defenses.", "In summary, Multi-Krum and Bulyan have stricter assumptions on the proportion of attackers than RFA and FoolsGold.", "In addition, while RFA and FoolsGold still assign potential outliers with extreme low weights, Krum (Multi-Krum, Krum-based Bulyan) directly removes them, making it impossible to inject backdoors if the malicious updates are obviously far from the benign updates.", "The centralized attack for four datasets totally fails under Multi-Krum and Bulyan while DBA can still succeed in some cases.", "We have included these results in Appendix A.6 of the revised version."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 24, "sentences": ["We very much appreciate your valuable comments, efforts and times on our paper.", "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "Q1. Comparison with [1, 2, 3, 4].", "The main difference between our method and [1, 2] is that we do not directly train the Gaussian mixture model, i.e., generative classifier but we post-process it on hidden feature spaces of pre-trained deep models.", "In addition, we study a robust inference method to handle noisy labels in training samples, while they did not.", "Next, [3,4] also assume clean training labels, and aim for detecting abnormal test samples after \u2019clean\u2019 training.", "Therefore, a comparison with [1, 2, 3, 4] is not straightforward as our goal is different.", "We clarified this in Section 2.1 of the revised draft.", "Q2. Computational cost.", "As you expect, estimating the parameters of LDA is very cheap compared to training original deep models like ResNet and DenseNet, since it requires only one forward pass to extract the hidden features.", "Q3. Version of backward/forward losses.", "As mentioned in Appendix B of the previous draft, we use the estimated noise transition matrices for backward/forward losses.", "We clarified more details of experimental setups in Appendix B of the revised draft.", "Q4. Updated abstract and performance evaluation.", "As AnonReviewer 3 mentioned, our main contribution is developing a new inference method which can be used under any pre-trained deep model.", "In other words, our goal is not outperforming the performance of prior training methods and complementary to them, i.e., our inference method can improve the performance of any prior training methods (see our common response to all reviewers).", "Nevertheless, we agree with your comments that it is more meaningful to emphasize our improvement over the state-of-the-art training methods.", "In the abstract of the revised draft, we report our improvement over Co-teaching [5] which is the most recent and state-of-the-art training method.", "Q5. Evaluation on adversarial attacks.", "In the revised draft, we also consider optimization-based adaptive attacks against our method under the black-box setup (see Table 5) and the white-box setup (see Table 10).", "In both setups, our inference method is shown to be more robust compared to the softmax inference.", "We further show that our method further improves the robustness of deep models optimized by adversarial training (see Table 6 and 11).", "Such experimental results support our claim that the proposed generative classifier can improve the robustness against adversarial attacks as it utilizes multiple hidden features (i.e., harder to attack all of them).", "We very much appreciate your valuable comments again.", "[1] Wen, Y., Zhang, K., Li, Z. and Qiao, Y., A discriminative feature learning approach for deep face recognition. In ECCV, 2016.", "[2] Wan, W., Zhong, Y., Li, T. and Chen, J., Rethinking feature distribution for loss functions in image classification. In CVPR, 2018.", "[3] Lee, K., Lee, K., Lee, H. and Shin, J., A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. In NIPS, 2018.", "[4] Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Houle, M.E., Schoenebeck, G., Song, D. and Bailey, J. Characterizing adversarial subspaces using local intrinsic dimensionality. In ICLR, 2018.", "[5] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: robust training deep neural networks with extremely noisy labels. In NIPS, 2018.", "Thanks a lot,", "Authors", "Dear AnonReviewer2,", "We hope that you found our rebuttal/revision for you and other reviewers in common.", "If you have any remaining questions/concerns, please do not hesitate to let us know and we would be happy to answer.", "Thank you very much,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_summary", "rebuttal_answer", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 25, "sentences": ["We thank the reviewer for their review.", "\u201cThe paper makes use of a result from the David MacKay textbook which defines the capacity of a single layer network to memorize the labelling of $n$ inputs in $d$-dimensional space. [...] It would be great if the paper also made some attempt to consider these connections. Or at least comment on how these factors could be incorporated into a more sophisticated analysis of the capacity of a network.\u201d", "We agree with the reviewer that our analysis of capacity in section 3 does not take into account the magnitude of the weights, nor the dependence on the depth of the network.", "Our objective in this section was to provide a empirical lower-bound on the capacity by designing a setup where we can vary the quantity of information contained in a dataset (in our case, N choose n), and evaluate empirically the effect of data augmentation.", "In relation to section 5, we aim at seeing how much a network can remember if it is explicitly trained to remember a given set of images.", "We understand the limitations of MacKay's analysis, which was presented to give a rough theoretical comparison point to our empirical evaluation.", "We will clarify this in the paper and improve the discussion along the lines discussed by the reviewer.", "\u201cThere is a slight oxymoron in the premise of the first set of experiments.", "The network is forced to memorize a set of positive examples relative to the negative set it sees during training. What is memorized I presume depends a lot on the negative set used for training (its diversity, closeness to the positive set and how frequently each negative example is seen during training).", "[...] Is there a training task which would allow one to more explicitly memorize the image (some sort of reconstruction task) as opposed to an in/out classification task?\u201d", "In these experiments, the set of positive and negatives is fixed (when varying data augmentation and architectures).", "During training, we feed to the network all positives and an equal number of negatives during each epoch.", "The performance does indeed depend on the closeness of the positive and the negatives, but this is similar to the membership inference problem presented in section 5, where it is difficult for a network to tell apart a seen image from an unseen, very similar image.", "A reconstruction task would suffer the same problems: the reconstruction is only approximate so we would need to evaluate the distance between our reconstruction, positives and negatives, which also depends on the closeness between positives and negatives.", "Also, the reconstruction task would need to remember the values of all pixels which requires more capacity.", "We agree that this specific deserves a short discussion and will add it to the paper.", "\u201cThis paper is a slightly difficult read [...] because there is not one main coherent argument or goal for the paper.[...]. Yes the different sections are related but it is does not feel like they build upon each other to help form a clearer picture of memorization within neural networks.\u201d", "The general goal of the paper is to empirically assess memorization in neural networks, and in particular the important question of implicit memorization, which is important for privacy: does a network trained for classification remember an image, or a set of images ?", "This aspect is empirically evaluated in sections 4 and 5, and section 3 is a preliminary study of the memorization capabilities for systems explicitly trained to memorize (this serves as a qualitative upper-bound for implicit memorization).", "We acknowledge that Section 3, which is a preliminary analysis, may confuse the reader in the first place.", "We decided to move it to an appendix after reading the feedback from the three reviewers.", "\u201cThe conclusions focus on the importance of section 3 and the results of the experiments performed. Do the conclusions accurately reflect the opinions of the author?\u201d", "We do not consider the conclusions of the experiments from Section 3  to be more important than those of the other sections, in fact quite the opposite.", "As mentioned above, we will move it to appendices.", "\u201c[...]In section 3 is a perturbed positive image considered a positive training image? And in the testing phase are only unperturbed versions of the positive images given to the ConvNet as input?\u201d", "When data augmentation is used, we consider that perturbed positive images are also positive images.", "In the testing phase, perturbed versions of the positive images are given to the ConvNet.", "\u201cLast paragraph page 4: \"when the accuracy gets over 60\\% and at 90\\%\". Is this training or validation accuracy?\u201d", "We decrease the learning rate when the training accuracy reaches these thresholds.", "We thank the reviewer for reporting typos, we will correct them in the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 26, "sentences": ["Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!", "We answer your questions and concerns in the following.", "> \"However, I do not understand how are the *discrete* output y is handled.\"", "For this toy problem, we represent labels y by standard one-hot encoding, and we directly regress one-hot vectors using squared loss instead of softmax.", "This allows us to input one-hot vectors into the inverted network to generate conditional x-samples.", "> \"I\u2019m not sure I understand what we are supposed to learn from the astrophysics experiments.\"", "We included this experiment to demonstrate that we are able to find multi-modal posteriors in a second real-world setting relevant to natural science.", "> \"INN outperforms other methods [...] over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better\"", "We indeed consider the calibration errors (reported in Sec. 4.2 (\u201cQuantitative results\u201d) and Appendix Sec. 6) the most meaningful of these comparisons, because they directly measure the quality of the estimated posterior distributions, and INNs have a clear lead here.", "We will add these numbers to Table 1 to emphasize their importance.", "> \"However, the real-world experiments are not necessarily the easiest to read.\"", "We understand, although we tried our best to condense the complicated nature of these applications.", "For the astrophysics setting, we provide more information in the appendix, Sec. 5, and for the medical application we refer to [1] for full details.", "[1] Wirkert et al.: Robust near real-time estimation of physiological parameters from megapixel multispectral images with inverse monte carlo and random forest regression. International Journal of Computer Assisted Radiology and Surgery, 2016.", "(https://link.springer.com/article/10.1007/s11548-016-1376-5 )", "We have uploaded a revised version of the paper, thank you again for your suggestions.", "The changes and additions are highlighted in red font for convenience.", "Please also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.", "If this presents a problem, we can attempt shorten the paper accordingly."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 27, "sentences": ["We thank reviewer #2 for the useful feedback.", "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added theorems and more formal statements in the main text,  compressed the appendix and enhanced the description of the experimental setup.", "We have updated the paper and kindly ask the reviewer to take another look.", "Robustness of the curvature sampling method: we provide confidence intervals in Table 2 of our results.", "We learn curvatures for each of the component spaces and show learned values together with confidence intervals in Appendix E. It can be seen that these variances are in the low regime."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 28, "sentences": ["Thank you for the comments!", "To review\u2019s feedback:", "- We pay attention to the term \u201cskill discovery\u201d and made it more clear about the connection between prior works and the current work in the revised version.", "Our method can also be combined with DIAYN to learn the skill-conditioned policy as mentioned in the paper.", "- We added both a theoretical connection and new experimental results to compare MISC and the empowerment method in the revised version.", "In the navigation tasks, we show that our method outperforms the empowerment method.", "- An intuition for why I(s_c, s_i) could be superior to I(a, s_i) is that in robotic tasks, the mutual information between the robotic sates, s_c, and the object states, s_i, could be easier to be estimated than the mutual information between the action, a, and the object states, s_i, as shown in Figure 4 in the paper.", "Therefore, the agent receives a higher MI reward more easily and learns to control s_i more efficiently.", "The context states can be seen as the summary information of the agent\u2019s action and the transition model of the environment, which could be more relevant in terms of estimating the object states in comparison to the agent\u2019s actions.", "- VIME and PER are used as described in their original papers.", "- We have added an appendix to provide more information about experiment details.", "- We also newly evaluated our method on gazebo-based robotic simulations, including the cases when there is no object, a single object of interest, and multiple objects of interests.", "A video showing new experimental results is available at https://youtu.be/l5KaYJWWu70?t=104", "In this experiment, we also compare MISC with two additional baselines, including ICM and empowerment (with state of interest), see Figure 4 in the paper.", "- We now mention that the states of interests vs context are given in the revised paper.", "However, when they are not given. They can also be automatically learned/selected by iterating over all possible combinations", ".", "Afterwards, an optimal combination can be chosen by the user via testing in the task at hand."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 29, "sentences": ["> I found the paper interesting to read and well written.", "The main contribution (backpropagating analytic gradients through imagined trajectories?) could potentially be highlighted more but otherwise the paper was clear.", "Thank you.", "Correct, our main contribution is to learn long-horizon behaviors by propagating analytic value gradients through imagined trajectories.", "Moreover, we show that this yields a scalable algorithm that solves control tasks of higher difficulty than was previously possible using model-based agents.", "> I wonder if the authors ever looked at how much the size of the latent vector determines the performance of the system? Is there an optional latent vector size across domains or is that optimal size task dependent?", "For our experiments, we used the same hyper parameters across all tasks, including the state size.", "We conducted an additional experiment where we trained Dreamer with latent states of 100, 200, 300, 400, 500 deterministic units and 10, 20, 30, 50, 100 stochastic units.", "We find that all sizes equal to or larger than the 200 and 30 used in our main experiments yield very similar performance, while smaller sizes result in suboptimal scores on some of the tasks, hinting at insufficient model capacity.", "> Additionally, how much variance is there in the imagined trajectories from a certain starting state? In other words, are the endpoints of most imagined trajectories similar or very different?", "We have not studied this quantitatively.", "Qualitatively, we find more diversity in the stochastic multi-step predictions near states that are more challenging to predict.", "For example, this includes collisions with the ground, the unstable equilibrium of an upright balanced pendulum that could either rotate left or right, or cheetah balancing on its front feet which might flip over on its back or fall back on its feet.", "> There is actually not too much for me to critique and I would suggest this paper should be accepted.", "Thank you."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_accept-praise"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 30, "sentences": ["We thank the reviewer for the positive comments.", "Below we address the main concerns.", "Q: \u201dApplying the model of Hartford et al. to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation... Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?\u201d", "A: Our goal in performing the synthetic experiments was to quantify the expressive power that is  gained by adding our basis elements to [Hartford et al. 18].", "We felt it is an informative experiment since [Hartford et al. 18] also discuss applying their model in the jointly exchangeable setting (page 3, second column, top paragraph).", "Having said that, we agree with the reviewer that [Hartford et al. 18] probably cannot handle such tasks by construction. As we mentioned in our response to Reviewer1 we will change the wording of this section to better reflect that this is *not* a failure of Hartford et al. but merely a setting outside their scope due to a different assumption on the symmetry group of the data.", "If the reviewers feel strongly about this experiment, we are open to replace it with a discussion.", "--------------------------------------------------------------------------------------------------------------------------------", "Q: \u201cSome of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al\u201918 and references in there) are missing\u201d.", "A: We did our best to survey and compare to the most related works on the dataset collection introduced in [Yanardag & Vishwanathan 2015].", "These datasets contain graphs from multiple origins, where some of them consist of highly varying graph sizes (within the same dataset).", "In any case we will make the code available as soon as possible.", "--------------------------------------------------------------------------------------------------------------------------------"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_by-cr", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 31, "sentences": ["Glad to know that you like our paper!", "1) Difference from parameter noise except for memory consumption:", "As stated in Section 3.3, we believe NADPEx is a generalization of parameter noise, with not only flexible memory consumption but also lower variance in gradients.", "This theory is examined in Section 4.2, where NADPEx shows faster convergence and lower variance in performance with different random seeds.", "Besides, comparing with [1], our work provides a theoretical modeling for the idea \"a hierarchy of stochasticity for exploration\".", "We model the NADPEx policy as a joint distribution of dropout random variables and actions, such that it could be combined seamlessly with existing on-policy policy gradient methods.", "One example is the policy space constraint stated in Section 3.2.", "We also provide another distribution i.e. Bernoulli distribution for stochasticity at high level, for which we derive gradient alignment and policy space constraint, as well as empirical results.", "As a minor point, in [1], the stochasticity at the high level i.e. the variance of parameter noise, is adjusted in a heuristic manner.", "NADPEx, in contrast, aligns the stochasticity throughout the hierarchy with end-to-end gradient update.", "2) Other good side effects:", "The robustness of the NADPEx policy is orthogonal to our current work, but will be an interesting direction for the future.", "Currently we only have some preliminary results.", "For example, it is more robust to adversarial neural attacks.", "In the future we will investigate how robust NADPEx policies could be when the environment is perturbed, e.g. agents are dragged slightly by humans as in [2, 3].", "That temporally consistent exploration is fairly important for physical robots is one of our motivations for this whole project.", "In the next step we will look for simulator environments with more authentic actuators to see how NADPEx could help solve that.", "Our ultimate goal is to find a safer and more efficient way for on-policy exploration on physical robots.", "We believe the application of NADPEx to off-policy exploration is straightforward.", "However, as stated in Section 1, off-policy methods benefit from stronger flexibility for experience sampler.", "This makes the gradient alignment and policy space constraint not as important as in the on-policy methods.", "As off-policy methods have the potential to be much more data-efficient, we will compare in the future how NADPEx performs comparing with auto-correlated noise in [4] and separate sampler in [5].", "[1] Plappert et al., \"Parameter Space Noise for Exploration\", ICLR 2018.", "[2] Tassa et al., \"Synthesis and stabilization of complex behaviors through online trajectory optimization\", IROS 2012.", "[3] Clavera et al., \"Learning to Adapt: Meta-Learning for Model-based Control\", arXiv 2018.", "[4] Lillicrap et al., \"Continuous control with deep reinforcement learning\", ICLR 2016.", "[5] Xu et al., \"Learning to explore via meta-policy gradient\", ICML 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_future", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 32, "sentences": ["We thank reviewer 1 for the detailed feedback.", "In this response, we clarify the accuracy-realism trade-off, revise the accuracy metrics, indicate reruns and new experiments, and address the individual questions.", "We updated Section 4.4 to indicate that it is to be expected that, although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG).", "A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.", "In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.", "Although the SVG generator is simpler than ours, ours is just a simple variation from Ebert et al. (2017).", "Since proposing a strong generator architecture is not the goal of this paper,", "any video generator (including the one from Denton & Fergus (2018)) could be used with our losses.", "We added this clarification to Section 3.4.", "Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).", "It's also worth noting that with a simpler feed-forward posterior and a unit Gaussian prior, our VAE ablation and SVG achieve similar performance on various metrics.", "We added Section 3.5 to point out the differences between the VAE component of our model and prior work.", "We have included a revised plot in Figure 14 (note that this temporary plot will be incorporated into Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the LPIPS metric (Zhang et al., 2018).", "LPIPS linearly calibrates AlexNet feature space to better match human perceptual similarity judgements.", "Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS.", "After examining the KTH results further, we realized that our results are likely weaker than they should have been, because we did not use the same preprocessing as prior work.", "The experiments from our original submission cropped the videos into a square before resizing, and thus discarded information from the sides of the video.", "We are currently rerunning the KTH experiments and we plan to update the results in the paper.", "We also didn't choose particular hyperparameters to ensure diversity for our models, and we expect some improvement in diversity in the new sets of experiments.", "Although the combination of VAEs and GANs have been explored recently for conditional image generation (Zhang et al. 2018), the video prediction task is substantially different, with unique challenges, due to spatiotemporal relationships and inherent compounding uncertainty of the future.", "Furthermore, while the individual components have indeed been known for video prediction, their combination is novel and not present in prior work, and we demonstrate that this produces state-of-the-art results in terms of diversity and realism.", "In addition, this work provides a detailed comparison of the effect of the losses on the various metrics.", "Furthermore, we are currently running experiments for various weightings of the KL loss and the adversarial loss, and we plan to include additional results that illustrate the trade-offs based on these hyperparameters.", "Although MoCoGAN performs well for videos with a single frame-centered actor, it struggles with multiple simultaneously moving entities.", "The authors of MoCoGAN also mentioned in personal correspondence that the conditional version (i.e. video prediction) was significantly harder to train.", "We noticed the same in earlier iterations of our model.", "In our case, we found that the model would degenerate to static videos or videos with a cyclic flickering artifact, which are issues that aren't a problem in conditional image generation.", "We added details to Section 3.4 describing the importance of a few components, such as spectral normalization and not conditioning the discriminator in the ground-truth context frames.", "The purpose of adding adversarial losses to a pure VAE is to improve on blurry predictions where the latent variables alone cannot capture the uncertainty of the data.", "However, that is typically not the case of synthetic datasets.", "In early experiments, we trained our pure VAE model on the stochastic shape movement dataset from Babaeizadeh et al. (2018), and our pure VAE was able to model the dataset without any blur and with perfect separation of the possible futures.", "We agree that plausibility is indeed important, and that's what our human subject studies try to capture.", "Since we provide predictions of the whole sequence to the human evaluator, we are not only evaluating for image realism but also for plausibility of the dynamics.", "Unlike the VAE models that implausibly erase the small objects that are being pushed in the BAIR dataset, our SAVP model moves those objects in a more plausible way.", "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077", "[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. 2018 PIRM Challenge on Perceptual Image Super-resolution. In Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018.", "https://arxiv.org/abs/1809.07517"], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 33, "sentences": ["We thank the reviewer for their helpful comments.", "Please could the reviewer clarify why they felt our work muddies the debate regarding large-batch training?", "We demonstrate that one can initially increase the batch size with no loss in test accuracy by simultaneously increasing the learning rate.", "However for very large batch sizes the test accuracy degrades under both constant epoch and constant step budgets.", "We agree that some of our observations under constant epoch budgets in sections 2 and 3 have been made in previous work.", "However there are also several important differences:", "1.", "Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).", "As we show in sections 4 and 5, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.", "A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.", "To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).", "2. Zhang et al. argued that Momentum only helps in the large batch limit.", "However, their analysis is based on the noisy quadratic model, which cannot explain the results we observed on the test set in sections 4 and 5.", "These experiments clearly demonstrate that, unlike the SDE perspective, the noisy quadratic model is not an appropriate model for predicting test set performance in deep learning.", "Their work also does not clarify the assumptions under which linear scaling of the learning rate should arise.", "3. Our empirical results in section 3 are similar to Shallue et al., however their work argues that there is no reliable relationship between learning rate and batch size.", "We draw a very different conclusion: the learning rate usually obeys linear scaling, but linear scaling only holds theoretically when the assumptions we specify are satisfied.", "Linear scaling may not hold in cases where these assumptions break down (e.g., language modelling).", "4. The observation that the test accuracy is independent of batch size in the noise dominated regime is a natural consequence of the SDE analogy, since any two training runs which integrate the same SDE should sample final parameters from the same probability distribution.", "We will clarify this in the updated text.", "Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.", "We apologise for this.", "It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.", "Turning to our generalization experiments in sections 4 and 5.", "It is true that a number of papers in recent years have claimed that SGD noise enhances generalization.", "However Shallue et al. recently argued no previous work had provided convincing empirical evidence for this claim.", "Indeed in their abstract, they state \u2018We find no evidence that larger batch sizes degrade out-of-sample performance\u2019.", "In another recent paper, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets.", "Crucially, to establish that SGD noise enhances generalization, one must show that small batch sizes generalize better than large batch sizes under constant step budgets, with realistic learning rate decay schedules, and one must independently tune the learning rate at each batch size.", "In section 4, we are the first authors to perform this experiment and confirm that the final test accuracy of SGD does degrade for very large batch sizes under both constant epoch and constant step budgets, contradicting the claims of both Shallue et al and Zhang et al.", "Furthermore, we show in section 5 that the optimal SGD temperature which maximizes the test accuracy is almost independent of the epoch budget.", "These results provide the first convincing empirical evidence that SGD noise does enhance generalization in well-tuned networks with learning rate decay schedules.", "We believe this is an important contribution."], "labels": ["rebuttal_social", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 34, "sentences": ["We thank the reviewer for the comments and suggestions.", "As the reviewer points out, the community currently lacks a strong theoretical understanding of minmax optimization, and we believe our work helps to fill this gap.", "We comment on the practical implications of our work below:", "1) While the exact form of the sufficiently bilinear condition may be somewhat unwieldy, the result gives concrete evidence that having higher bilinearity can aid convergence for certain algorithms, even for settings that are not purely bilinear.", "This indicates that one should pay attention to the magnitude and condition number of the off-diagonal of the Jacobian when constructing a min-max problem and choosing an algorithm to solve the problem.", "2) In non-convex-concave settings, HGD will converge to all types of stationary points, as the reviewer points out.", "We propose some modifications to HGD to allow it to work in non-convex settings in Appendix A, which essentially amount to explicitly determining the local curvature of the problem and running a modified algorithm, such as Hamiltonian Gradient Ascent, near undesirable critical points.", "This would allow us to show similar local convergence guarantees to those proven by other works in the area (see Appendix A).", "However, as the reviewer points out as well, the HGD analysis is also useful because it implies similar convergence results for CO, which is a practical algorithm.", "3) Our result for CO shows that as long as $\\gamma \\ge 4L_g/\\alpha$, then CO will converge in sufficiently bilinear settings (currently it\u2019s written as $\\gamma = 4L_g/\\alpha$ but we will change this in the final version).", "This indicates that increasing $\\gamma$ may speed up convergence when we are in a sufficiently bilinear region (and in particular, the algorithm may not converge if $\\gamma$ is too small and the region has a very large bilinear term).", "If $\\gamma$ is too large, CO will converge to stationary points that are not local min-maxes, so these two phenomena must be traded off.", "One could potentially detect which regime one is in by computing a few eigenvalues of the Jacobian (using a logarithmic number of Hessian-vector products) during or after training.", "Other comments:", "-We thank the reviewer for pointing out Azizian et al. 2019.", "This work was released concurrently to ours on Arxiv and indeed seems to have some similar findings.", "We will include a reference to it in our revised version.", "-We thank the reviewer for the comment on notation and will incorporate it into the final version.", "References:", "Azizian, Wa\u00efss, et al. \"A Tight and Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games.\" arXiv preprint arXiv:1906.05945 (2019)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 35, "sentences": ["Many thanks for the detailed review!", "Main comments:", "1/ The DIP approach critically relies on regularization in order to make the method work (both by adding random noise in each optimization step to the input, as well as early stopping).", "As the first reviewer noted ``In fact, the DIP of Ulyanov et al. can hardly be considered \"a model\" (or a prior, for that matter), and instead should be considered \"an algorithm\", since it relies on the early stopping of a specific optimization algorithm''.", "However we follow the reviewers' suggestion and made clear that the idea to use a deep network without learning as an image model is not new and rewrote the item to ``The network itself acts as a natural data model.  Not only does the network require no training (just as the DIP); it also does not critically rely on regularization, for example by early stopping (in contrast to the DIP).''", "Before that, in the introduction, in the original and revised version, we have a paragraph devoted to the DIP explaining that Ulyanov et al. introduced the idea of using a deep neural network without learning as an image model.", "2/ Regarding the theoretical contribution: We fully agree that a limitation of the theorem is that it pertains to a one layered version of the decoder.", "We are currently extending this to the multilayer case, but still have to address a technical difficulty in counting the number of different sign pattern matrices.", "Regarding the assumptions: The proposition uses the assumption that k^2 log(n_0)  / n <= 1/32.", "Here, the constant 1/32 is not optimal.", "k^2 is essentially the number of parameters of the model, and n is the output dimension.", "The proposition is only interesting if k^2 log(n_0)  / n <= 1/20 even without this assumption (due to the right hand side of the lower bound) therefore this assumption is not restrictive.", "The bound is applicable if the number of parameters, k^2 is smaller than a logarithmic term times the number of output parameters, i.e., it allows the number of parameters to scale almost linearly in the output dimension.", "This is the regime in which the deep decoder operates throughout the paper.", "We agree that many natural noise patterns have structure, and that those can be better approximated with deep models, and are thus more difficult to remove.", "3/ We have added the sentence ``In the default architectures with $d=6$ and $k=64$ or $k=128$, we have that N = 25,536 (for k=64) and N = 100,224 (k=128)", "out of an RGB image space of dimensionality 512\\times512\\times3=786,432 parameters.'' to specify the number of parameters.", "Thanks for the suggestion to try second order method like LBFGS; we have tried LBFGS as a response to the reviewer's comment.", "It converges in significantly fewer iterations, but each iterations is so much more expensive that overall it optimizes slower than ADAM or gradient descent.", "Minor comments:", "1/ Figure 4: We have added labels and the sentence ``Early stopping can mildly enhance the performance of DD; to see this note that in panel (a), the minimum is obtained at around 5000 iterations and not at 50,000.'' in the caption to clarify.", "Also, we have added the sentence ``Models are fitted independently for the noisy image, the noiseless image, and the noise.'', and rewrote the paragraph", "Thanks for pointing this out!", "We agree that here we present only results for one image, but we did carry out simulations for many images, and those plots are qualitatively the same for all the images considered.", "Thus our conclusions about the model do not only hold for one image.", "2/ Normalization is applied channel wise.", "Let z{ij} be the j-th column in the i-th layer.", "Then z{ij} is normalized independently of any of the other channels.", "3/ We have reworded the corresponding paragraphs to make clear that while we do not use convolutions, and thus this is not strictly speaking a convolutional neural network, it shares many structural similarities with a conventional neural network, as pointed out by the reviewer.", "4/ The equation is correct in that the parameter choices in the paper are such that the deep decoder has much fewer model parameters N than its output dimension. Thus N is much less than n.", "5/ We agree that it is not optimal to use unintroduced notation at this point, but we made this compromise so that we can illustrate the performance of the deep decoder without introducing its details, but wanted to give a reader the chance to later see exactly what parameters we used.", "6/ Unfortunately choosing k=6 is too small to have a small representation error, i.e., to represent the image well.", "We have, however not hand-selected the 8 images shown out of the 64, and the other 64-8 images look very similar.", "We have all the images in the jupyter notebook that comes with the paper.", "7/ Great question, it is faster to optimize the deep decoder since the adam/SGD steps are cheaper, but it indeed seems to require slightly more iterations for best performance than the DIP."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_reject-request", "rebuttal_concede-criticism", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 36, "sentences": ["We would like to thank the reviewer for providing valuable and detailed feedback.", "We have addressed the clarity concerns in the updated paper.", "Figure captions, metrics used in the table, etc, as mentioned in the presentation section of the review have been carefully examined and updated in the paper.", "We will reorganize the experiment section to better present the comparisons under different experimental settings.", "(1) Factorized Latent Variables:", "The factorization of latent space with respect to the modalities provides a way to differentiate observed and unobserved modalities.", "Therefore, VSAE is capable of handling partially-observed data where the missing modalities can be arbitrary.", "In addition, the embeddings are intuitively more meaningful as input to unimodal encoders is now limited to only observed modalities, eliminating the effect of missing modalities.", "When performing imputation/generation, however, we want to capture the dependencies between modalities.", "In other words, unobserved modalities should be imputed based on the information extracted from observed modalities.", "For experiments, we design this by conditioning decoders on all latent variables, essentially accessing information from all observed modalities.", "This is not in contradiction to the factorized latent variable assumption.", "Instead, the encoders try to embed each modalities individually, while decoders learn the dependencies between different modalities.", "(2) Multimodal Experiments:", "We apologize for unclear description of experimental settings.", "In general, we believe multi-modal data is more general than conventional image-text or video-text pairs.", "By unifying tabular data also as multi-modal (with each attribute as one modality), we show that VSAE provides us a principled way for imputation, capable of generalizing to more data families.", "Specifically, we conducted experiments on two types of data:", "(1) low-dimensional tabular data, and (2) high-dimensional data (pixel or text) as \"multimodal\" to better define the overall task of learning from partially-observed data.", "Upon request, we have included more extensive experiments following [1] on MNIST/FashionMNIST, and [2] on CMU-MOSI/ICT-MMMO.", "Results are reported in Table 10 and Table 11 (Appendix C.5).", "As shown, VSAE consistently outperforms baseline models across the added experiments as well.", "(3) Discussions on Comparison with Upper Bound Methods:", "Models trained with fully-observed data in theory should have better performance, thus we treat them as upper bound methods.", "However, it is very interesting to observe that in some cases, VSAE have superior performances.", "One possible explanation is that missing modalities introduces extra noise into the model as regularizer, thereby, increasing the generalization ability.", "However, detailed experiments and more discussions need to be carried out to back up this explanation.", "[1] Wu et al. Multimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018.", "[2] Tsai et al. Learning Factorized Multimodal Representation, ICLR 2019."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_done", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 37, "sentences": ["We thank the reviewer for the comments.", "As correctly observed by the reviewer, Arora et. al. 2015 suffers from a bias in estimation both in the analysis and in the empirical evaluations.", "The source of this bias term is an irreducible error in the coefficient estimate (formed using the hard-thresholding step).", "NOODL overcomes this issue by introducing a iterative hard-thresholding (IHT)-based coefficient update step, which removes the dependence of the error in estimated coefficient on this irreducible error, and ultimately the dictionary estimate.", "Intuitively, this approach highlights the symbiotic relationship between the two unknown factors \u2014 the dictionary and the coefficients.", "In other words, to make progress on one, it is imperative to make progress on the other.", "To this end, in Theorem 1 we first show that the coefficient error only depends on the dictionary error (given an appropriate number of IHT iterations R), i.e. we remove the dependence on x_0 which is the source of bias in Arora et. al. 2015.", "We have added the intuition corresponding to this in the revised paper after the statement of Theorem 1 in Section 3.", "Analysis of Computational Time \u2014 We have added the average per iteration time taken by various algorithms considered in our analysis in Table~4 and Appendix E.", "The primary takeaway is that although NOODL takes marginally more time per iteration as compared to other methods when accounting for just one (Lasso-based) sparse recovery for coefficient update, it (a) is in fact faster per iteration since it does not involve any computationally expensive tuning procedure to scan across regularization parameters; owing to its geometric convergence property (b) achieves orders of magnitude superior error at convergence, and as a result, (c) overall takes significantly less time to reach such a solution; see Appendix E for details.", "We would like to add that since NOODL involves simple separable update steps, this computation time can be further lowered by distributing the processing of individual samples across cores of a GPU (e.g. via TensorFlow) by utilizing the architecture shown in Fig. 1.", "We plan to release all the relevant code as a package in the future.", "In this revision, we have added comparison to Mairal '09, a popular online DL algorithm.", "Further, we have also added a proof map, in addition to the Table 3, for easier navigation of the results."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 38, "sentences": ["Thank you for your thoughtful feedback!"], "labels": ["rebuttal_social"], "confs": [1.0]}
{"abstract_id": 39, "sentences": ["We thank the reviewer for their insightful comments.", "Q1 \"all the experiments except the last row of Table 2 concern adaptation between two domains. Given the paper title, the reviewer would have expected more experiments in a multiple domain context.\"", "A1 A main difference between domain adaptation and MDL is the fact that the former aims to minimize the target error, while the latter aims to minimize the average error.", "In this sense, our goal (and the validation experiments on Cell) are focused on MDL.", "Q2", "\"Although the paper introduces the generalization bound for MDL, it does not give new formulation or algorithm to handle MDL\" [...] \"There is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.\"", "A2 This issue is related to the above: the new generalization bound extends that of Ben David et al. in the sense that it considers all pairs of domains involved, thus bounding the *average* risk; and this bound is the one underlying the proposed algorithm and its MDL experiments.", "We have clarified this in the manuscript.", "Q3 \"the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset. \"", "A3: We added 3 domain experiments for Office, which are now displayed in Appendix E.1 table 6.", "As discussed in [3], we also find that the addition of a second source is not necessarily beneficial to target accuracy.", "Q4: Comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.\"", "A4: ADDA, an unsupervised DA method, proceeds by training sequentially a classifier on Source, then learning the Target feature space by making it indistinguishable from the Source one.", "However this is not applicable to the semi-supervised setting: either target labels would not be used in the first training step, or they would be used but without any domain loss to account for the fact that two domains are being used at the same time.", "Thus, the classifier would actually learn two sub-classifiers: one for each domain, which would turn counter-productive in the second step where this strong distinction between source and target would have to be un-learned.", "We are re-programming DSN and experimental results will be added.", "We thank the reviewer for the suggestion.", "Q5: \"The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.\"", "A5 Zhao et al. [3] consider the multiple source context; they define a weighted scheme where the weight of a source depends on its H-divergence with the target, plus its own classification error.", "The feature extractor is trained either from the best source only (in the sense of this weight), or from a weighted sum of the sources.", "When interested in multi-domain learning (thus aiming to minimize the average risk), it seems that there are two possibilities: a single feature extractor; or a feature extractor per domain.", "In the former case, the feature extractor might be overly conservative; in both cases, scalability w.r.t. the number of domains might be an issue.", "Hoffman et al. [4] also consider the multiple source context, assuming that the target is a unknown mixture of the sources (or not too far thereof in terms of Renyi divergence).", "Their experiments follow this assumption (using as target a mixture of sources Amazon, Webcam and DSLR).", "In our case this assumption does not hold, e.g. the joint distribution of England(x,y) is *not* a mixture of Texas(x,y) and California(x,y) (as can be seen by eye, and confirmed by experiments).", "The adversarial change of representation only enforces the merge of the marginals.", "Q6 \"The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier. Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples\"", "A6", "As the reviewer suggests, there are indeed misclassifications of samples using their entropy ranking in early training stages.", "We mention this section 4.3.", "This misclassification is the reason why it is better that hyper-parameter p slightly underestimates p* than is equal to it, as can be seen in Fig. 1, right (except when p*=1 as one could expect).", "Q7", "\"Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).\"", "A7 L is the cardinal of the union of classes with labeled examples in at least one domain."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 40, "sentences": ["1- There are two reasons that concept and problem embedding are performed in this work.", "Considering concept continuity is an important matter in education.", "Having concept embedding, concept continuity can be reached as is discussed in the last paragraph on page 7 and some other examples are given in table 2.", "By just having the most sophisticated concept extractor, the concept continuity cannot be retrieved.", "Furthermore, problem embedding is used by the recommender system to project the performance of students on the problems they solved onto other problems that they have not solved.", "This way, we have an idea of what problems should be recommended to them and which problems should not by having an evaluation of their ability to solve unseen problems and recommend problems in the boundary of their capacity, not way beyond, and to recommend problems in a way that covers all concepts necessary for students to learn.", "We have observed interesting patterns, e.g. similar problems are more likely to be solved correctly at the same time or wrong at the same time.", "Note that by just having the concepts of problems that are not in numerical form, performance projection may not be feasible and there is a need for using other methods like embedding.", "2- The data size being small", "is just the nature of the application.", "Creating new problems is a creative process and is not easy, given that with the insight we have on the application, the data size seems to suffice.", "Furthermore, since Prob2Vec is performing well for not a relatively big data set, it would definitely do well for big data sets since the more data we have, the more precise the concept and problem embedding are.", "The easy-tough-to-beat method proposed by Arora et al. is the state of the art in unsupervised sentence embedding that we compared our algorithm with.", "Please let us know if we missed anything.", "Pre-training is a common practice in transfer learning (one-shot learning).", "The objective function does not differ from the objective function used for post training.", "Training on only negative samples with lower training epochs than the training epochs in post training just adjusts the weights of the neural network to a better starting point.", "If the training epochs in pre-training is relatively smaller than the training epochs in post training, due to curse of dimensionality, the warm start for post training results in better performance for NN classifier.", "To make it more clear what it means to train the neural network on a pure set of negative data samples, think about batch training.", "It's not likely, but possible, that a batch only has negative or positive samples.", "In the pre-training phase of our method, we intentionally used a pure set of negative samples (with fewer training epochs) to have a warm start for post training.", "As table 3 shows, our proposed method outperforms one-shot learning.", "Please look at part 1 of our response to reviewer2 and part 2 of comment titled \"Response to Question on Negative Pre-Training\" below."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 41, "sentences": ["First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers,  and apologize for the late response due to additional experiments and modifications of the paper.", "Remark 1. \"the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).\"", "A : The purpose of our experiments is to show that the SST algorithm is comparable to the conventional SSL algorithms.", "Therefore, we experimented with the popular setting.", "The following is the experimented dataset in other papers.", "- Temporal ensembling & \u03a0 model [1]: CIFAR-10 (4k), SVHN (500, 1k), CIFAR-100 (10k)", "- VAT [2] : CIFAR-10 (4k), SVHN (1k), CIFAR-100 (no experiment)", "- Mean Teacher [3]: CIFAR-10 (1k, 2k, 4k), SVHN (250, 500, 1k), CIFAR-100 (10k)", "We took the reviewer\u2019s comment judiciously and have added CIFAR-10 (1k, 2k) experiments in Section 6.5 of the supplementary material and their accuracies are comparable with those of the conventional SSL algorithms. (It took a long time to perform 5 runs of test for all additional experiments.)", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "The result of CIFAR-10 (1k, 2k) with 5 runs", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "(error     / standard deviation) |", "1k        |        2k", "supervised                       | ( 38.71 / 0.47 ) | ( 26.99 / 0.79 )", "\u03a0 model [1]                      | ( 27.36 / 1.2 )  | ( 18.02 / 0.60 )", "mean teacher [3]", "| ( 21.55 / 1.48 ) | ( 15.73 / 0.31 )", "SST                              | ( 23.15 / 0.61 ) | ( 15.72 / 0.50 )", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "[1] Laine, Samuli, and Timo Aila. \"Temporal ensembling for semi-supervised learning.\" arXiv preprint arXiv:1610.02242 (2016).", "[2]Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. arXiv preprint arXiv:1704.03976, 2017.", "[3] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep", "Remark 2. \"Why does the SST algorithm use different epsilon policies for synthetic vs organic datasets?\"", "A : There are different network structures in the synthetic and organic dataset (table 4-5 in the supplementary materials in the new version).", "And, because there are only 12 initial points in the synthetic, it needs much higher confidence than organic datasets.", "(Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added in the organic dataset.", "However, in synthetic data, unlabeled data is added when epsilon begins at (10^(\u22125)).", "Therefore, We have changed the epsilon value so that no data is added at the beginning of the iteration.)", "Remark 3.", "What is the problem in SVHN (balance problem or dataset or both)?", "A : We have experimented with SVHN with data balancing.", "In SVHN, 1,000 images are used as the labeled data and 45,000 balanced unlabeled images are used.", "As a result, the SST is still worse than other algorithm [1].", "Therefore, we have modified our expression and added the result in Section 6.4 of the supplementary material.", "Remark 4. \"You should also show the performance of regular SSL methods in the setup on Table 4.\"", "A : We have performed experiments of self-training without threshold and SST with softmax output.", "Although the experimental setting is a bit different from [1], the setting of 100% of non-animal unlabeled data is the same.", "They have shown that the performance degraded when the unlabeled dataset contained 100% of non-animal data in figure 2 in [4].", "The approximate score in Figure 2 of [4]", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "100% out-of-class (error)", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "supervised learning : about 23.5%", "\u03a0 model : about 26.3 %", "Mean Teacher : about 26.3 %", "VAT : about 26%", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "[4] Odena, Augustus, et al. \"Realistic Evaluation of Semi-Supervised Learning Algorithms.\" (2018). ( https://arxiv.org/abs/1804.09170 )", "Remark 5. combining SST and other SSL", "A : Combining and the additional cost is expensive. Therefore, we have modified our expressions."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_none", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 42, "sentences": ["Thanks for your feedback.", "We reply to all reviewers jointly in our comments above."], "labels": ["rebuttal_social", "rebuttal_summary"], "confs": [1.0, 1.0]}
{"abstract_id": 43, "sentences": ["We thank the reviewer for their comprehensive review.", "We updated the paper with better results over more tasks, either matching or outperforming the human baseline in terms of final accuracy, and outperforming the model-free baseline in all cases.", "We also included results over multiple runs of all experiments, showing the minimum, maximum and mean accuracy.", "1. While it is true that the manually-tuned baseline we provided is simple, it is a standard practice in the field to adjust the learning rate during training and keep the rest of the hyperparameters constant.", "Adjusting all of them requires significantly more effort and is infeasible in many cases.", "2. Due to time constraints, we have not benchmarked our method against more hyperparameter-tuning baselines yet.", "We agree that it would be a very valuable comparison and leave that for future work.", "Nevertheless, please note that the human baselines we use for Transformer have been tuned by researchers using auto-tuners among other tools.", "3. [1] successfully use PPO with an LSTM policy on a challenging, partially-observable environment.", "It is equally principled to use a Transformer policy, since both would operate on the same sequence of observations.", "The SimPLe algorithm runs PPO on an MDP approximated by a powerful model that handles stochasticity well, which is also a valid approach.", "4. We updated the paper with a justification of our action discretization scheme.", "Such a discretization has a number of benefits, including multi-modality, which cannot be achieved using a parameterized Gaussian policy.", "[2] show that discretization of the action space improves the average performance, stability and robustness to hyperparameters of reinforcement learning agents on a range of continuous control tasks.", "5. While we have not included such transfer experiments in our current work, we do believe that a model trained on enough architectures and tasks will generalize to new ones.", "For instance, in the updated version of the paper, we show that the learned policy employs similar learning rate and weight decay rate adjustment schemes across very different tasks.", "Substantiating this claim in the general case will likely require a large-scale study, which we plan to perform in the future.", "[1] OpenAI et al. \u201cLearning Dexterous In-Hand Manipulation\u201d, arXiv preprint arXiv:1808.00177 (2018)", "[2] Tang et al. \u201cDiscretizing Continuous Action Space for On-Policy Optimization\u201d, arXiv preprint 1901.10500 (2019)"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 44, "sentences": ["We very much appreciate your valuable comments, efforts and times on our paper.", "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "Q1. Updated proof.", "To address your concerns, we provided more detailed explanations of our proof arguments in the revised draft (see Appendix F).", "We also re-organized our proof completely for better understanding.", "Q2. Relation to Tandem approach.", "As you pointed out, our method is somewhat related to Tandem approach [1] in that both post-process a generative model on top of hidden features extracted by DNNs.", "However, the main purpose of Tandem is not for handling noisy labels.", "In particular, the Tandem approaches utilize the EM algorithm that should be highly influenced by outliers, while our method is specialized to be robust against them.", "We clarified this in Section 2 of the revised draft.", "[1]  Hermansky, H., Ellis, D.P. and Sharma, S., Tandem connectionist feature extraction for conventional HMM systems. In IEEE ICASSP, 2000.", "Thanks a lot,", "Authors", "Dear AnonReviewer1,", "We hope that you found our rebuttal/revision for you and other reviewers in common.", "If you have any remaining questions/concerns, please do not hesitate to let us know and we would be happy to answer.", "Thank you very much,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 45, "sentences": ["Thanks for your valuable comments and feedback.", "We attempted to much  improve the clarity of our paper by adding further explanations and correcting typos in many places.", "New experiments have also been added.", "Please find bellow our answers to your specific remarks.", "R: \"In the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, ...  Beyond this simplification, I am not clear if that is actually intended by the authors.\"", "A: You are totally right, eq.10 simplifies to $\\log p(D) \\geq", "\\mathbb{E}_{I \\sim", "q^D}", "\\left[ \\sum\\limits_{i=1}^{|D|-1} \\log p(D_i |D_{<i},I_{<i})  + \\sum\\limits_{v \\not\\in U^D} \\log p(v \\not\\in U^D| D_{\\leq |D|-1}, I)\\right]$. We were aware of this but for simplicity and the conciseness of presentation we chose to not give this final derivation.", "Also, this was because $P(D_i,I_i|D_{<i},I_{<i})$ is much easier to compute (and more efficient) than $P(D_i |D_{<i},I_{<i})$, given that the computation of $P(I_i| D_{\\leq i},I_{<i})$ is in both case required for sampling ($P(D_i,I_i|D_{<i},I_{<i})$ involves a simple product while $P(D_i|D_{<i},I_{<i})$ involves a sum of products).", "But we agree this was not a good choice, since this simplification appears obvious to the reader.", "We hesitated a lot on this, our decision was taken to present things as closely as possible to our implementation (by the way there was a mistake in the algorithm 2 resulting from this hesitation - the implementation for our experiments was correct however).", "We agree that this may appear confusing.", "Particularly since it gives the feeling that parents of infections are not involved anymore in the computation.", "But there are actually, since there remains $P(I_i|D_{<i},I_{<i})$ terms in the expectation.", "The process learns parameters that tend to give high probabilities to the most likely parent vectors regarding infections from the episode.", "In the revision of the paper, we gave the final derivation you suggested, since it is better for comprehension (It also allowed us to discuss more precisely on what is optimized by considering the given gradient update), and we discuss about its implementation in the appendix as an explanation for the algorithm 2 (that only slightly changed to correct the aforementioned mistake).", "Thanks for this remark that helped us to much improve the paper.", "R: \"The authors explain how they trained their own model but there is no mention on how they trained benchmark models\"", "A: You are totally right, it is missing.", "Baseline models are trained on the same training set as our model following the methods proposed in their original paper.", "Our model and the baselines were tuned by a grid search process on a validation set for each dataset (whose size is given in the description of the datasets),", "although the best hyper-parameters obtained for Arti1 remained near optimal for the other ones.", "For every model with an embedding space (i.e., all except CTIC), we set its dimension to $d=50$ (larger dimensions induce a more difficult convergence of the learning without significant gain in accuracy).", "We added this explanation in the new version of the paper."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 46, "sentences": ["Thank you for your fruitful comments.", ">>", "What would make it stronger imo is to address the issue of how much is gained from a discrete vs. continuous representation.", "Discrete representations by themselves are not better than continuous ones (cf. Table 1, wav2vec vs. vq-wav2vec).", "However, discretization enables the application of existing algorithms from the NLP literature which were designed for discrete inputs.", "We show that the BERT model can be directly applied to discretized speech.", "BERT can better model context than (vq-)wav2vec.", ">> The authors take it as a given that discrete is good because it allows us to leverage work in NLP. That makes sense -- but at what cost?", "Chaining vq-wav2vec and BERT requires more computational effort than just wav2vec, however, it does improve accuracy as our results show (cf. Table 1).", "Running BERT requires roughly as much computational overhead as just vq-wav2vec.", ">> The state of the art on LibriSpeech is not Mohamed at al. 2019.", "See e.g. Irie et al. Interspeech 2019 for better result.", "Thanks for pointing this out, we fixed this in the updated version of the paper we just posted.", ">> The Conclusion is very sparse.", "We broadened conclusion and delineated additional future work."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 47, "sentences": ["We thank the reviewer for the great feedback.", "We have simplified the architecture described in the paper by combining the networks $\\sigma$ and $\\omega$, and included the results from this architecture as well, producing a more robust architecture that performs better for multiple rewrite steps (while keeping the original, more complicated solution as one of the baselines).", "As suggested, we have added further analysis of failure cases.", "We also corrected the typos and clarified the definitions of True, Pred (One Step) and Pred (Multi Step) variants.", "We are very grateful for the review that helped to improve the paper significantly."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 48, "sentences": ["Thank you for your comments and suggestions.", "We will address the issues you mentioned.", "1.", "ARTNet is not very much related to our V4D.", "Basically, ARTNet is an alternative for 3D CNNs by replacing 3D convolution layers with SMART blocks.", "The SMART blocks are two branch units, with one branch for learning static appearance features and one branch for learning motion features.", "ARTNet is a clip-based method for learning short-term representations while our V4D is a video-level method for learning both short-term and long-term representations.", "2.\tDuring training, we uniformly divide the whole video into U sections and randomly select one action unit from each section.", "So there are no overlaps for training.", "For testing, there might be overlapping during sampling due to the limit length of video.", "However, our V4D inference algorithm in section 3.4 guarantees that only the non-overlapping action units will interact with each other during testing.", "3.", "Sure.", "We train all the models with 8 GPUs of GTX 1080 with memory capacity of 11178MB.", "For the inference speed, our V4D ResNet18 takes 0.67s per video and V4D ResNet50 takes 1.22s per video.", "In addition, we also reported the GFLOPs of V4D and compared it with other typical methods in Table 2(b).", "For training on Mini-Kinetics, V4D ResNet18 takes a bit more than 1 day while V4D ResNet50 takes a bit more than 3 days.", "4.", "We can only find the results of ARTNet ResNet18 in the published paper.", "After communication with the authors of ARTNet, we confirm that there are no results published for ARTNet ResNet50.", "So instead we implement ARTNet ResNet50 by ourselves and the top1 accuracy on Kinetics-400 is 74.3%.", "This is still lower than our V4D ResNet50 whose top1 on Kinetics-400 is 77.4%.", "Also, ARTNet ResNet18 reports an average metric of 81.4%, which is the average of top1 and top5 accuracy.", "While our V4D yields an average score of 85.3%.", "5.", "Thank you for providing this related work and we now cite this paper in the second version.", "This paper utilizes 4D CNN to process videos of point cloud so that their input is 4D data.", "Instead, our V4D processes videos of RGB frames so that our input is 3D data.", "This basically makes the methods and tasks quite different.", "Actually we was going to cite this paper yet considering the significant difference we finally did not cite it in the original version.", "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 49, "sentences": ["We thank reviewer for his insightful comments.", "1.", "We agree with the reviewer that sinkhorn iteration is a way to obtain an upper bound on Wasserstein distance.", "However, based on the original paper, they solve the Sinkhorn divergen with T iterations, later when they solve the generator based on the estimated distance, the gradient has to backpropagate through those T iterations, which is expensive and infeasible.", "We also note that there is new work, IPOT (Xie et al., 2018), which can get rid of backpropagating through the T iterations as what we adopted (Bertsekas, 1985) in the paper.", "Combining PC-GAN with IPOP or other future works could be an interesting future work.", "2.  The variance of the sandwiched estimator can be higher, but we are more concerned about bias in this work, which can be treated as a bias-variance trade-off.", "3. The 20:1 mixture used in practice do not directly correspond to s in theory, because the distances we compute are not scaled.", "For example, if the f_\\phi, the discriminator of GAN, is k-Lipschitz, the lower bound estimate should be divided by k.", "However, k is unknown in practice.", "Therefore, we just numerically did a coarse grid search and find the best mixture ratio.", "Also, we try different ratios as we replied to R2 above.", "Ratio                   D2F (Distance to Face)", "Coverage", "1:0", "6.03E+00                        3.36E-01", "40:1", "6.06E+00                       3.41E-01", "20:1", "5.77E+00                       3.47E-01", "10:1", "6.85E+00                       3.56E-01", "0 :1", "9.19E+00                       3.67E-01", "4. We do not consider W_s to be very close from W_U. As can be seen from Figure 6, for the aeroplane examples, W_U fails to capture aeroplane tires while W_s can.", "Similarly for Chair example, W_s recovers better legs than W_U. Quantitatively, we highlight that W_s outperforms W_U consistently as shown in Table 1.", "Thus, we consider both W_U and W_L is needed to generate good quality point clouds."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_none", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 50, "sentences": ["Thank you for your positive review, we address your reasonable concern for the applications of the proposed method in below:", "Q1:  It would be nice to see a better case made for spherical convolutions within the experimental section.", "A1", ": We believe the best case is the non-rigid shape classification and retrieval.", "Our method achieves a state-of-the-art classification and retrieval performance on Shrec\u201911.", "The good performance on non-rigid shape is mainly contributed by the use of bijective spherical parameterization method, which obtains the input spherical image without topological information losses.", "When using spherical projection method to represent 3D shape, there will be information loss if the object is non-convex.", "The lossy input affect the performance of rigid shape analysis to some extent.", "Q2: The experiments on SHREC17 show all three spherical methods under-performing other approaches.", "It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.", "Is there a task that this representation significantly outperforms other spherical methods and non-spherical method?", "A2: The experiments on SHREC17 does show all three spherical methods slightly under-perform some other state-of-the art approaches, We believe this is mainly due to the information losses introduced in the spherical projection process which retains only the convex portion geometric information.", "Future improvements can be added by using:  (1) less lossy input in spherical projection methods (e.g. on top of SEF, and EDF, we can also add other statistic information such as the minimum distance of intersection, mean distance of intersections or standard deviation of the intersection and so on to reduce the information losses); (2) extend the spherical parameterization method on to the general 3D shapes.", "Currently, the spherical parameterization method only works for genus-0 closed object.", "The 3D models presented on ModelNet and Shrec\u201917 are of arbitrary genus which prevents us from using spherical parameterization method.", "Generalization of spherical parameterization methods to objects with arbitrary topology will be one of the future work.", "Compared to non-spherical method, spherical image is one of the most compact representation for 3D shape analysis, the spherical convolution methods rely on no data augmentation (for Type I and Type II) or reduced data augmentation (for Type III, only SO(2) rotation augmentation is required).", "Other non-spherical method (such as volumetric or multi-view based methods)  can only be generalized into unknown orientations using SO(3) rotation augmentations, their representation of 3D shapes are either too sparse (voxel model) or too redundant (multi-view projections).", "Compared to SO(3) spherical convolution method (Cohen et al. 2018), our network is computationally efficient (in terms of network model), but we have to admit that this is at the price of required data augmentation", ".", "Another advantage is that our network allows local filters and local-to-global multi-level spherical features extraction.", "The other two spherical convolution methods (Cohen et al 2018 and Esteves et al 2018) use a lat-lon grid and conduct convolution in the Fourier space, which has a common disadvantage: the Fourier transform does not support local spherical filters.", "Another disadvantage is the use of lat-lon grid which introduces unevenness of the perception field (due to the high resolution near the poles, and low resolution at the equator).", "Experimentally, we compared the three spherical convolution methods in Table 3 using Shrec\u201917 perturbed shape retrieval experiment.", "Cohen et al 2018 and ours obtain similar performances because both of the methods used anisotropic filters, the former achieves rotational invariant using SO(3) rotations for filters, while ours achieves rotational invariant using alt-az rotation of filter and SO(2) rotation augmentation of input shapes.", "As expected, anisotropic filter perform  better than the isotropic filter proposed in Esteves et al 2018 which limits the model capacity.", "Q3:  Is there a specific useful application where spherical methods in general outperform other approaches?", "A3: As mentioned in Cohen et al 2018, perhaps the most exciting future application of the Spherical CNN is in omnidirectional vision.", "Although very little omnidirectional image data is currently available in public repositories, the increasing prevalence of omnidirectional sensors in drones, robots, and autonomous cars makes this a very compelling application of our work.", "Omnidirectional vision is a better application to show the strength of the spherical convolution method."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 51, "sentences": ["Thank you for your suggestion of increasing the discussion of the results.", "We\u2019ve expanded the discussion of the results as much as possible.", "For now we would prefer to keep the actual bar plot of individual module performance in the appendix in the interest of space, and keep the dataset description in the main part, as this was appreciated by the other two reviewers.", "As you say, the ability to generalize is very important in mathematics.", "The paper contains an extrapolation test set to do exactly this - these include generalization tests on larger numbers, longer sequences, more function compositions (which is similar to having more variables), etc (see Appendix B for more details).", "We haven\u2019t attempted to be exhaustive in types of generalization, but the extrapolation test set can be extended in the future to allow for this.", "None of the modules currently include \u201cunsolvable\u201d as an answer, but this is something that would definitely fit within the framework.", "(As an aside: there would be no need to have a special character; we could simply select some consistent word like \u201cUnsolvable\u201d; neural models trained so far seem to have no problem outputting \u201cTrue\u201d or \u201cFalse\u201d.) More generally, there are many further types of problems, that could be included in the dataset - but we hope for now that the current range is comprehensive in types of reasoning required for school-level mathematics.", "We always welcome contributions to the dataset that extend the range of questions in a consistent manner."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_future", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 52, "sentences": ["Thanks for your careful review! As mentioned in the intro, we are trying to give some principled insight into benefits of BN, which has proved tricky.", "Also, it is noted in the paper that BN probably has many desirable properties, of which auto-rate tuning is just one.", "(i) Speed of SGD vs GD:", "Note that \u201ctime\u201d here refers to number of iterations, not epochs.", "We are not aware of results establishing SGD is faster in this measure.", "(As noted on p2,  we are working within the standard paradigm of convergence rates in optimization.", "The only new part is the automatic rate tuning  behavior shown for most parameters when BN is used.)", "(ii) \u201cusually training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.\u201d", "We\u2019re assuming training proceeds until gradient is small (stationary point).", "We are not aware of any prior analysis of speed of convergence that deviates from this assumption.", "Perhaps the reviewer is thinking of early stopping in context of better generalization?", "(iii) \u201cclarify difference from Wu et al. (2018)\u201d", "Wu et al. 2018 introduces a *new* algorithm inspired by weight normalization (WN) and studies its convergence rate to stationary point.", "This algorithm can be seen as an explicit way to tune the learning rate (thus it is conceptually analogous Adagrad).", "They don't have any results about WN or BN itself.", "Their analysis could be adapted to GD on one-neuron network with WN or BN without scale-variant parameters (gamma and beta).", "Even this adaptation is not immediate because the goal of this work is to find a stationary point on the unit sphere rather than R^d.", "Finally, they prove no results for SGD, whereas our paper does.", "(iv) \u201csingle learning rate doesn\u2019t apply for all parameters\u201d", "Correct.", "The algorithm can use a single learning rate for scale-invariant parameters but needs a tuned rate for the scale-variant ones.", "In feedforward nets, the number of scale-variant parameters scales as the number of nodes and the number of scale-invariant parameters scales as the number of edges (up to weight sharing).", "Thus the vast majority of parameters are scale-invariant.", "(v) \u201cRelation between original loss and loss using BN.\u201d", "Our results hold for the loss of batch-normalized network (\u201cBN-loss\u201d)  which is different from the loss of the original network (\u201cBN-less loss\u201d).", "Probably the reshaping of loss function due to BN is very important but currently hard to analyse theoretically because we lack a good mathematical understanding of the loss landscape (even BN-less)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_followup", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 53, "sentences": ["Dear Reviewer 2, thank you for your constructive feedback and for taking the time to review our work.", "Your specific points are addressed below.", "> \u201cThe results are not strong. And, unfortunately, the model contribution currently is too modest.\u201d", "Indeed, the model is a minor contribution and, especially in light of a more thorough evaluation of RGCN, the sum attention RGAT results do not improve on those in Schlichtkrull et al. (2017).", "However, we would like to highlight that despite being a simple modification, the implementation of such a model that is trainable in a reasonable time frame is non-trivial.", "We plan to make our implementation public to aid future research in the area.", "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN: Gated Attention Networks for Learning on Large and Spatiotemporal Graphs", "(Zhang et al. 2018)", ".", "We believe that this additional investigation, accompanied by marginally improved results on Tox21 for dot product attention (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) enhance model contributions of the paper.", "We also consider the poor results on MUTAG to be significant and informative for the rest of the community in developing more powerful models that can be applied to relational graphs.", "> \u201cWe should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)\u201d", "We agree that including experiments on the other data sets presented in Wu et al. (2018) would be a valuable addition to the paper.", "Unfortunately, we are unable to perform the required thorough hyperparameter exploration required to draw meaningful conclusions within the remaining time of the rebuttal period.", "This is something we will investigate in the future.", "> \u201cMy introduction suggestion: do not talk about Convolutional neural networks (CNNs).\u201d", "Thank you for this stylistic critique.", "We see how the approach taken did not provide an intuition about the problem as well as it could have.", "We agree with your point regarding the wealth of graph neural network studies.", "We feel that the field of geometric deep learning, from which part of the direction of this work originated, is important to keep as part of the development of graph-based machine learning models.", "Some recent publications in the area of graph based ML have put less emphasis on geometric deep learning, the generalisations of convolutions from grids to graph, and the modifications of convolutions to achieve non-basis dependent methods.", "We felt that it is important to keep these concepts associated with the field of graph based ML.", "This introduction could, however, talk less in detail about CNNs themselves, and deal more with graphs - the main focus of the paper.", "We will produce a reworked introduction where graphs play a larger role.", "This should provide a more intuitive introduction to our work, whilst maintaining cornerstone concepts."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_by-cr", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 54, "sentences": ["We thank reviewer #3 for the extensive feedback.", "We have incorporated it as stated below.", "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work and added the suggested new interesting references, added theorems and more formal statements in the main text, compressed the appendix and enhanced the description of the experimental setup.", "Proofs that the space \"interpolates smoothly with curvature\": we added formal proofs (see theorems 2 and 3) that all the operations are differentiable, i.e. the gradients are equal from both the left and right at 0, w.r.t. curvature, for the chosen models of hyperbolic and spherical geometry.", "k-addition definiteness in the spherical setting: we have added the formal condition that the k-addition be well-defined, and a proof that for two points this condition indeed recovers x != y / (k ||y||^2) - see Theorem 1.", "Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.", "This is a desirable property for Riemannian vector averaging.", "Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.", "Other comments:", "-Synthetic tree: contains |V| - 1 edges.", "We corrected this mistake.", "-Curvatures: are learned as we state in the paper section 4 and appendix F.", "-Citations: fixed.", "-Working with the Poincare ball as opposed to the hyperboloid model: this allows us to use gyrovector spaces which are defined either for the Poincare ball or the Klein model, as well as to connect those with the Riemannian geometry of the space.", "Moreover, as we show in the paper, we can now smoothly interpolate between all constant curvature spaces which is beneficial for learning curvatures without a priori deciding on their signs.", "-Statement about \u201cgeneral class of trees\u201d replaced by \u201call weighted or unweighted trees\u201d.", "[1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips\u201919", "[2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips\u201919"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 55, "sentences": ["We appreciate your constructive feedback.", "Specifically, your comments about our motivation and development of our idea greatly help us to improve the quality of our paper.", "If we correctly understand reviewer 2\u2019s concerns, the concerns can be divided into two folds:", "1. Our suggestion to mitigate the catastrophic forgetting looks a naive combination of well-known concepts. Thus, it is more system engineering than science.", "2. Each component described in Figure 1 is not explained enough.", "Also, there is no description of the complete task.", "[Response for 1]", "As we explained at the common response, we started our research from clear open questions.", "Our first open question was that why other GR-based algorithms [1, 2] assume unit Gaussian priors even though they integrate classification loss into their VAE formulation.", "Since they do not consider the conflict between the unit Gaussian prior and discriminative loss for the latent variable z, their models generate ambiguous samples that negatively affect the performance of incremental learning, which is discussed in section 4.1 in our paper.", "This leads us to a more theoretical formulation for classification-regularized VAE.", "By introducing class conditional priors induced by the mutual information maximization, DiVA yields class-wise discriminative one mode Gaussians for latent variable z.", "Naturally, DiVA can conduct both class prediction and class conditional sample generation with one integrated model.", "The second open question was that why GR-based algorithms suffer from serious catastrophic forgetting in natural image datasets, even though generated samples are not completely noisy.", "We assumed that this is due to the vulnerability of neural networks [3] triggered by different distributions of pixel values between real and generated images.", "Thus, we defined the two domains: real domain and sample domain.", "To narrowing the distribution gap, we needed a solution that satisfies two conditions (also described in section 5):", "1. We should translate only the style (a global pattern of a specific domain) as keeping outline patterns of given images.", "2. We should consider an unpaired domain translation between real and generated images because the generated images are sampled randomly.", "Fortunately, we were able to find an existing solution that satisfies the requirements: CycleGAN. Any other domain translators that satisfy the conditions can be used or newly studied.", "With the solution, we could make a breakthrough for GR-based methods.", "To the best of our knowledge, this is the first successful approach for a GR-based algorithm to start to resist the catastrophic forgetting problem with a natural image dataset.", "[Response for 2]", "Figure 1 is a conceptual description of our proposed model, DiVA.", "Each component is explained in section 4, below Equation 2, and justified in section 4.1.", "Also, for an easy understanding of the whole CL process with DiVA, we added another figure in Appendix E.", "[References]", "[1] van de Ven, Gido M., and Andreas S. Tolias. \"Generative replay with feedback connections as a general strategy for continual learning.\" arXiv preprint arXiv:1809.10635 (2018).", "[2] Mundt, Martin, et al. \"Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition.\" arXiv preprint arXiv:1905.12019 (2019).", "[3] Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. \"One pixel attack for fooling deep neural networks.\" IEEE Transactions on Evolutionary Computation (2019)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 56, "sentences": ["We are grateful to the reviewer for the comments.", "In this revision, we have corrected the minor typos, added additional comparisons, and added a proof map for easier navigation of the results.", "Specific comments are addressed below.", "1. Regarding exact recovery guarantees \u2014 NOODL converges geometrically to the true factors.", "Therefore, the error drops exponentially with iterations t. In other words, as t \u2014> infinity A_i \u2014> A^*_i for i in [1,m] and x_j \u2014> x^*_j for j in [1,m], where x_j is in R^m.", "We have added this clarification in Section 1.1.", "2. On tuning parameters \u2014 There are primarily three tuning parameters, namely eta_x (step-size for the IHT step), tau (threshold for the IHT step), and eta_A (step-size for the dictionary update step.) Our main result prescribes the theoretical values of these as shown in assumptions A.5 and A.6.", "Here, eta_x = Omega_tilde(k/sqrt(n)), tau = Omega_tilde(k\u02c62/n), and eta_A = Theta(m/k).", "We have updated A.6. to include the order of these parameters.", "The specific choices of these parameters, like other similar problems, depend on some a priori unknown parameters (e.g. the sparsity k, and the incoherence mu) which makes some level of tuning unavoidable.", "This is true for Arora '15 and Mairal '09, as well, where tuning is required for the choice of step-size for dictionary update, and for choice of regularization parameter and the step-size for coefficient estimation via FISTA.", "Note that, in our experiments we fix the step-size for FISTA as 1/L, where L is the estimate of the Lipschitz constant (since A is not known exactly).", "Alternately, since NOODL involves gradient-based updates for the coefficients and the dictionary, tuning (the step-sizes and the threshold) is relatively straightforward in practice, since it is based on a gradient descent strategy.", "In fact, to compile the experiments presented in this paper, we fixed step-size, eta_x, and threshold, tau, and tuned the step-size parameter eta_A only (Theta(m/k)).", "The choices of eta_A are 30 for k = 10,20 and eta_A = 15 for k =50,100, as shown in Fig.2.", ", eta_A mostly effects the convergence rate as long as it is chosen in Theta(m/k).", "Also, as shown in Table 4 (Appendix E), the tuning process for l1-based algorithms (i.e. FISTA) takes more time, since one needs to scan over the range of the regularization parameter to find one that works.", "This (a) adds to the computational time, and (b) since the dictionary is not known exactly, may guarantee recovery of coefficients only in terms of closeness in l2-norm sense, due to the error-in-variables (EIV) model for the dictionary.", "In this sense, NOODL is (a) simple to tune, (b) assures guaranteed recovery of both factors, and (c) is fast due to its geometric convergence properties.", "These factors highlight its applicability in practical DL problems.", "3. Definition of Hard Thresholding (HT) \u2014 As per the recommendation of the reviewer, we have repeated the definition of hard-thresholding (HT) initially presented in the \"Notation\" sub-section, in Section 2 for clarity.", "4. Comparison to other Online DL algorithms \u2014 As correctly observed by the reviewer, the overall structure of NOODL is similar to successful online DL algorithms.", "These successful algorithms (such as Mairal '09) leverage the progress made on both factors for convergence, however, do not guarantee recovery of the factors.", "On the other hand, the state-of-the-art provable DL algorithms focus on the progress made on only one of factors (the dictionary), and do not have good performance in practice, since they incur a non-negligible bias; see Section 5 and Appendix E.", "NOODL bridges the gap between these two.", "In addition to our main theoretical result, which establishes conditions for exact recovery of both factors at a geometric rate, NOODL also has superior empirical performance, leading to a neurally-plausible practical online DL algorithm with strong guarantees; see Section 3 and 4.", "Our work also paves way for the development and analysis of related alternating optimization-based techniques.", "On reviewer's recommendation, we compare the performance of NOODL with one of the most popular alternating minimization-based online DL algorithm used in practice -- Mairal `09 -- in Fig. 2 and Table 4 (Appendix E).", "In this work, the authors show that alternating between a l1-based sparse approximation step and dictionary update based on block co-ordinate descent converges to a stationary point.", "The other comparable techniques shown in Table 1, are not ``online\u2019\u2019 and/or require stringent initializations, in terms of closeness to the true dictionary, as compared to NOODL.", "Our experiments show that due to the geometric convergence to the true factors, NOODL outperforms competing state-of-the-art provable online DL techniques both in terms of overall computational time, and convergence performance.", "These additional expositions further showcase the contributions of our work both on theoretical and practical online DL front."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 57, "sentences": ["We thank our second reviewer for his comments.", "We first refer to your main comments and then answer each point in part.", "The work of Lakshminarayanan et al. indeed showed that deterministic ensembles can improve on the performance of MC-dropout techniques and provides a foundation for ours.", "And as Beluch et al. (2018) showed, this can be valuable in an active learning setting.", "However, our work differs in two major ways:", "i) We focus on showing the uncertainty representation in these methods suffer from overconfident predictions and that combining the two methods into a stochastic ensemble can be of great benefit and improve on the quality of the uncertainty.", "ii) We believe the true novelty to be in applying them in an active learning setting, and in particular on a small dataset problem (i.e. the size of the final dataset acquired during AL is only a small fraction of the entire available unlabelled dataset).", "As you mentioned, data is notoriously scarce and deep learning methods rarely work on small dataset problems.", "We thank the reviewer for pointing us to the work of Huand et al. Indeed this is an interesting method that would allow us to most likely achieve similar or better results with less computational overhead.", "This is definitely something we will consider for future work, but it is somehow out of the main scope of the paper, which was to show the power of combining MC-dropout with ensembles in the active learning setting.", "Taking into account more advanced ensemble methods is definitely of interest.", "In terms of the Bayesian Optimization literature, this is definitely of interest if we are to focus on hyper-parameter tuning for our models, but we fail to see the connection of the work you mentioned to our active learning examples.", "Our focus was not on fine-tuning our models.", "In relation to your specific points, we answer these below:", "1) Gal has already showed in his PhD thesis that MC-Dropout almost always performs best in terms of prediction accuracy and uncertainty quality assessment when compared to alternative Bayesian neural network approaches such as Probabilistic Back Prop and other variants of stochastic gradient MCMC methods.", "The aim of our paper was to improve upon MC-Dropout in the context of active learning, which would invariably translate into better performance w.r.t. other Bayesian NN approaches.", "2) Beluch et al. (2018) showed that going beyond 3 networks in their deterministic ensemble method does not add any significant improvements in terms of performance.", "Therefore, we used this number when benchmarking against their method.", "3) The aim of the paper was to improve upon the state-of-the-art in active learning for the image classification task.", "We specifically chose this task due to its relevance to the real world especially in the medical imaging industry.", "We agree that a more comprehensive study could be done in order to asses the viability of our method for ML tasks other than image classification.", "As for other neural network architectures, we chose the one used in the benchmarked methods.", "4) Results are averaged over 5 multiple independent runs.", "We will include both this and confidence scores in a revised version of our paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 58, "sentences": ["We thank the referee for finding our paper clearly written and in an interesting domain.", "1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.", "We have shown in our experiments (see Fig.\u00a03) that using a single RBM as a pre-processor will not result in increased adversarial resistance.", "A comparison of mean field training vs. constrastive divergence for RBMs has been made by [III,IV].", "Nevertheless, we have added a comparison between our method and the exact solution for a small Boltzmann machine of 16 units in the appendix.", "While we don't claim that this explains the performance of the method under all circumstances, it gives an indication of how well it works.", "We agree that a more thorough study of the training method would be desirable but in this article we are concentrating on reporting the results on increased adversarial resistance.", "We believe that these results should be interesting to the community even if some doubts about the training procedure remain.", "Lastly, we want to obtain a differentiable building block that can be used in standard neural nets.", "The unrolling of the mean field iterations (see Fig.\u00a01) provides a straightforward to achieve this.", "Propagating a gradient through a sampling based building block, while possible, would be considerably harder.", "2. In [III,IV] the authors have succesfully used a mean field approximation beyond the trivial first order to train RBMs.", "In [II] a first order approximation is used.", "In our approach we use the approximation up to fourth order in the coupling J.", "3. The approach of the authors in [III,IV] and also our approach is based on free energy.", "The systematic expansion of the free energy in the coupling constant forms the basis of our approach.", "4. D is calculated as the relative entropy over a batch of 10000 examples.", "The reference probabilities are taken to be uniform and the model probabilities are calculated according to the procedure outlined in section 2.", "Due to the approximations involved it is possible that the sum of the model probabilities exceeds one.", "In this case we rescale the unclamped free energy to limit the total probability to one.", "We have added a note to that effect to the article.", "5. We have included a comparison to the, to our knowledge, currently most adversarially resistant model on MNIST.", "Since our results are derived for MNIST we can only compare to methods in the literature that are evaluated on MNIST.", "We acknowledge that other methods perform very well on more sophisticated tasks and have added a reference.", "6. While we have not explicitely labelled white box and black box attacks we are using a strong white box attack (gradient based) and the, to our knowledge, strongest black box attack (boundary method).", "We have added the labels in the text and Table 1 to make this more clear.", "7. We have added some more attacks on the most robust model.", "We completely agree that more experiments on other datasets are needed to show the universality of the method.", "Due to the computational complexity this will need to remain a topic for a follow up article.", "[I] Seyed-Mohsen Moosavi-Dezfooli, Ashish Shrivastava, Oncel Tuzel: Divide, Denoise, and Defend against Adversarial Attacks. arXiv 1802.06806 (2018).", "[II] Ruslan Salakhutdinov, Geoffrey Hinton: Deep Boltzmann Machines. Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, 448-455, (2009)", "[III] Pawe\u0142 Budzianowski. Training Restricted Boltzmann Machines Using High-Temperature Expansions. Master\u2019s thesis, University of Cambridge, 2016.", "[IV] Marylou Gabri\u00e9, Eric W. Tramel, and Florent Krzakala. Training restricted boltzmann machines via the Thouless-Anderson-Palmer free energy. CoRR, abs/1506.02914, 2015."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_reject-request", "rebuttal_future", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 59, "sentences": ["We would like to thank Reviewer 1 for their review and constructive suggestions.", "Our responses inline:", ">Can you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance?", "-The primary reason is to ensure that training is invariant to the per-device batch size.", "When scaling from resolution 128x128 to 256x256, we increase the number of devices but maintain the same overall batch size, reducing the per-device batch size.", "Cross-replica BatchNorm ensures that the smaller per-device batch size does not affect training.", "Switching to per-device BatchNorm at 128x128 results in a performance drop, albeit not a crippling one: for a model which would otherwise get an IS of 92 and an FID of 9.5, switching to per-device BatchNorm results in an IS of 78 and FID of 13.", ">It is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs.", "Providing such analysis would be also helpful for the community.", "-The goal of this work is to explore GANs at large scale; the exploration of small or medium scale models would indeed be interesting for another study.", "Having said that, we do evaluate BigGAN on conditional CIFAR-10 (mentioned briefly in Appendix C.2) and obtain an IS of 9.22 and an FID of 14.73 without truncation, which to our knowledge are better than any published results.", ">How do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?", "-Any of the proposed techniques could be applied to standard GANs for text or other sequential data in principle, but we have not experimented with these applications ourselves."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 60, "sentences": ["1. We apologize for typos and if any term is not defined at the appropriate places.", "We  fixed all the typos and define the abbreviation for IPM at the first occurrence.", "Please check the revision.", "P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3, is defined in the subsequent two sentences in the same paragraph.", "By the notation G_theta(u) \\sim p(theta), we mean that we want to train the generator G_theta such that when fed a random variable u \\sim p(u), the distribution of G_theta(u) matches that of p(theta).", "Sorry if it is confusing, but G_theta is not parameterized by theta, it just indicates that its the generator for theta. (Like G_x indicates that it is the generator for x).", "2. The training of G_theta is described in the subsection titled \u201cHierarchical Sampling\u201d.", "As correctly pointed out by the reviewer, that G_theta does not appear in the objective function (4).", "Using (4), we train G_x and Q networks.", "After training G_x and Q, we use trained Q to collect inferred Q(X), for each point cloud X.", "Then we train the generator G_theta using ordinary WGAN formulation to produce samples from same distribution as that of the samples Q(X) for each point cloud X. In addition to such two step training, a joint training also works, but is slower computationally, thus we report only the two step training in the paper.", "3. Quantitative evaluation of generative modeling performance is unfortunately very hard for real world problems like point clouds, which is the probable cause for it being missing from much of GAN literature.", "Thus, to provide some quantitative results for generation, we resorted to the toy problem.", "In the toy problem, we can accurately gauge the generation capabilities as can be seen from Figure 5.", "(We did not explicitly provide numbers like KL divergence, as it is evident from the Figure that PC-GAN would be significantly better than AAEs if we evaluate the numbers.) The same protocol can be extended for measuring the quality of the final hierarchical sampling.", "4. To showcase the effect of varying s, we chose the reasonable sized ModelNet10 dataset and ran for s=0, s=1, and three values s_1<s_2<s_3 in between.", "The results are as follows:", "D2F (Distance to Face)       Coverage", "s=0                        6.03E+00                     3.36E-01", "s1", "6.06E+00                     3.41E-01", "s2", "5.77E+00                     3.47E-01", "s3", "6.85E+00                     3.56E-01", "s=1", "9.19E+00                     3.67E-01", "4. Yes the model nicely captures simple topological features of the object, like presence of holes versus being one solid object. Even in the latent space, objects with hole group together."], "labels": ["rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 61, "sentences": ["We thank the reviewer for the valuable feedback! We reply to the answers and comments in the order they were raised.", "Note, that regarding the methodology there were some misunderstandings (which we try to avoid for future readers in the revised version).", "(1) The equations are indeed very related.", "Note however, that In a standard Bayesian neural network (BNN), one would assume that \\theta is a global random variable (i.e. does not depend on input x), whereas in the CDN, we assume that \\theta depends on x and is thus a local random variable.", "Furthermore, in a Bayesian setting p(theta|...) would play the role of a approximate posterior, which would require variational inference (VI), and thus a different objective,  to estimate it.", "(2) In Equation (4) we followed with  p(D | \\psi) a standard notation for \\sum_n p(y_n | x_n; \\psi) (i.e. summation of Equation (3) wrt all data in D) which also can be found e.g. in the work of Graves (2011) [4] and Blundell et al. (2015) [5].", "The objective we introduce for CDNs differs from the ELBO-based objective in VI in the way the logarithm is placed in the first term of the objective: in the ELBO we have a logarithm inside the expectation, while the logarithm is outside the expectation in the CDN objective (note however, that the sample-based approximations get equivalent if only one sample is used).", "Furthermore, in the ELBO we have a fixed value of \\lambda = 1.", "We added a new Section 4 in the revised version of the paper discussing these differences.", "Moreover, we investigated the impact of the different objectives empirically and found that the CDN-based objective led to significantly better results, as shown in the newly added Section 6.4 in the revised manuscript.", "(3) Indeed we need the probabilistic version of hypernetworks to implement the model we described in Equation (3).", "We just wanted to point out that this is in contrast to the vanilla  hypernetworks proposed by Ha et al. (2016) [1] and Jia et al.", "(2016) [2] which would produce a point estimate for \\theta.", "(4) We used a matrix-variate normal (MVN) to reduce the parameters of the model.", "Using a diagonal MVN for X \\in R^{p x q} one needs pq+p+q parameters.", "In contrast a fully-factorized diagonal Gaussian needs pq+pq.", "But you are right, we could easily extend our approach to account for more flexible distributions by using a \"diagonal plus rank-one\" structure diag(a)+uu^T, with vectors a and u, as noted by Louizos and Welling ( 2016) [3] (the increase of parameters is negligible: adding additional vector u).", "We will investigate the benefits of more flexible mixing distributions in future work.", "(5) Thanks for this valuable comment!", "We have revised the toy experiments to include a heteroscedastic regression task (see Section 6.1.).", "It shows that CDNs are able to quantify the heteroscedastic aleatoric uncertainty.", "However, we kept the OOD experiments on MNIST and notMNIST in the paper as well, since we consider it as very interesting that the CDN, while being designed for modelling aleatoric uncertainty, is very competitive on this task.", "Moreover, we investigate the mixing distribution learned in Appendix G.", "(6) Of course, you are right! Previously we showed the test accuracy in Appendix F.  To make it more directly accessible, we have now added the test accuracy achieved by the different models into the legends of the plots, showing that CDN achieves similar predictive power as the baselines.", "We now present the validation accuracy instead in Appendix F.", "(7) Sorry, for this unfortunate formulation!", "We observed that generally as \\lambda increases, the uncertainty is increasing, while the accuracy is decreasing.", "Therefore a simple and effective heuristic for choosing \\lambda is to look at the validation set of MNIST and choose the highest \\lambda that still results in high accuracy (e.g. >. 0.97).", "We have made this procedure clear in the revised manuscript.", "References:", "[1] Ha, David, Andrew Dai, and Quoc V. Le. \"Hypernetworks.\" arXiv preprint arXiv:1609.09106 (2016).", "[2] Jia, Xu, et al. \"Dynamic filter networks.\" Advances in Neural Information Processing Systems. 2016.", "[3] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016.", "[4] Graves, A. (2011). Practical variational inference for neural networks. In Advances in neural information processing systems (pp. 2348-2356).", "[5] Blundell, C., Cornebise, J., Kavukcuoglu, K. & Wierstra, D.. (2015). Weight Uncertainty in Neural Network. Proceedings of the 32nd International Conference on Machine Learning, in PMLR 37:1613-1622"], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 62, "sentences": ["We thank the reviewer for a detailed review.", "We agree with the reviewer that a uniqueness result based on the axioms is desirable, but we don\u2019t have it.", "While we\u2019re able to show that the paths at the input and at the hidden layer must be coupled (i.e. non-oblivious), we just don\u2019t understand the space of non-oblivious methods that well.", "Mathematically, we don\u2019t have a handle on how the path at the hidden layer can vary as the network below the hidden layer is changed.", "Partition consistency is the only axiom about the network below the hidden layer, but it is not applicable to all networks.", "We probably need another axiom to prove uniqueness.", "Another key observation made by the reviewer is the interpretability vs importance of neurons.", "While those are not the same, we demonstrate that conductance can give us some insights about the network (Sections 5.1 and 6.1)."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 63, "sentences": ["Regarding comment 1: Thanks for recognizing the merit of our idea.", "As also mentioned by Reviewer #2, the proposed MAD implicitly assumes that classifiers in the competition are reasonably accurate.", "Otherwise, the selected counterexamples may be less meaningful.", "We will make this assumption explicit in the revised manuscript.", "From this perspective (and other reasons mentioned in the discussion section), MAD should be viewed as complementary to, rather than a replacement for, the conventional accuracy comparison for image classification.", "When two classifiers perform at a reasonable level and achieve very close accuracy numbers (e.g., VGG16BN and ResNet34 on ImageNet validation set), MAD provides the most efficient way of differentiating the two models by maximizing their discrepancies over a large-scale image set.", "We want to emphasize that MAD is especially useful on image classification tasks where most cutting-edge classifiers achieve very close performance.", "In these situations, the MAD competition ranking, which is obtained by evaluating on corner examples searched from web-scale unlabeled dataset, is more convincing than something like 1% accuracy advantage on the validation set.", "For problem domains where there are few sufficiently accurate models, we may still apply the underlying principle behind MAD to create adaptive test sets such that the strengths and weaknesses of the models are most easily revealed.", "In those scenarios, we conjecture that we need increase k to a reasonably larger number, thus at the cost of efficiency.", "Regarding comment 2: Thanks for the comment.", "As long as two (or multiple) models differ (even in slightly different ways), MAD provides the highly efficient way of spotting such differences by exploring a large-scale unlabelled dataset.", "However, these differences are less likely to be revealed using a fixed and small test set (i.e., they will probably have the same accuracy numbers as models to be compared are very similar and are biased in similar ways).", "For the extreme case that two models are exactly the same (i.e, they are biased in identical ways and make identical prediction errors), both MAD and traditional accuracy-based methods will draw the same conclusion - the two models have the same performance.", "Accuracy-based evaluation methods arrive at this conclusion by comparing model predictions with ground truth labels and outputting the same accuracy numbers.", "In contrast, MAD arrives at the same conclusion without any human labeling since the set S for subjective testing is empty.", "So in this extreme case, both MAD and accuracy fail to compare those two models.", "In summary, to reliably compare the relative performance of computational models, all evaluation methodologies (including MAD) rely on the assumption that the models to be compared should be diverse to a certain extent, and the proposed MAD makes this assumption more explicit.", "In fact, MAD makes the best use of model discrepancies (even if models are biased in very similar but not identical ways) to rank the model performance.", "As a matter of fact, based on our experiments, we find that state-of-the-art ImageNet classifiers do have their own biases. (See figure 8 in the appendix.)", "Regarding comment 3: Thanks for the excellent question.", "We believe that the parameter k is task-dependent.", "For problem domains where there are reasonably accurate models (e.g., imageNet classification in our example), we may obtain a stable ranking with a relative small k (e.g., k=15 in the imageNet classification example).", "For problem domains where there are no good models, we may increase k to the limit of human labeling budget in order to obtain reasonable performance comparison.", "Regarding comment 4: Thanks for the comment.", "In our current setting, we restrict the dataset D to the domain of interest that contain natural images of mainly 200 classes.", "However, as the construction of D is noisy and coarse, D contains plenty of open-set images, which do not belong to any class of interest.", "Since we do not perform any manually data screening at this stage, some of the open-set images may even be selected to construct the dataset S.", "This means that although the selected open-set image is out of the domain of interest, the associated two classifiers make different, high-confident (with threshold set to 0.8), but incorrect predictions (Case III).", "As a result, we consider it as a strong counterexample of the two classifiers.", "Note that this situation rarely happens, at least in our experiments because the competing classifiers tend to give open-set images low-confidence, and therefore are automatically filtered out.", "We agree with the reviewer that selecting images that contain only one salient object requires a lot of human effort.", "So we did not eliminate Case I. It turns out that keeping Case I does not seem to affect the comparison and analysis of competing models."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 64, "sentences": ["Thank you very much for your constructive comments.", "First, with respect to baselines, we have updated the results tables to include the additional baselines mentioned, as well as runs for VAT(+EntMin) with lower numbers of labels on CIFAR-10.", "After updating these baselines, we note that our method still achieves state-of-the-art performance in the regime where 1000 and 2000 labels are used for training on CIFAR-10, with and without data augmentation.", "We have also updated the text to tone down the claims.", "In addition, we note that the highest performance in many of the mentioned baselines (and with VAT) are obtained with a combination of multiple approaches.", "When our method is compared head-to-head against the proposed method in the mentioned papers, it is competitive and sometimes outperforms them, for instance, in experiments on CIFAR-10 with 4000 labels", "With augmentation:", "Adversarial Dropout [1] (11.32) vs ours (11.79 +/- 0.25)", "Without augmentation:", "Improved GAN + SNTG [2] (14.93) vs ours (14.34 +/- 0.17)", "Defining the best combination of techniques to achieve the highest performance is an interesting direction of future work; our preliminary experiments combining Mean Teacher with manifold regularization have shown some improvements and we will include the results in the final version of the paper.", "Second, with respect to novelty, we would like to re-iterate our contributions since they may not have been clear.", "First, while manifold regularization has been explored in (Kumar et al 2017) and (Qi et al 2018), we proposed an efficient and effective approximation of manifold regularization that is far easier to compute than the involved method in (Kumar et al 2017).", "Moreover, we point out issues with the standard finite difference approximation to the Jacobian regularization and propose a solution to this problem by ignoring the magnitude of the gradient and using only the direction information.", "Moreover, we showed manifold regularization provides significant improvements to image quality and linked it to gradient penalties used for stabilizing GAN training, which were not shown by (Qi et al 2018).", "We did try to use spectral normalization but did not observe any gains for semi-supervised learning.", "Finally we would like to emphasize the conceptual differences between our method and other smoothing methods like spectral normalization - such methods perform isotropic regularization, whilst ours performs anisotropic smoothing along the manifold directions of generated data-points.", "We showed through experiments using (isotropic) ambient regularization that anisotropic regularization is more beneficial in the case of semi-supervised learning."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 65, "sentences": ["We would like to thank Reviewer 2 for their review and constructive suggestions.", "Our responses inline:", ">Discussions sometimes lack depth or are absent.", "-We have added an additional section (Appendix G) expanding on our discussion and providing additional insight into the observed instabilities.", ">For example, it is unclear to me why some larger models are not amenable to truncation.", "Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?", "-Truncation introduces a train-test disparity in G\u2019s inputs--at sampling time, G is given a distribution it has effectively never seen in training.", "The observation that imposing orthogonality constraints improves amenability to truncation is empirical.", "Our suspicion is that if G is not encouraged to be \u201csmooth\u201d in some sense, then it is likely that G will only properly generate images given points from the untruncated distribution.", "We hypothesize that models which are not amenable end up learning mappings which, when given truncated noise, either attenuate or amplify certain activation pathways, leading to extreme output values (hence the observed saturation artifacts).", "We speculate that encouraging G\u2019s filters to have minimum pairwise cosine similarity means that, when exposed to distribution shift, the network\u2019s features are less correlated and less likely to align and amplify an activation path it would otherwise have learned to scale properly.", ">Were samples from those networks better without using truncation? Why would this be?", "-Samples from those networks without truncation do not have measurably different quality, and their training metrics (losses, singular values) show no differences.", "Aside from empirically testing each network individually for amenability to truncation, we found no other way to check for that amenability.", "> Authors report how wider networks perform best, and how deeper networks degrade performance.", "Again, discussions are lacking, and it doesn\u2019t seem the authors tried to understand why such behaviors were shown.", "Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.", "-We are wary of explanations for which we do not have evidence.", "For each of the modifications introduced in Section 3, we offer a succinct conjecture as to why that change improves performance, but we are not aware of any existing reliable, informative metric which we could employ to understand or trace the source of each observed behavior, particularly with respect to GAN stability or performance.", "Regarding depth vs width: This paper is empirical, and we only briefly experimented with increasing depth analogously to increasing width.", "While increasing width provided an immediate measurable benefit, increasing depth did not.", "We felt that it was better to report the results of this brief investigation than to omit it for a lack of investigatory depth.", "> In Section 3.1 : \u201cAcross runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.\u201d For me, this is not particularly clear.", "Is this something the reader should understand from Table 1?", "-This means that of all the models we trained for the study presented in Table 1 which did not use Orthogonal Regularization, only 16% were amenable to truncation.", "Of all the models which we trained for the study presented in Table 2 which did use Orthogonal Regularization, 60% were amenable to truncation.", "This is not reflected in Table 1, which is merely a presentation of how the introduced modifications impact performance.", ">I question the choice of sections chosen to be in the main paper/appendices.", "I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.", "However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.", "In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.", "However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.", "With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.", "-We appreciate this suggestion.", "While we recognize that this paper generally has a strong focus on implementation details, we felt that this instability was one of the most salient behaviors we observed, and that future work would be best served by presenting our investigations and attempts to understand its source, even if these methods did not improve performance.", "The information in Appendix B and C is intended to be of interest to those who want to reproduce our experiments, so it largely comprises hyperparameters and architectural details that we felt were not necessary to understand the main results of the paper.", ">In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says \u201closses\u201d.", "-Thanks! This was indeed an error, which we\u2019ve corrected in the updated draft.", ">I would also be curious to see the proposed techniques applied on simpler datasets. Can this be useful for someone having less compute power and working on something similar to CelebA?", "-The goal of this work is to explore GANs at large scale; the exploration of small or medium scale models would indeed be interesting for another study.", "Having said that, we do evaluate BigGAN on conditional CIFAR-10 (mentioned briefly in Appendix C.2) and obtain an IS of 9.22 and an FID of 14.73 without truncation, which to our knowledge are better than any published results."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 66, "sentences": ["Thank you for your appreciation of our work!"], "labels": ["rebuttal_social"], "confs": [1.0]}
{"abstract_id": 67, "sentences": ["We thank the reviewer for their helpful comments.", "We agree that our most surprising results are for SGD under constant step budgets or unlimited epoch budgets.", "However the behaviour of SGD under constant epoch budgets has generated a lot of debate in the literature in recent years, and we felt it was important to address this simple case first.", "We agree that some of the observations in sections 2 and 3 have already been made in previous work, however there are also several important differences:", "1. Ma, Bassily and Belkin also introduced the notion of two regimes, however their theory holds for convex losses in the interpolating regime.", "We will discuss their contribution explicitly in the updated text.", "Our discussion in section 2 clarifies why the two regimes arise in practical deep learning models for which these conditions may not hold.", "2.", "Our paper is the first to relate the two regimes of SGD to the popular analogy between SGD and stochastic differential equations (SDEs).", "As we show in later sections, this perspective is crucial to understanding the influence of batch size and learning rate on test accuracy.", "A common criticism of this analogy is that SGD noise is not Gaussian when the batch size is small.", "To our knowledge, we are the first to show that the analogy between SGD and SDEs holds for non-Gaussian short-tailed noise (appendix B).", "3. We clarify the differences to some other recent papers in our reply to reviewer 1.", "Two reviewers complained that it was difficult to tell from the text which contributions are novel and which also appear in previous works.", "We apologise for this.", "It was not our intention and we will edit sections 1 and 2 to ensure that this is resolved and that the above points are reflected in the text.", "Turning to our generalization experiments in sections 4 and 5.", "We agree that many authors have proposed that SGD noise enhances generalization.", "Most notably, Keskar et al. argued that large minibatches perform worse than small minibatches on the test set, even when both achieve similar performance on the training set.", "However their experiments do not provide convincing evidence for this claim, because they tuned the learning rate with small batches and then used the same learning rate value with large batches.", "A convincing experiment should independently tune the learning rate at all batch sizes under a constant step budget, and it should use a realistic learning rate decay schedule.", "Indeed, Shallue et al. recently argued that no existing paper has provided convincing evidence that small batch sizes generalize better than large batch sizes under constant step budgets, and they state in their abstract \u2018We find no evidence that larger batch sizes degrade out-of-sample performance\u2019.", "Meanwhile, Zhang et al. argued that optimization in deep learning is well described by a noisy quadratic model which predicts that increasing the batch size should always enhance performance under constant step budgets.", "To our knowledge, our experimental results in section 4 are the first to provide convincing evidence that very large minibatches do perform worse than small batch sizes on the test set, even under constant step budgets and when the learning rate is independently tuned.", "We believe this is an important contribution.", "Meanwhile, our results in section 5 suggest that SGD has an optimal temperature early in training which promotes generalization and is independent of the epoch budget.", "In response to the reviewer\u2019s specific comments:", "1) Looking at Figure 1c, while the optimal learning rate at 8k with Momentum is 4, the error bars at this batch size range from 4 to 32.", "These error bars can be very large in the curvature regime, precisely because the optimal learning rate is close to instability.", "2) Yes, Momentum will help under constant step budgets if the batch size is large, since it enables us to achieve larger effective learning rates which are beneficial for generalization.", "We will add additional experiments to the text to clarify this.", "3) We will clarify the meaning of warm up, epoch budget and step budget as requested."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 68, "sentences": ["We very much appreciate your valuable comments, efforts and times on our paper.", "Our responses for all your questions are provided below. Our major revisions in the new draft are colored by red.", "Q1. More related works", "We updated the introduction by including more recent works [1, 2, 3, 4, 5] related to deep learning with noisy labels.", "In the previous draft, we only included the relevant literature which involves a single network/classifier.", "The updated related works utilize multiple networks, e.g., an ensemble of classifiers or meta-learning model.", "We also added new experimental results for them in Table 4 of the revised draft, as we mentioned in our common response to all reviewers.", "Thank you very much for the suggestions.", "Q2. Comparison with VAT [6].", "We remark that a targeted setting of VAT [6] is different from ours in that it is designed for improving the performance on semi-supervised learning, while our main goal is handling noisy labels in the training dataset.", "Due to this, we skip the comparison with VAT.", "Instead, as we mentioned in our common response to all reviewers, we consider more training baselines (such as MentorNet [2] and Co-teaching [3]) focusing on handling noisy labels, and show that our inference method can improve all of them.", "Q3. L-FBGS adversarial attacks [8].", "We remark that L-FBGS [8] is known to fail easily due to the near-zero gradient of loss function [7].", "Instead, we consider CW attack [7] which is known to be much stronger.", "[1] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.", "[2] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing very deep neural networks on corrupted labels. In ICML, 2018.", "[3] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: robust training deep neural networks with extremely noisy labels. In NIPS, 2018.", "[4] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.", "[5] Eran Malach and Shai Shalev-Shwartz. Decoupling\u201d when to update\u201d from\u201d how to update\u201d. In NIPS, 2017.", "[6] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.", "[7] C. Nicholas and W. David. Towards evaluating the robustness of neural networks. In IEEE Symposium on SP, 2017.", "[8] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2013.", "Thanks a lot,", "Authors"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 69, "sentences": ["We thank the reviewer for the valuable review comments and suggestions! Please find our point-by-point response as follows.", "Q1: Is there only 1 adversarial party?", "A1: There is only one adversarial party in centralized attack.", "But we make sure that the total injected triggers (e.g., modified pixels) of DBA attackers is close to and even less than that of the centralized attacker.", "We stressed this setup in Section 3.2.", "That is, the ratio of the global trigger of DBA pixels to the centralized is 0.992 for LOAN, 0.964 for MNIST, 0.990 for CIFAR and 0.991 for Tiny-imagenet.", "Q2: What\u2019s the result for centralized attacks with the same number of scaling times as DBA, but each update includes 1/4 number of poisoning samples?", "A2: Following your suggestion, we conducted two sets of new experiments.", "1. Change the poison ratio into 1/4: We decrease the fraction of backdoored samples added per training batch into 1/4.", "2. Change the data size into 1/4: We divide the local dataset into 4 parts and use 1/4 dataset for each update and keep the poison ratio unchanged.", "We have included the results and discussion in Appendix A.4 of the revised version.", "Q3: If the decomposition is also useful for trigger patterns that are not necessarily regular shapes?", "A3: It\u2019s also useful for irregular shape triggers.", "1. We study the irregular pixel logo \u2018ICLR\u2019 for three image datasets.", "Specifically, we use \u2018ICLR\u2019 as the global trigger pattern and decompose it into \u2018I\u2019, \u2018C\u2019, \u2018L\u2019, \u2018R\u2019 for local triggers.", "2. We also use the physical trigger glasses (Chen et al.,2017) on Tiny-imagenet and decomposed the pattern into four parts.", "The results are in Appendix A.3 of our revised version.", "DBA is also more effective and this conclusion is consistent in different colors of glasses.", "Q4: Can the authors show concrete examples on how the attacks are generated? The details are especially unclear on LOAN.", "A4: We note that we have mentioned our attack formulation and algorithm in Section 2.2;", "We have also provided more details about LOAN dataset and how we attack in Appendix A.1."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 70, "sentences": ["We thank the reviewer for their positive evaluation of our study.", "We agree that the prediction from CSLB features is particularly interesting, and we are currently working on improving this further by interpolating to other objects in a semi-supervised manner (similar to what was proposed by the reviewer).", "We also strongly agree that testing additional embeddings would be very interesting!", "For the present work, we focused on synset embeddings because they represent a closer match to the meaning of each individual object than word embeddings would and provide a one-to-one match for the meanings.", "For example, our list contains four different meanings for the object named by the word \u201cbaton\u201d, referring to (1) an item in relay races, (2) in twirling, (3) a weapon used by police, and (4) an item used by a musical conductor.", "Due to the novelty of this line of research, to our knowledge there are no other synset embeddings available than the ones we used, and we included both a 50d dense and a 300d dense version.", "In addition, we would have liked to include sparse positive synset embeddings as a reference, however those are currently not available; for that reason, we included NNSE word embeddings instead.", "In the future, we would like to add sparse positive synset embeddings and test their interpretability relative to our similarity embedding.", "We hope this will underline the unique contribution of a behavior-based similarity embedding presented here.", "In addition, we would like to thank the reviewer for their idea on how to extend the embedding.", "Indeed, we are currently working on predicting similarities for other concepts and images from pretrained synset vectors and activations in deep convolutional neural networks.", "However, this effort is still in its early stages and beyond the scope of the present work."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 71, "sentences": ["We are thankful for the reviewer 2 to provide us with his/her feedback,", "The reviewer mentioned: \"the interpolation mechanism is also too simple\":", "We would like to highlight that despite the simplicity of interpolated samples, there has been demonstrated the effectiveness of using such samples on developing more regularized and generalized neural networks (Zhang et al, 2018) as well as on making them more secure", "(Zhao et. al. 2018)", ".", "Thus, we believe that simplicity does not necessarily lead to ineffectiveness.", "The reviewer mentioned \u201cmany hidden assumptions on the images source or the base classier\u201d:", "As this statement is not clear for us, we would appreciate if the reviewer could elaborate more on it.", "The only assumption we made is on the fact that the out-distribution samples should be statistically and semantically different than the in-distribution samples.", "Then among such out-distribution sets, we propose a measurement for identifying the most representative one among those available.", "The reviewer stated \"There exists more principled approaches for *selecting* out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors\":", "While there are the approaches that aim to detect out-distribution sets, they have not been designed for the selection purposes as we do.", "By mentioning this statement, if the reviewer means the missing of some principled approaches like ODIN in our comparisons, we would like to inform the inclusion of ODIN results in the revised version of the paper.", "Reference:", "- Zhao, Jake, and Kyunghyun Cho. \"Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples.\" arXiv preprint arXiv:1802.09502 (2018).", "- Zhang, H., Cisse, M., Dauphin, Y. N., & Lopez-Paz, D. (2017).", "mixup: Beyond empirical risk minimization.", "ICLR 2018 (arXiv preprint arXiv:1710.09412)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 72, "sentences": ["We thank the reviewer for the effort, however we believe there is a mis-understanding.", "As for the synthetic curves experiment, we updated the paper with a justification.", "This task, while simple, showcases the ability of Transformer to model a distribution over curves of similar shape to real training curves with varying speeds of convergence.", "It has been designed so it is easy to quantify the diversity of generated curves and the fit between the distribution generated by the model and the real one.", "Furthermore, we included two additional tasks, attesting to the ability of Transformer to model a wide range of distributions over training curves.", "We also updated the citation of the paper you mentioned with an arxiv URL.", "We still believe that while focusing on the synthetic task the reviewer might have missed the main point of the paper, namely that time-series forecasting with Transformer works really well, at least in the context of modeling deep learning dynamics.", "The general problem has been studied in the community for many decades and we believe that we made significant progress, so we kindly encourage the reviewer to reconsider their assessment of our contributions."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 73, "sentences": ["We thank Reviewer 1 (R1) for their review and for asking interesting questions that helped us to understand where our paper may have been unclear.", "In our response below we will try our best to better explain our motivation for building and using SQOOP, as well as address R1\u2019s other questions and concerns.", "A key concern that R1 expressed in their review is that we perform our study on the new SQOOP dataset, instead of using an available one (for example CLEVR or Abstract Scenes VQA).", "Though we appreciate the concern (it has spurred us to rethink and rephrase how we justify SQOOP) we still believe that the SQOOP dataset is the best choice for precisely testing our ideas.", "We kindly invite R1 to consider the following arguments in favor of doing so:", "The goal of our study was to perform a thorough investigation of systematic generalization of language understanding models.", "To that end, we wanted a setup that is as simple as possible, while still being challenging by testing the ability to extend the relational reasoning learned to unseen combinations of seen words.", "We therefore choose to focus on simplest relational questions of the form XRY, as they also allow us to factor out challenges of discrete optimization in choosing the right module layout (required for Stochastic N2NMN).", "The simplicity is also useful because most models get to 100% accuracy on the training set of SQOOP, which allowed us to put aside any remaining optimization challenges and just focus our study on systematic generalization.", "In contrast, we find that the popular CLEVR dataset does not satisfy our requirements and if we did modify it sufficiently, we believe that it would only differ from SQOOP in the actual rendering and would not affect our conclusions.", "Though visually more complex, CLEVR has only 3 object types: cylinder, sphere and cube.", "Therefore, it would only allow for 3x4x3=36 different XRY relational questions.", "This is arguably not enough to sufficiently represent real world situations, and would definitely hinder our experiments.", "Specifically, we would not be able to sufficiently vary the difficulty of our generalization challenge when allowing 1,2,4,8 or 18 possible right hand-side objects in the questions (we clarify why splits with lower #rhs/lhs are more difficult than those with higher #rhs/lhs later in this response).", "Hence, we did not find the original CLEVR readily appropriate for our study.", "We could, in theory, introduce new object types to CLEVR and rerender a new dataset in 3D using Blender (the renderer that was used to create CLEVR) with different lighting conditions and partial occlusions.", "Though enticing, we believe that such a 3D version of SQOOP would lead to exactly same conclusions, because the vision required to recognize the objects in the scene would still be rather trivial.", "The Ying and Yang dataset is clearly a valuable resource (and we thank the reviewer for the pointer), but we do not think it is readily suitable for the kind of study that we aim to perform.", "The dataset, to the best of our understanding, uses crowd-sourced questions (as the questions are taken from Abstract VQA dataset, whose captions were entered by a human, according to the original VQA paper https://arxiv.org/pdf/1505.00468v6.pdf).", "Using crowd-sourced questions would not allow us to control our experiments at the level of precision that we wanted to achieve (e.g. we would not know the ground-truth layouts, it would be harder to construct splits of varying difficulty, etc.).", "As well, Abstract VQA contains only 50k scenes, and from our experience with SQOOP we know that this number would be not sufficient to rule out overfitting to training images as a factor.", "We thank R1 for their constructive suggestion to consider NMNs that form a DAG.", "We are currently investigating a chain-structured NMN with shortcuts from the output of the stem to each of the modules, and we will soon report these additional results in the upcoming revision of the paper.", "We hope that these results, combined with further qualitative investigations we are conducting, will answer the legitimate question of R1 as to why Chain-NMN performs so much worse than Tree-NMN.", "We acknowledge that the text of the paper can be improved to explain better why splits with lower #rhs/lhs are generally harder than those with higher #rhs/lhs, and we thank R1 for pointing this", "out", ".", "Our reasoning is that lower #rhs/lhs are harder because the training admits more spurious solutions in them.", "In such spurious regimes models adapt to the specific lhs-rhs combinations from the training and can not generalize to unseen lhs-rhs combinations (i.e. generalizing from questions about \u201cA\u201d in relation with \u201cB\u201d to \u201cA\u201d in relation to \u201cD\u201d (as in #rhs/lhs=1) is more difficult than generalizing from questions about \u201cA\u201d in relation to \u201cB\u201d and \u201cC\u201d to the same \u201cA\u201d in relation to \u201cD\u201d (as in #rhs/lhs=2).", "We will update the paper to be more explicit in explaining these considerations.", "We would like to conclude our response by replying to the higher-level concern of R1 that the findings of our study may not \u201cgeneralize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more\u201d.", "While we fully agree that more complex datasets with more complex questions would bring new challenges, these are ones we purposely put aside (such as the general unavailability of ground-truth layouts for vanilla NMN, the need to consider an exponentially large set of possible layouts for Stochastic N2NMN, etc.) We believe that it is highly valuable for the research community to know what happens in the simple ideal case of SQOOP, where we can precisely test our specific generalization criterion.", "This knowledge (e.g. the superiority of trees to chains, the sensitivity of layout induction to initialization, the emergence of spurious parameterization in end-to-end learning), will guide researchers in choosing, designing and troubleshooting their models, as they now know what to expect modulo the optimization challenges that they may face.", "The field of language understanding with deep learning is not easily amenable to mathematical theoretical investigations and, with that in mind, rigorous minimalistic studies like ours are arguably very important.", "To some extent, they play the role of the former: they inform researcher intuition and lay a solid foundation for scientific dialogue.", "We purposely traded breadth for depth in our investigations, and we will go even deeper in the additional experiments that the upcoming revision will contain.", "We believe that the total of our results makes a complete conference paper.", "All that said, we would welcome specific suggestions of additional experiments that we could carry out in order to better validate our claims.", "We hope that this response has clarified to R1 what our paper was insufficiently clear about. A new revision with additional experiments and fixed typos will soon be uploaded to OpenReview, and we hope that R1 takes this response and the changes that we will make to the paper into account."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 74, "sentences": ["We thank the reviewer for the evaluation.", "Please see our detailed response to several recurring issues at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X.", "In that response we address the following issues:", "(1) We emphasize fundamental differences between Cakewalk and CE.", "(2) How the sampling distribution should not be considered as a part of Cakewalk, and that it is mostly provided as an example, and a basis for the reported experiments.", "(3) The experiments include results two tasks.", "Nonetheless, it appears the paper doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "Next, we\u2019ll try to provide some intuition as to why a sampling distribution that assumes independence between the different dimensions can be useful in some cases.", "The simple explanation is that in some problems the conditional expectation of the objective given that some x_i=j is much better than for other values x_i=k. In such cases, for each dimension the algorithm will tend to sample values which are useful to many possible solutions.", "In the clique problem for example, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.", "In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.", "Lastly, we note that these kind of factorized distributions have a long history of being useful  in machine learning.", "In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.", "In different contexts, such distributions have also been used as naive mean field approximations in variational inference."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 75, "sentences": ["We thank the reviewer for the feedback.", "In the following, we address the comments individually:", "-  Comparison with conventional triplet methods using images and their corresponding RGB images", "We did not consider comparisons with conventional triplet approaches: the message of our paper was not to demonstrate the utility of ordinal embedding approaches over conventional (representation-based) triplet approaches.", "It was rather to show that when input representations are NOT available, we provide a scalable approach to solve the ordinal embedding problem.", "We agree with the reviewer that such a comparison would be possible but the experiments, we believe, would not reflect the message of the paper.", "-  More synthetic experiments comparing the various ordinal embedding approaches", "There is a large literature that compares existing ordinal embedding approaches, and in order to not overload the figures, we had decided to just compare against the most popular traditional algorithms. But we can definitely add more comparisons in the revision of the paper.", "-", "The \u201cclaim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems\u201d", "We would like to clarify that our claim was merely that we use neural networks to address ONE instance of an NP-hard optimization problem.", "We want to bring attention to the generic idea of using neural networks as optimization toolboxes to directly solve non-convex optimization objectives instead of merely for learning problems."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 76, "sentences": ["We thank you for your constructive feedback.", "We reply to all reviews in a general response above."], "labels": ["rebuttal_social", "rebuttal_structuring"], "confs": [1.0, 1.0]}
{"abstract_id": 77, "sentences": ["Thank you for the instructive review!", "Our algorithm 1 minimizes the empirical learnable noise risk (Eq. 4), which does not assume that X_{t-1}^{(j)} follows a diagonal gaussian distribution.", "Originally, to justify the I^u=1/2 \\sum_l log(1+Var(X^(j)_{t-1,l})/ \\eta_{j,l}^2) term used in our experiments for estimating mutual information, we used diagonal Gaussian assumption for X_{t-1}^(j) in the experiment.", "In fact, a better way to justify this is to note that I^u provides an upper bound for the mutual information subject to the constraint of known variance of marginal distributions of X^(j)_{t-1}, and the upper bound is reached with the diagonal Gaussian distribution, as is proved in Appendix C in the revision.", "Therefore, the assumption of diagonal Gaussian assumption is dropped for the experiments in the revision.", "Practitioners can choose to optimize an upper bound of the learnable noise risk for better efficiency (as is also used in the experiments in this paper), or use differentiable estimate of mutual information for better accuracy, as has also been pointed out in the paper.", "In the revision, we have also added a more detailed comparison with other methods in sections 4.2 and 4.3, showing the strength of our method.", "For example, in section 4.2, our method correctly identifies important causal arrows, while the four other comparison methods either have more false positives and false negatives, or completely fail to discover causal arrows.", "In section 4.3", ", we compare with the results in previous literature.", "We note that although all compared methods correctly identify the causal relations, our method have the advantage that the inferred causal strength does not decay with increasing history length (we also analyzed that in the original submission)."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 78, "sentences": ["Regarding your first concern on the comparison with CW: In short, MarginAttack is able to achieve a higher attack success rate than CW AND a shorter running time.", "The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference.", "To show this point clearly, we would like to refer you to the results in our response to reviewer 3, where we scanned through the number of binary search steps and measure the success rate and running time.", "As can be seen, MarginAttack has a higher success rate than all the versions of CW.", "There is a success-rate-efficiency tradeoff in CW, as a smaller binary search step number leads to a lower success rate.", "However, even with 10 binary search steps, CW is still unable to outperform MarginAttack in terms of success rate.", "On the other hand, with very small numbers of binary search steps, CW still runs slower than MarginAttack.", "Hope these results will clarify your major concern.", "Regarding your minor concern:", "In the theorem, we did not assume convexity.", "The assumption with the name 'convexity' is saying that the constraint set should not be 'too concave'.", "Please check the following figure where we listed what decision boundaries are permitted by our theorem and what not.", "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf", "As can be seen, the convexity assumption permits a wide variety of decision boundaries.", "Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does.", "In this case, the critical point becomes a local maximum rather than a local minimum."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 79, "sentences": ["Thank you for the review and accurate summary of our submission!", "> I am reluctant to give a higher score due to its incremental contribution.", "Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.", "SVG clearly differs from Dreamer in that it only considers 1-step model predictions in SVG(1) or multi-step predictions without value function in SVG(\u221e).", "SVG(0) does not use a dynamics model.", "In addition, Dreamer propagates gradients through transitions in a learned features, making it effective for high-dimensional control tasks.", "> Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.", "Besides the important technical difference described above, we highlight the empirical performance of Dreamer.", "A conclusion of the SVG paper was that the model did not yield substantial practical benefits beyond 1-step predictions.", "We found it important to revisit this topic in the light of recent substantial improvements to dynamics models (see below).", "> Effectiveness on very long horizon trajectories: Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.", "This is an open issue in model-based RL.", "While current dynamics models still cannot accurately predict full episodes, this is rarely needed in practice.", "Recent works successfully use learned dynamics for control from both proprioceptive inputs (Chua et al. 2018, Shyam et al. 2019, Wang & Ba 2019) and from images (Hafner et al. 2019, Zhang et al. 2019).", "Dreamer shows that the relatively short model predictions (H=20) yield high-quality policy gradients, and that an additional value function in the latent space is effective for solving tasks that require longer-term credit assignment (e.g. with sparse rewards).", "Our experiments provide evidence that combination is effective in practice.", "> However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).", "This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).", "I think this point should be discussed in the paper.", "That is, the issue still exists, and Dreamer is less effective with very long horizon.", "We address the challenge of long horizons not using long-term model predictions but by learning a value function that estimates the infinite sum of discounted future rewards.", "Figure 4 in our submission shows that this gives Dreamer robustness to the imagination horizon compared to two baselines.", "> Inapplicability to discrete controls:  One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.", "This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.", "Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.", "This restriction should be noted in the paper.", "We applied Dreamer to environments with discrete actions using the DiCE estimator (Foerster et al. 2018) locally for the da/d\u03bc and da/d\u03c3 derivatives.", "This was a drop-in replacement for the reparameterization estimator and slightly outperformed a Gumble-softmax actor.", "We find that with this 1 line change, Dreamer solves discrete action tasks of the Atari suite and a 3D DMLab environment.", "> There is no mention about variance of policy gradient estimates.", "Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.", "Dreamer uses reparamterization gradients that already have low variance (Kingma & Welling 2013, Rezende et al. 2014); although see Miller et al. (2017).", "Learning baselines for variance reduction is common for Reinforce estimators as used in A3C and PPO (Mnih et al. 2016, Schulman et al. 2017) but not for reparameterization estimators as used in Dreamer, SVG, and SAC (Heess et al. 2015, Haarnoja et al. 2018)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 80, "sentences": ["Thank you for your thoughtful review. We will address your concerns in turn.", "Q1: So you provide the general framework where somebody has to specify only the F?", "A1: Yes, and that is the motivation of this work, to avoid training new models for slightly different situations.", "Q2: The efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.", "A2: Yes, so we use WGAN-GP, a strong and elegant implementation, as our trained GAN.", "Q3: Is parameter Omega estimated individually for each degraded image?", "A3: Yes.", "Thank you again for your positive reviews which give me some confidence, I really appreciate it."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 81, "sentences": ["We thank the reviewer for their time and their clear understanding of the key aspects of the paper.", "We address the reviewer\u2019s questions in the following:", ">  How much does the image matter for the single-image data set?", "The reviewer raises an important point about the tested single images.", "Less crowded images could lead to many patches having no gradients (e.g. showing only the sky), leading to a failure of at least RotNet, if not also BiGAN on many samples of the augmented dataset.", "Our image choices were thus motivated by striving for simplicity and not further adding a pipeline that would, for example, extract only patches with sufficiently large image gradients.", "We are training DeepCluster now on a significantly less busy image and will report results in the coming days.", ">  How general is the proposed approach?", "We believe that this method will work well for pretext tasks that rely on learning via detecting and learning invariances, such as Exemplar [1], Colorization [2], and Noise-as-targets [3].", "Methods such as Context [4] and Jigsaw [5] could potentially work less well as they would potentially easily find a way to cheat given the limited amount of original data of one image.", "However, as the authors note in the paper cited by the reviewer, the accuracy of a pretext task does not translate to downstream task performances, so even a method that is simple on one image\u2019s patches does not necessarily fail.", "This is an interesting avenue for research and we hope that this paper could inspire follow-up work on this topic.", "> [1] found that the network architecture for self-supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved.", "Indeed, the paper mentioned by the reviewer shows that the performance of various self-supervised methods for ResNets does not degrade with the depth as it does for VGG and AlexNets due to the skip-connections.", "However, as ResNets have not been originally used to train the methods analyzed in our paper, we have stayed in the bounds that are required for fair comparisons and only used AlexNet.", "We agree with the reviewer that it would be good to check if ResNets, in general, can also be trained in such a manner (e.g. could global pooling destroy the signal?), so we are running an experiment on a ResNet-18 and will report results in the upcoming days.", "> Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?", "MonoGAN trained without any exploding gradients or other problems frequently encountered by GANs.", "As we have suggested in the paper, this might be due to the fact that image-patches from one image follow a simpler distribution than in-the-wild images of a complete dataset.", "\u2014", "[1] A. Dosovitskiy et al. \"Discriminative unsupervised feature learning with exemplar convolutional neural networks.\" TPAMI 2015", "[2] R. Zhang et al. \"Colorful image colorization.\" ECCV 2016.", "[3] P. Bojanowski et al. \"Unsupervised learning by predicting noise.\" ICML 2017.", "[4] D. Pathak et al. \u201cContext Encoders: Feature Learning by Inpainting\". CVPR 2016.", "[5] M. Noroozi \"Unsupervised learning for visual representations by solving jigsaw puzzles.\" ECCV 2016"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 82, "sentences": ["We are aware of the related work you mention.", "Please note that unfortunately the \u201cSemantically Conditioned LSTM\u2026\u201d is not directly comparable because, as they state in their paper, \u201cthe generator is further conditioned on a control vector d, a 1-hot representation of the dialogue act (DA) type and its slot-value pairs\u201d.", "Our goal is to work with arbitrarily complex questions that map to correspondingly arbitrarily complex logical forms and not a very restricted set of logical forms that could be represented in a one-hot fashion.", "Please do note that we ran 2 sets of human evaluations (Adequacy and Fluency), as is standard in Machine translation in order to deal with the evaluation bias problem you describe - we took this into account when conducting experiments and will make it more clear in a revised version.", "We also observe significant improvements in both human evaluations, suggesting that the improvement comes from our method and not from evaluation bias.", "Our dataset only contains a single logical form for each question and vice-versa, making it impossible to evaluate quantitative metrics (bleu, rouge, meteor) in the multi-reference setting you describe.", "Please also note that metrics like bleu and rouge have been commonly used in a non multi-reference setting by significant work in the natural language processing community.", "We thank the reviewer for their comments and will take them into account in a revised version."], "labels": ["rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 83, "sentences": ["The main topic of the paper is how to evaluate models that use confidence thresholding.", "The primary purpose is to compare *defenses*. However, to justify the attack strategy that we propose to use, we also compare *attacks*. Specifically, we provide an experiment demonstrating that our attack actually is stronger than the baseline.", "However, it is not really necessary to provide multiple experiments demonstrating that MaxConfidence is more powerful because the superiority of MaxConfidence is theoretically guaranteed."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 84, "sentences": ["Thank you for the fruitful comments!", "We addressed your main concern and updated Section 1 of the paper to better situate it in the existing literature.", ">> Would it be possible to train the vq-wav2vec model jointly with BERT, i.e. as one model? [...] Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end-to-end model, e.g. by randomly masking some of the continuous input speech?", "The focus of this paper is a quantization approach for audio.", "Replacing the two-step training process by an adaptation of BERT to continuous data (using a wav2vec/CPC-like objective function instead of the cross entropy) is an interesting direction for future work (and we amended the future work section accordingly).", "However, our current paper is a proof of concept that a pre-training scheme based on masked inputs (BERT) can improve over previous methods in the speech domain.", ">> What exactly does \"mode collapse\" refer to in this context?", "In several configurations (especially for one and two groups) considerably less codewords than theoretically possible are used.", "We loosely refer to mode collapse as the phenomenon when very few codewords per group are used (cf. Appendix A).", "We updated the paper to also refer to the appendix where we outline the number of codewords that the model uses.", "We observed that in the \u201cfew group regime\u201d (G=1...4), only a few of the available centroids per group are used and refer to this phenomenon as mode collapse \u2014 for BERT training, this is actually favorable e.g. in the G=2, V=320 setting as it yields a codebook of acceptable size for NLP model training (13.5k/23k).", "Mode collapse could potentially be circumvented by strategies like embedding re-initialization used in classical k-means and this is an interesting avenue for future work.", ">> [...] BERT is required on top of the vq-wav2vec discrete symbols.", "Is it possible that the output acoustic model is simply better-matched to continuous rather than discrete input (direct vq-wav2vec gives discrete while BERT gives continuous)? Would it make sense to train the wav2vec acoustic model on top of the vqvae codebook entries (e) instead of directly on the symbols?", "We actually did what you suggest: when we train acoustic models on top of vq-wav2vec, we input the dense embedding vectors corresponding to the discrete codewords.", "On the other hand, we also trained an NLP sequence to sequence (Section 6.3) which takes the quantized audio codes as input and then generates the transcriptions.", "This gives reasonable accuracy and suggests that the discrete codes by themselves, and without the learned continuous representations, are useful.", "We clarified this in the updated version of the paper.", "We believe the reason the dense embeddings for the discrete codewords work less well", "is because they do not encode as much detailed context information as a representation built by wav2vec or BERT.", "The information in the codebook is ultimately less detailed than a context vector specific to the current input sequence."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 85, "sentences": ["Thank you very much for your time, and your constructive comments, we are looking forward to further discussions!", "We answer your questions and concerns in the following.", "> \"The advantage of INN is not crystal clear to me versus other generative methods such as GAN and VAE.\"", "It is indeed possible to adapt other network types to the task of predicting conditional posteriors.", "We are currently setting up experiments for detailed analysis of the respective advantages and disadvantages and will report about these results in a future paper.", "In the present paper, we focus on demonstrating that high-quality posteriors can actually be learned using bi-directional training as facilitated by INNs.", "Concerning the comments/questions:", "1.", "> \"could the authors elaborate on the comparison against cGAN\"", "cGAN generators are at an inherent disadvantage relative to INNs, because they never see ground-truth pairs (x,y) directly -- they are only informed about them indirectly via discriminator gradients.", "This it not a problem for simple relationships, e.g. between images x and attributes y, and cGANs work very well there.", "However, it makes learning of complicated forward processes much harder and may cause the resulting posteriors to be inaccurate.", "Moreover, INNs are forced to embed every training point x somewhere in the latent space, whereas cGAN generators may fail to allocate latent space for some x, because this is never explicitly penalized.", "This can lead to mode collapse and insufficient diversity.", "> \"Can cGAN be used to estimate the density of X (posterior or not)?\"", "cGANs can in principle do this by choosing a generator architecture with tractable Jacobian (using e.g. coupling layers or autoregressive flow), but we are not aware of published results about this possibility.", "2.", "> \"For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)?\"", "Yes, the weights of the losses are considered as hyperparameters, because the magnitude of MMD-based losses depends on the chosen kernel function.", "Hyperparameter optimization suggested an up-weighting of MMD-based losses by a factor of 5, to give them approximately equal impact as the supervised loss.", "For the iterations, we accumulated gradients over one forward and one inverse network execution before each parameter update.", "We also tried alternating parameter updates after each forward and backward pass, which resulted in equal accuracy, but was a bit slower.", "We did not experiment with other ratios than 1:1.", "3.", "> \"Is this to effectively increase the intermediate network dimensions?\"", "This is precisely the reason: It improves the representational power of the INN, as mentioned in Sec. 3.2 and discussed in our response to reviewer 1.", "At present, we find this is only necessary for the toy problem in Fig. 2.", "> \"It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).\"", "This is correct.", "We explicitly prevent information from being hidden in the padding dimensions in the following way:", "A squared loss ensures that the amplitudes are close to zero.", "In an additional inverse training pass, we overwrite the padding dimensions with noise of the same amplitude, and minimize their effect via a reconstruction loss.", "We will add this to the relevant paragraph in the paper.", "4.", "> \"I am curious if this model could succeed on higher dimensional data\"", "Works such as [1, 2, 3] (also cited in our paper) have shown that the coupling layer architecture in general works well with images.", "These works use maximum likelihood training, i.e. exploit the tractable Jacobians to maximize the likelihood of the data embedding in latent space.", "To scale-up our approach, we may need to replace MMD loss with maximum likelihood as well, and first experiments with this show promising results, see", "https://i.imgur.com/ft09Pk9.png .", "[1] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv:1605.08803, 2016.", "[2] Diederik P Kingma and Prafulla Dhariwal.", "Glow: Generative flow with invertible 1x1 convolutions. arXiv:1807.03039, 2018", "[3] Schirrmeister, Robin Tibor, et al. \"Generative Reversible Networks.\" arXiv:1806.01610, 2018", "We have uploaded a revised version of the paper, thank you again for your suggestions.", "The changes and additions are highlighted in red font for convenience.", "Please also note that by adding these changes, our page count increased by half a page beyond the recommended 8 pages.", "If this presents a problem, we can attempt shorten the paper accordingly."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 86, "sentences": ["Except for the learning rate, all the hyper-parameters were chosen according to the values suggested by the authors of AdaGrad, and Adam.", "The learning rate was chosen as 1/K, with K=100 being the number of examples used to estimate the CDF.", "As our stated goal is to present an algorithm which can be blindly applied with some fixed set of hyper-parameters to any possible objective, one of the goals of the experiments is to show that in such a setting some methods will work, while others will fail.", "Thus, as a controlled experiment for this hypothesis, we first fixed the set of all hyper-parameters for all methods, and then proceeded to apply them to various problems.", "In this setting therefore, tuning the learning rate or any other hyper-parameter for that matter will compromise the validity of our results.", "Regarding table 3, we accept the reviewer\u2019s suggestion, this is a good point. We particularly like the suggestion of writing NA or some such value, and we will use it to correct the paper.", "We thank the reviewer for the evaluation.", "Please see our detailed response to several recurring issues at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X.", "In that response we address the following issues:", "(1) We emphasize fundamental differences between Cakewalk and CE.", "These go beyond the differences the reviewer mentions.", "(2) How the sampling distribution should not be considered as a part of Cakewalk, and that it is mostly provided as an example, and a basis for the reported experiments.", "(3) The experiments include results two tasks.", "Nonetheless, it appears the paper doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "Next, we try to answer the specific issues the reviewer mentions.", "First, we address the suggestion of introducing Cakewalk as a generalization of CE.", "While we were writing the paper we in fact considered presenting Cakewalk as the reviewer suggests.", "We eventually decided against this approach as CE is a method for adapting an importance sampler, and its convergence guarantees only apply when it is treated as such.", "The convergence guarantees of REINFORCE on the other hand still apply under our surrogate objective framework.", "This property allows us to explore various surrogates, where one such construction allows us to interpret CE as a policy gradient method, and another makes the basis for Cakewalk.", "Second, we address the issue of using a sampling distribution that assumes independence between the different dimensions.", "As the author correctly states, such a distribution will not always be useful, and one can design a problem for which this distribution will lead to a poor local optimum.", "Note however that a global maximizer for the objective suggested by the reviewer can be easily found just by random sampling: sampling such a maximizer has the same probably as sampling an odd integer - half.", "Nonetheless, for the clique problem such a distribution can be effective.", "Intuitively, if some node i is part of a large clique, then sampling x_i=1 is likely to result in a good objective as there are many nodes that are connected to i, and the chance of not sampling any of them decreases with the clique size.", "In this way, over time the probability for sampling such nodes becomes higher, and the chance of sampling all of them together increases.", "A similar reasoning applies for the k-medoids problem.", "We note that these kind of factorized distributions have a long history of being useful in machine learning.", "In a similar context to the one studied in the paper, such distributions have been studied by Rubinstein in his paper which discusses CE as an algorithm for combinatorial optimization, and in the classical bandit papers Exp3 is applied independently to several dimensions to study game theoretic problems.", "In different contexts, such distributions have also been used as naive mean field approximations in variational inference.", "Next, we address the question regarding the gradient update types.", "One intuitive explanation for why an algorithm that maintains a \u2018memory\u2019 of previous gradient updates like AdaGrad or Adam is required", "is that they protect against sampling biases.", "Consider for example the case when the execution is at the start, and the sampling distribution still has maximum entropy.", "Due to the combinatorial nature of the solution space, the examples that have been sampled thus far create a distorted representation of the solution space.", "In this case we could get that some x_i=j will occur few times, while some other x_k will not receive the value j at all.", "Now if we apply vanilla gradient updates this can skew the sampling distribution in random directions.", "Gradient updates such as those of AdaGrad and Adam on the other hand will lessen the impact of such deviations as the importance of each case is inversely proportional to the number of previous observations.", "As such deviations will inevitably occur whenever we rely on polynomially sized samples to represent a combinatorial solution space, without such corrections a gradient based adaptive sampling algorithm will almost surely fail.", "Indeed, as can be seen in tables 1,2 and 4, SGA almost never leads to a locally optimal solution.", "Furthermore, this reasoning explains why AdaGrad is superior to Adam: AdaGrad corrects against sampling biases that entail all the examples that have been encountered, while Adam does this only within some exponentially moving time window.", "Indeed, this phenomenon is studied in detail in the AdaGrad paper (though without assuming a data distribution), and sparse data like ours (one can say our data points are N indicator vectors of length M) is the first motivating example in their paper."], "labels": ["rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 87, "sentences": ["Overall", "We thank you for your kind comments, in acknowledging that the work is well motivated, and the problem is an important one, currently not studied under the meta-learning paradigm.", "We have made substantial improvements based on your suggestions, and other reviewers\u2019 comments, and hopefully we are able to address most of your concerns.", "Concern 1: Reproducibility", "Our code is built on top of existing code (Prototypical Networks and Image-to-Image Translation from CycleGAN).", "Thus, we adopt the same hyperparameters and architectures as the prior work, and as a result our work is fairly easy to reproduce.", "We will of course release the code.", "As suggested, we have utilized the appendices to give detailed information about the experimental setup.", "Concern 2: Size of unlabelled test set, data-split information", "We did provide some details on the first version (in the appendix).", "In light of the reviews, in the revised version, we have expanded the appendix to give more details on the experimental protocol.", "Concern 3: Jointly learning vs Freezing GAN", "Training in joint manner can be very tricky, and may often cause stability issues.", "You are right in your suggestion, that it is similar to first styling, and then applying meta-learning.", "Having said that, this is a common strategy in several state of the art domain adaptation techniques, where the GAN-based domain adaptation and task-specific classifier are trained in multiple steps.", "For example, see training protocol in [1,2,3, etc.].", "[1] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alyosha Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018", "[2] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition (CVPR), volume 1, pp. 4, 2017.", "[3] Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., & Krishnan, D. Unsupervised pixel-level domain adaptation with generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017"], "labels": ["rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 88, "sentences": ["Thank you for your feedback and additional references! To address the comments:", "- As we explained in Thm.1, \\eta_{\\mathcal{H}} is a constant measuring how well the model family \\mathcal{H} can fit the true models from both domains.", "Estimating this term requires *labelled* target samples, which is usually unavailable in domain adaptation.", "However, when we have access to a handful of labelled target data, we can certainly estimate this term and perform model selection (e.g., choosing neural network models \\mathcal{H}) better, meaning that we can find better values for alphas, and so achieve even better adaptation performance.", "- Yes, there are many methods that conduct single-source to single-target adaption in the literature.", "However, our main focus is *multi-source* to single-target adaptation.", "This is why our comparisons focus on similar methods, that also use multiple sources at the same time.", "They are generally more competitive than single-source methods.", "Following Reviewer #1's suggestions, we added one additional state-of-the-art competitor, Moment Matching for Multi-Source Domain Adaptation (M3SDA) (Peng et al., 2019), to our experiments.", "Moreover, we also add the challenging Office-Home dataset as you suggested[D]; results can be found in the new Section 5.3.", "The results with the new competitor, and on both the earlier datasets and the new one, show that our method outperforms the competition, especially on the Office-Home dataset, in which we achieve state-of-the-art performance.", "If you have any comments or concerns, feel free to leave a message here and we can discuss further. Thank you!"], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 89, "sentences": ["Thanks for the review.", "There are two significant inaccuracies:", "1. GGT does not take the view of a low-rank *approximation*. This is a central point of the paper.", "2. Re: iterative methods: the preconditioner is a -1/2 power of the Gram matrix, not the inverse.", "More details below:", "@Inverse square root: We are fully aware of the distinction.", "- Note that iterative solvers like conjugate gradient do not immediately apply here, as we are solving a linear system in M^{1/2}, not M.", "- Krylov subspace iterative solvers suffer from a condition number dependence, incurring a hard tradeoff between iteration complexity and \\eps. [1]", "- We actually *did* try polynomial approximations to M^{-1/2} as an alternative to our proposed small-SVD step.", "We saw worse approximation (the condition number dependence kicks in) and worse GPU performance (parallel computation time scales with polynomial degree).", "@Full-matrix terminology: The use of \u201cfull-matrix\u201d to distinguish from \u201cdiagonal-matrix\u201d is standard, and taken directly from [2].", "@Full-matrix vs. full-rank: Note that we do not consider the windowed Gram matrix to be an \u201capproximation\u201d of the \u201cfull\u201d gram matrix.", "The window is for the purpose of forgetting gradients from the distant past, motivated by (1) our theory, (2) the small-scale synthetic experiments, and (3) the extreme ubiquity of Adam and RMSprop, which do the same.", "Note that we do no approximation on the windowed Gram matrix, the fact that it is low rank is a feature.", "@Location of \\mu definition: Is the reviewer\u2019s suggestion simply to move this definition into the intro?", "@Comparison with second-order methods", ": Please refer to our response to Reviewer 1 for some additional comments.", "@Tweaks: We don\u2019t believe that any of the tweaks should be so controversial.", "- The \\eps parameters are present in *every* adaptive optimizer, for stability.", "The interpolation with SGD is just another take on this.", "- The exponential smoothing of the first moment estimator is a subtler point.", "As we point out in Appendix A.2, in the theory for Adam/AMSgrad [3,4], \\beta_1 *degrades* the moment estimation, yet everyone uses momentum in practice.", "Even if this is unconvincing, the performance gap upon removing this tweak is minor, and our empirical results hold without this tweak.", "We are simply offering a heuristic that we have observed to help training unconditionally, just like momentum in Adam.", "@Informal main theorem: By \u201cinformal\u201d we truly mean that we are suppressing the smoothness constants (L, M) for readability and space constraints. We are simply adopting the widespread practice of deferring the non-asymptotic mathematical statement to the appendix.", "[1] Tight complexity bounds for optimizing composite objectives. Blake E Woodworth, Nati Srebro. NIPS 2016.", "[2] Adaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer. JMLR 2012.", "[3] Adam: A Method for Stochastic Optimization. D.Kingma,J. Ba. ICLR 2015.", "[4] On the Convergence of Adam and Beyond. S. Reddi, S. Kale, S. Kumar. ICLR 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_followup", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 90, "sentences": ["We\u2019re glad that you found the paper interesting and well-written. To address your comments and questions:", "1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN.", "Also, NPN can probably be modified to output spans of a sentence.", "I will be curious to know how it performs.", "The NPN model requires a pre-defined lexicon of action types (i.e., verbs), such as cut, bake, boil, etc.", "For the recipes dataset, the action types and their causal effects were manually collected and defined.", "Since the ProPara dataset does not have these annotations, we would have to manually identify action types to apply NPN to it.", "Also, NPN treats the state change as a classification problem (of about 260 classes that are also manually defined).", "In contrast, KG-MRC finds the state-describing span in the text directly, which we believe is a more generic approach.", "2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper.", "We agree that more detail would help readers to understand the model better.", "We\u2019ve made some hopefully significant updates to Section 4 (model description and notation) to improve clarity, and we hope you\u2019ll take the time to read the new manuscript.", "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "3. What are the results when using the whole training set of Recipes ?", "We\u2019ve completed an experiment on the full Recipes dataset and updated the paper to describe the result (this experiment did not finish in time for the initial submission).", "The model\u2019s F1 score improves from 51.64 on the partial data to 54.27 on the full data, surpassing the previous state of the art by a more significant margin."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 91, "sentences": ["We thank the referee for their review and the summary of our results", "1. We have included some more attacks on the most robust model (a transfer attack and a Gaussian random noise attack).", "2.", "(a) We have evaluated the adversarial resistance when training a Boltzmann machine with 256 fully connected latent variables directly on the 8x8 patches.", "The version with only 128 hidden units was not able to reduce the relative entropy to the values of the larger, stacked machine.", "We find that the model without stacking is not able to increase the adversarial resistance.", "It is possible that we are unable to complete the training due to the approximations involved.", "For a small machine (16 units) of full hidden connectivity we can observe the noise rejection behaviour, as shown in appendix D.", "(b) We have trained a machine with the same connectivity as the stacked machine directly on the 8x8 patches.", "This training gives similar results to the training in stages.", "(c) From the result in (b) we conclude that the particular manner of the pre-training does not matter.", "Therefore also the choice of first training set (98% coverage or full coverage) does not influence adversarial resistance.", "(d) There are a total of 28800 parameters in the Boltzmann machine.", "We will gladly provide files with the trained weights and also fully trained neural networks on request.", "3. We currently do not have a full explanation for the large adversarial resistance, but noise resistance must play a part in it.", "The very strong rejection of Gaussian noise and the observations in Fig.\u00a04 point in this direction."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_none", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 92, "sentences": ["With all our due respect to Reviewer #3\u2019s valuable time and effort in reviewing our manuscript, we must admit that we are a bit upset by this last late review, due to the apparent lack of understanding before placing comments, and several factual errors that make the current comments at least poorly grounded.", "We understand that the idea of \u201cmodel falsification as model comparison\u201d might not be trivial to understand for people primarily from practical deep learning backgrounds.", "The idea is deeply rooted in a successful series of studies from image perceptual assessment research: a basic introduction can be found in (Wang & Simoncelli (2008)).", "We notice that Reviewer #2 also kindly points out another interdisciplinary foundation of MAD in software differential testing.", "We hope Reviewer #3 can carefully read the below explanation, and reconsider the rating to a more serious and appropriate one.", "Q1:", "One of the main advantages is that it can select a sample set from an arbitrarily large unlabeled images.", "However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods.", "Response:", "Our method is very efficient in terms of human annotation budget compared with traditional methods, which is one of the main claims we elaborated in our paper.", "We are disappointed that this major important point was not well understood.", "In fact, MAD provides the very first and efficient solution (in the context of image classification) to exploit a large-scale image set under the constraint of the very limited budget for human labeling.", "We have noticed that the other two reviewers agree with us and appreciate this point.", "For example, quote Reviewer #2: \u201cBecause of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload\u201d.", "To evaluate the relative performance of two ImageNet classifiers, traditional evaluation methods compute accuracy on a fixed test set.", "For ImageNet validation set, human annotations for 50,000 images need to be provided.", "This number is large in terms of human labeling effort, but is extremely small compared to the set of all natural images (the natural image manifold).", "As also mentioned by the reviewer, annotation for each image is a 1000-class classification task, which makes the labeling task more difficult compared to a binary classification problem.", "In contrast, rather than comparing fixed test sets which are typically small, the proposed MAD adaptively samples a test set from an arbitrarily large corpus of unlabeled images so as to maximize the discrepancies between the classifiers, measured by the distance over WordNet hierarchy.", "Human labeling is only required on the resulting small and model-dependent image sets, which contains only k=30 images (for each pair of classifiers) on the ImageNet experiment as reported in our paper.", "Our experiments show that the MAD ranking stabilizes at around k>15 (see figure 5) and successfully tracks the recent progress in image classification .", "For comparing 11 classifiers, the total labeled images needed are 1,650 (see page 6): it is obviously smaller than 50,000 and leaves much room to compare more classifiers (before it reaches 50, 000).", "In conclusion, our method is apparently much more efficient in terms of human annotation budget compared with traditional methods.", "In addition, despite the fact that the selected set by MAD is small (as a way of maximizing the efficiency of human labeling), it provides the strongest examples to let classifiers compete with one another.", "Quote Reviewer #2: \u201cThe proposed MAD competition distinguishes classifiers by finding their respective counterexamples. It is therefore an \"error spotting\" mechanism\u201d.", "Their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix), which sheds light on potential ways to improve the classifiers or combine them into a better one.", "Those gains are way beyond the scope of collecting random image samples.", "Q2: Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.", "Response:", "We agree with the reviewer that k is a critical parameter in MAD.", "We want to however draw the reviewer\u2019s attention to the ablation study and figure 5, if they were accidentally missed in the first reading.", "Based on them, we cannot concur with the statement \u201cif k is relatively small the method seems very sensitive to selected examples\u201d.", "When we apply MAD to compare imageNet classifiers, we find that the MAD ranking stabilizes very quickly when around k>15.", "We would like to also emphasize that despite the small size of labeled images, MAD successfully tracks the steady progress in image classification, as verified by a reasonable Spearman rank-order correlation coefficient (SRCC) of 0.89 between the accuracy rank on ImageNet validation set and the MAD rank on our test set.", "As also pointed out by Review #2, the selected top-k images provide the strongest examples to let classifiers compete with one another.", "Through this process, their respective strengths, weaknesses as well as biases can be most easily revealed (see figures in the appendix).", "Q3: The authors invite five volunteer graduate students to annotate the selected example.", "However, for many categories, it\u2019s nor easy for normal people to distinguish.", "So the experiments in this paper is also not convincing.", "Response:", "As veterans in performing subjective studies, we understand and agree with the reviewer that querying ground truth labels for a 200-class classification problem is difficult. That is exactly why we have carefully designed our subjective experiment.", "Given an image x, which is associated with two classifiers f_i and f_j , we pick two binary questions for human annotators: \u201cDoes x contain an f_i(x)?\u201d and \u201cDoes x contain an f_j (x)?\u201d.", "For each question, we follow  the original ImageNet instructions and include the definition of f_i(x) (or f_j(x))  with a link to a corresponding Wikipedia page.", "We also show several example images of f_i(x) (or f_j(x)) sampled from the ImageNet validation set.", "Moreover, if more than three of our five human annotators find difficulty in labeling x, it is discarded and replaced.", "When both answers to the two binary questions are false (corresponding to Case III), we cease to source the ground-truth label of x for reasons mentioned by the reviewer, and treat x as a strong counterexample for both f_i and f_j.", "Based on the above, we cannot concur with the judgement \u201cthe experiments in this paper is (are) also not convincing\u201d."], "labels": ["rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 93, "sentences": ["Thank you very much for the instructive and detailed review!", "For the first and second comments, we appreciate the detailed example you proposed.", "Specifically, we agree with the 1) and 2) of your analysis.", "For 3), although x^(1)_{t-2} is useful for predicting x^3_{t}, due to the causal chain and the presence of independent noise in the response function Eq. (1), x^(2)_{t-1} is even more useful for predicting x^(3)_{t}. When Eq. (2) is minimized w.r.t. both f_\\theta and all \\eta, with appropriate \\lambda, it is likely that \\eta_1 will go to infinity and \\eta_2 will be finite, leading to the correct conclusion that X^(1)_{t-1} does not directly structurally cause x^(3)_t.", "For example, in the new Appendix B.3, we show analytically and numerically that for a linear Gaussian system, the global minimum of the learnable noise risk lies on I(x^(1)_{t-2}; \\tilde{x}^(1)_{t-2})=0, i.e. \\eta_1->\\infty, for a wide range of \\lambda.", "To study the general landscape and global minimum of the learnable noise risk, we first carefully inspect Theorem 2, and find that its original statement is not true in general.", "We have replaced the original Theorem 2 with a detailed analysis of the loss landscape of the learnable noise risk.", "Specifically, there are four properties that the minimum MSE (MMSE, the first term of the learnable noise risk after minimizing w.r.t. f_\\theta) must obey, as demonstrated in the new Appendix B. In particular, we prove that the MMSE based only on the uncorrupted variables that directly structurally cause x^(i)_t is the minimum among all MMSE based on any set of uncorrupted variables.", "These properties will likely lead to nonzero mutual information for the variables that directly structurally cause x^(i)_t, at the global minimum of the learnable noise risk, as we ramp down \\lambda from infinity.", "In a sense, the learnable noise risk behaves similarly as an L1 regularized risk.", "Whereas L1 encourages sparsity of the parameters of the model, the mutual information term in the learnable noise risk encourages sparsity of the influence of the inputs, where the strength of sparsity inducing depends on \\lambda.", "As also pointed out in the \u201crelated works\u201d in the revision, the learnable noise risk is invariant to model structure change (keeping the function mapping unchanged) and rescaling of inputs, while L1 or group L1 do not, making learnable noise risk suitable for causal discovery where the scale of data may span orders of magnitude and the model structure may vary.", "For the third and fourth comment, thanks for pointing out and we have added the constraint-based methods in the related works section, and stressed that we are dealing with \u201ccausality in mean\u201d in section 3.1."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 94, "sentences": ["We thank the reviewer for their comments.", "Although our primary contributions are empirical, we also provided a detailed theoretical discussion in section 2, where we give a clear and simple account of why the two regimes arise.", "Although previous authors have also discussed some of these results, there are differences between our conclusions, as we discussed in our responses to the other two reviewers.", "We would also like to emphasize that we make a significant contribution to the debate regarding SGD and generalization.", "While many papers have proposed that small batches may generalize better than large minibatches, it was recently pointed out by Shallue et al. that none of these experiments provide convincing evidence for this claim, because no experiment to date has compared small and large batch training under a constant step budget with a realistic learning rate decay schedule while independently tuning the learning rate at each batch size.", "We are the first to run this experiment and conclusively establish that SGD noise does enhance generalization in popular models/datasets.", "We believe this is an important contribution.", "We also provide intriguing results as we vary the epoch budget, which demonstrate that the optimal learning rate which maximizes the test accuracy does not decrease as the epoch budget rises.", "This supports the notion that SGD has an optimal \u201ctemperature\u201d which biases it towards solutions that generalize well.", "Additional experiments in the appendix G go further and study how the optimal learning rate schedule changes as we increase the epoch budget."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 95, "sentences": ["We would like to thank the reviewer for the time and useful feedback.", "Our response is given below.", "- Interpretation of self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture.", "Overall, self-modulation appears to yield the most consistent improvement for the deeper ResNet architecture, than the shallower, more poorly performing, SNDC architecture.", "Self-modulation doesn\u2019t help in the SNDC/Spectral Norm setting on the Bedroom data, where the SNDC architecture appears to perform very poorly compared to ResNet.", "For the other three datasets, self-modulation helps in this setting though.", "- The ablation study shows that the impact is highest when modulation is applied to the last layer (if only one layer is modulated).", "It seems modulation on layer 4 comes in as a close second.", "I am curious about why that might be.", "Figure 4 in the Appendix contains the equivalent of Figure 2(c) for all datasets.", "Considering all datasets: (1) Adding self-modulation to all layers performs best.", "(2) In terms of median performance, adding it to the layer farthest from the input is the most effective.", "We believe that the apparent significance of layer 4 in Figure 2(c) is statistical noise.", "- I would like to see some more interpretation on why this method works.", "We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.", "As a first step, we provide a careful empirical evaluation of its benefits.", "While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.", "Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.", "- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?", "A 10% change in FID is visually noticeable.", "However, we note that FID rewards both improvements in sample quality (precision) and mode coverage (recall), as discussed in Sec 5 of [1].", "While we can easily assess the former by visual inspection, the latter is extremely challenging.", "Therefore, an improvement in FID may not always be easily visible, but may indicate a better generative model of the data.", "[1] https://arxiv.org/abs/1806.00035", "- Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).", "We view this contribution as a simple yet generic architecture modification which leads to performance improvements.", "Similarly to residual connections, we would like to see it used in GAN generator architectures, and more generally in decoder architectures in the long term."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 96, "sentences": ["We are afraid that there seems to be some confusion regarding our paper.", "We apologize if this is caused by the lack of clarity in the use of abbreviation \u201cES\u201d (see general response).", "In the latest revision, \u201cEvolutionary structure search\u201d is abbreviated as \u201cESS\u201d for clarity.", "We emphasize that in the paper, NO \u201cevolutionary strategy\u201d but rather PPO is used to train the policy (see Section 2.1 and 3.2).", "We hope the reviewer can take time to revisit the paper in the light of this inconsistency.", "Also, we now have 5 baselines from previous research and modern variants, which we believe further showcases our contributions.", "Q1: The experiments do not include any strong baseline", "We added more baselines to further strengthen the significance of our work with respect to the previous approaches.", "The baselines now include (a)\u201cESS-Sims\u201d (Sims, 1994), (Cheney, 2014), (Taylor, 2017), (b) ESS-Sims-AF, (c) ESS-GM-UC, (d) ESS-BodyShare and (5) Random graph search.", "We refer to the details of each baseline in the general response.", "|      NGE         | ESS-Sims  | ESS-Sims-AF  | ESS-GM-UC | ESS-BodyShare |  RGS", "fish         | **70.21**    |", "38.32     |", "51.24         |", "54.40", "|", "54.97         |", "20.96", "Walker   |", "**4157.9** |", "1804.4   |", "2486.9        |", "2458.19   |", "2185.1        |", "1777.3", "The results show that NGE is significantly better than previous approaches and baselines.", "We did an ablation study by sequentially adding each sub-module of NGE separately.", "The table shows that submodules are effective and increase the performance of graph search.", "Q2: a) Optimizing both the controller and the hardware has been previously studied in the literature.", "Is it worth using a neural graph?", "b) All algorithms should optimize both G and theta for a fair comparison.", "By \u201coptimizing both G and theta\u201d, we meant to indicate that the learned controllers can be transferred to the next generation even if the topologies are changed (instead of throwing away old controllers).", "We note that only NGE among all the baselines has the ability to do that.", "Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.", "To the best of our knowledge, the traditional methods require re-optimizing theta from scratch for each different topology, which is computationally demanding and breaks the joint-optimization.", "NGE approximately doubles the performance of previous approach (Sims, 1994) as shown in Q1.", "Please refer to Section 3.1 and Section 3.4 for more details.", "Q3: You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.", "Again, we apologize for the confusing use of \u201cES\u201d abbreviation.", "Evolutionary strategy is not used in the paper.", "We invite the reviewer to re-read our paper, since it seems to have led to a major misunderstanding.", "CMA-ES updates and utilize the covariance matrix of sampling distribution, which is not directly applicable to discrete structure optimization.", "We believe it will be a valuable future research direction.", "Q4: Providing the same computational budget seem rather arbitrary and depends on implementation.", "We are unsure what the reviewer is indicating, and would appreciate the additional clarification.", "In terms of the computational budget for each experiment, we compared different algorithms under different computational budget metrics, more specifically,  \u201cwall-clock time\u201d, \u201cnumber of updates\u201d, and the \u201cfinal converged performance\u201d.", "NGE performs best among all algorithms.", "We emphasize the fact that wall-clock time is a more common and realistic metric for comparing the structure search in practice.", "We agree that computational budget depends on implementation, and the curves in the paper are plotted based on the number of iterations/parameter update, which is independent of the implementation.", "Q5: The writing of the paper", "We sincerely thank the reviewer for the suggestions.", "We updated the changes in the latest version accordingly."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_refute-question", "rebuttal_social", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_followup", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 97, "sentences": ["We thank for the reviewer for their comments on our work, and we share our responses below.", "1) We agree that we did not provide a clear definition of \"task\".", "In the present paper there are two tasks: classification into primary labels, and classification into secondary labels.", "We did not mean to imply that the classification of a specific class is a task on its own.", "We agree however that a clearer introduction of the terminology would be clearly helpful and we plan to add this to the final submission.", "2) This comment is not entirely correct and we would like to apologies for any confusion in the paper.", "Actually, the update of the generator depends on the improvement of the classifier for the *principal* labels on the *meta-training* data, i.e. the improvement in generalisation to unseen data.", "Thus, the optimal auxiliary labels are not the ground-truth labels for the principal classes, since this would make both terms in the minimisation for $\\theta_1$ (the second equation in 3.2) identical and not allow any leveraging of the meta-training data.", "Also, we would argue that the KL-divergence, rather then introducing noise, allows us to avoid collapsing classes which we would claim are due to dying neurons (again, there is not loss/mechanism drawing the auxiliary labels to be the same as the primary ones).", "These claims are supported by showing that providing random labels does not lead to any improved performance and by our experience that using hard labels does indeed improve performance.", "3) Providing fair comparisons across a range of very different methods is not easy when other methods aim to solve a different problem.", "Concerning the comparison with prototypical networks, we do agree that this is not a fair comparison and we would like to change the phrasing in the paper.", "The original reason for associating this to the prototypical network was that we employ their zero-shot setup: i.e. we use a VGG network to obtain prototypes on the meta-data and then use these prototypes to define an auxiliary task on the training-data.", "4) We do agree that requiring the class hierarchy is a current limitation of the work.", "While it is still general enough for solving classification tasks (we merely have to choose a fixed number of sub-classes per task, e.g. 5 without having to provide anything else), we would want to look at more general auxiliary task in future.", "One option we are considering is employing an auxiliary regression task, where the generator network would provide vectors and the corresponding loss would be simple regression.", "However, since this is the first work to use a double gradient method for auxiliary task generation, we believe that presenting results with a comparison to human auxiliary labels, which itself also requires this hierarchy, is a good starting point.", "5) We would very much like to test our approach on more complex datasets with more varied classes, and this will be part of future work.", "However, we would like to repeat that our approach can work with an arbitrary hierarchy (e.g. assigning the same number of sub-classes to every class).", "The reason why we only used 100 classes in our experiments is for allowing the comparison with human-defined classes, but in principle we could use any number of sub-classes per primary class.", "In the CIFAR10 dataset in which a hierarchy is not defined, we show that using 6 different hierarchies all lead to a better generalisation."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 98, "sentences": ["Dear Reviewer 1, thank you for taking time to read and review our paper and for your useful comments.", "Hopefully the new results in our response will better aid discussion. Your specific points are addressed below.", "> \u201c...the additions proposed are small modifications to existing algorithms", "We concede that the modifications to the existing models is a minor contribution.", "We would like to highlight that despite being a simple modification, producing an implementation that trains in a reasonable about ot time is non-trivial - this is confirmed by community comments requesting for implementation details.", "We plan to make our code public to aid research in the area.", "To generalise the model, following the recommendations in one of the comments, we have investigated applying the Transformer-style dot product attention that was presented in GaAN (Zhang at al. 2018 https://arxiv.org/abs/1803.07294).", "This generalises the notion of RGAT, and we believe that this increase the contributions in terms of model modification that our paper offers.", "> \u201c...and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017).\u201d", "Unfortunately we cannot directly compare our approach to Velickovic et al. (2017) on relational graphs since their proposed model doesn\u2019t support relationship types.", "Hence, we only compare to Schlichtkrull et al. (2017) and the conventional baselines, whereas Velickovic et al (2017) reports results on the non-relational graphs Cora, Citeseer, Pubmed and PPI.", "In the case of the RDF tasks, our model hyperparameter search space does include a model very similar to vanilla GAT.", "In the case where we only have both one basis kernel for each of the convolution and the attention, i.e. where B_V and B_v in equation (8) are set to 1, then the difference between RGAT and GAT is only a set of learnable relative scale factors living inside the basis coefficients c_b and c_d.", "During our hyperparameter search, however, no favourable points for evaluation set performance were discovered with basis sizes lower than 10 (a basis size of 5 is permitted in the search).", "This leads us to conclude that vanilla GAT would not perform well on the RDF tasks.", "As mentioned above, we have now also evaluated the dot-product style attention of the transformer and have included it in our results.", "As one of the public comments mentioned, a study comparing these types of attention models to the more recurrent based models like Gated Graph Neural Networks (Li et al. 2015) would be extremely worthwhile.", "We we feel that evaluation lies outside of the scope of this work, however, which is mainly concerned with evaluating how the introduction of an attention mechanism into RGCN modifies its behaviour.", "> \u201c...the results achieved in the experiments are very small improvements compared to the baseline of RGCN", "\u2026\u201d", "We agree that any improvements compared to RGCN are marginal.", "In light of your third comment (below) regarding hyperparameters, the new RGCN baseline on Tox21 significantly closes the gap between the baseline and sum-attention RGAT.", "The new dot product attention results on Tox21 (Mean test AUCs: WIRGAT 0.838 +/- 0.007, ARGAT 0.837 +/- 0.007) are slightly improved compared those of the sum attention, however, due to the retrained RGCN baseline, the relative gap between the best performing RGAT and RGCN is now smaller than it was before.", "We also see value in reporting these negative results.", "It was expected that an attention mechanism like GAT should cope with the node-degree imbalances observed to be present in the MUTAG dataset [a statement along these lines was made in Schlichtkrull et al. (2017)].", "The most natural route to tackle this problem turns out to fail - a result which we believe is informative for the community who are trying to solve this problem.", "On the other hand, the newly evaluated dot-product attention does no worse (or better) than RGCN, indicating a more promising research direction to pursue.", "> \u201c...often these small variations in results can be compensated with better baselines training", "\u2026\u201d", "We also suspected this was a possibility, and in the same of sum-style attention it turns out to be true.", "To determine whether this was the case, we performed the same hyperparameter optimisations to our implementation of RGCN.", "In the case of the RDF datasets AIFB and MUTAG, we observe no meaningful difference between our retrained RGCN benchmark and the original benchmark provided in Schlichtkrull et al. (2017).", "On the other hand, our retrained implementation on the graph classification task TOX21 raised the performance above that of the RGCN reported in Wu et al, 2017 ti match the performance of the sum-style attention mechanism RGAT performance.", "This new RGCN performance is not higher than the observed performance of the dot-style attention mechanism, however, although this is not significant as discussed above."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_by-cr", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 99, "sentences": ["Thank you for your time and effort of reviewing our paper. Please see our response below.", "\\kappa is an assistant notation to remove the ambiguity of the two \\gammas in G_{\\gamma}^{q_{\\gamma} (y)}. \\kappa stands for the parameter/variable of which the gradient information is needed.", "For example,", "(i) g_{\\kappa}^{q_{\\gamma}(y)} = frac{-1}{q_{\\gamma}(y)} \\nabla_{\\kappa} Q_{\\gamma}(y)}, where \\kappa is \\gamma, as in Theorem 1;", "(ii) g_{\\kappa}^{q_{\\gamma}(y|\\lambda)} = frac{-1}{q_{\\gamma}(y|\\lambda)} \\nabla_{\\kappa} Q_{\\gamma}(y |\\lambda), where \\kappa could be \\gamma or \\lambda.", "Eqs. (7) and (8) are the foundations GO is built on, but they are not our GO.", "GO is defined in Eq. (9) of Theorem 1.", "For Eq. (9), yes, y_{-v} is selected from one sample y in the experiments.", "But GO is not the local expectation gradient (Titsias & Lazaro-Gredilla, 2015), because GO uses different information (the derivative of the CDF and the difference of the expected function).", "As pointed out in the last paragraph of Sec. 3, when y_v has finite support and the computational cost is acceptable, one could use the local idea from Titsias & Lazaro-Gredilla(2015) for lower variance, namely analytically evaluate a part of expectations in Eq. (9).", "For a detailed example, please refer to Appendix I.", "The main difference between the local expectation gradient and the proposed GO is that the latter is applicable to where the former might not be applicable, such as where y_v has infinite support or the computational cost for the local expectation is prohibitive.", "Please note our GO is defined in Eq. (9).", "As pointed out in the last paragraph of Sec. 3, calculating Dy[f(y)] (requiring V+1 f evaluations) could be computationally expensive.", "We also stated there, \u201cfor f(y) often used in practice special properties hold that can be exploited for ef\ufb01cient parallel computing\u201d.", "We took the VAE experiment in Sec 7.2 as an example and gave in Appendix I its detailed analysis/implementation, in which you might be interested.", "More specifically, the two bullets after Table 4, should be able to address your question on fast speed.", "Also, as noted in the penultimate paragraph of Sec. 7.2, less parameters (without neural-network-parameterized control variant) could be another reason for GO\u2019s efficiency.", "As for computation complexity, since different random variables (RVs) have different variable-nabla (as shown in Table 3 in Appendix), GO has different computation complexity for different RVs.", "After choosing a specific RV, one should be able to obtain GO\u2019s computation complexity straightforwardly.", "For quantitative evaluation, the running time for each experiment has been given in the corresponding Appendix.", "Please check there if interested.", "Thank you for pointing out the concern on multi-sample-based REINFORCE.", "We have added another curve labeled REINFORCE2 to the one-dimensional NB experiments (see Fig. 8 for complete results), where the number 2 means using 2 samples to estimate the REINFORCE gradient.", "In this case, REINFORCE2 uses 2 samples and 2 f evaluations in each iteration, whereas GO uses 1 sample and 2 f evaluations.", "As expected, REINFORCE2 still exhibits higher variance than GO even in this simple one-dimensional setting.", "Multi-sample-based REINFORCE for other experiments is believed unnecessary, because (i) the variance of REINFORCE is well-known to increase with dimensionality; (ii) after all, if multi-sample-based REINFORCE works well in practice, why we need variance-reduction techniques?", "Please refer to Sec. 7.2 and Appendix I, the author released code from Grathwohl(2017) (github.com/duvenaud/relax) were run to obtain the results of REBAR and RELAX.", "We adopted the same hyperparameter settings therein for our GO.", "So, we do not think the hyperparameter settings favor our GO in the reported experiments.", "Please refer to the first paragraph of Sec. 7.2, \u201cSince the statistical back-propagation in Theorem 3 cannot handle discrete internal variables, we focus on the single-latent-layer settings (1 layer of 200 Bernoulli random variables).\u201d", "If you are interested, as stated in the last paragraph of Sec 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.", "We believe that procedure might be useful for the inference of models with discrete internal RVs (like the multi-layer discrete VAE).", "Please refer to the last paragraph of Appendix I, where we explained this misunderstanding in detail.", "In short, GO does not suffer more from overfitting; one reason is GO can provide higher validation ELBO.", "Actually, we believe it is GO\u2019s efficiency that causes this misunderstanding.", "We hope your concerns have been addressed. If not, further discussion would be welcomed."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_other", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_followup", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 100, "sentences": ["We thank the reviewer for their time and detailed reading of the paper.", "In the following, we address each of the reviewers comments:", "> Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features [\u2026] Scattering is a perfectly fine approach for initial features.", "Our aim is to investigate the \u201cpower\u201d (or lack thereof) of current self-supervision techniques when applied to standard deep network models.", "This is of interest because self-supervision is a hot topic of research.", "Finding whether e.g. the Scattering Transform can replace the first few layers of a network is interesting, for example to know if handcrafted features can also do as well as (self) supervision for the first few layers, but not susbtitutive of our core investigation (furthermore, we also look ad deeper layers, where these features are unlikely to be competitive).", "Still, such an experiment can help put our findings in context.", "This is why we do include them in Table 2 of the paper, where we show that scattering is not quite as good as even single-image self-supervision.", "We do think that the suggestion of finetuning/retraining the rest of the model is also interesting after replacing the first few layers is also interesting.", "Still, we think that this can complement but not replace linear probing as the latter is a more direct way of finding what the probed layers can do.", "For instance, it is likely possible to learn a good network even by replacing the first layer with the identity function \u2014 it is just a slightly less deep model.", "For these two reasons, we are running the requested experiments and we hope to be able to update Table 4 in the following days.", "> Can the authors clarify how the neural style transfer experiment is performed?", "Indeed, the method by Gatys et al. uses deeper layers as well, which we also use \u2014 straight from the self-supervised method, without fine-tuning or anything else.", "We will update the paper with these details.", "> While I appreciate the computational burden of testing more images, it does feel that Image A and B are quite cherry-picked in being very visually diverse. [...]  It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties.", "Thank you, finding the best single training image, or finding useful synthetic images, are both very interesting ideas. While we are happy to consider doing so as a next step, it is next to impossible to do so in time for the rebuttal (we do not have access to thousands of GPUs).", "Nevertheless, we would argue that the paper stands on by making some interesting observation on the ability of self-supervision to extract useful information from more than one (or few) images, and by investigating the role of data augmentation in this process.", "We hope that the reviewer will agree that the community will be interested in hearing about these findings.", "> I disagree with the claim of practicality in the introduction (page 2, top). While training on one image does reduce the burden of number of images, the computational burden remains the same.", "Our intention wasn\u2019t to say that we can save compute time, but data collection effort (which is also a practical issue in some applications).", "Nevertheless, we agree that our findings have mostly a theoretical value, so we have adjusted the wording to reflect that.", "> And as mentioned above, it doesn\u2019t seem likely that *any* image would work for this method.", "It is true that we did not quite prove that, so we have reworded the text to tone down this claim.", "To be a bit more specific, obviously a blank image would not work, and textureless images would probably not work well either.", "However, we did use in the paper the first two images we manually selected from Google Image Search (while we did select images with some texture, they have not been otherwise been optimized for good performance in our evaluation)", ".", "Thus, we think that it is extremely likely that many other images would work just as well.", "> Finally, more images are needed to learn the deeper layers for the downstream task anyway.", "True, but even for deeper layers a single image achieves two thirds of the performance that self-supervision can squeeze out of a million images, which we think is interesting."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 101, "sentences": ["We thank the reviewer for their time and welcome feedback, which we are incorporating into the revised version.", "R: - Theoretical contribution", "A:", "-- Just a clarification regarding the first contribution: WE from the current task is not used to generate a saliency map for the next task; it is instead used to instruct the learner of the next task about which input areas are more important than others via the attention mechanism.", "This becomes a part of the future learning procedure, not just a post hoc visualisation method as in the original WE .", "As such, the first contribution is not solely about generating new visualisations; it is more about using the learned saliency maps from the past to attend in future learners.", "We therefore believe that the potential of this first contribution as a conceptual framework via which a learner can understand, attend, and then enhance its attention for future tasks, is not small.", "-- Importantly, we are the first to combine interpretability with continual learning and show that interpretability can help continual learning. It is a significant step to bring these two communities together.", "-- It is worth noting that VCL has achieved very good results on most of these benchmarks, so it is very hard to outperform it with a large margin.", "Nevertheless, although the experimental increases might not seem dramatic, they are statistically significant (we have added the statistical significance results to the appendix in the revised version).", "R: - Related work", "A:", "-- Thank you.", "In addition to the locations specified by the reviewer, the first two paragraphs of Section 1 (especially the second paragraph) discuss several other related works.", "We have added more to the revised version.", "R: - https://arxiv.org/pdf/1805.09733.pdf", "A:", "-- Thank you. We have cited the paper in the revised version, and plan to take it into further consideration in future work.", "R: - Yellow color in plots", "A:", "-- We have changed the yellow colour in Figures 2, 3 and 4 to black. Yellow is now no longer used in any plots."], "labels": ["rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_summary", "rebuttal_summary", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 102, "sentences": ["We are thankful for the valuable feedback.", "The main concern of this review is the quality of the writing and experimental details.", "We updated the paper to clarify all the names and ensure that all terms are introduced before they are used.", "The two tower design was necessary since the decision whether theorem T can be rewritten using parameters P requires both pieces of information, so we need to feed them to the network.", "In fact $\\omega$ does not need to predict p, but it gives extra supervision signal and therefore regularizes the prediction.", "The random baseline is necessary because of the unbalanced nature of the rewrite success, this is hard to control, so we added an extra baseline that shows that our results are better than just ignoring any of the input expressions (theorem or parameter).", "In addition, we have simplified the architecture described in the paper by combining the networks $\\sigma$ and $\\omega$, and included the results from this architecture.", "We have significantly improved the experimental section by further clarifying the experiments and expanding them with more supporting measurements.", "We have also moved the two non-baselines out of the baselines section.", "Finally, thank you for the insightful questions!", "With our current setup, the goal is to simply perform reasoning steps in latent space without specifically proving any statements.", "There are several approaches to make the network predict a closed goal, for example by predicting a fixed embedding such as the zero vector.", "We expect that most semantic aspects of the formula could be recovered, but not superficial features as the naming of the variables should not affect the rewriteability of formulas.", "The question how much of the formula can be recovered is probably dependent on the theorem database, since only those aspects that manifest in different rewrite successes are expected to be recovered.", "We don't have much intuition on the decomposability of embeddings, but it seems like a fascinating research direction.", "We are grateful for the feedback which has helped to make the paper much clearer and more readable."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 103, "sentences": ["We thank Reviewer 3 for the constructive feedback.", "Here is our point-to-point response to the comments and questions raised in this review:", "1. \u201cThe novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily\u201d", "GAN inference and adversarial training seek different goals.", "Adversarial training addresses a supervised learning task while GAN inference focuses on an unsupervised learning problem.", "Due to the inherent difference between supervised and unsupervised learning problems, the notion of generalization is defined differently between them.", "Arora et al. (2017) provide the standard definition of generalization error for GANs which is very different from the standard generalization error considered in supervised learning.", "Furthermore, no work in the literature theoretically guarantees that spectral normalization closes the generalization gap for either adversarial supervised learning or GAN unsupervised learning.", "2. \u201cIt is not clear to me that these are some novel results that can better help adversarial training\u201d", "Our work\u2019s main contribution is the theoretical generalization guarantees for spectrally-normalized adversarially-trained DNNs.", "Introducing the adversary can significantly grow the capacity of a DNN.", "Therefore, existing DNN generalization bounds are not applicable to adversarial training settings.", "Our work, to our best knowledge, is the first to show that the adversarial learning capacity of a DNN for FGM, PGM, WRM training schemes can be effectively controlled by regularizing the spectral norm of the DNN\u2019s weight matrices.", "Our numerical results further support our theoretical contribution."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 104, "sentences": ["(1) Prior Network:", "During training phase, we sample from prior network to generate \"pseudo\" observations for unobserved modalities.", "The pseudo observations are then used to estimate the conditional likelihood for such modalities (E_x_j in the ELBO).", "Practically, we follow a two-stage method in our implementation.", "At each iteration, the first stage imputes unobserved modalities (with latent code sampled from approximate posterior for observed modalities, and prior for unobserved modalities), followed by the second stage to estimate ELBO based on the imputation and backpropagate corresponding gradients.", "(2) Conditioning on Ground-Truth Mask:", "We conduct experiments with decoder p(x|z, m) conditioned on the original mask in training set, and observe comparable performance and convergence time.", "The mask distribution might be easier to learn as compared to data distribution (since the mask is fully-observed)", ".", "However, we argue that jointly learning the mask distribution and data distribution provides us an opportunity to further analyze the missing mechanism and potentially can facilitate other down-stream tasks.", "(3) Image Inpainting:", "We appreciate the reviewer's suggestion on evaluate the effectiveness of our model on image inpainting task.", "However, with our current setup, an encoder is trained for each modality respectively, making it difficult to scale to inpainting task, if we treat each pixel as an individual modality.", "Nevertheless, we believe this is an interesting extension.", "The backbone models and mathematical formulations can be very similar, if not the same.", "A potential solution could be to employ patch level encoders to reduce the total number of encoders needed."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 105, "sentences": ["We appreciate your time and effort of reviewing our paper, and thank you for the insightful and constructive comments.", "For simplicity of the main paper, we moved all the detailed proofs to the Appendix.", "More specifically, the proofs for Theorem 1, Lemma 1, Theorem 2, Corollary 1, and Theorem 3 are given in Appendix A, C, D, E, and F, respectively.", "Thanks a lot for pointing out the smoothness conditions for reparameterization; we have carefully revised our paper to remove the misleading statements and to make it clearer when our method (and also the reparameterization trick, Rep) is applicable.", "For your comments wrt discrete random variables (RVs), unfortunately, we haven\u2019t found a principled way to back-propagate gradient through discrete internal RVs (like in multi-layer sigmoid belief networks).", "However, as stated in the last paragraph of Sec. 7.2, we presented in Appendix B.4 a procedure to assist our methods in handling discrete internal RVs.", "We believe that procedure could be useful for the inference of models like the multi-layer sigmoid belief networks.", "As for the conditional independency, it is actually removed after marginalizing out additional continuous RVs (which could be non-reparameterizable RVs like Gamma).", "Also note that one can strengthen the aforementioned procedure by inserting more additional continuous internal RVs into the inference model to enlarge its (marginal) description power.", "The notations are chosen for harmony and also to keep consistency with the main literature.", "For example, one can add another expectation wrt the true data distribution q(x) to the ELBO in Eq. (1), that is, E_{q(x)} [ELBO] = E_{q(x) q(z|x)} [log p(x,z) \u2013 log q(z|x)]  \\propto  - KL[q(x)q(z|x) || p(x,z)].", "For dropout, since the dropout rate is a tunable hyperparameter that need not be learned (thus no back-propagation is required)", ", one can use Rep to construct the q distribution you defined.", "If we understand correctly, in that case we cannot demonstrate our advantages.", "Currently, the proposed method cannot be directly applied to multi-layer sigmoid belief networks (without the procedure in Appendix B.4).", "We have made an explicit statement of this in the revised manuscript.", "Thank you for pointing this out.", "However, it\u2019s believed that Rep cannot be applied to Gamma distributions [1,2].", "We have revised our statement to \u201cThere are situations for which Rep is not readily applicable, e.g., where the components of y may be discrete or nonnegative Gamma distributed\u201d.", "[1] F. Ruiz, M. Titsias, and D. Blei. The generalized reparameterization gradient. In NIPS, pp. 460\u2013468, 2016.", "[2] C. Naesseth, F. Ruiz, S. Linderman, and D. Blei. Rejection sampling variational inference. arXiv:1610.05683, 2016.", "Yes, Lemma 1 shows that our deep GO will reduce to Rep when Rep is applicable.", "We are not sure whether you were asking about the difference in Fig. 1 or Fig. 2.", "So, two responses are given below.", "(A) In Fig. 1, the difference comes from the definition of node y^(i).", "For deterministic deep neural networks, node y^(i) is the activated value after an activation function, where deterministic chain rule can be readily applied; while for deep GO gradient, node y^(i) might be the sample of a non-reparameterizable RV, where deterministic chain rule is not applicable.", "Please also refer to the main contribution (ii) of our response to Reviewer 2.", "(B) If you were interested in the difference in Fig. 2 (a)(b), the reasons include (1) the standard Rep cannot be applied to Gamma RVs; (2) both GRep and RSVI are designed to approximately reparametrize Gamma RVs; (3) GO generalizes Rep to non-reparameterizable RVs; or in other words, GO is identical to the exact Rep for Gamma RVs.", "Yes, the sticking approach was implicitly adopted for all the compared methods when it is applicable.", "We have made a clear statement in the revised paper.", "Since stochastic computation graph (SCG) is based on REINFORCE and our method is based on GO, the comparison between SCG and our method is (roughly speaking) identical to that between REINFORCE and GO.", "That is, SCG is more generally applicable but with higher variance; the proposed method has less generalizability but with much lower variance.", "We have added the following discussion into Related Work.", "\u201c\u2026as the Rep gradient (Grathwohl et al., 2017).", "SCG (Schulman et al., 2015) utilizes the generalizability of REINFORCE to construct widely-applicable stochastic computation graphs.", "However, REINFORCE is known to have high variance, especially for high-dimensional problems, where the proposed methods are preferable when applicable (Schulman et al., 2015).", "Stochastic back-propagation\u2026\u201d", "Thank you for pointing out these fundamental conditions, which we have added into the revised manuscript.", "We hope your concerns have been addressed. If not, further discussion would be welcomed."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 106, "sentences": ["Thank you for your review!", "> You have the phrase \"allowing to imagine thousands of trajectories in parallel\". I would like some elaboration on this. I think you have ideas of what is happening in the latent space that I am not following.", "The latent states are defined as 330 dimensional activation vectors with 300 deterministic and 30 sampled components.", "We can predict imagined trajectories for thousands of initial states in parallel since they fit into the memory of the GPU at once.", "Specifically, Dreamer predicts imagined trajectory of length 20 from each of the 50x50=2500 latent states for the current training batch.", "Performing the same amount of imagination steps with a dynamics model that generates images during inference would be challenging.", "For example, we could only fit up to 500 trajectories of length 10 into GPU memory with the SV2P model (Babaeizadeh et al. 2017).", "Besides the memory constraints for predicting multiple trajectories in parallel, predictions in the latent space are often an order of magnitude faster than in pixel space.", "> I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. How does the effort compare to the 1E9 steps of the base line your refer to?", "We will include more details in the final version.", "Dreamer was run for 2e6 environment steps (20 hours) compared to D4PG that was run for 1e9 environment steps (24 hours).", "Both algorithms used a single GPU each.", "As outlined in our previous answer, Dreamer performs 10 billion imagination steps throughout training.", "Please note that imagination steps are often considered free for robotic learning, because the bottleneck is the time of physical interaction with the real world.", "> [...] understanding what structures get learned in latent space, are the in fact compact, diverse?", "The amount of information in the latent representation is upper bounded by the KL divergence loss.", "We observed a typical KL divergence of 15 bits per time step,", "compared to the 64 x 64 x 3 x 8 bits of the corresponding images.", "This bounds the compression ratio to at least 1 : 6500.", "We make no claims regarding diversity.", "However, since the behaviors are learned purely in latent space, there must be a sufficient amount of diversity to solve the presented tasks.", "We agree that exploring the semantics of the latent space is an interesting orthogonal direction for future work.", "> Perhaps there is room for memory/memories in the latent space?", "It would be interesting to combine Dreamer with external memory modules.", "Gregor et al. (2019) provide a comparison of such modules.", "However, this would better be addressed in a separate work to keep the paper focused on the main contribution of learning behaviors by latent imagination."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 107, "sentences": ["The reviewer feels that the proposed model is too simple, and suggests comparing against more complex models, suggesting a few in particular.", "Our response is twofold:", "- The accuracy of the proposed simple model exceeds the accuracy of far more complex models by a wide margin, and this consistently over a range of networks (all commonly used networks in this literature), against a range of baselines (all either commonly used baselines, or methods known as achieving state-of-the-art accuracies), and on two important tasks (link prediction and multi-label classification).", "We do not agree that its simplicity reduces its merit, we think it rather contributes to its merit.", "- We thank the reviewer for suggesting additional comparisons with specific more complex models, although we feel that calling these methods 'stronger' requires some clarification or support.", "Of the suggested methods, graph convolutional and message passing neural networks need attributed graphs as inputs, and are thus not applicable.", "We have in the meantime been able to include struc2vec in the evaluation, again showing superiority of CNE by a wide margin -- showing that it is maybe more complex but certainly not 'stronger' as in 'more accurate'.", "The paper will be updated very soon to include these results.", "Perhaps the reviewer is incredulous regarding this large increase in performance a method as 'simple' as CNE achieves w.r.t. the state-of-the-art.", "We believe that this is due to the conceptual advance made in CNE.", "In our opinion a conceptual advance that achieves a strong boost in accuracy without increasing complexity, is at least as valuable as a method that achieves the same boost in accuracy while also increasing complexity.", "Also note that all code is provided, and we invite the reviewer to replicate our experiments."], "labels": ["rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 108, "sentences": ["We thank the reviewer for the comments on our paper.", "Designing more efficient streaming algorithms with machine learning techniques is a relatively new research topic and we have included more related work in our updated version of the manuscript (highlighted in the blue color)."], "labels": ["rebuttal_social", "rebuttal_done"], "confs": [1.0, 1.0]}
{"abstract_id": 109, "sentences": ["Thanks a lot for your review.", "We address your remarks below:", "\"RNN with an accumulator / too minor a contribution \"", "We want to emphasis that the accumulator implemented in the newly proposed architectures has an inherently different nature than accumulators used so far: While accumulators such as LSTM cells accumulate knowledge about the state of a sequence, our architectures produce meaningful intermediate results, which can simply summed up to estimate the final set utility.", "Producing such intermediate results, which model the nature of the problem much better than previous approaches, is the key idea presented in this paper and a major benefit of the proposed architectures.", "This also follows the idea of the Choquet integral.", "\"Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.\"", "To improve reproducibility, we published the data and the code.", "We implemented in our work the most basic version of our idea as well as the most basic version of each reference model.", "Hence, the code of the implemented architectures only consists of few lines and can be checked easily.", "We think that it is not fair to simply mistrust our results since we made our work fully transparent."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 110, "sentences": ["We thank the reviewer for the constructive feedback.", "The use of a fixed embedding space $L$ and a separate space $L^\\prime$ was useful as it naturally prevents the collapse of embeddings.", "However this could be counteracted by stopping the gradient at the right place in the simplified architecture  which was suggested in the original paper and is now described in the updated paper.", "As suggested, we have added further analysis of failure cases, and describe strategies for negative mining from these examples.", "In addition, we have included a brief description of the graph neural network architecture used in Paliwal et al (2019).", "We also include further details on the construction of training set.", "Training a decoder to predict the results of rewrites from the latent space is an interesting idea, but is technically challenging and we felt it was out of scope for this paper.", "We managed to counteract the noisiness of predicted embedding by training on noisy embeddings which trains the network to be robust to random changes and improves the prediction of multi-step rewrites significantly.", "We are grateful for the suggestions that contributed significantly to improving the quality of the paper."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_reject-request", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 111, "sentences": ["We thank you for the constructive feedback and are glad that you enjoyed the paper.", "Here we discuss some of your comments.", "R1: \"missing in the paper is the comparison to two other class of RL methods: count-based exploration... In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.\"", "=> We chose to focus on dynamics-based approaches in this paper because we found them more straightforward to efficiently parallelize than the published pseudo-count methods.", "This allows us to be able to run more and larger experiments on many environments.", "Interestingly, increased parallelization also significantly helped the exploration strategies as shown in Figure 3(a).", "Further, we will add the details of preliminary experiments using pseudo-count in the supplementary.", "In particular, we were not able to find any official public implementation of the pseudo-count methods.", "We experimented with a third party implementation trying to see if it could play Breakout without extrinsic rewards, but did not achieve sufficient success and found it to be too slow for scaling it up to a large-scale study.", "R1: \"the experiments around VAE... While the paper shows experimentally that they aren't as successful... there's no further discussion on the reasons for poor performance.\"", "=> We found that VAEs overall worked well and were sometimes better than other representation learning methods, but often were causing instability at training.", "We don't claim such instability is an inherent property of the VAE feature learning method, but probably stems from the continually changing data distribution as agent makes progress.", "Indeed modeling the density of a non-stationary distribution, with modes appearing and disappearing, is a challenging and an active research problem.", "We will clarify this in the final version.", "R1: \"An interesting area for future work could be on early stopping techniques for embedding training\u2026 maybe somewhere in between could be the sweet spot with training\"", "=> Thank you for the excellent suggestion.", "We agree that there may be some optimal tradeoff between features that are stable and features that adapt to the environment.", "Such tradeoffs would be interesting to investigate, and might be crucial to getting learned features to perform significantly better than fixed random features.", "We will add this in the discussion/future work section of paper.", "R1: \"What are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).\"", "=> Thank you. We will add more details on the architectures to the appendix."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 112, "sentences": ["Thank you very much for your strong recommendation!", "1) Intuition about the improvement", "Though not explained in Section 4.", "The intuition for NADPEx is given in Section 3.", "Interpretation for as efficient or even faster exploration in dense environment (4.1) is that NADPEx could encourage more diverse exploration, while absorb experience from it in a relatively efficient way.", "For sparse environments (4.2), where temporally consistent exploration is crucial for learning signal acquisition, NADPEx outperforms vanilla PPO.", "It could also beat parameter noise if difficulty is increased, because intuitively low variance in gradients is a boon for faster learning.", "Improvement in 4.3 and 4.4 are basically from the theoretical grounding of NADPEx, which we believe is one of our contributions.", "Specifically, improvement in 4.3 is from high level stochasticity's adaptation to the low level; while that in 4.4 could be interpreted with the idea of trust region, that policy should be updated to somewhere near the sampling policy in the policy space, such that collected experience are usable (on-policy).", "In NADPEx, trust region also contains the meaning that dropout policies are close to each other for more efficient exploration.", "2) Limitation of NADPEx", "One of the limitation we see from NADPEx is that dropout policies are not directly interpretable from their network structures, while interpretability and composibility are prerequisites for reusing them in more complicated tasks.", "Luckily, modeled as latent random variables, an information term could be added to the objective as in [1, 2].", "This is also a direction for future research work.", "[1] Florensa et al., \"Stochastic neural networks for hierarchical reinforcement learning\", ICLR 2017.", "[2] Hausman et al., \"Learning an Embedding Space for Transferable Robot Skills\", ICLR 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 113, "sentences": ["First of all, thank you for taking your time to review our paper and providing feedback. We have judiciously taken the comments of the reviewers, and apologize for the late response due to additional experiments and modifications of the paper.", "Remark 0. It needs other theoretical explanation (ex. co-training)", "A: We have modified our paper and added some theoretical explanation in the introduction on page 2.", "Remark 1. \" Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter. \"", "A: We do not agree with your opinion.", "The formulae of the softmax and sigmoid are as follows.", "The softmax function : exp(f_j(z)) / sigma(exp(f_k(z))", "= 1 / ( 1 + exp(-f_j(z)) \u00d7 (exp(f_1(z)) + ... + exp(f_j-1(z)) + exp(f_j+1(z)) + ... exp(f_n(z)))", "The sigmoid function : 1 / (1 + exp(-g(z)))", "where z, f(z), and g(z) represent the final layer of the backbone network, classification network, and selection network respectively.", "As you said, if f_j(z) is very high and the other f(z)s are moderate, it can work like sigmoid.", "However, even if f_j(z) is not much high, the softmax output can be close to 1 with extremely smaller values for other f(z)s", "because:", "The softmax output : 1 / ( 1 + exp(-f_j(z)) \u00d7 0) = 1", "We experimented with a high softmax output threshold (epsilon = 10^(\u22124)).", "Although epsilon was 10^(\u22124) (threshold = 0.9999), an average of about 800 unlabeled data was added for the case of 100% of the non-animal data.", "Even at 0% of the non-animal data, performance is lower than the fixed mode of the sigmoid.", "This shows the limitation of thresholding with softmax.", "The result of new SSL problems on the CIFAR-10 dataset with 5 runs are as follows:", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "activation function / softmax (error/added data) / sigmoid (error/added data)", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "supervised", "/", "( 22.27 / 0 )", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "0%", "/", "(18.27 / 4,306.8)", "/", "(17.84/2,338.8 )", "25%", "/", "(18.35 / 3,350.4 )", "/", "(18.38 / 1470.0 )", "50%", "/", "( 18.72 / 2580.0 )", "/", "(19.04 / 811.2 )", "75%", "/", "(20.33 / 1,711.2 )", "/", "( 20.07 / 315.6 )", "100%", "/", "(20.71 / 864.0 )", "/", "( 20.24 / 1.2 )", "\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014", "Remark 2. Expression, and details (ex. number of iterations, stopping criteria, typos and grammar errors)", "A : We apologize to the reviewer for the lack of clarity in the manuscript.", "We have modified our expression, typos and grammar errors.", "Regarding the details on hyper-parameters:", "- We set parameters as follows.", "The number of training iteration and thresholding epsilon are very important parameters in our algorithm and have a considerable correlation with each other.", "In the first experiment, the iteration number remains fixed and the growth rate of epsilon is adjusted so that the validation accuracy saturates near the settled iteration number.", "While the validation accuracy is evaluated using the cross-validation, we set the number of training iteration to be 100 so that the model is trained enough until it saturates.", "Epsilon is increased in log-scale and begins at a very small value (10^(\u22125)) where no data is added.", "The growth rate of epsilon is determined according to when the validation accuracy saturates.", "The stopping criterion is that the accuracy of the current iteration reaches the average accuracy of the previous 20 steps.", "If the stopping iteration is much less than 100 times, the epsilon growth rate should be reduced so that the data is added more slowly.", "If the stopping iteration significantly exceeds 100 iterations, the epsilon growth rate should be increased so that the data is added more easily.", "We allow 5 iterations as a deviation from 100 iterations and the growth rate of epsilon is left unchanged in this interval.", "(In previous versions, the growth ratio of epsilon for CIFAR-10 was applied to SVHN and CIFAR-100.", "However, since the epsilon growth rate is different for each dataset, as the reviewer mentioned, we have performed the cross-validation for SVHN and CIFAR-100 and modified our results.) As a result, the epsilon is gradually increased in log-scale by 10 times every 33 iterations in CIFAR-10 and SVHN.", "In the case of CIFAR-100, the epsilon is increased by 10 times in log-scale every 27 iterations.", "In the second experiment, we leave the epsilon fixed and simply train the model until the stopping criterion is satisfied.", "Other details are the same as those of the first experiment.", "(In previous versions, the training iterations of fixed mode had been fixed.", "Thanks to the comment from the reviewer, we were able to rearrange the content and set training iteration by cross-validation.)", "Remark 3. \"SST itself is only comparable with or even worse than the state-of-art methods.\"", "A : As mentioned in our paper, SST has comparable performance to other conventional SSL algorithms.", "In Table 2 of our paper, SST achieves 34.89% on CIFAR-100, which is higher than TempEns[1](38.65%), 11.82% on CIFAR-10, which is slightly worse than VAT+EntMin[2](10.55%), and perform worse 6.88% on SVHN.", "However, SST can solve the real problem of the existence of out-of-class unlabeled data.", "[1] Laine, Samuli, and Timo Aila. \"Temporal ensembling for semi-supervised learning.\" arXiv preprint arXiv:1610.02242 (2016).", "[2] Miyato, Takeru, et al. \"Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning.\" arXiv preprint arXiv:1704.03976 (2017).", "Remark 4. \"Combining SST with other existing techniques can help.", "However, the additional cost is expensive.", "Further demonstrations are necessary for the proposed SST method.\"", "A : It is true that combining and the additional cost is expensive.", "Therefore, we have modified our expressions in the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 114, "sentences": ["Thank you for your thorough assessment and helpful comments! To answer your two questions:", "1) Upper bound", "You are right that it would be ideal to optimize the target loss L_T(h, f_T) directly.", "However, this is not possible because we do not have labelled target data (i.e., f_T is unknown).", "Minimizing an upper bound is arguably the only viable option *with theoretical generalization guarantee*. It is a common practice in the domain adaptation community (Mansour et al., 2009a, 2009b; Ben-David et al. 2007, 2010; Cortes and Mohri, 2011), and it is essentially the key idea of PAC learning and generalization analysis (Gy\u00f6rfi et al., 2006; Sch\u00f6lkopf et al., 2002; Vapnik, 2013).", "Besides, our method *directly* optimizes the upper bound without resorting to heuristics, unlike prior methods.", "Ref:", "- Gy\u00f6rfi, L., Kohler, M., Krzyzak, A. and Walk, H., 2006. A distribution-free theory of nonparametric regression. Springer Science & Business Media.", "- Sch\u00f6lkopf, B., Smola, A.J. and Bach, F., 2002. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press.", "- Vapnik, V., 2013. The nature of statistical learning theory. Springer science & business media.", "2) More experiment", "Thank you for pointing out these datasets.", "We have conducted further experiments on the Office-Home dataset,  using the ResNet50 as the backbone architecture and changing the classification head to 65 classes.", "As we can see in the new Section 5.3, our method achieve state-of-the-art performance and outperform all alternatives", "; these results are statistically significant.", "In addition, we have added one more competitive baseline (M3SDA) from the DomainNet paper you mentioned, using their public code with a few necessary adjustments for each dataset (e.g., network architecture, etc).", "We ensure that all methods use the same backbone architecture for a fair comparison.", "Again, our method outperform M3SDA in all datasets.", "If you have any other comments or concerns, we are happy to provide further feedback.", "Thank you!"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 115, "sentences": ["We thank our third reviewer for his comment.", "We do understand your concern about the significant increase in computational time.", "However, we believe that in the context of active learning, the main problem is not related to computational power, rather to the scarcity of data.", "Therefore, a better way of making the most out of little data is critical.", "For example, a 10 \\% increase for only 300 samples acquired, could make a huge difference in a critical field where active learning is most valuable.", "We believe that this is exactly what we manage to achieve with our method and this comes as a result of a better representation of uncertainty during AL.", "Furthermore,  Beluch et al. (2018) showed that going beyond 3 networks in their deterministic ensemble method does not add any significant improvements in terms of performance.", "Therefore we use 3 stochastic ensembles for our method.", "As for the novelty of this method, although it seems more like an engineering solution, we believe that it makes a significant contribution in the field of deep active learning."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 116, "sentences": ["We thank the reviewer for their time and welcome feedback, which we are incorporating into the revised version.", "R: - \"important features for the new task should be in similar locations ...\"", "- \"the locations for important features should be comparatively stable ...\"", "A:", "-- Continual learning typically assumes a degree of similarity among the tasks.", "If tasks are completely different from each other, then most continual learning frameworks will somehow struggle.", "For example, the standard Split MNIST benchmark is in line with this \u201clocations of important features\u201d assumption.", "Having said that, we acknowledge that more agility to, at least, discover that early on would be beneficial.", "More importantly, a normalization strategy on top of our attention map would help enhance its invariance properties, potentially leading to a more robust treatment of the locations of important features.", "In page 4 in the revised version (footnote 3), we have clarified this and notified its potential for future work.", "-- Thank you for the suggestion regarding the fixed attention map.", "We tried an experiment using the fixed attention map as a baseline, and as expected it performs significantly worse than ours.", "We have added that to the revised version (see p.6 and Appendix A).", "R: - FSM vs. Classification performance", "A:", "-- It is true that evaluating the FSM is not necessarily the same as the classification results, which is precisely the reason why we show both in our results.", "As specified in page 2, \u201cHere we propose a new measure ...\u201d - our point in this regard is to propose another (different) manner via which catastrophic forgetting can be estimated, which is not the same as the classification accuracy.", "The goal is that (as we know and agree they are two different measures that might agree or disagree in their judgments on catastrophic forgetting) both can be used to inspect the degree of catastrophic forgetting.", "We have further clarified that in Section 6.2 in the experiments by stressing that the obtained FSM results \u201calong with the classification results\u201d denote the significance of the whole framework in addressing catastrophic forgetting.", "It is definitely a good idea to analyse the correlation between changes in classification accuracy and in FSM values, thank you. We will rigorously investigate this in future work.", "R: - Statistical significance", "A:", "-- Thank you.", "We have added the statistical significance results to the revised version.", "Since we were concerned that adding this information to the plots would make them harder to read", ",  statistical significance of the the average accuracy and FSM results obtained after completing the last two tasks from each dataset, i.e. the corresponding values of the last two tasks of all the plots in Figures 1, 2, 3 and 4, are now displayed in the tables in Appendix A.", "Checking cases where the learner incorrectly classifies the image in the second time step is sound and will be inspected in future work.", "R: - Clarity", "A:", "-- We have fixed the typos in the revised version, thank you: i) The first sentence of the third paragraph in Section 4 now reads: \u201cFor input images of ..., the averaged weight of evidence matrix  is referred to as $\\text{WE}_{\\bm{i}}(\\bm{x}) \\in \\RR^{\\bm{r} \\times \\bm{c}}$.\u201d  ii) In page 6: \u201cThe size of the surrounding square \u2026 is 16 $\\times$ 16 pixels."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 117, "sentences": ["Thank you for the comments!", "To review\u2019s questions:", "- As the experimental results shows, with position information alone, the agent is able to learn to push or pick up the object, therefore we consider position information alone (without velocity information) is sufficient in our case.", "- For MISC, the method needs to know what are the states of interests and what are the context state.", "While, the states of interest can be any states that users are interested in, such as a part of the robot states or the object states.", "The context states are some other states, which are different from the states of interest.", "In robotic tasks, the states of the robot and the object states are normally available [Andrychowicz et al 2018, Plappert et al 2018].", "To automatically detect the state of interests and the context states, we can train the agent with random state splits and then chose the combination, which is suitable for the tasks at hand.", "References:", "[1] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pp. 5048\u20135058, 2017.", "[2] Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow- ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce- ment learning: Challenging robotics environments and request for research. arXiv preprint arXiv:1802.09464, 2018.", "- Yes, MISC can deal with the case, when there are multiple objects of interest.", "We added new experiments showing the agent can learn to manipulate two balls.", "We define the mutual information intrinsic reward as I(S^i_{1}, S^c)+I(S^i_{2}, S^c).", "The experimental results are shown in the new video at https://youtu.be/l5KaYJWWu70?t=148, where we show that a robot car can learn to manipulate two balls in the same episode.", "- Equation (4) is not the mutual information between two trajectories of states.", "It is an estimation of mutual information between two sets of states. And the states are sampled from the same trajectory.", "Therefore, we do not need to decompose Equation (4) to evaluate Equation (3).", "- We add the experimental details in the Appendix.", "- The discriminator is trained along with the policy.", "For example, in the case that we update the agent 200 times in each epoch, then we also update the MISC 200 times per epoch.", "For more detailed information, please refer to our code at https://github.com/misc-project/misc", "- Compared to the dense reward, with the negative L2 distance between the robot and the object, the robot can only learn to reach the object but will not learn to push or pick up the object because when the robot reaches the object, the negative L2 distance is already zero.", "However, MISC has the advantage that it not only enables the agent to learn to reach but also learn to push and pick & place.", "- If we train the MISC and DIAYN at the same time, the DIAYN reward might be dominant. Subsequently, The agent might not learn to control the states of interests with MISC.", "- MISC-p works similarly to PER.", "The main difference is that MISC-p uses the estimated mutual information quantity as a priority, while PER uses the TD-error as a priority for replay.", "For more detail on PER, please refer to the original PER paper [Schaul et al 2016].", "Reference:", "Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In International Conference on Learning Representations, 2016.", "- We first scale the intrinsic and the extrinsic reward between 0 and 1 and then use equal weights for these two rewards.", "- For the opposite situation, we can use negative mutual information rewards to encourage the agent to learn to \u201cavoid\u201d some objects.", "- The discriminator uses the same amount of (s,a,s') experience as VIME consumes because the discriminator is fixed after pre-training.", "VIME can only be trained along with the policy.", "VIME cannot be pre-trained, otherwise, it won\u2019t detect novel states.", "- Transfer the learned discriminator from Push to the Pick&Place should still help the agent to learn the pick & place task because the transferred discriminator will help the agent to learn to reach the object at least. As long as the state inputs for the discriminator are the same, then MI discriminator can be transferred among different tasks."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 118, "sentences": ["We thank the reviewer for his comments and observations.", "Following are the answers to each question you have raised.", "R1Q1(a): \u201cThe classification of base class into super classes seems questionable to me.", "In the meta-learning language, the author attempts to learn a good representation of graphs", "based on different graph classification tasks generated by a task distribution", ".", "In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).\u201d", "R1A1(a): Our model has two major components, $C^{sup}$ (for super-class prediction) and $C^{GAT}$ (for graph label prediction).", "During the training phase of our classification, $C^{sup}$, which is a MLP layer, learns the super-class labels of the samples based on GIN\u2019s extracted feature vectors (which represent base class labeled graphs).", "While, $C^{GAT}$ takes as input the \u201cgraph of graphs\u201d (supergraph) which models the latent inter-class as well as intra-class information and is constructed in every training batch, along with base-class labels, to learn the associated class distribution.", "Then, during the fine-tuning phase on graphs with novel class labels, the feature extractor\u2019s (GIN) parameters are fixed and $C^{sup}$ is used to infer the super-class label of the novel class labeled graphs.", "Then, the parameters learned by $C^{GAT}$ get updated and further \u201cfine-tuned\u201d for better performance on the novel samples.", "In addition to our brief overview, you could also find a very neatly detailed summarization of our method in reviewer 2\u2019s comments (paragraphs 1-4).", "The meta-learning framework, where batches are sampled as \u201cepisodes\u201d with N-way K-shot setting, does not perform as well in our few-shot setting on graphs for the following reasons:", "1) We have very limited total number of training classes (in order of 10s), when compared to the image domain (order of 100s and 1000s).", "This limitation hampers learning across tasks and generalization to new unseen tasks.", "2) In each of our batches, we randomly sample a fixed-size of training samples belonging to the set of N labels chosen.", "Therefore, when building our supergraph, we end up with k-NN graphs of \u201cvariable size\u201d per super-class, compared to fixed size (K nodes) k-NN graphs that we would have got using episodic learning.", "We suspect this further allows our GAT to learn and generalize better to unseen graphs.", "Furthermore, in [1], the authors use a similar strategy in their \u201cbaseline++\u201d method and produce good results.", "Their findings are also in sync with our empirical finding.", "[1] Chen et~al. \u201cA closer look at Few-Shot Classification\u201d, ICLR 2019", "R1Q1(b): Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.\u201d", "R1A1(b): In every batch of graphs during both training and fine-tuning phase, each graph is associated with its corresponding graph label.", "In case of training, its a base-class and in the case of fine-tuning its a novel class", ".", "In the case of $C^{GAT}$, the graph is accompanied by a regular class label and in case of $C^{sup}$, the graph is accompanied by a superclass label.", "R1Q2: \u201cThough seemingly very important to the architecture, the purpose of constructing the super-graph $g^{sup}$ in the training of $C^{CAT}$ seems to be unclear to me.\u201d", "R1A2: What makes few-shot learning particularly difficult compared to common machine learning settings is the dearth of training examples, which results in a bad empirical risk approximation for the expected risk and therefore gives rise to an empirical risk minimizer that is sub-optimal.", "Reducing the required sample complexity can result in a better empirical risk minimizer.", "Therefore, given a very large space of hypotheses H, our goal is to further restrict and constrain H using some prior knowledge because a reduced H has reduced sample complexity and thus requires fewer training samples to be trained.", "We provide this \u201cprior knowledge\u201d in the form of a \u201cgraph of graphs\u201d, namely our super-graph $g^{sup}$, which captures both the latent inter-class and intra-class relationships between classes.", "Observe that in $g^{sup}$, we build a k-NN graph PER super-class, restricting any flow of information between super-classes, thus further restricting H. We force our model to jointly learn both the superclass and graph class labels.", "This way similar classes (grouped under a superclass) together contribute to learning a general prior representing the superclasses and each superclass also provides \u201cguidance\u201d to better train with the few samples assigned to that superclass.", "The introduction of this prior knowledge in the form of a supergraph in $C^{GAT}$ during training also helps generalize better to the novel samples that are presented to our model in the fine-tuning stage.", "Additionally, we would also like to draw attention to the supergraph usage summary provided by reviewer 2 in paragraph 3 of their comments."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 119, "sentences": ["Thank you very much for your supportive remark! We are happy that the writing is clear to you. Below we provide additional comments regarding your questions.", "1). Low-rank assumption:", "This work was primarily motivated by the observation that many systems exhibit strong relationship among states and actions, governed by potentially simple dynamics.", "This might eventually lead to structures within the optimal solution.", "We hope that this empirical study would motivate further theoretical analysis on structures within the community.", "Below are some thoughts on the potential theoretical motivation for this assumption:", "a) It is possible that the states and actions in consideration have some latent variable representations.", "If the optimal Q function is a piecewise analytic function on the latent variables, then there are works arguing the approximately low-rank property of the resulting matrix [1].", "b) There are theoretical works in RL and Markov process that assume that the transition kernel can be decomposed to a low-dimensional feature representation [2,3].", "These assumptions on the transition kernel may lead to low-rank optimal Q matrices.", "c) For continuous problems, theoretical analysis often needs to assume some sort of smoothness in the Q function [4,5].", "It is possible that such smoothness in the Q function will result in a low-rank Q matrix when evaluated at finite but fine enough discretized grid.", "[1] Udell, Madeleine, and Alex Townsend. \"Why Are Big Data Matrices Approximately Low Rank?.\" SIAM Journal on Mathematics of Data Science 1.1 (2019): 144-160.", "[2] Yang, Lin, and Mengdi Wang. \"Sample-Optimal Parametric Q-Learning Using Linearly Additive Features.\" International Conference on Machine Learning. 2019.", "[3] Sun, Yifan, et al. \"Learning low-dimensional state embeddings and metastable clusters from time series data.\" Neural Information Processing Systems 2019.", "[4] Yang, Zhuora, Yuchen Xie, and Zhaoran Wang. \"A theoretical analysis of deep Q-learning.\" arXiv preprint arXiv:1901.00137(2019).", "[5] Shah, Devavrat, and Qiaomin Xie. \"Q-learning with nearest neighbors.\" Advances in Neural Information Processing Systems. 2018.", "2). Dynamical manner for the number of incomplete observations and whether the strategy can be adapted to the nature of the problem:", "This is a great point and definitely an interesting future direction.", "In the current work, it is not immediately that one could easily detect the rank and adapt the algorithm in a principled manner.", "As one practical solution, it may be possible to dynamically adjust the regularization in a manner similar to cross validation.", "At each step, for the submatrix, one could randomly sample a portion of the entries for ME, while keeping another fraction of the remaining entries as a validation set.", "If the recovered matrix via ME has a low reconstruction error on the validation set, it is likely that a suitable low-rank approximation is sufficient and has been found by the ME oracle.", "In contrast, if the reconstruction error is large, the algorithm might have been too aggressive on finding a low-rank solution while a higher rank solution is indeed necessary.", "As such, one could then adjust the algorithm to increase the number of observations for ME or try to reduce the level of low-rank regularization.", "The above cross validation scheme might be an interesting complement to our current approach.", "Overall, we believe that principally solving those questions you posted are meaningful and important directions that worth further investigations.", "3). Beyond the low-rank assumption and use a more elaborate type of prior:", "Thank you for your inspiring advice.", "Without any elaborate prior information, rank is a natural point to study the global property of a matrix.", "In principle, understanding structures in MDP could also be potentially explored, and we believe that it is possible to extend to other types of scenarios with prior information about the MDPs.", "However, at the current stage, we do not have a particularly systematic approach to explore more elaborate type of structures in MDPs.", "While this paper is focusing on low-rank structures, as the reviewer noted, there can be other structures to be explored.", "We hope that our paper could serve as an example, and further motivates future studies for exploiting structures in MDP.", "4). Notation in Section 4.2:", "Thank you for the suggestion.", "We will expand the definition of the notation to make them clearer."], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_future", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_social", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 120, "sentences": ["We would like to thank Reviewer 3 for the review and constructive suggestions.", "Our responses inline:", ">it would be nice to also propose unconditioned experiments.", "-We agree; this was simply not within the scope of the work we conducted.", ">I understand that no data augmentation was used during training?", "-This is correct, and consistent with previous works (Spectral Normalization and WGAN-GP).", "We briefly experimented with data augmentation (random crops and horizontal flips) but did not notice any measurable performance difference.", ">clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?", "-Yes, this can effectively be seen as modifying the PDF of z to have no mass outside of the truncation threshold.", "TensorFlow offers a built-in implementation with tf.random.truncated_normal.", ">A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.", "-We have revised the abstract to explain the truncation trick as controlling the tradeoff between fidelity and diversity by reducing the variance of the Generator\u2019s input.", ">A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.", "-Thanks for the pointer! We have added this reference.", ">It would be nice to add a figure of random generations.", "-In the caption of Figure 5, we include a link to an anonymous drive folder with sample sheets at different resolutions and truncation values, with 12 random images per class.", ">make the bib uniform: remove unnecessary doi - url - cvpr page numbers", "-Thanks, we have fixed this."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 121, "sentences": ["We thank the reviewer for the thoughtful review.", "Responses:", "- We clarified the unclear parts, and will upload the revised version asap. (Also see below for specific responses.)", "- It is unclear to us if the reviewer thinks the computational complexity is high, or the mathematical complexity.", "With regards to mathematical complexity, we believe the model is actually rather simple (see also other reviews).", "Thus, we assume computational complexity is meant.", "Computational complexity is discussed in detail in the manuscript though, and is certainly not higher than competing methods (in part thanks to the low mathematical complexity of the model).", "See also next point.", "- The datasets we used are as large as the datasets used in other related work in the area.", "To demonstrate CNE's superior scalability, we included another network with around 200.000 nodes and around 1.000.000 edges (http://snap.stanford.edu/data/loc-Gowalla.html), run on a basic single CPU laptop.", "Again, CNE outperforms all other methods in accuracy by a wide margin, and is substantially faster as well.", "The results are included in the revised manuscript.", "Detailed comments:", "- \"In the introduction, \"it is in general impossible to find an embedding in R^d such that ...\", [...]?\"", "We apologize for having been a bit brief here, we will clarify this in the revision (uploaded asap).", "We meant to say that in network embedding methods that aim to model first-order proximity (where proximity in the embedding space implies a higher probability of being linked), this is a requirement (otherwise, proximity of v and v' would imply they are likely to be linked).", "Thus our argument only applies to such first-order proximity methods.", "Methods that aim to model second-order proximities (where proximity in the embedding space implies a greater overlap between the sets of adjacent nodes), however, are similarly vulnerable.", "For example, there can be a 50% overlap (which is highly significant in sparse networks) between the neighborhoods of nodes A and B, as well as between the neighborhoods of nodes B and C, but zero overlap between the neighborhoods of nodes A and C.", "This would mean that nodes A and B need to be embedded close to each other, nodes B and C as well, but nodes A and C distant from each other.", "The triangle inequality makes this hard.", "Finally, these are but examples of how a Euclidean embedding on its own lacks representational power.", "We believe that our empirical results also demonstrate this without having to refer to easy-to-identify problematic situations for pure embedding-based methods.", "- \"In Equation (2), How is P_ij defined exactly [...]?\"", "They are not parameters: they are numbers between 0 and 1 representing the prior probability of a link between nodes i and j (i.e. prior to seeing the embedding).", "These numbers are such that the prior knowledge of the types described are satisfied in expectation.", "In other words, they are implied and can be computed automatically and highly efficiently based on prior work, after one has decided on which prior knowledge to use.", "For details about how P_ij are fitted given such prior knowledge constraints, and how they can be represented efficiently, we have to refer to Adriaens et al. (2017) and van Leeuwen et al. (2016).", "We have however summarized the relevant aspects: the fact that all probabilities P_ij, although there are n^2 of them, can be represented using much fewer parameters, and the fact that they can be fitted highly efficiently (in our experiments, even on the largest networks, this always took only a tiny proportion of the total computation time).", "In the new version to be uploaded soon, this will be further clarified.", "- \"In Equation (6), the posterior distribution should be P(X|G) [...]?\"", "No, the equation is correct as stated.", "Footnote 2 warned the reader about this, as we know it is unusual.", "The posterior is a distribution for the network, such that finding the best embedding is indeed a maximum likelihood problem (not a maximum a posteriori problem), even though the likelihood function is computed as a posterior given a prior for the network and a conditional for the embedding given the network.", "We suspect that it is this unusual aspect of the CNE formulation that makes it original with respect to the state-of-the-art.", "- \"In Table 2 and 3, how are [...]?\"", "Equation (6) defines the posterior of the network given the embedding, which we maximize w.r.t. the embedding.", "It explicitly depends on the prior probabilities P_ij, which are computed based on the prior knowledge about the degrees or about the block density structure of the adjacency matrix.", "Thus, this information is brought into the model by considering the posterior distribution for the network, where the prior models the degrees of the nodes, or the block structure."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 122, "sentences": ["1. We aimed to provide a broad variety of example applications (playing tennis, walking, fencing, dancing), while mainly focusing on the most complicated (tennis) application, for a thorough analysis of our method.", "Unfortunately, we cannot release the dataset, since we do not own the videos, but we will share our code.", "2. The Pose2Pose and Pose2Frame networks are trained separately.", "Specifically, the P2F network is trained on the original data, and not on the output frames of the P2P network.", "You are correct that some artifacts are added to the final P2F output at test time, yet they are minor due to the structural stability of the poses generated by the P2P network.", "Furthermore, training the P2F network with the P2P outputs is problematic, since we do not have the ground-truth for the new pose generated by the P2P network.", "3. The mask loss proposed in the review is similar to our implementation, except that we make a distinction between an inner-mask control and an outer-mask control.", "Our mask regression losses consist of a first loss penalizing the mask from being active outside the densepose mask, and a second loss penalizing the mask from being inactive inside the densepose mask.", "Combining them both results in the suggested loss."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 123, "sentences": ["Thanks for your thoughtful review.", "We have given serious considerations of your concerns and revise our manuscript to accommodate your suggestions.", "Please see the details below.", "Q1: \u201cThere are a few grammatical/spelling errors that need ironing out.\u201d", "A1: We have fixed the typos and grammatical errors in the revision.", "Q2: \u201cPioneering work is not necessarily equivalent to \"using all the GPUs\"\u201d", "A2: This claim is indeed not accurate we have delete this claim in the revision.", "Q3: \u201cThere are better words than \"decent\" to describe the performance of DARTS, as it's very similar to the results in this work!\u201d", "A3: We have changed the word to \u201cimpressive\u201d in the revision. However, DSO-NAS indeed outperforms DARTS on ImageNet dataset as illustrated in Table2.", "Q4: \u201cFrom figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?\u201d", "A4: In the search stage, the scaling factors are only used to indicate which operators should be pruned.", "The value of scaling factors do not represent the importances of kept operators since they can be merged into the weights of convolution.", "We also add experiments in CIFAR-10 to compare the performance between keeping the non-zero weightings and equal weightings.", "The result shows that both of them yield similar performances.", "---------------------------------------------------------------------------------------------------------------------------", "Architecture         \t                    params(M)", "test error", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-share+c/o", "3.0                        \t2.84", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-share+c/o+k/w", "3.0                              2.88", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-full+c/o", "3.0                        \t2.95", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-full+c/o+k/w", "3.0                        \t2.96", "---------------------------------------------------------------------------------------------------------------------------", "where \u201cc/o\u201d represents that training the searched architectures with cutout and \u201ck/w\u201d represents keeping the non-zero weightings in the architectures.", "Q5: \u201cWhy have you chosen the 4 operations at the bottom of page 4?\u201d", "A5: These four operations were used by ENAS and commonly included in the search space of most NAS papers.", "Q6: \u201cHow do you specifically encode the number of surviving connections?\u201d", "A6: We don\u2019t directly encode the number of surviving connections.", "Instead, the number of surviving connections is determined by the weight for L1 regularization, which can be incorporated with certain budget.", "Q7: \u201cMeasuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?\u201d", "A7: All of our experiments were conducted by NVIDIA GTX 1080Ti GPU, which was also used by ENAS and DARTS.", "We have added it in the paper."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 124, "sentences": ["We thank the reviewer for the insightful comments.", "We address the questions in the following:", "- How many images did you have in the experiment?", "We had 7500 images in total.", "We had 3 concept classes, and 2500 images for each concept.", "We will mention the total number in the main text.", "- The proposed network is not deep, but shallow", "We agree that a clear distinction line between shallow and deep networks does not exist.", "So we will make a note on that issue.", "- More experiments on the number of layers", "We had experimented with fewer layers.", "We realized that in this case the width of the network should be increased to compensate for the representation power of the network.", "As we already had an extensive set of experiments, we decided not to report that.", "As the proposed architecture already performs well to solve the ordinal embedding problem, we found it unnecessary to try deeper networks.", "- \"I don't see a clear conclusion of how to pick the width of hidden layers, maybe a better representation could be used.\"", "There exist three parameters in this experiment, which makes it hard to come up with the most conclusive representation.", "We also generated line plots (multiple curves in one plot) and 3D mesh plots to show the dependency.", "In the end, we found the heat-map more informative.", "In the revision, we will add the other plots to support the claim.", "- \"I don't see a discussion about the downsides of the method\"", "One of the drawbacks is that our method needs GPUs, while the more traditional algorithms run on CPUs.", "This can be of disadvantage if non-machine learning experts want to use our method.", "However, this is the case for most recent ML methods based on neural nets.", "The number of required triplets is theoretically lower bounded by nd log n, and this is also being confirmed by our experiments (our algorithm, as well as our competitors, break down when they get fewer triplets).", "Therefore, in a setting with passive triplet answers, and without extra information, it is impossible to overcome this problem.", "- \"in section 4.4", "when comparing the proposed approach with another method why not use more complex datasets (like those used in section 4.3)\"", "Independent of the dataset complexity, provided with enough triplet answers, all methods can yield less than 5% triplet error.", "However, the computation time is significantly lower for our proposed method.", "Due to the iterative nature of all algorithms, the computation time does not depend on the data distribution, but on the number of input points.", "Thus, a simple uniform dataset could serve to show our intention in this section.", "- \"in section 4.3, there is no guarantee that the intersection between the training set and the test set is empty.\"", "Yes, in theory that is true, but in practice this is negligible: the total number of possible triplets is about 10^9. So the likelihood that two sets of size 1000 intersect is close to 0.", "- \"in section 4.3", "how is the reconstruction built (Figure 3b)?\"", "Figure 3b is the exact output of the ordinal embedding in two dimensions.", "The colors are the initial labels of the input items.", "There are two or three labels assigned to demonstrate the quality of reconstruction.", "Note that the ordinal embedding output is unique only up to isometric transforms.", "In other words, every valid output is still valid with rotation, scaling and translation."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 125, "sentences": ["Many thanks for the detailed review!", "1/ We agree that there are many elements of our architecture that are similar to that of a convolutional network, however the network does not perform convolutions.", "To reflect both points, we have revised the text to:", "``The network does not use convolutions.", "Instead, the network does have pixelwise linear combinations of channels, and just like in a convolutional neural network the weights are", "are shared among spatial positions.", "Nonetheless, they are not convolutions because they provide no spatial coupling between pixels, despite how pixelwise linear combinations are sometimes called `1x1 convolutions.' '',", "and we have also added a subsection comparing the compression performance of our architecture to that of a decoder with convolution layers.", "In a sense, what the deep decoder is doing is separating multiple roles that proper convolutional layers fill:  the DD breaks apart the spatial coupling inherent to convolutions from their channel dependence and equivariance.", "Further, it says that the spatial coupling need not be learned or fit to data, and can be directly imposed by upsampling.", "2/ Yes, the upsampling analysis in Figure 5 also extends to two-dimensional images.", "We agree that natural images are only approximately piece-wise smooth after all, and the deep decoder only provides an approximation of natural images (albeit a very good one).", "3/ We agree and have changed `batch normalization' to `channel normalization' throughout.", "4/ Great point; we have added the sentence ``The optimal $k$ trades off those two errors; larger noise levels require smaller values of $k$ (or some other form of regularization).", "If the noise is significantly larger, then the method requires either choosing $k$ smaller, or it requires another means of regularization, for example early stopping of the optimization.", "For example $k=64$ or $128$ performs best out of $\\{32,64,128\\}$, for a PSNR of around 20dB, while for a PSNR of about 14dB, $k=32$ performs best.''", "5/ We do not mention the standard deviation, but do specify the SNR throughout (e.g., in table 1 in column identity).", "We have clarified this in the caption of the table.", "6/ It essentially produces smooth noise then.", "The weights learned by the deep decoder pertain to the source noise tensor.", "We have added a corresponding figure to the jupyter notebook for reproducing Figure 6."], "labels": ["rebuttal_social", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 126, "sentences": ["Thank you for your thoughtful review.", "We appreciate that you think our problem and approach are interesting! We have made some substantial revisions to clarify the presentation, we hope that these will help address your concerns.", "We respond to some specific comments below:", "> No comparisons were provided to any baselines/alternative methods.", "We do compare to a number of lesions in the supplement, and to an alternative method (performing tasks from a description alone).", "We have moved this latter comparison to the main text at the suggestion of Reviewer 1.", "> in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.", "Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively.", "This makes a comparison with MAML even more desirable. Without any comparisons it\u2019s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.", "Our main contribution is to propose a meta-mapping framework for zero-shot task performance, and parsimonious method for performing these meta-mappings.", "MAML as such is not a method of zero-shot task performance, it requires examples to learn", "from", ".", "We could therefore compare to MAML for our basic-meta-learning results, but those are simply a sanity check.", "We also compare to a variety of baselines, including chance and optimal performance, untransformed representations, and the most correlated task experienced (in the cards domain).", "However, if our paper is accepted, and you feel that the comparison to MAML for basic meta-learning is useful, we will run MAML on our tasks before the camera-ready submission.", "One might take inspiration from our framework try to use MAML for zero-shot task performance by transforming task representations would require adopting our meta-mapping framework, as well as a number of ideas of our architecture (where do the task representations come from, and how are they used?), and so its not clear to us that this is an appropriate baseline, rather than simply another implementation of our technique.", "Instead, we feel the more appropriate baseline is performing tasks from a natural language description, as many prior zero-shot works have, and so we have moved this result to the main text, as noted above.", "> The experimental results presented were all done on small synthetic datasets and it\u2019s hard to evaluate whether the method is practically useful.", "We already have more material than fits in this paper, especially now that we have added clarifications that all reviewers requested.", "We remained with synthetic tasks for two reasons: 1) to illustrate the method in settings we thought would be clearer, 2) because as we highlight in the future directions, there is a lack of meta-learning datasets that contain as structured of relationships between tasks as we consider.", "(Taskonomy, for example, has at best a notion of \"similarity\" in terms of transfer.) We are working on creating several such datasets, but we think that this paper as it stands is a useful contribution that illustrates the concept and ideas -- the datasets themselves will also require further description, and including them in a paper of this length would likely result in even more material being cut, and so a less clear presentation.", "We do think this is an important direction, but we think this paper makes a useful contribution by highlighting this new perspective. We hope that you agree.", "> Also, the work by Platanios, et al. on contextual parameter generation is very relevant to this work as it tackles multi-task learning using HyperNetworks.", "It may be worth adding a short discussion/comparison to that work as it also considers zero-shot learning.", "Thanks, we weren't familiar with this very interesting work! We have added a reference and a brief discussion of the relationship.", "- Capitalize: \u201csection\u201d -> \u201cSection\u201d, \u201cappendix\u201d -> \u201cAppendix\u201d, \u201cfig.\u201d -> \u201cFigure\u201d.", "Sometimes these are capitalized, but the use is inconsistent throughout the paper.", "Thanks for pointing this out, fixed.", "- \u201cHold-out\u201d vs \u201cheld-out\u201d", ".", "Be consistent and use \u201cheld-out\u201d throughout.", "We are making a grammatical distinction here -- \"held-out\" is an adjective and \"hold-out\" is a noun. That is, one might refer to a \"held-out task\" or to a \"a hold-out.\" Hopefully this clarifies things.", "We hope that this clarifies things, and will help to address your concerns. Please let us know if further changes would be useful."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 127, "sentences": ["We thank the reviewer for their comments and for noting correctly that our modification is quite effective, particularly regarding the large improvements on human evaluations.", "Our method is simpler in both conception and implementation than coverage, while requiring less parameters and being twice as likely to be chosen as better by human judges.", "We agree with the reviewer on the simplicity of our method, which we believe to be an asset.", "In addition to that, we believe the Scratchpad Encoder is fundamentally interesting as a mirror to the \u2018attentive read\u2019 common in seq2seq models.", "We also appreciate the reviewer taking their time to draw our attention to how to better emphasize the novelty and simplicity of our work."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_accept-praise", "rebuttal_answer", "rebuttal_concede-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 128, "sentences": ["We would like to thank the reviewer for the comments and for raising some subtle yet important questions.", "We address and clarify specific comments below.", "We have also made corresponding changes in the revised paper, and have added a proof map, in addition to the Table 3, for easier navigation of the results.", "We have also added comparisons with Mairal `09, and experimental evaluation of computational time.", "1. Noise Tolerance \u2014 NOODL also has similar tolerance to noise as Arora et. al. 2015 and can be used in noisy settings as well.", "We focus on the noiseless case here to convey the main idea, since the analysis is already very involved.", "Nevertheless, the proposed algorithm can tolerate i.i.d. sub-Gaussian noise, including Gaussian noise and bounded noise, as long as the ``noise\u2019\u2019 is dominated by the ``signal\u2019\u2019.", "Under the noisy case, the recovered dictionary and coefficients will converge to a neighborhood of the true factors, where the neighborhood is defined by the properties of the additive noise.", "In other words, the noise terms will lead to additional terms which will need to be controlled for the convergence analysis.", "Specifically, the noise will add a term to the coefficient update in Lemma 2, and will effect the threshold, tau.", "For the dictionary, the noise will result in additional terms in Lemma 9 (which ensures that the updated dictionary maintains the closeness property).", "A precise characterization of the relationship between the level of noise the size of convergence neighborhood requires careful analysis, which we defer to future effort.", "2. On eps_t and A.4. \u2014  Indeed, we don\u2019t need to assume that eps_t is bounded.", "Specifically, using the result of Lemma 7, we have that eps_0 undergoes a contraction at every step, therefore, eps_t <= eps_0.", "For our analysis we fix eps_t = O^*(1/log(n)), which follows from the assumption on eps_0= O^*(1/log(n)) and Lemma 7.", "On reviewer\u2019s comments, we have updated A.4., and moved the note about eps_t = O^*(1/log(n)) to the Appendix A.", "3. Exact recovery of factors \u2014 Also, we would like to point that NOODL recovers both the dictionary and coefficients exactly at a geometric rate.", "This means that as t\u2014> infinity both the dictionary and coefficients estimates converge to the true factors without incurring any bias.", "We have added a clarification corresponding to this in the revised paper in Section 1.1 and after the statement of Theorem 1 in Section 3."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 129, "sentences": ["Thank you for the detailed review.", "We appreciate your extremely useful pointers to existing dependency testing techniques, many of which are new to us. Before delving into them, here's our initial thoughts.", "First and foremost, we would like to know more details on the reasoning behind the rejection rating.", "It seems that the criticisms are on citations to other dependency testing approaches and time-dependency of fMRI data.", "The suggestions are invaluable and we'll gladly include them (work in progress).", "But comparison with them is not apples to apples , so we are not sure to what extent that adds value to our work, where we already compared to the KSG estimator for MI estimation and Pearson's correlation for dependency testing.", "Comparing to other dependency testing approaches, our technique allows the use of arbitrary neural networks and directly tests MI>0, the necessary and sufficient condition of dependency, is more general and is complementary to other techniques that make stronger assumptions about the data.", "We want to point out that a simple and most widely used dependency testing technique is Pearson's correlation through test of linear correlation (sufficient condition of dependency), which we compared to and show that our technique provides complementary value on sine wave and the fMRI dataset.", "We are aware that low sample complexity dependency tests exist and can be achieved by making additional assumptions on data, and we discussed that in the conclusion section, but our technique makes less assumptions and is applicable to general datasets which may not satisfy stricter assumptions.", "We are very interested in the HSIC-based techniques which seem to be popular and we could show complementarity. But at the same time the conclusion will be the same, so we have question on what value does it add for the audience over Pearson's correlation.", "Regarding other methods for dependency testing through mutual information, after following the line of work by Barrett et al. 2017, we reached this concise summary http://ims-vilnius2018.com/content/pdf/ivc293.pdf, which explains the smoothness assumptions made to data, as well as the fact that they used an asymptotic variance which holds when a large number of samples is given (page 2 top).", "It implies that the resulting confidence intervals as well as the test results are asymptotic and not guaranteed, which puts the resulting statistical tests into question.", "We have to admit that we did not thoroughly understand this line of work because of our background, so please comment if we are wrong about that.", "Instead, our dependency test does not make assumptions about data.", "Our lower bound and its confidence interval are not asymptotic.", "Theorem 1 provides a guaranteed confidence interval for arbitrary number of samples (so do our baselines, MINE and Pearson's r).", "We compared with the KSG estimator, which also only has asymptotic confidence intervals in literature.", "In addition, the proof provided in our work is concise and is easy for readers to understand and verify.", "Regarding time dependence and test threshold, it's important and thanks for pointing it out! We think that there are two ways time affects dependency listed below.", "1) First, we assume non-overlapping windows of TRs as the basis for computing number of i.i.d fMRI samples, but did not mention that in our current draft. We'll update and make it clear.", "2) Second, we can see an argument on whether or not segments of fMRI signals qualify as i.i.d, although not sure if this is a problem for our MI estimation approach.", "If things change very little from one second to the next, the signals could be very similar and may not really be, intuitively, independent samples and may bias result of the study.", "However, which independence assumptions to use is not in scope for our paper, because our fMRI study is trying to show that dependency testing works and is complementary to Pearson's correlation, not so much on drawing neuroscience conclusions.", "On a side note, it may turn out that which independence assumptions to make is a deeper question that doesn't yet have a clear answer.", "Regarding the level of the test, our theorem 1 already provides a proof based on Hoeffding inequality.", "This proof could be experimentally verified through computing MI on 300 test set samples and see how the estimate changes if there were >1million test set samples, repeat say 1000 times using different random seeds."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_followup", "rebuttal_followup", "rebuttal_by-cr", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 130, "sentences": ["We would like to thank the reviewer for the time and useful feedback.", "Our response is given below.", "- The paper is mainly empirical, although the authors compute two diagnostic statistics to show the effect of the self-modulation method.", "It is still not clear why self-modulation stabilizes the generator towards small conditioning values.", "We consider self-modulation as an architectural change in the line of changes such as residual connections or gating: simple, yet widely applicable and robust.", "As a first step, we provide a careful empirical evaluation of its benefits.", "While we have provided some diagnostics statistics, understanding deeply why this method helps will fuel interesting future research.", "Similar to residual connections, gating, dropout, and many other recent advances, more fundamental understanding will happen asynchronously and should not gate its adoption and usefulness for the community.", "- It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].", "It seems that the authors are not aware of this difference.", "We are aware of this key difference and we apply the sigmoid function to scale the output of the discriminator to the [0,1] range for the non-saturating loss.", "Thanks for carefully reading our manuscript and noticing this typo which we will correct.", "- In addition to report the median scores, standard deviations should be reported.", "We omitted standard errors simply to reduce clutter.", "The standard error of the median is within 3% in the majority of the settings and is presented in both Tables 5 and Table 6."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 131, "sentences": ["Thank you for your fruitful comments.", ">> 1.", "[...]", "One observation from the submission is that the token set may need to be very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive [...] I think some more motivation or exploration (what kind of information did BERT learn) is needed to understand why that is the case.", "Our BERT vocabulary sizes (13.5k for the gumbel version and 23k for the k-means version) compare favorably to the setups commonly used in NLP where vocabularies are double or triple of our sizes.", "We agree that it would be interesting to perform an in-depth analysis on the embeddings learned by BERT and we will investigate this in future work.", "Here we focus on a new quantization method evaluated via downstream performance in phone and speech recognition settings by employing models that worked well (and were extensively tuned) in NLP contexts.", ">> 2.", "A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.", "Yes, this is an interesting avenue for future work!", "We did not follow this direction due to two motivations: first, our aim is to contribute a new quantization scheme for audio data that is trained to predict the context in a self-supervised way.", "Second, we wanted to show that good performance can be achieved with discretized audio on actual speech tasks.", ">> 3. One concern I have with discrete representation is how robust they are wrt different dataset.", "We agree that an ablation study on robustness of the embeddings across different datasets would be very interesting.", "Here we are mostly focusing on relatively clean data (WSJ, TIMIT, Librispeech) following the original wav2vec paper but we would be interested in exploring robustness in the future.", "However we note that representations transfer at least well across datasets from the \u201cclean speech\u201d domain: vq-wav2vec and BERT is only trained on Librispeech and never tuned on TIMIT/WSJ.", ">> 4.", "Another curious question is whether the features would still provide as much improvement when a stronger ASR system than AutoSeg (e.g., Lattice-free MMI) is used.", "The original wav2vec paper (Schneider et al., 2019) reports better results than LF-MMI on the WSJ benchmark, however, the two setups are not strictly comparable.", "In some sense, the LF-MMI result has an edge because it is based on a phoneme-based ASR system which is typically stronger than the character-based ASR system used with wav2vec.", "We agree that evaluation on stronger baselines is an important future direction though."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 132, "sentences": ["Thank you for your comments and suggestions.", "We will address the issues you mentioned.", "1.\tThank you for the insightful suggestion.", "We now have added related work about video compositional methods in section 2.3 in the second version of the paper.", "2.", "In the original version of the paper, all experiments are conducted on trimmed video classification datasets.", "Although most papers in this field only report results on the trimmed video datasets, we do agree that more complicate cases should be tested.", "Additionally, we evaluated our V4D for untrimmed video classification on ActivityNet v1.3, which contains videos of 5 to 10 minutes and typically large time lapses of the videos are not related with any activity of interest.", "The very competitive result is reported in the appendix of the second version of paper, which demonstrated the generalization and robustness of our V4D.", "In fact, unlike previous video compositional methods, even when local events are not well aligned or misclassified, long-term modelling with 4D convolution and video-level aggregation with global average pooling are very likely to correct the partial error.", "3.About complexity, in the original version of the paper, we have reported parameters and FLOPs of V4D and compared it with other baseline methods in Table 2.", "4. We have already corrected the typo in title in the second version of the paper. Yet it seems that we are not able to modify the title on OpenReview. Thank you for pointing it out.", "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 133, "sentences": ["Thank you for your constructive comments.", "We are glad that you found our experiments extensive and that our approach provides significant improvements.", "In response to your comment that \"similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts\" we would like to take this opportunity to clarify the novelty of our approach.", "First, with regards to (Mescheder et al 2018), our method is not simply the application of existing gradient penalties (GPs) in the context of semi-supervised learning.", "Our approach is conceptually different since the regularizer proposed by (Mescheder et al 2018) is an (isotropic) ambient regularizer in the input space, whereas the regularizer we used performs (anisotropic) smoothing on the manifold parametrized by the latent generative model.", "We believe we are the first to show the benefits of anisotropic Jacobian regularizers in the context of semi-supervised learning.", "Moreover, an important contribution of our work is the efficient computation of such gradient penalties in the context of semi-supervised learning.", "Current application of such penalties uses the exact Jacobian which is especially computationally expensive in the case of semi-supervised learning as it is now a tensor (one matrix per class in the case of Improved GAN), which quickly becomes intractable with large numbers of classes.", "We proposed and demonstrated the effectiveness of an efficient (non-obvious) approximation of the Jacobian-based regularizer which significantly accelerates training.", "We provide responses to further questions/comments below:", "Q: \"A comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).\"", "A: Methods such as (Kipf and Welling 2017) are designed for semi-supervised learning on graphs; here a key challenge is in defining the structure (edges and edge weights) of the graph.", "Defining the graph structure is not trivial for the image datasets commonly used as benchmarks.", "In this light, one of the advantages of our approach is that the manifold (graph structure) is implicitly learned by the GAN, thus avoiding the need to explicitly define it.", "That said, it is an interesting direction for future work and we thank the reviewer for the suggestion.", "Q: \"How do the FID/Inception improvements compare to (Mescheder et al 2018)?\"", "A: We cannot directly compare our image generation scores with those reported in (Merscheder et al 2018) as we used different GAN architectures; for reference, they reported an Inception score of 6.2.", "We have updated the paper with Inception/FID scores from the ambient regularizer on CIFAR-10 (Table 4), which is an approximation of the proposed regularizer in (Merscheder et al. 2018) using stochastic finite differences.", "As mentioned earlier, it is not practical to compare the non-approximated regularizer due to the substantial increase in computational complexity in the semi-supervised GAN setting.", "We observe that ambient regularization gives better image generation scores; however it does not perform as well on semi-supervised learning.", "This tradeoff between image generation and semi-supervised learning performance was previously reported in (Salimans et al., 2016) \"Improved Methods for Training GANs\".", "Q: \"It would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.\"", "A: We re-checked our FID computation for this case and fixed a bug.", "We have updated the paper with updated FID scores; we note there is a high variance in the FID so while there is an improvement on average, it occasionally may not be better.", "Q: \"Although there is a clear improvement in FID scores for Cifar10. It would be informative to show the generated images w/ and w/o manifold regularization.\"", "A: We have included generated images with and without manifold regularization in the Appendix (Figure A5 for CIFAR-10, Figure A6 for SVHN) - these show clear improvements as well.", "Q: \"More analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.\"", "A: We note that although on average the method of (Kumar et al 2017) performs better on SVHN, the standard deviation is also much higher than many other methods (including ours) on both SVHN and CIFAR-10 indicating that it is not as robust.", "Q: \"It should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach.\"", "A: We have revised the tables such that bold values represent the best results for clarity."], "labels": ["rebuttal_social", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_future", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_done", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 134, "sentences": ["Thank you for the insightful review.", "We updated the paper with better results and more tasks.", "We show that our method outperforms the human baseline in terms of training speed and either matches or outperforms the human in terms of final accuracy on all tasks.", "While it is true that the human baseline does not require any additional computational resources for training, it does require domain expertise acquired through years of learning, which is arguably even more costly.", "Notably, in all 4 problems where we compare to the human baseline, we believe that human researchers used a similar or higher number of runs as our tuner to design the baseline schedules that we compare against.", "We also updated the paper with more details regarding Transformer and Proximal Policy Optimization.", "Thank you for mentioning the existing learning curve modeling methods.", "We added an explanation of differences of our method with those works.", "[1] learn a probabilistic model of one training curve using a handcrafted basis of nonlinear functions of shapes similar to the training curves being modelled.", "Our method does not make any assumptions about the shape of the modelled curves and is able to jointly model many training curves - in our experiments, training and validation loss and accuracy.", "[2] learn a deterministic model of a learning curve, while our method also models stochasticity, hence providing diverse experience for training a reinforcement learning agent.", "Also in contrast to [1] and [2], our method allows the hyperparameters to change over the course of training and models the influence of those changes on the training metrics.", "[1] Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).", "[2] Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 135, "sentences": ["We thank the reviewer for the comments on our paper.", "A prior work of Hsu et al. (ICLR'18) showed that heavy hitter oracles exist and that they can be constructed using machine learning techniques.", "We are using the same type of oracles in our current submission.", "Similar oracles have been studied in previous works too, e.g., membership oracles for Bloom filters in Kraska et al. Both the previous works and the experiments in the current submission demonstrate that it is reasonable to make such an oracle assumption.", "We thank the reviewer for the questions on optimization ability, generalization error, etc. These are interesting research directions.", "The answers are very likely to depend on the application, data sets, etc., which we plan to study in the future.", "The prior work by Hsu et al. showed that the oracle trained by deep learning has high accuracy (see Section 5.3 in their paper): for Internet traffic data, the AUC score is 0.9, and for search query data, the AUC score is 0.8.", "The performance of a simple online algorithm would likely depend on the type of classifier used and input feature representation.", "Linear classifiers with IP addresses represented as individual bits are unlikely to work well because their expressive power is limited.", "For instance, at the very least, we would like our classifier to express a DNF hypothesis of the form:", "(IP address = a1) or (IP address  = a2) or ...", "We have updated the introduction to rephrase and clarify the lower bound claims.", "The added/modified text are highlighted in the blue color."], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 136, "sentences": ["We thank the reviewer for a detailed review.", "We have implemented the suggestions for improving clarity.", "Regarding our use of \u2018axioms\u2019: We follow the economics literature in using axioms as normative concepts, i.e., to denote desirable properties that a neuron importance methods.", "And not the use in the mathematical literature, which is to denote statements that are self-evidently true.", "We have clarified this in the submission."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 137, "sentences": ["We thank the reviewer for the thoughtful review.", "The reviewer points out that CNE works well for link prediction and visualization.", "We wish to point out that our experiments indicate CNE consistently outperforms the state-of-the-art not only for these tasks but also for multi-label classification.", "Our responses:", "a) Variational inference is useful in particular when the partition function is hard to compute, which is not the case here.", "So we believe it would be overkill in this case.", "In any case, it is not needed to achieve the performances CNE achieves at a very modest computational complexity and fast practical runtimes.", "b) No hand-engineering is needed to use CNE even when using more informative priors modeling node degrees and block structure.", "The degree of each node can simply be computed on the training set, so no hand-engineering is needed (just the choice to include it or not -- and it always better to include it).", "The block structure, on the other hand, will often be part of the data specification or meta-data of the nodes.", "For example, the network may be a multi-partite network representing a relational database, or it may be a company social network where the nodes are employees, and generic job titles are known for each of the employees (as attributes of the nodes).", "The entity types in the first example, and the job titles in the second example, would then define the blocks, and the density of the parts of the adjacency matrix between any two such blocks can again easily be computed on the network.", "Our method imposes no constraints on such blocks (e.g. they may even be partially overlapping).", "Again, all that is needed is choosing whether to use a block prior for any specified attribute (in the two examples: entity type, and node attribute).", "Again, empirically, including it always appears to be better, so one could even avoid having to make the choice.", "Inferring structural properties of the graph to be used in the prior (e.g. using GraphRNN), as we understand the reviewer suggests, certainly sounds potentially interesting.", "However, while it may improve accuracy, we do not believe that it adds value in reducing the amount of hand-engineering needed, as the amount of hand-engineering needed is very minimal already.", "The fact that CNE could be combined with such inferred structural properties increases its potential impact though, and this remark of the reviewer further underscores the need for methods such as CNE that can take such structural information into account.", "The boost in accuracy achieved by CNE, using a model that is arguably also a lot simpler than the state-of-the-art network embedding approaches, is thus achieved without any increased need for hand-engineering."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 138, "sentences": ["Thank you for your review and valuable comments!", "Summary: our response includes: (1) Clarification on language translation baselines; (2) Discussion on image translation evaluation; (3) Reference and clarification.", "** Language Translation Baselines **", "1. For the baseline models reported:", "1.1) We use the transformer model with \"transformer_big\" setting [1], which is a strong baseline that outperforms almost all previously popular NMT models based on CNN [2] and LSTM [3].", "Transformer is the state-of-the-art NMT architecture.", "Our numbers of the baseline transformer model match the results reported in [1].", "1.2) In addition to the standard baseline models, we also compare our method against all the relevant algorithms including knowledge distillation (KD) and back translation (BT).", "1.3) As can be seen in many well-known and recent NMT works ([4], [5])", ", it is a common practice to use transformer as the robust baseline model.", "Furthermore, it is also shown from these works that it is hard to improve over the transformer baseline, and 0.5-1 BLEU score improvement is already considered substantial.", "2. We further add newly obtained results on the WMT18 challenge.", "We compare our method with both the champion translation system MS-Marian (WMT18 En->De challenge champion).", "Our method achieves the state-of-the-art result on this task.", "---------------------------------------------------------------------------", "WMT En->De", "2016", "2017", "2018", "---------------------------------------------------------------------------", "MS-Marian (ensemble)", "39.6", "31.9          48.3", "Ours (single)", "40.68", "33.47       48.89", "Ours (ensemble)", "41.23", "34.01       49.61", "---------------------------------------------------------------------------", "Please refer to Section 3.4 \"Study on generality of the algorithm\" for more details and Table 4 for full results in our updated paper.", "** Image Translation Evaluation **", "For image-to-image translation tasks, we further add two quantitative measures: (1) We use the Fr\u00e9chet Inception Distance (FID) [6], which measures the distance between generated images and real images to evaluate the painting to photos translation.", "(2) We use \"FCN-score\" evaluation on the cityscape dataset following [7].", "The results are reported in Table 6 and Table 7 respectively.", "Multi-agent dual learning framework can achieve better quantitative results than the baselines.", "We are not sure what you meant by \u201cHow does their ensemble method compare to just their single-agent dual method?", "\u201d", ".", "The standard CycleGAN model (baseline) already leverages both primal and dual mappings, which is equivalent to our \u201cDual-1\u201d model in NMT experiments, i.e., the dual method with only one pair of agents f_0 and g_0.", "Our model involves two additional pairs of agents (f_1 and g_1, f_2 and g_2) during training.", "Unlike ensemble learning, only one agent (f_0 for forward direction, or g_0 for backward direction) is used during inference.", "** Reference **", "Thanks for pointing a reference paper \"Multi-Column Deep Neural Networks for Image Classification\" (briefly, MCDNN) and we have added reference to it (Section 4).", "Although MCDNN also uses multiple agents (i.e., several columns of deep neural networks), it differs from our model in two aspects: (1) Our work leverages the duality of a pair of dual tasks while this paper does not; (2) In an MCDNN framework, during the training phase, all the columns are updated by winner-take-all rule; and during inference, all columns work like an ensemble model through weighted average.", "In comparison, we only update one primal and one dual agent during training, and use one agent for inference.", "** Clarity **", "Thanks for pointing out that our original introduction to the names of baselines and models is not very clear.", "Please kindly refer to first paragraph in Section 3.3.", "You may check our updated paper with clarification and new experimental results.", "Thanks for your time and feedbacks.", "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" In NIPS. 2017.", "[2] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional Sequence to Sequence Learning. In Proc. of ICML, 2017.", "[3] Wu, Yonghui, et al. \"Google's neural machine translation system: Bridging the gap between human and machine translation.\" arXiv preprint arXiv:1609.08144 (2016).", "[4] Chen, Mia Xu, et al. \"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.\" In Proc. of the ACL, 2018.", "[5] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proc. of NAACL, 2018.", "[6] Heusel, Martin, et al. \"Gans trained by a two time-scale update rule converge to a local nash equilibrium.\" In NIPS, 2017.", "[7] Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" In CVPR, 2017", "Dear AnonReviewer1,", "Before the final decision concludes, do you have further questions regarding our rebuttal and updated paper?", "Our paper revision includes reorganization of the introduction to our framework (Section 3.1), the additional experiments on WMT18 English->German translation challenge (Section 3.4), the additional study on diversity of agents (Appendix A), and quantitative evaluation on image-to-image translations (Section 4.3 and 4.4) following your suggestions.", "In particular, we would like to highlight that:", "(1) The calibration of BLEU score: We would like to point out that our improvement over the previous state-of-the-art baselines is substantial.", "For example, on the WMT2014 En->De translation task, the performance of the transformer baseline is 28.4 BLEU score [1] (our baseline matches this performance).", "The improvement over this baseline is 0.61 in [2], 0.8 in [3] (1.3 BLEU improvement over the re-implemented 27.9 baseline in [3]) and 0.9 in [4], while ours is 1.65 BLEU score.", "(2) The baselines: As we explained in the previous response, we are using the state-of-the-art transformer as our backbone model, and comparing against all the relevant algorithms including KD, BT and the traditional 2-agent dual learning (Dual-1).", "Moreover, we also show on WMT18 En->De challenge that our method can further improve the state-of-the-art model trained with extensive resources (Section 3.4 of our updated paper).", "We hope our rebuttal and paper revision could address your concerns.", "We welcome further discussion and are willing to answer any further questions.", "[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.", "[2] He, Tianyu, et al. \"Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation\". Advances in Neural Information Processing Systems. 2018.", "[3] Shaw, Peter, Jakob Uszkoreit, and Ashish Vaswani. \"Self-Attention with Relative Position Representations.\" In Proc. of NAACL, 2018.", "[4] Anonymous. Universal transformers. In Submitted to International Conference on Learning Representations, 2019.", "URL https://openreview.net/forum?id=HyzdRiR9Y7.", "Under review as a conference paper at ICLR 2019"], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_followup", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 139, "sentences": ["Thank your very much for your review.", "We have updated the manuscript with more details in the derivation of the first order approximation of KL divergence.", "1) Elaborated derivation of Eq. 10", "Q1: We have added one more line to explain the derivation.", "Basically a baseline is subtracted, and GAE is introduced.", "2) Gradient update on \\phi from KL divergence", "The gradients w.r.t. \\phi from the KL divergence is stopped for variance reduction with acceptable bias, which we prove with MuProp [1].", "Details could be found in Appendix C.", "Q3: Rather than [2], we employ MuProp to reduce variance in our development of NADPEx.", "Thank your for your suggestion.", "Q4: Yes \\theta and \\phi are jointly and simultaneously optimized at Eq. 12, though the gradients w.r.t. \\phi from the KL divergence is stopped.", "Q7: Due to the stop-gradient manipulation in the KL divergence, gradients w.r.t. \\phi remains the same as in stated in last subsection.", "3) Mean policy in the KL divergence", "What motivates the mean policy is not variance reduction, but the idea that dropout policy had better to be close to each other.", "As intuitively \\phi is controlling the distance between dropout policies, it would further remedy the little bias mentioned above.", "However, the computation complexity for \"close to each other\" would be O(N^2), with N being the number of dropout policies in this batch.", "We employ mean policy to make it linear. And it could be regarded as an integration on a Gaussian approximation of the Monte Carlo estimate according to [3].", "Details could be found in Appendix C.", "Q2: No the mean policy is not used due to the likelihood ratio trick.", "And the approximation of using mean policy is discussed in [3], with a sound deduction.", "Q3: Mean policy is not motivated by variance reduction, which is addressed as introduced above.", "Thank you for your suggestion.", "Q5: In the updated version, we have explicitly pointed out that the gradients w.r.t. \\phi from KL divergence is stopped. Thanks for this suggestion.", "Hope our response addresses your concerns!", "[1] Gu et al., \"MuProp: Unbiased Backpropagation for Stochastic Neural Networks\", ICLR 2016.", "[2] Titsias et al., \"Local Expectation Gradients for Black Box Variational Inference\", NIPS 2015.", "[3] Wang et al., \"Fast dropout training\", ICML 2013."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_social", "rebuttal_done", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 140, "sentences": ["Thank you for your insightful feedback and suggestions! To address your comments:", "- Following your suggestion, we have added one experiment on a synthetic regression task in Appendix C. Here, we show that our method can learn meaningful models for target domains, and also learn the source domain weights in a way that selects only relevant source domains for training.", "More importantly, we can learn the model and the domain weights simultaneously, unlike many existing works that use two-stage learning (learn the weights then the model).", "- We also followed your suggestion and show the domain weights of MDMN on the Amazon dataset for comparison (see the new Section 5.4).", "As you mentioned, our weights are theoretically justified while theirs are only heuristically computed.", "We see that the weights provided by MDMN are not very stable, changing from one source domain to another drastically during training.", "This instability makes their weights difficult to interpret.", "- W.r.t. the naive method of using (fixed) coefficients.", "Note that whichever domain discrepancy is used, it has to depend on the data representation.", "For complicated models like deep neural networks, the feature representations (hidden layers) are changing constantly during training and arguably there is no \"right\" way to compute *fixed* coefficients/weights", "based on ever-changing representations", ".", "Computing the coefficients directly from the images is extremely difficult, if not impossible, because calculating the domain discrepancy using such high-dimensional data is not feasible.", "Note that MDMN DOES use W1-distance to compute domain weights, and our comparison in Section 5.4 shows that it is not very stable, as mentioned above.", "To summarize, there is no easy way to compute meaningful fixed coefficients and our method is indeed suitable for dynamic representations during neural network training.", "We hope our explanation and additional results address and resolve your concerns. If you have any other comments, we are happy to have further discussions.", "Thank you!"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 141, "sentences": ["We thank the reviewer for their review.", "The reviewer notes the need to emphasize how and why to use this approach.", "In the new revision, we have added a discussion section to make a case for this.", "We will publish the code to compute conductance after the blind-review phase.", "The reviewer also mentioned that the paper doesn\u2019t compare directly against various attribution methods.", "For this, we refer the reviewer to our response for the comment by anonymous."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 142, "sentences": ["1.", "The purpose of the counterexample is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds.", "We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions.", "A good generator and discriminator would definitely be a solution as well.", "However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions.", "We found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging, however, it does sometimes results in reasonable generations (although worse than proposed PC-GAN).", "So, we do not consider the argument to be unrealistic as we often observe the degeneracy.", "So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training.", "Other architectures like RNN might work, but they are not permutation invariant, which is a desirable property for set data like point clouds.", "More comparisons between using RNN and DeepSets for other tasks on set data can refer to Zaheer et al., (2017).", "2.", "As we discussed in the end of Section 3, ALI and BiGan\u2019s goal is to match (z, G(z)) and (Q(X), X), which aims to infer the random noise z and enforce the latent code to follow noise  distribution (e.g. Gaussian).", "On the other hand, we do not enforce Q(X) to follow from Gaussian.", "Instead, we train the other G_theta(u) to match Q(X), which is more similar to AAE-like works (Engel et al., 2017; Kim et al., 2017; Achlioptas et al. 2017).", "The difference of the interpretation between PC-GAN and  those AAE-like work is also explained in the second paragraph of Sec 4.", "3. We followed the same protocol that we trained on ShapeNet55 and tested on ModelNet40 testing set.", "Please check Table 3 in the revision.", "PC-GAN achieves 86.9% accuracy which is better than AAE (84.5%),  3D-GAN (83.3%) and other unsupervised learning approach."], "labels": ["rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 143, "sentences": ["We thank the referee for their review.", "1. We are not training Restricted Boltzmann Machines (RBMs), but Boltzmann machines where the hidden units can be fully connected.", "2. The complete connectivity graph for our Boltzmann machine, as presented in Fig 2, can be interpreted as having two hidden layers.", "The graph has bipartite connectivity between the visible units and the first 128 hidden units and bipartite connectivity between the first 128 hidden units and the second 128 hidden units.", "We thank the referee for bringing the article [V] to our attention and we now have acknowledged the prior work properly in our introduction.", "We agree that it would be very instructive to evaluate the model in [V] for adversarial resistance, but we would argue that this evaluation is beyond the scope of this article.", "3. Due to the complexity of the network compared to e.g. LeNet and the higher adversarial resistance the optimisation procedure to find adversarial images takes a long time, making it hard to evaluate 10000 images for all training stages and different attacks.", "We have now evaluated the adversarial resistance throughout the article for 1000 images randomly selected from the 10000 MNIST test images.", "This should avoid placing too much emphasis on the cleaner images in the beginning of the MNIST test set.", "Fig.\u00a03 and other evaluations have been updated for the new test set.", "4. To our knowledge the boundary method is the strongest black box attack.", "The succesful transfer attack on CapsNet is based on transfer of adversarial images from a different model (LeNet).", "We have implemented this attack and added it to our evaluation.", "[V] Norouzi, Mohammad Ranjbar, Mani Mori, Greg: Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning. 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2735-2742 (2009)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_contradict-assertion", "rebuttal_answer", "rebuttal_done", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 144, "sentences": ["Thank you for your comments and suggestions!", "Summary: our response includes (1) Clarification on math equations; (2) Analysis on diversity of additional agents; (3) Quantitative analysis for image translation.", "** Clarification on Mathematics in Section 3.1 **", "We apologize for the confusions in Section 3.1.", "We have reorganized this section, as shown in our updated paper.", "For your questions:", "1. About equation 8, indeed there is a typo and should be a \"partial\" sign in front of the \"\\delta\" function in the numerator.", "Thanks for pointing this out.", "2. The details of derivative estimation can be found in Section 3.1 (especially equation 9 and 10 in our updated version.", "** Study on diversity of agents **", "1. You are right.", "We obtained distinct \"agents\" f_i and g_i through multiple independent runs with different random seeds and different input orders of the training samples.", "As far as we know, there's no common quantitative metric to measure the diversity among models in NMT.", "But we agree with you that intuitively, more diversity among agents leads to greater improvements.", "2. Following your suggestions, we add a study on the diversity of agents (presented in Appendix A of the updated paper).", "We design three group of agents with different levels of diversity: (E1) Agents with the same network structure trained by independent runs, i.e., what we use in Section 3.3; (E2) Agents with different architectures and independent runs; (E3) Homogeneous agents of different iteration, i.e., the checkpoints obtained at different (but close) iterations from the same run.", "We evaluate the above three settings on IWSLT2014 De<->En dataset.", "The diversity of the above three settings would intuitively be (E2)>(E1)>(E3)", ".", "We present full results in Figure 4 (Appendix A), where the BLEU scores with Dual-5 model are:", "--------------------------------------------------------", "E1", "E2             E3", "--------------------------------------------------------", "En -> De", "35.44", "35.56       34.97", "De -> En", "29.52", "29.58       29.28", "--------------------------------------------------------", "From the above results, we can see that diversity among agents indeed plays an important role in our method.", "There are, of course, many other ways to introduce more diversity, including using different optimization strategies, or training with different subsets as you suggested.", "All of these can potentially bring further improvements to our framework, yet are not the focus of this work.", "From the current studies, we show that our algorithm is able to achieve substantial improvement with a reasonable level of diversity.", "We leave more comprehensive studies on diversity to future work.", "Please kindly refer to Appendix A for more detailed results.", "** Quantitative analysis for image translation **", "Thanks for your suggestions.", "We add two quantitative measures on image translation tasks: (1) We use the Fr\u00e9chet Inception Distance (FID score) [1], which measures the distance between generated images and real images to evaluate the painting to photos translation.", "(2) We use \"FCN-score\" evaluation on the cityscape dataset following [2].", "The results are reported in Table 6 and Table 7 respectively.", "Multi-agent dual learning framework can achieve better quantitative results than the baselines.", "*", "* Term usage of \"multi-agents\" **", "Although with the same term, the \"multi-agent\" or \"agent\" in this paper has no relationship with multi-agent reinforcement learning.", "You are right in that the term \"agent\" in our context refers to \"mapping\" or \"network\".", "To avoid further confusion in the discussion period, currently we decide not to change the term usage throughout the paper during rebuttal; instead, we will change the term after the acceptance/rejection decision.", "You can check our updated paper with clarification and new experimental results.", "Thanks for your time and valuable feedbacks.", "[1] Heusel, Martin, et al. \"Gans trained by a two time-scale update rule converge to a local nash equilibrium.\" Advances in Neural Information Processing Systems.", "2017.", "[2] Isola, Phillip, et al. \"Image-to-image translation with conditional adversarial networks.\" In CVPR, 2017"], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_social", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_summary", "rebuttal_future", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 145, "sentences": ["Thank you for your careful review and useful comments!", "Overall, in response to your review and that of referee 3 we will include a more intuitive discussion of our results in the next revision of our text.", "To reply to your other specific comments,", "1) The intuition for batchnorm can be put in a more general setting.", "If a function f: X -> Y tends to spread out small clusters in the input space almost evenly in the output space, then one can expect that its gradients will be large typically.", "In our case, a batchnorm network can be understood as a function that sends a batch of inputs to a batch of outputs.", "In the appendix, we showed that the correlation between two different batches tend to a constant value independent of the input batches.", "No matter how close two input batches are, the output batches will have the same \u201cdistance\u201d from each other -- small movements in the input space leads to large movements in the output space.", "Thus we can expect the gradients to be large as well.", "We have added a new figure to the Appendix to further support this intuition.", "In it, we pass through a linear batchnorm network 2 minibatches.", "Both minibatches contain points on the same circle and 1 point off the circle that is unique to each minibatch.", "While the circle in each minibatch will remain an ellipse as they are propagated through the network, the angle between the planes spanned by them increasingly becomes chaotic with depth.", "3) As observed in [1] and [2], depthwise convergence to covariance fixed points is bad for training, and the best networks are either moderately deep or initialized such that the depthwise convergence rate to the fixed point is as slow as possible.", "We observe that deep networks whose activation statistics resemble a non-BSB1 fixed point typically feature worse gradient explosion than BSB1 networks.", "This seems to be because the nonlinearities that induce these fixed points increase rapidly (for example, polynomials with high degrees), so that the corresponding derivatives are also large, causing gradient explosion.", "(The reason that rapidly increasing nonlinearities don\u2019t converge to BSB1 fixed points is that, after a spontaneous symmetry-breaking, begins a \u201cwinner-take-all\u201d covariance dynamics, in which the activations of a few examples in the batch suddenly dominates those of the others in the batch, and this dominance persists across each layer.)", "4) We were a bit confused by what was meant by \u201cpractice\u201d here.", "We have thoroughly verified that for realistic input distributions (MNIST and CIFAR10) and common initialization strategies (weights that are randomly distributed) our theory makes accurate prediction.", "Moreover, we have shown that these predictions can be connected to practice in the sense that they predict whether or not the network can be trained.", "Having said this, if by practice you meant that the neural network is accurately described by our theory during training then we do not expect this to be true. We are happy to emphasize this in the camera ready.", "If this did not properly address your question, please feel free to let us to know and we will improve this response!", "[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein.", "Deep Information Propagation (https://arxiv.org/abs/1611.01232)", "[2] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)"], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_by-cr", "rebuttal_social", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 146, "sentences": ["We would like to thank the reviewer for their feedback.", "We address each comment below individually with appropriate headings.", "- Summary", "We would like to point out that the reviewer in the summary incorrectly described that our approach uses the \"triplet loss as a convex relaxation of the ordinal embedding problem\".", "Using the triplet loss as a proxy does not make the problem convex.", "- The relation between data distribution and hardness of ordinal embedding", "Ordinal embedding is NP-hard independent of the data distribution.", "The paper \u201cLandscape of non-convex quadratic feasibility\u201d (Bower et al. 2018) can shed more light on this.", "The equation (1) in this paper rephrases the ordinal embedding problem as a homogeneous quadratic feasibility problem.", "The constraint matrices of the problem (P_i in the paper), which correspond to the triplet inequalities, are all indefinite which makes the whole optimization NP-hard.", "Moreover, many of our experiments in this paper feature the uniform distribution, which does not satisfy any nice structural assumptions.", "- Using a convex solver", "As we pointed out earlier, using the triplet loss does not make the optimization problem convex and hence using a convex solver would not be possible here.", "- \u201cEquations (3) and (4):  isn't this the same as using the hinge loss to bound the zero-one loss?\u201d", "Yes, that is true."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 147, "sentences": ["Thank you for the detailed comments.", "Our goal is to develop a quantitative understanding of AlphaGo Zero (AGZ), moving beyond the intuitive justification for the algorithms in the original work.", "We believe that a rigorous mathematical analysis is crucial to provide a solid foundation for understanding AGZ and similar algorithms.", "This requires developing (i) a precise mathematical model, (ii) a quantitative performance bound within the model.", "Our work takes an important step in this direction by modeling AGZ\u2019s self-play and its supervised learning algorithm accurately.", "In particular, we use the turn-based game model to capture the self-play aspect.", "We develop a quantitative bound in terms of cross-entropy loss in supervised learning, which is the \u201cmetric\u201d of choice in AGZ.", "While the cross-entropy loss seems intuitive, using it as a quantitative performance measure requires careful thought.", "For example, in Appendix F (page 19, 2nd paragraph), we discussed a scenario where this intuition is incorrect under a careless measure.", "That is, seemingly \u201cobvious\u201d algorithms can fail in the absence of a rigorous mathematical proof.", "We agree that there is a gap between AGZ and our model.", "As mentioned in our paper, MCTS converges to the optimal policy for both classical MDPs and stochastic games.", "Hence in this paper, we model the AGZ\u2019s MCTS policy by the optimal policy, and mainly focus on the other two key ingredients of AGZ, self-play and supervised learning.", "It will be interesting to study how the error between MCTS and the optimal policy affects the iterative algorithm.", "This is a research direction we think is worth pursuing in the future.", "We also agree with the reviewer that some of our statements might be too strong.", "We will revise accordingly. Instead of ``immediate justification``, we believe this work does provide a first-step, formal framework towards a better theoretical understanding.", "We will also revise the title, perhaps to ``applying AGZ`` so as to make the connection to MDP more clear in our paper."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_future", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 148, "sentences": ["1. Comment:", "However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.", "1. Response:", "We study seq2seq classification tasks since they have been widely used in real world applications for RNNs.", "To name a few, in speech recognition, [1] hybridizes hidden Markov model with RNNs to label unsegmented sequence data; In computer vision, [2, 3] demonstrate scene labeling with LSTM and RNNs, achieving higher accuracy than baseline methods; In healthcare, [4] proposes a model, Doctor AI, to perform multiple label prediction (one for each disease or medication category).", "In addition, [5, 6] both apply RNNs to real-world healthcare datasets (MIMIC-III, PhysioNet, and ICU data) for mortality prediction and other multiple classifications tasks.", "We establish bounds for classification because it is typical in learning theory and is easy to compare among existing literature.", "On the other hand, our analysis applies in other tasks as long as a suitable Lipschitz loss function is chosen.", "Specifically, Lemma 4 establishes an upper bound for empirical Rademacher complexity of general Lipschitz loss functions (the last line in Appendix A.4).", "By replacing the loss function in Lemma 1, we can derive generalization bounds for various tasks other than classification.", "References", "[1] Graves, Alex, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. \"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks.\" In Proceedings of the 23rd international conference on Machine learning, pp. 369-376. ACM, 2006.", "[2] Byeon, Wonmin, Thomas M. Breuel, Federico Raue, and Marcus Liwicki. \"Scene labeling with lstm recurrent neural networks.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3547-3555. 2015.", "[3] Socher, Richard, Cliff C. Lin, Chris Manning, and Andrew Y. Ng. \"Parsing natural scenes and natural language with recursive neural networks.\" In Proceedings of the 28th international conference on machine learning (ICML-11), pp. 129-136. 2011.", "[4] Choi, Edward, Mohammad Taha Bahadori, Andy Schuetz, Walter F. Stewart, and Jimeng Sun. \"Doctor ai: Predicting clinical events via recurrent neural networks.\" In Machine Learning for Healthcare Conference, pp. 301-318. 2016.", "[5] Che, Zhengping, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. \"Recurrent neural networks for multivariate time series with missing values.\" Scientific reports 8, no. 1 (2018): 6085.", "[6] Xu, Yanbo, Siddharth Biswal, Shriprasad R. Deshpande, Kevin O. Maher, and Jimeng Sun. \"RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient Monitoring Data.\" In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2565-2573. ACM, 2018."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 149, "sentences": ["Thank you for your encouraging comments especially with regards to novelty and thoroughness of our experiments.", "We have addressed the minor issues you highlighted; answers to your questions are also provided below:", "Q: \"It would be helpful to note in the description of Table 3 what is better (higher/lower).\"", "A: We have updated the table description (now Table 4) as suggested.", "Q: \"Also Table 3 seems to have standard deviations missing in Supervised DCGANs.\"", "A: The results we reported were taken from (Gulrajani et al. 2017) which did not include standard deviations for this setting.", "Q: \"And is there an explanation on why there isn\u2019t an improvement in the FID score of SVHN for 1000 labels?\"", "A: We re-checked our FID computation for this case and fixed a bug.", "We have updated the paper with updated FID scores; we note there is a high variance in the FID so while there is an improvement on average, it occasionally may not be better.", "Q: \"What is the first line of Table 4? Is it supposed to be combined with the second?\"", "A: It is the reference from which all 3 results (Supervised, Pi Model and Mean Teacher) were taken from; we have made this clearer in the revised paper.", "Q: \"And is the Pi model missing results or can it not be run on too few labels?\"", "A: We have updated the table with results on fewer labels reported in (Tarvainen & Valpola, 2017); results on fewer labels were not reported in (Laine & Aila 2017).", "Q: \"In Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there ?\"", "A: This is because the classifier is not smooth in this region.", "The classifier is probably not constrained enough.", "Q:\"What are the differences of the 6 pictures in Figure A7? Iterations?\"", "A: These are the results from 6 different runs."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 150, "sentences": ["Thank you for your review of our work.", "The following are your concerns of our work:", "a. Limited contribution to the qualitative understanding of the optimizers", "a.i. Informativeness of the proposed w-tunability metric", "b. Using wall-clock time instead of number of HPO oracle calls", "We address these concerns one-by-one.", "a. Limited contribution to the qualitative understanding of the optimizers:", "We consider the three main contributions of our work to be 1) a systematic evaluation protocol of optimizers, with off-the-shelf HPO to account for the cost of tuning of hyperparameters.", "This is missing in existing papers, which consider best attained performance alone.", "The importance of a proper hyperparameter search protocol is emphasized by Choi et al., 2019 (published after our submission and under review at ICLR).", "2) a \u201cw-tunability\u201d measure of the cost of hyperparameter optimization, and 3) under the experiments considered we find that Adam (with default beta and epsilon values) is the most tunable.", "a.i. Informativeness of the tunability metric:", "We propose w-tunability as a metric to incorporate the HPO tuning too in reporting the performance of an optimizer, and compute it as a linear combination of the incumbents of the HPO algorithm, though one can use an arbitrarily complex function trading off interpretability.", "It is true that Figures 4-7 essentially contain all the information needed to judge about the optimizer\u2019s tunability.", "However, a metric that is easy to compute, interpret and compare optimizers across tasks is crucial, for which we propose w-tunability.", "This is analogous to computing specific quantities like accuracy, FPR, TPR from the confusion matrix, even though a confusion matrix contains all the information (and is quite cumbersome to compare).", "The summary metric in Figure 2 provides a different interpretation: It reports a normalized performance i.e., the  normalized incumbent performance at iteration $k$. This doesn\u2019t explicitly include information about the previous $k-1$ iterations (which is our central argument).", "Thus our proposed tunability metric provides more information than the summary statistics plot (figure 2).", "Due to this novelty, we argue that our setup does contribute to the qualitative understanding of the optimizers.", "In fact, it yields to a drastically different valuation of adaptive gradient methods than popular previous work (Wilson et al, Shah et al).", "You mention that our work is incremental to the work on benchmarking of optimizers.", "Can you please provide respective references?", "We have modified parts of our paper to reflect these arguments better.", "b. Using wall-clock time instead of HPO oracle calls:", "Our reason for using a number of configuration trials instead of a time budget is that measuring number of hyperparameter configuration searches required is more relevant to understand the optimizers\u2019 dependence on the hyperparameters.", "However, we completely agree with you that computational budget is a relevant factor from the practitioner\u2019s point of view, and added a discussion of this in Appendix E of the paper.", "As you rightfully point out, the adaptive optimizers tend to converge in fewer number of epochs, amplifying the results that favor Adam over the variants of SGD.", "References:", "Wilson, Ashia C., et al. \"The marginal value of adaptive gradient methods in machine learning.\" Advances in Neural Information Processing Systems. 2017.", "Shah, Vatsal, Anastasios Kyrillidis, and Sujay Sanghavi. \"Minimum norm solutions do not always generalize well for over-parameterized problems.\" arXiv preprint arXiv:1811.07055 (2018).", "Choi, Dami, et al. \"On Empirical Comparisons of Optimizers for Deep Learning.\" arXiv preprint arXiv:1910.05446 (2019)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 151, "sentences": ["We thank the reviewer for the comments and suggestions.", "Our paper indeed focuses on theoretical results, but we believe the theory has some practical implications as well.", "In particular, while the exact form of the sufficiently bilinear condition may be somewhat unwieldy, the result gives concrete evidence that having higher bilinearity can aid convergence for certain algorithms, even for settings that are not purely bilinear.", "This indicates that one should pay attention to the magnitude and condition number of the off-diagonal of the Jacobian when constructing a min-max problem and choosing an algorithm to solve the problem."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 152, "sentences": ["We thank the reviewer for their encouraging words.", "\"There are better baselines for MNIST to MNIST-M\": yes, the presented method might not outperform all other methods on all other datasets. Still, we hope to convince the reviewer that the results on the other datasets are worth being considered.", "\"Office is not suitable unless one wants to be in a few-datasample regime or work with data with noisy labels\": we would like to point that this regime is quite realistic in Bioimage informatics (noisy, with few samples per class).", "\"The results on Cell are not convincing\": as our goal is multi-domain learning on this dataset, the relevant performance indicator is the average risk over all domains.", "Table 2 details what happens in various categories of cases (on classes with/without labelled samples).", "Despite the (well-known) degradation of the results on labeled classes when one also considers unlabelled classes, the bottom line is that -- regarding the average risk -- our method outperforms the baseline.", "Importantly, MuLANN results on Cell are significantly better than the baseline on all rows which involve unlabeled classes (rows  3, 6, 9, 13), while remaining not significantly different to the baseline on 6/9 of the other rows.", "\"Comparison with other methods did not take into account a variety of hyperparameters\".", "The reviewer is right.", "Complementary experiments have thus been performed, and tables 1, 2 updated.", "We investigated the impact of varying the learning rate, the weight lambda on the discriminator loss, the weight dzeta of the known-unknown discrimination loss, the learning rate schedule, lambda schedule as well as using different learning rates for pre-trained layers versus from scratch layers (see Table 5 for more detailed information).", "These results show a moderate sensitivity of MuLANN, MADA and DANN wrt hyper-parameters and confirm that MuLANN outperforms both MADA and DANN (detailed results available here https://drive.google.com/file/d/1NjtMKF53qmnx4_Jyvh-ofxb0WjzcDvow/view?usp=sharing)."], "labels": ["rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 153, "sentences": ["1. Comment:", "Missing comparison with parameter counting bounds [1, 2].", "1. Response:", "[1, 2] are early works on RNNs and their analysis is based on very simple network architectures, which is not directly comparable to our results.", "Specifically, [1] only consider RNNs taking the first entry of the hidden state as output and restrict their discussion to polynomial or sigmoid activation functions.", "Moreover, their analysis is based on unwinding RNNs as feedforward neural networks and adopt a layer-wise analysis, which fails to incorporate the parameter sharing in RNNs, and the established bounds are far from satisfactory (O(d^8 t^2) for sigmoid activation).", "[2] simply focus on linear RNNs for binary classification problems, its extension to general settings are quite unclear.", "2. Comment:", "Vacuous bounds in the regime \\beta >1.", "2. Response:", "We have clearly indicated that our bound is always polynomial in d and t in the introduction on page 3, which is not vacuous.", "Moreover, both bounds of Theorem 3 are obtained under the same assumptions as in Theorem 2 with additional norm constraints on weight matrices.", "The exponential term stems from the layer wise covering argument rather than the range of the output.", "The bound in Theorem 2 is still polynomial in d and t, since we exploit the parametric form of RNNs and construct the covering by weight matrix coverings.", "Existing literature has shown that keeping the spectral norm of weight matrix U close to 1 stabilizes the training of RNNs.", "This can be achieved by orthogonal initialization and imposing extra constraints or regularization [3-5].", "We further discuss the trade-off between representation and generalization beneath Theorem 2 on page 4: \\beta \\approx 1 helps balance the generalization and representation of RNNs.", "3. Comment:", "Technical contribution: marginal.", "3. Response:", "We provide new understandings of RNNs by connecting their generalization properties to their empirical success.", "We establish generalization bounds using the PAC-learning framework by instilling our simple but new complexity analysis for RNNs.", "In particular, we characterize the Lipschitz property of the output of RNNs with respect to weight matrices (Lemma 2).", "This key step allows us to construct a covering on RNNs from simple coverings of weight matrices, which results in a complexity bound of RNNs polynomial in d and t.", "We further extend our analysis to establish the first generalization bounds for MGU and LSTM RNNs.", "The proposed analysis is quite different from [6], which adopts a layer-wise evaluation for neural networks and constructs the overall covering via a complicated matrix covering argument.", "Moreover, as compared in Section 4 Table 1, our generalization bound is much tighter when \\beta > 1.", "To sum up, the main contributions are listed as follows (see paragraphs 2 - 5 on page 3):", "1) We develop new techniques for effectively characterizing the model class of RNNs, which allows us to establish tight generalization bounds.", "2) We establish generalization bounds for both MGU and LSTM RNNs, and demonstrate their advantages over vanilla RNNs in generalization (under Theorems 4 and 5).", "Given very limited literature on RNNs and their variants (not even mentioning that most of existing theoretical results are negative, e.g., exponential complexities), we believe this paper has its novel contributions.", "4. Comment:", "Missing experiments to validate nature of bounds.", "4. Response:", "Please refer to the revised version for numerical evaluations in Section 6.", "In particular, we illustrate that our obtained generalization bound (Theorem 2) is much smaller than existing bounds even for \\beta > 1.", "References", "[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86, no. 1 (1998): 63-79.", "[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" In Advances in Neural Information Processing Systems, pp. 204-210. 1996.", "[3] Vorontsov, Eugene, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. \"On orthogonality and learning recurrent networks with long term dependencies.\" arXiv preprint arXiv:1702.00071 (2017).", "[4] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. \"Unitary evolution recurrent neural networks.\" In International Conference on Machine Learning, pp. 1120-1128. 2016.", "[5] Simonyan, Karen, and Andrew Zisserman. \"Very deep convolutional networks for large-scale image recognition.\" arXiv preprint arXiv:1409.1556 (2014).", "[6] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" In Advances in Neural Information Processing Systems, pp. 6240-6249. 2017."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 154, "sentences": ["1- We briefly mentioned the way problem embedding with similarity metric is used in the recommendation system in this work, but here is more explanation on that.", "The most similar problem is not necessarily recommended to a student.", "On a high level, if a student performs well on problems, we assume he/she performs well on similar problems as well, so we recommend a dissimilar problem and vice versa.", "More specifically, we project the performance of students on problems they solved onto the problems that they have not solved.", "This way, we have an evaluation of the performance of students on unseen problems.", "A problem is recommended that is within the capacity of students close to their boundary to help them learn, and at the same time recommendation is done so that all the concepts necessary for students are practiced by them.", "An evaluation on real students is presented in part 2 of the comment titled \u201cResponse to questions about Prob2Vec\u201d on this page, and we observed that similar problems are more likely to be solved correctly at the same time or wrong at the same time.", "The math expressions are not ignored in our proposed Prob2Vec method.", "In the example given in the last paragraph on page 3 for example, math expressions are used to extract the concept n-choose-k.", "We both use math expressions and text to label problems with appropriate concepts.", "2- Prob2Vec only uses expert knowledge for rule-based concept extractor, but does not use selected informative words.", "The effort put for rule-based concept extractor is negligible compared to effort needed for annotation of all problems with their corresponding concepts.", "We both annotated all problems manually and used rule-based concept extractor for annotation.", "In the former method, we observed 100% accuracy in the similarity detection test and observed 96.88% accuracy in the latter method.", "However, the rule-based concept extractor needs much less manual effort than manual problem annotation and is capable to provide us with relatively high level of accuracy we need in our application.", "Note that our method is scalable as long as problems are in the same domain as the rule-based concept extractor is automated for a single domain, but for the case that problems span many different domains, it is the natural complexity of the data set that requires a more sophisticated rule-based concept extractor.", "Furthermore, in most realistic cases for education purposes, problems span a single domain not multiple ones.", "We also like to grab your attention to the negative pre-training method proposed for training on imbalanced data sets.", "You may want to refer to part 2 of comment titled \u201cResponse to Question on Negative Pre-Training\u201d and part 1 of our response to reviewer2."], "labels": ["rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 155, "sentences": ["Q: Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?", "A: Yes we did and interestingly, it didn't show improvement.", "The results are reported in Appendix A, Table A1 in the row titled: \"Ours + classifier after\" and discussed in Section 4.2.4.", "Note that because the bypassed latent, z, is included along with the canonicalized latents, z_canon, the classifier is trained on both the original representation and the canonicalized representations together.", "Q: For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?", "A: For our experiments, each training example was bypassed and canonicalized by four different transformations: C_1, C_2, C_1C_2, and C_2C_1.", "So latents are canonicalized in both possible orderings.", "We discuss this point at the bottom of page 4 after equation 6 and will further clarify.", "Q: Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).", "There might be other constructions that are more efficient and less restrictive.", "A: Structuring the matrix to enforce properties like idempotency is definitely an interesting direction to explore in future work.", "While not equivalent, as a first step towards this, we did try to examine different levels of idempotency enforcement.", "First, at the reconstruction loss, since pairs of canonicalizers are applied in different order we enforce the the decoded results are similar.", "However, adding a stricter enforcement of similar values before decoding only hurt performance, hinting that the suggested idea of enforcing idempotency at the strictest level may hurt performance further.", "Q: I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?", "A: Treating a canonicalizer as a standard linear projection, we explore z - P*z which should contain only the factor of interest (intuitively it is the difference between a representation and a version of it stripped off of the specific factor so that the factor is isolated).", "Creating a set of such latent samples (here we took all examples to be of the same digit for visualization purposes) we ran PCA to get a dimension with the most significant variance.", "If the above mentioned assumption indeed holds, the font should be the only change in the set.", "We order the samples according to that axis and plot them in a row from left to right.", "We see that indeed the font (bottom row) and angle (top row) present a good correlation with the value along the axis."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 156, "sentences": ["The following link is a figure that explains the difference between zero-confidence attack and fix-perturbation attack.", "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure1.pdf", "As can be seen, the zero-confidence attack finds the closest point on the decision boundary; while fix perturbation-attack finds adversarial samples within a fix perturbation.", "Both attacks are equivalent if we only want to compute the attack success rate under a given perturbation level.", "However, we will be better off with zero-confidence attacks if we want to", "1) Compute the margin of each individual example; and", "2) Probe and study the decision boundary of a classifier", "Of course, we can also measure the margin of each example using a fix-perturbation attack, for example PGD, by binary searching over the perturbation levels.", "However, the computation cost will significantly increase.", "Consider, for example, the CIFAR-10 dataset.", "Since for our model, most margins fall within 10, so let\u2019s assume the binary search range is 10 (for adversarially trained models this number will be much higher).", "If we want to achieve a accuracy of 0.1, then we need at least 7 binary search steps.", "In other words, the computation complexity increases by 7 times.", "In fact, CW applies a similar binary search idea to achieve zero-confidence attack, and that is why its computation cost is high.", "The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently, but it at least provides a perspective on why zero-confidence attack is challenging, and why the complexity reduction as well as accuracy improvement of MarginAttack is valuable.", "Although this is not the major focus of your comment, we would like to revisit the theorem assumptions.", "While there are nine assumptions, these assumptions are in fact more realistic than expected.", "Take the convexity assumption, which you mentioned in your review, as an example.", "This assumption does not say that the constraint has to be convex.", "It only says that the constraint should not be \u2018too concave\u2019.", "In particular, the curvature of the of the decision should not exceed that of the L2 or L-infinity ball.", "For better illustration, we plotted some decision boundaries that are allowed by the assumption, and some that are not.", "Please check the following link:", "https://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf", "As can be seen, the convexity assumption permits a wide variety of decision boundaries.", "Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does.", "In this case, the critical point becomes a local maximum rather than a local minimum.", "The other assumptions are also more realistic than their names sound.", "The differentiability assumption does not stipulate that the constraint has to be differentiable.", "It actually permits countably infinite jump discontinuities.", "The Lipchitz continuous assumption does not assume Lipchitz continuity everywhere, but only at x*. We are not saying that the assumptions are very loose, but they are realistic enough to shed some light on the actual convergence property of MarginAttack.", "Nevertheless, we are considering adding a 2D toy example as you suggested.", "We will post further responses if there are further updates."], "labels": ["rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 157, "sentences": ["Q: \"...I find these assumptions too strong for the task of learning disentangled representation.\"", "A: We wish to emphasize that:", "(1) we only require access to meta-labels on the source set", "(2) our goal is not to find disentangled representations; Our goal is transferability so that we can learn on real data with minimal supervision.", "Q: \"you can simply train a network to predict the canonicalizations by simple supervised learning\"", "A: Finding the canonicaliers is not our goal.", "These are used as auxiliary constraints which guide representation learning and allow for latent data augmentation on our limited target data.", "Specifically, predicting the factor values will not help us in manipulating the target samples (note the significant improvement we get by using the majority vote).", "Q: method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].", "A: The suggested methods [1-3] are self-supervised methods using less information than the baselines we have included (for example, the AE+classifier baseline is trained on the same synthetic data with the same access to digit labels).", "We therefore expect that these methods will perform worse than our baselines.", "We have included  a comparison with method [3] (see general comment to all reviewers)."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 158, "sentences": ["Q: \"I had hard time to understand latent canonicalization...\"", "A: \"latent canonicalization\" is a procedure by which the representation is being modified such that a factor of variation assumes a certain prescribed value. While the values of the factors are pre-specified, the canonicalizers are learned linear operators.", "Q: The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?", "A: as we discuss in the general comments, the method is best suited for problems of interest where a synthetic simulator is available.", "In those cases full access to factors of variation is available as this is used to generate the data.", ", The question we focus on is how to best utilize the knowledge of factors of variation to improve the transferability of the learned representation to real data with limited labels.", "We are not focusing on a setting where the source domain has no labels.", "That said, our method does not assume access to all factors for all samples (in fact we only use two factors per sample).", "Q: How can the proposed method be generalized to non-image data?", "The experiments were only done on simple image datasets.", "I am wondering this method can be applied to other complex datasets whose latent factors are unknown.", "A: Our method is not specific to the image domain and there is no reason it could not be applied to other input types.", "In order to use our method, we only require a corpus of data with partially (at least two) labeled factors of variation and a corpus of data which is sparsely labeled for a downstream task.", "Critically, these two sources of data need not be identical!", "This means that an easy way to acquire a corpus with labeled factors of variation is to use a simulator to generate this data,  as we do in this study.", "Q: I do not understand this: \"to fit well the method overfitting rate\" in Section 3.3.", "Thank you for pointing this sentence out!", "It is indeed unclear.", "All we were trying to say is that each baseline\u2019s training duration was chosen independently to prevent overfitting.", "We have updated the draft to be more clear."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 159, "sentences": ["We thank the reviewer for the comments and suggestions.", "We address points individually below:", "1) The Higher-order Lipschitz condition is necessary for us to use the PL convergence guarantee.", "This condition is similar to assumptions made in convex optimization, especially where higher-order updates are involved (see eg. Agarwal et al. 2017 and Bubeck et al. 2019).", "If the iterates of the algorithm always have bounded norm (eg. due to constraints or regularization), then three-times differentiable functions will satisfy the Higher-order Lipschitz condition for our purposes.", "This is because it suffices for the condition to hold for only the iterates of the algorithm ($x^{(1)},x^{(2)},...$), rather than for all of $\\mathbb{R}^d$.", "2) We thank the reviewer for this remark.", "We wanted $L_H$ to be defined for our theorem statements, but we can see how it is confusing as is. We will make it clear that $L_H$ is the smoothness parameter of $H$.", "3) Theorem 3.4 holds in the broadest setting out of all of these results.", "Theorems 3.2 and 3.3 have slightly tighter bounds for their respective settings.", "We will clarify this in the surrounding text.", "4) It is true that these results rely on the PL condition, and this is unavoidable for our current results.", "The novel perspective in this paper is that we consider the PL condition on a different objective, namely the squared gradient norm, rather than on the game objective $g$. This perspective allows us to prove our new bounds, although we still require some nontrivial linear algebra.", "The PL condition also allows us to easily prove our stochastic HGD results.", "5) The 1/sqrt(2) should cancel out on both sides of the guarantee in Theorem 5.2 (and eg. in equation 68).", "Minor suggestions:", "-We appreciate the suggestion for page 5 and will make this change in the final version.", "We thank the reviewer for recognizing our theoretical contributions.", "We would be happy to include some further experiments in the final version comparing HGD with other algorithms such as extragradient.", "References:", "Agarwal, Naman, and Elad Hazan. \"Lower bounds for higher-order convex optimization.\" COLT 2018.", "Bubeck, S\u00e9bastien, et al. \"Near-optimal method for highly smooth convex optimization.\" COLT 2019."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_accept-praise", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 160, "sentences": ["Thank you for your thoughtful review. We will address your concerns in turn.", "Q1: The idea is very related to Yeh et al.\u2019s work which is not mentioned at all.", "A1: The entire first paragraph of our related work section is focused on Yeh et al.\u2019s work.", "As we explained in the paragraph, there is a major theoretical flaw in their method.", "Yeh et al. (2017) use the discriminator loss of a trained GAN as an indicator of how realistic their restoration is.", "However, Goodfellow et al. (2014) already prove that the discriminator is unable to identify how realistic an input is after several steps of training, if the GAN has enough capacity.", "Ideally the generator will have all the information of the data distribution while the discriminator will have none.", "That is why we use the generator of a trained GAN as an implicit probability density model in our method.", "Another difference between their work and ours is that they only focus on image inpainting problem, while our method applies to various image restoration problems.", "Q2: Total variation regularization can also handle different degradations.", "A2: We think you underestimate the difficulty of those restoration problems.", "Please check the degraded images in Table 3.", "These images are damaged so badly that TV cannot recover any meaningful thing.", "As a handcrafted prior, TV performs much worse than our data-driven baseline method in these tasks.", "Q3: Does the proposed method learn the image inpainting mask as well?", "What are the parameters of the degradation in the applications?", "A3: The image inpainting mask is known and fixed.", "We use four different kinds of degradation to test the generality of our method.", "The first three kinds of degradation are 7\u00d7 downsampling, making a 14\u00d714 square hole in the center of the image, and adding Gaussian white noise with a standard deviation of 1.0, respectively.", "The last kind of degradation is a composition of a series of degradation in order, which are (a) adding linear motion blur by at most 14 pixels in any direction, (b) 4\u00d7 downsampling, (c) adding uniform noise between -0.05 and 0.05, (d) randomly removing 10% of the pixels."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 161, "sentences": ["We thank the reviewer for its valuable and insightful comments.", "We are reviewing our work from a theoretical point of view and will update the paper very soon to reflect this.", "Even though we have not yet proved the above, we have empirically showed that the benefit of DEBAL over plain ensemble methods consists of a better representation of uncertainty, that is paramount in active learning.", "By better we mean", "1) more meaningful and closer to what one would expect (Fig 4 & Fig 6 (right))", "2) better calibrated (Fig 6 (left)).", "Our initial aim was not to compare stochastic ensembles with deterministic or single MC-dropout but to correct for the mode collapse issue in estimating posteriors with MC-dropout.", "We have empirically shown that adding ensembles to this, greatly improves the MC-dropout technique and outperforms the deterministic ensembles as well.", "We had similar doubts about the benefit of adding MC-Dropout to an ensemble.", "Therefore, we contrasted the performance of DEBAL against the plain ensemble method and showed empirically that DEBAL gives rise to better measures of uncertainty.", "Finally, as we strive to make our assumptions hold theoretically, we agree that adding theoretical Bayesian support to our method is of great importance if we are to further improve the understanding of Bayesian deep learning.", "For your final point, although Beluch et al. (2018) showed better performance for ensembles, we have shown this in the context of a small dataset problem (i.e. the size of the final dataset acquired during AL is only a small fraction of the entire available unlabelled dataset), which we believe is more relevant to the real world cases if AL is to become a widely used method.", "As for the figures, we are aware of this and will try to make them more clear in a revised version."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 162, "sentences": ["We would like to thank reviewer #1 for the constructive critics.", "We have significantly improved the exposition in this paper, ameliorated the discussion of the related work, added new theoretical results in the main text, compressed the appendix and enhanced the description of the experimental setup.", "We have updated the paper pdf and kindly ask the reviewer to take another look.", "THEORY:", "Section 2 now formalizes our contribution of unifying Riemannian geometry and gyrovector spaces for all stereographic spaces of constant curvature of any sign, formally proving that the interpolation is smooth at 0.", "Please note that gyrovector spaces and their properties were stated only for negative curvature spaces prior to our work.", "Theorem 1 formally states when k-addition is defined for spherical spaces.", "Theorem 2 formally derives formulas of exponential map, logarithmic map, geodesics and distance w.r.t. Gyro-operations in the setting of positive curvature, by making use of the Egregium theorem.", "Theorem 3 proves smoothness of the main quantities appearing in Theorem 2 w.r.t. curvature, i.e. that they are differentiable at 0, by showing that left and right derivatives from both models actually match.", "It also gives the derivative of the distance function w.r.t. curvature around 0.", "Theorem 4 describes two simple properties of our proposed left-matrix-multiplication, in particular that it is Mobius-scalar-associative.", "Theorem 5 shows that our proposed left-matrix-multiplication is an intrinsic averaging for Riemannian manifolds of constant curvature, i.e. it does commute with isometries when the matrix A is right-stochastic, which is not the case of right-matrix-multiplication.", "This is a desirable property for Riemannian averaging.", "Theorem 6 shows that weighted combinations in the tangent space used in the very recent works of [1,2] (appeared after ICLR submission deadline) is also an intrinsic averaging for Riemannian manifolds of constant sectional curvature, i.e. it does commute with isometries.", "LITERATURE:", "A more careful review of the literature has been incorporated into the introduction section.", "We have also incorporated the recent works [1,2] to appear soon at Neurips\u201919 (their text became available very recently and after the ICLR submission deadline).", "WRITING:", "We have unified all notations: bold for multidimensional quantities, i.e. vectors, matrices, embeddings.", "We have significantly improved the writing, re-done the bibliography and citing, and organized the most important theorems and definitions into a clearer presentation.", "MOTIVATION:", "We have rewritten the introduction and detailed the motivation of using spaces of constant curvature, i.e. they better represent certain classes of data such as hierarchical structures, scale-free graphs, complex networks and cyclical or spherical data.", "This is in accordance with the previous related work on non-Euclidean embeddings.", "Moreover, we have detailed some limitations of Euclidean spaces in Appendix B.", "EXPERIMENTAL SETTINGS & HYPERPARAMETERS:", "Are added to section 4 and appendix E", "EXPERIMENTAL RESULTS:", "Our methods outperform the Euclidean GCN models for the synthetic datasets for minimizing distortion, while being better or competitive for node classification.", "We believe further experimental investigations are needed to better train non-Euclidean graph neural network models.", "[1] Hyperbolic Graph Neural Networks, I. Chami et al., Neurips\u201919", "[2] Hyperbolic Graph Convolutional Networks, Q. Liu et al., Neurips\u201919", "We would love to hear your feedback on our substantially improved paper (based on your suggestions).", "Unfortunately, ICLR's tight schedule would prevent us to answer any additional questions after 15th of November. Thank you!"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 163, "sentences": ["Thank you for your time and effort of reviewing our paper. Please see our response below.", "Our main contributions include:", "(i) For single-layer random variables (RVs), we propose a unified gradient named GO by exploiting the integration-by-parts idea, which is applicable to continuous/discrete RVs.", "In the special case of single-layer continuous RVs where GO recovers Implicit Rep or pathwise gradients, we consider it\u2019s our contribution to provide a principled explanation (via integration-by-parts) why Implicit Rep and pathwise gradients have low Monte Carlo variance; or in other words, we prove that their implicit differentiation originates from integration-by-parts.", "(ii) For multi-layer RVs, our main contribution is the discovery that with GO (or in other words, the introduced variable-nabla), one can back-propagate gradient information through a nested combination of nonlinear functions and general RVs (including non-reparameterizable continuous RVs, back-propagating through which is challenging).", "Another interpretation of this contribution is that GO enables generalizing the deterministic chain rule to a statistical version.", "Here, we refer to deterministic chain rule as back-propagating gradient through deterministic functions (like neural networks) or reparameterizable RVs (like Gaussian).", "By contrast, statistical chain rule is referred to as back-propagating gradient through more general RVs (including non-reparameterizable ones).", "Of course, statistical chain rule recovers deterministic chain rule for deterministic functions and reparameterizable RVs, because GO recovers the standard Rep.", "(iii) Another 2 minor contributions include Lemma 1 and Corollary 1.", "In Lemma 1, we explicitly prove that our deep GO gradient contains the standard Rep as a special case, in general beyond Gaussian.", "Note neither Implicit Rep nor pathwise gradients can recover Rep in general, because a neural-network-parameterized reparameterization usually leads to a nontrivial CDF.", "In Corollary 1, we reveal the fact that the proposed method degrades into the classical back-propagation algorithm under specific settings.", "Finally, we believe it is interesting to create a consistent architecture, which unifies (a) a GO gradient which contains many popular gradients as special cases, and (b) a more general statistical chain rule developed based on GO which recovers the well-known deterministic chain rule under specific cases.", "For your comments not addressed above, please see our additional response below.", "(1) We have made clearer the relationships among the standard Rep, Implicit Rep/pathwise, and our GO in the revised manuscript.", "In the revised paper we have explicitly pointed out that the experiments from (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) additionally support our GO in the special case of single-layer continuous RVs.", "(2) Please refer to our main contributions summarized above, where other contributions, beyond GO for discrete RVs, are clarified.", "(3) Please refer to our main contributions (ii)-(iii).", "As stated in our paper, many works tried to solve the problem of stochastic/statistical back-propagation.", "We consider our contributions in Secs. 4 and 5 as one step toward that final goal.", "Please note that what\u2019s done in Secs. 4 and 5 is not straight-forward and has not been reported before.", "Since stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) focuses mainly on reparameterizable RVs, deterministic chain rule as mentioned in main contribution (ii) can be readily applied.", "By contrast, we target towards more general situations in Secs. 4 and 5 where deterministic chain rule might not be applicable, such as for non-parameterizable (continuous) RVs.", "We prove that one can utilize our GO to sequentially back-propagate gradient though non-parameterizable continuous RVs, namely the statistical chain rule mentioned in main contribution (ii).", "We have revised the last paragraph of the Introduction to make a more explicit summation of our main contributions, as mentioned above.", "We hope your concerns have been addressed. If not, further discussion would be welcomed."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 164, "sentences": ["Your interpretation of section 3 is exactly right.", "Thank you for suggesting additional experiments to better understand the behavior of the scratchpad component.", "We would like to note that beyond the gains across all evaluated quantitative metrics (bleu, rouge, meteor), our method shows substantial gains on human evaluations.", "In future work we propose to use our method to generate a large dataset and evaluate its performance.", "We don\u2019t claim to be the first to generate questions from logical form, but the experiments within show that our approach is superior to standard approaches in the literature."], "labels": ["rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 165, "sentences": ["Many thanks for the review!", "Good point regarding the negative results; we have added a subsection in the revised paper entitled ``A non-convolutional network'', where we compare to a convolutional decoder and conclude that ``Our simulations indicate that, indeed, linear combinations, yield more concise representations, albeit not by a huge factor.''.", "Regarding the minor points, we have reworded the paragraph on regularizing, and changed `compression ratio' to `compression factor', and reworded such that `large compression factor' means large compression."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 166, "sentences": ["We thank the reviewer for the suggestions.", "Q1: Robot design were explored in (Sims, 1994) etc. The novelty of the paper is fairly incremental.", "We respectfully disagree and believe our contributions are significant.", "We note that only NGE among all the baselines has the ability to optimize both the graph G and the controller parameters.", "Graph neural network formulation is KEY here, enabling it to perform this efficient policy transfer.", "To the best of our knowledge, the traditional methods (such as (Sims, 1994)) require re-optimizing parameters of the controllers from scratch for each different topologies, which is computationally demanding and breaks the joint-optimization.", "To further showcase our work with respect to prior art, we added (Sims, 1994) as an additional baseline in the latest revision.", "We refer the reviewer to the general response for details.", "NGE has about 2x performance of (Sims, 1994) in both fish and walker environments.", "Moreover, we argue the videos of (Sims, 1994) might be confusing as it mixes the results of policy evolution from human-designed robots and structure evolution.", "Q2: Can it be applied to more complex morphologies?", "Humanoid etc.", "maybe?", "NGE can be applied to evolve humanoids, however, there are two major difficulties in doing that in practice.", "1. Training humanoid controllers is of orders of magnitude more difficult than training cheetah (Schulman, 2017).", "2. To evolve realistic humanoid structure (e.g. hands, symmetrical limbs), one would need to have more realistic environments that better reflect tasks and complexity in the real world.", "However, we agree that this is a very interesting direction for the future.", "Q3: Comparison to more baseline, for example models with no message passing.", "We thank the reviewer for pointing out the baseline of no message passing in GNN, which we named as ESS-BodyShare.", "In the latest revision, we have 5 baselines from previous research and modern variants, which further showcases the significance of our work.", "In general, NGE has significant improvement both quantitatively and qualitatively.", "We refer the reviewer to the general response for further information.", "Specifically for ESS-BodyShare baseline:", "|", "NGE     | ESS-BodyShare", "fish         |  70.21   |  54.97", "(78.3% of NGE)", "Walker   |", "4157.9 |   2185.1 (52.5% of NGE)", "In environment where global information is needed (for example, walker with multiple rigid body contact), the performance is jeopardized. But in an easier environment, message passing is less needed.", "Q4: Clarification of Figure-4 (Section-4.2)", "Our aim was to show that in the case where the human-engineered topology needs to be preserved, it is better to co-evolve the attributes and controllers with NGE rather than only training the controllers (controllers are trained from scratch for both NGE and baselines).", "The x-axis was scaled according to the number of updates.", "We apologize for the lack of clarity.", "We revised the x-axis from \u201cgenerations\u201d to parameter \u201cupdates\u201d in the latest revision.", "In the latest revision, we also included the curve where the topologies are allowed to be changed, which leads to better performance, but does not necessarily preserve the initial structure.", "Schulman, 2017. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 167, "sentences": ["Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "Here we respond to your specific comments.", "\"Some technical details are missing.", "In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing.", "Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\"", ">>> Thanks for pointing out the details.", "We want to clarify the few-shot setting.", "We follow the widely-used episodic paradigm proposed by Matching Networks [1].", "In each episode (training batch), our algorithm solves a small classification problem which contains N classes each having K support and Q query examples (e.g., N=5, K=1, Q=15, totally 80 examples).", "The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset.", "This is very fast and efficient.", "In deep neural networks, there is a common trick in computing the gradient of operations non-differentiable at some points, but differentiable elsewhere, such as Max-Pooling (top-1) and top-k.", "In forward computation pass, the index position of the max (or top-k) values are stored.", "While in the back propagation pass, the gradient is computed only with respect to these saved positions.", "This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch.", "In our paper, we use the tensorflow function tf.nn.top_k() to compute k-nearest neighbor operation.", "\"Does episode training help label propagation? How about the results of label propagation without the episode training? \"", ">>> In our paper, the length scale parameter \\sigma is trained in an example-wise and episodic-wise way, as described in section 3.2.2 and Figure 4 of Appendix A. In order to investigate the benefit of episodic training, we combine the heuristic-based label propagation methods [2] with meta-learning to serve as a transductive baseline.", "Please refer to Table 1 and Table 2 line \"Label Propagation\".", "It can be seen that TPN outperforms naive label propagation with a large margin, thus verifying the effectiveness of episode training.", "[1] Vinyals, Oriol et al. \"Matching networks for one shot learning.\" NIPS. 2016.", "[2] Zhou, Denny et al. \"Learning with local and global consistency.\" NIPS. 2004."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_summary", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 168, "sentences": ["Thank you for your careful consideration and feedback.", "Following your request, we updated the paper to include mean learning curves for different models in Figure 6 in Appendix C. Our models converge faster than DNC.", "Some of them (especially DNC-MD) also have significantly lower variance than DNC."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 169, "sentences": ["We thank Reviewer 1 for the constructive feedback.", "Here is our point-to-point response to the comments and questions raised in the review:", "1. \u201cThe numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper\u2026 I wonder why the numbers are so different.\u201d", "Table 1 of \"Obfuscated Gradients Give a False Sense of Security\" reports an accuracy of 47% under 0.031 norm-inf perturbation for the CIFAR10 dataset (55% is reported for the MNIST dataset), approximately the same as the 44% accuracy in our Figure 5.", "The difference in performance stems from how we preprocessed the CIFAR10 images: exactly in the manner described by (Zhang et al., 2017)\u2019s ICLR paper \u201cUnderstanding deep learning requires rethinking generalization\u201d (we whiten and crop each image).", "2. \u201cWhat's the training time of the proposed method compared with vanilla adversarial training?\u201d", "We have added Table 2 to the Appendix which reports the increase in runtime for each of the 42 experiments discussed in Table 1 after introducing spectral normalization.", "For 39 of the cases, our TensorFlow implementation of the proposed method results in longer training times (from 1.02 to 1.84 times longer).", "In the 3 cases of iterative adversarial attacks with the Inception architecture, the proposed method actually results in faster training time.", "This is likely due to how TensorFlow handles training in the backend.", "We provide the code for full transparency.", "3. \u201cThe idea of using SN to improve robustness has been introduced in the following paper: \"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\" (but this paper did not combine it with adv training).\u201d", "Thank you for bringing this recent work to our attention.", "We cite and discuss this NIPS paper in our updated draft."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 170, "sentences": ["Pose2Pose -- An ablation study for the P2P network can be found in Table 2, with quantitative results for each contribution.", "We do not add a qualitative ablation study for the P2P network, since still-images (as opposed to videos) do not convey the temporal improvement in this case.", "Pose2Frame -- A qualitative ablation study can be found in Fig. 16.", "As can be seen, the results justify each component used.", "pix2pixHD -- the Pose2Frame network can be directly compared with the pix2pixHD network, since they both act as mapping functions between dense-pose representations to realistic images.", "A quantitative comparison can be found in Table 1, as well as a qualitative comparison in Fig. 14.", "As can be seen, the use of our different components described in the P2F ablation study (blending mask and regularization, object channel, two pose inputs, discriminator attention on character, etc.), results in much fewer artifacts, making the Pose2Frame network suitable for this application.", "Combining the Pose2Pose and pix2pixHD networks, would yield significant artifacts (as seen in Fig. 14), and is not suitable for this kind of application."], "labels": ["rebuttal_done", "rebuttal_reject-request", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 171, "sentences": ["We thank reviewer 3 for the detailed feedback.", "We are glad that the reviewer found the extensive evaluation appropriate, and that our model behaves well for the realistic and diversity measures.", "We now address all the individual questions.", "We added Section 3.5 to point out the differences between the VAE component of our model and the SV2P and SVG models from prior work.", "In Section 3.4, we clarified what frames the discriminator takes, and in Section 4.3 we added a description of the deterministic version of our model.", "In Section A.1.1, we provided a better description of how frames are predicted at each time step.", "In Section 3.5 and A.1.2, we clarified that the latent variables are sampled at every time step.", "We updated Section 4.4 to indicate that it is to be expected that although our SAVP model improves on diversity and realism, it also performs worse in accuracy compared to pure VAE models (both our own ablation and SVG from Denton & Fergus (2018)).", "A recent result [1] proves that there is a fundamental tradeoff between accuracy and realism, for all problems with inherent ambiguity.", "In fact, a recent challenge held at ECCV 2018 in such a problem [2] evaluates all algorithms on both of these axes, as neither adequately captures performance.", "Note that proposing a generator architecture is not the goal of this paper.", "Instead, we provide a systematic analysis of the effect of the loss function on this task (which could be applied to any generator).", "We use a warping-based generator, from prior work (Ebert et al. 2017), and include a comparison to SVG for completeness.", "In the updated draft, we clarify in Section 3.4 that the warping component assumes that videos can be described as transformation of pixels, but that any generator (including the one from Denton & Fergus (2018)) could be used with our losses.", "Since evaluating generator architectures is not the emphasis of this paper, we did not test the importance of the warping component nor test on videos where this hypothesis is less suitable.", "We have included a revised plot in Figure 14 at the end of the Appendix (note that this temporary plot will be incorporated to Figure 6), where we use the official implementation of SSIM and replace the VGG metric with the Learned Perceptual Image Patch Similarity (LPIPS) metric (Zhang et al., 2018).", "LPIPS linearly calibrates  AlexNet feature space to better match human perceptual similarity judgements.", "Aside for the first two predicted frames, our VAE ablation and the SVG model both achieve similar SSIM and LPIPS performance.", "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077", "[2] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. 2018 PIRM Challenge on Perceptual Image Super-resolution. In Perceptual Image Restoration and Manipulation (PIRM) workshop at ECCV 2018.", "https://arxiv.org/abs/1809.07517", "We have included a revised plot in Figure 15 at the end of the Appendix (which will be incorporated to Figure 7) that fixes the KTH dataset preprocessing.", "Our VAE-only model now achieves substantially higher accuracy and diversity than SVG (Denton & Fergus, 2018).", "As before, the GAN-only model mode-collapses and generates samples that lack diversity.", "Our SAVP method, which incorporates the variational loss, improves both sample diversity and similarities, compared to the GAN-only model.", "Our SAVP model also achieves higher accuracy than SVG.", "The experiments from our original submission (1) cropped the videos into a square before resizing, and thus discarded information from the sides of the video, and (2) did not filter out the empty frames, and thus our models were trained on uninformative frames.", "We fixed those issues to match the preprocessing used by Denton & Fergus (2018).", "In addition, we have also included experiments where we condition on only 2 frames instead of 10 frames, in order to test on a setting with more stochasticity."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 172, "sentences": ["Thanks for your valuable review! We've added an experiment section in the new revision, showing how BN helps convergence in the training process."], "labels": ["rebuttal_done"], "confs": [1.0]}
{"abstract_id": 173, "sentences": ["We thank the reviewer for their review.", "We address the different remarks below.", "\u201cIt is unclear whether the data augmentation techniques is applied only at training time or also at test time. In other words: at test time, do you present the original images only or transformed images too?\u201d", "We apply the data augmentation both at training and test time.", "\u201cIn section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?\u201d", "We agree that the full distribution of the softmax layer provides more information, but there is no straightforward way to extend the Kolmogorov-Smirnov distance to multi-dimensional distributions, beyond the two- and three-dimensional cases.", "We focus on confidence as a proxy to the loss, and we assume that the loss is the quantity that should be the most different between training and testing, as the optimization phase explicitly minimizes the loss on the training set.", "Moreover, early experiments showed that using the outputs of intermediate layers provide no improvement for membership inference (on preliminary CIFAR-10 experiments, we obtained respectively 67.7 accuracy with the output layer and 66.5 when using all layers).", "\u201cSection 5 is somewhat less clear than the previous sections. The authors should more clearly define what the private, public and evaluation sets are, right from the beginning. The purpose of the public set is explained only in section 5.2.\u201d", "We will update this section to make it clearer.", "\u201cThe experimental results of section 5.2 are somewhat disappointing. Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.", "Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).", "This seems to be too low to be of practical use.", "This might be because the Bayes and MAT attacks are too simplistic.", "Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?\u201d", "We agree that better performance could be obtained by running the initial model for more epochs, but our goal is to stay close to the standard training of Imagenet models, i.e. 90 epochs with an initial learning rate of 0.1, divided by 10 every 30 epochs.", "We emphasize that the last line of Table 3 corresponds to the most difficult setup, where the network has been trained with a strong data-augmentation, and we only use the intermediate layers of the network (which amounts to less than 62% of the parameters for e.g. Resnet101), this is why the performance is significantly impacted.", "We experimented with more sophisticated models, and it did not bring any improvement (see shadow models in appendix E, e.g. the performance before the softmax layer is 58.2 for Resnet101 and 60.8 for our method)."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 174, "sentences": ["We thank the reviewer for their evaluation.", "Please see our response at https://openreview.net/forum?id=Hkx-ii05FQ&noteId=HygFbNmL6X, where we also discuss our experimental framework.", "Even though we present results on two tasks, it appears the paper structure doesn\u2019t convey this clearly, and we suggest two possible ways how to update the paper in this regard.", "We note in this context that even though we would also like to see Cakewalk evaluated on the domains mentioned by the reviewer, these are not part of our own research agenda, and accordingly our suggestions refer to other problems in combinatorial optimization.", "Next, we address other issues raised by the reviewer.", "First, we\u2019d like to emphasize that the clique problem studied in the paper is far from being a toy problem.", "All the algorithms are evaluated on the DIMACS clique dataset which was published as part of the second DIMACS challenge which specifically focused on combinatorial optimization.", "Over the years, this dataset has become a standard benchmark for clique finding algorithms, and results on it are regularly published.", "In this respect, this dataset is an important benchmark for clique algorithms very much like CIFAR10 and CIFAR100 are for image classification methods.", "Notably, Cakewalk approaches the performance of the best clique finding algorithms that directly search a graph, and which are tailored to this specific task.", "Note that none of the tested methods were given enough samples even to recover the graph itself, as most graphs have more than 100 nodes, and we\u2019ve allowed only for 100 |V| samples in each execution.", "To us this seems as a rather challenging setup, not just for the algorithms we\u2019ve tested in this paper, but for any clique finding algorithm.", "Next, we wonder how would the reviewer correct the confounds mentioned with regard to the clique experiment.", "Providing a controlled experiment is always challenging, though the elements mentioned by the reviewer were specifically selected as to reduce various confounds.", "The main research question we try to address is whether algorithms that only rely on function evaluations can recover locally optimal solutions.", "Since the objective is the only source of information for such algorithms, an all-or-none kind of objective would not be very useful.", "Instead, the objective is designed in a manner that provides information even for partial solutions, thus allowing the tested algorithms to gradually improve the objective.", "In terms of the sampling distribution, as our focus is on the update step, we decided to use the simplest possible sampling distribution we can think of.", "In such a regime, we can attribute any performance gains to the algorithms themselves, and not to any prior knowledge that is reflected by the structure of some complex sampling distribution.", "Next, we agree that local optimality is a mean rather than a goal (the objective itself).", "Nonetheless, as in the problems we seek to address the global optimum cannot be found in polynomial time", ", the second best approach is first to design a method that can recover locally optimal solutions.", "Once such a method is available, repeated applications of that method can allow one to select a good solution, very much like the standard practice of repeated applications of k-means which the reviewer mentions.", "This reasoning however is dependent on a method\u2019s capability of recovering locally optimal solutions, and therefore studying this ability makes for a worthwhile effort.", "Answers to the last comments:", "- Table 3 is indeed confusing, this is a good point. We will correct it.", "- Methods that apply a surrogate objective work best with AdaGrad.", "In this case, our data is a classical use which is explored in the AdaGrad paper uses as a motivating example.", "Not surprisingly, both Cakewalk and OCE work best with it.", "REINFORCE however is sensitive to the objective values, and it appears that Adam somewhat mitigates this problem.", "However, this is not as effective as applying a surrogate objective, and REINF_Z with Adam is outperformed by OCE_0.1 with AdaGrad in all measures.", "- Our frame of reference were algorithms that could be applied to any combinatorial problem, and which only rely on function evaluations.", "Control variates and reward shaping methods are mostly useful when tied to the particularities of a given objective, and thus do not fall into this category.", "In neural combinatorial optimization the study is focused on designing a sampling distribution that reflects some prior knowledge about a problem, and thus, we consider this line of work as orthogonal to ours.", "Having said that, we see how these areas might seem related, and we will revise the related work section to better emphasize the aforementioned differences.", "- We selected the name \u2018Cakewalk\u2019 after consulting with a few colleagues. Following a joint discussion, we concluded that this name has the best chance for increasing our work\u2019s impact."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_by-cr", "rebuttal_reject-request"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 175, "sentences": ["We really appreciate your constructive comments.", "We respond to each comment as follows.", "1. Meta dropout does not regularize the variational framework because there is no variational inference framework.", "- Thank you for your comment.", "We agree with you that the current lower bound is not a variational form due to the assumption of q=p. In Section 3.2, we toned down the original expression \u201cLearning to regularize variational inference\u201c into \u201cConnection to variational inference\u201d, and corrected the corresponding sentences.", "Still, there exists a clear connection between standard variational inference and our learning framework.", "Thus we believe that discussion in Section 3.2 will be helpful to readers who want to understand the meaning of learning objective Eq.(2) in depth.", "2. Improving adversarial robustness experiment.", "- Thank you for the helpful suggestion.", "During the rebuttal period, we conducted additional experiments on adversarial robustness as you suggested:", "a) We replaced the previous FGSM attack with stronger PGD attack (200 iter.), with $L_1$, $L_2$, and $L_\\infty$ norm constraints.", "b) We included more baselines (e.g. Mixup, VIB, and information dropout), and show that our meta-dropout largely and consistently outperforms all of them.", "c) We added more detailed descriptions of the adversarial meta-learning baseline and in-depth analysis on the results.", "d) We further show that the learned perturbation from our Meta-dropout also generalize across different types of adversarial attacks with $L_1$, $L_2$, and $L_\\infty$ attacks.", "The generalization to different types of attacks is an important problem in adversarial learning, and most existing models fail to achieve this goal.", "Please see the corresponding section in the revision.", "We believe that the adversarial robustness part of our paper has become much stronger than before, thanks to your suggestion."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 176, "sentences": ["Thank you very much for the constructive comments.", "We tried to strengthen our claims by adding more experimental data which the Reviewer requested.", "1. The proposed \"Multi-bit-quantization + Viterbi-based binary code encoding\" requires slightly larger memory footprint than \"Multi-bit quantization only ([4])\" because some of the Viterbi encoded bits have different indices from their corresponding quantization bits.", "Hence, the \"Multi-bit quantization only\" requires 10 % to 20 % smaller memory footprint than \"Multi-bit-quantization + Viterbi-based binary code encoding\" case.", "However, the main reason why we apply the Viterbi weight encoding is that parallel sparse-to-dense matrix conversion can be done by applying same Viterbi encoding process to the non-zero values and indices of the non-zero values in parallel.", "This parallel sparse-to-dense conversion makes the speed of feeding parameters to PEs 10 % to 40 % faster compared to [1] (Figure 6c).", "2. Per Reviewer\u2019s suggestion, the experimental results for the effectiveness of \"Don\u2019t Care\" term have been moved to Section 4.1.", "3. Per Reviewer's suggestion, we measured accuracy differences before and after Viterbi encoding for several quantization methods such as linear quantization ([2]), logarithmic quantization ([3]), and alternating quantization ([4]) methods with the same quantization bits (3-bit).", "The result shows that combination with alternating quantization and Viterbi weight encoding had only 2 % validation accuracy degradation after the Viterbi encoding was applied first right after the quantization and the accuracy was easily recovered with retraining.", "On the other hand, the combination with the other quantization methods and Viterbi weight encoding showed accuracy degradation as much as 71 %, which was too large to recover the accuracy with retraining.", "The accuracy difference mainly results from the uneven weight distribution.", "Because weights of neural networks usually are normally distributed, the composition ratio of '0' and '1' is not equal when the linear or logarithmic quantization is applied to the weights of neural networks.", "As we stated in the manuscript, Viterbi encoder tends to produce similar number of '0' and '1'.", "Therefore, we can conclude that under the same bit condition, alternating quantization method shows best accuracy and compatibility with our bit-by-bit Viterbi encoding scheme regardless of the type of neural networks.", "4. We conducted additional simulations to compare sparse matrix reconstruction speed of [1] and the proposed method.", "We used a random 512-by-512 size matrix with various pruning rate ranging from 75 % to 95 %.", "We conducted the simulations under the assumptions described in Figure 6c.", "The simulation results are shown in Figure 6c in updated manuscript.", "We could observe that the proposed method could feed 10 % to 40 % more nonzero weights and input activations to PEs in same 10000 cycles compared to [1].", "Proposed method could also feed parameters to PEs 20 % to 106 % faster compared to baseline method, which reads dense weight and activation matrices directly from DRAM.", "The improvement in the proposed scheme mainly comes from the parallelized process of assigning non-zero values to their corresponding indices in the weight matrix.", "While preparing addition data for the rebuttal, we realized that our simulation model did not fully exploit the parallelized weight and index decoding process of the proposed method.", "After further optimization, we could observe that the parameter feeding rate of the proposed method increased compared to the reported data in original manuscript.", "Therefore, we updated Figure 7 in original manuscript to Figure 6c in updated manuscript according to the new data.", "5. We added the change of the exact weight representation at each process in Figure 1 to clarify the flowchart.", "Reference", "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "[2] Darryl D. Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. Fixed point quantization of deep convolutional networks.", "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pp. 2849\u20132858. 2016.", "[3] Daisuke Miyashita, Edward H. Lee, and Boris Murmann. Convolutional Neural Networks using Logarithmic Data Representation. CoRR, abs/1603.01025, 2016. URL https://arxiv.org/abs/1603.01025.", "[4] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 177, "sentences": ["We thank you for the constructive feedback and discuss some of your comments below.", "R3: \"it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards\"", "=> We would like to highlight that evaluating success of pure curiosity-driven exploration (no extrinsic rewards for training) by measuring the extrinsic score of game is just a proxy to evaluate exploration.", "Our results show that exploration via curiosity has striking correlation with game scores.", "But we expect that when environments have a well-defined (and well-shaped!) extrinsic reward, a policy trained using that extrinsic reward should outperform the policy trained with only curiosity especially when the performance is measured by the extrinsic return.", "There are, however, examples, such as the Bowling Atari game, where a policy trained with only curiosity does *better* than a policy trained with extrinsic rewards.", "The purely curious agent learns to play the game better than agents trained to maximize the (clipped) extrinsic reward directly.", "We think this is because the agent gets attracted to the difficult-to-predict flashing of the scoreboard occurring after the strikes.", "We expect such examples to come from environments with misleading or poorly-shaped extrinsic rewards.", "R3: \"...it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.\"", "=> In the paper, Section 2.1, we discuss that random features have advantages that they are they are stable, compact, and tend to include most relevant information about the observation.", "However, in our opinion, a more interesting question is not why random features perform so well, but rather why the feature learning methods perform so poorly (relative to this baseline).", "Learning the features introduces non-stationarity that confounds the effects of learning the dynamics.", "We believe that if methods are developed to address this non-stationarity, or environments that are more visually complex are used, then the benefits of the learning the features will become more noticeable."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_summary", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 178, "sentences": ["Thanks for your comments.", "To our knowledge, close to 2% improvement of accuracy is not small in CIFAR100. Because we only change pooling layers while keeping others exactly the same.", "Now, we respond to your questions one by one:", "1.", "The results of AA-pooling and F-pooling are not the same.", "In Fig. 1, we show the results of average-pooling and F-pooling.", "If you carefully look at the corner of curves, you can find the differences.", "Without convolution, AA-pooling is similar to average-pooling (both of them are low-pass filters but with sightly different kernels.", "So AA-pooling gives different results for sine waves.", "2. We believe F-pooling plays a more important rule in applications where shift-equivalent is serious, such as object detection and object tracking.", "Because we need to predict the location or shifts of an image object.", "Moreover, F-pooling may be better for complex-valued CNNs, such as [1].", "3. The limitation of imaginary part is easy to overcome: set the resolution of F-pooling\u2019s output to an odd number or padding it to an odd number when the resolution is an even number.", "In this way, the imaginary part is zero.", "Moreover, the word shift in this paper means circular shift.", "So it is better to use circular padding in convolutional layers.", "However, we find circular padding slower the training speed in PyTorch.", "If we use zero paddings as in most situations, the beneficial of F-pooling is reduced.", "Our current experiments use zero paddings.", "See our general response for what happens when we replace zero paddings with circular padding.", "4. In all experiments of our current paper, the imaginary part is already ignored.", "We can\u2019t directly measure how the imaginary part affects the performance unless we use complex-valued CNNs.", "Ignoring this part will destroy the reconstruction optimality, but the effect is small.", "Suppose the output size of F-pooling is 2N+1.", "We first transform a signal into frequency domain and keep 2N+1 components with the lowest frequencies: f(-N), \u2026 , f(0), \u2026 ,f(N).", "Then we transform it back into time domain.", "In this case, the imaginary part in time domain is zero because of symmetry.", "Now, suppose the output size is 2N+2: f(-N), \u2026 , f(0),", "\u2026 f(N), f(N+1)", ".", "In this case, the imaginary part is not zero.", "However, if we set f(N+1) to 0, it imaginary part becomes zero again.", "Thus, the error of ignoring imaginary part is not larger than ||f(N+1)||.", "Fig.4 shows an example of odd and even output size of F-pooling.", "[1] Deep complex networks, ICLR2018"], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 179, "sentences": ["Dear Reviewer:", "Thank you for your valuable comments.", "We have addressed typos in the revision accordingly.", "And please find our response as follows.", "-  Can you be more specific about the gains in training versus inference time?", "We would like to emphasize that the our goal is to speed up the inference time for softmax, so we do not include any comparisons in terms of training time.", "According to our experiments, most speedup can be achieved in few epochs (given all other layers are pre-trained) so that the training time increase is not significant compared to the original one.", "- You motivate some of the work by the fact that the experts have overlapping outputs. Maybe in section 3.7 you can address how often that occurs as well?", "Thanks for the suggestion.", "We demonstrate that ambiguous words are often overlapped between clusters as illustrated in Figure 3(b).", "We added one more Figure in Appendix B, Figure (b), to demonstrate the distribution of overlapping.", "- It wasn't clear how the sparsity percentage on page 3 was defined?", "Sorry for the possible confusion.", "The sparsity in page 3 means the percentage of pruned words.", "We have added more clarifications in the revised version.", "- Can you motivate why you are not using perplexity in section 3.2?", "We use top-k accuracy, instead of perplexity, because approximating top-k is required for most inference tasks in practice (see [1]).", "Perplexity captures the normalized log-likelihood of all possible words, while top-k accuracy is a better measure for inference speedup for top-k retrieval.", "For example, in some extreme cases, if a word only has a very small probability which makes it unpredictable at all (i.e. couldn\u2019t be retrieved by top-k for any reasonably small k)", ", it could still have a huge impact in terms of perplexity, but has a much smaller impact on top-k accuracy, which seems more reasonable given the goal of top-k retrieval.", "[1] Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS), NIPS 2014"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 180, "sentences": ["We appreciate your constructive feedback.", "Specifically, your comments about the motivation and problem definitions greatly help us to improve the quality of our paper.", "(Importance and motivation)", "To step forward to artificial general intelligence, we should further consider making an agent that can learn and remember many tasks incrementally [1].", "However, this is particularly challenging in real-world settings: the agent may observe different tasks sequentially, and an individual task may not recur for a long time.", "In this settings, a learned model might overfit to the most recently seen data, forgetting the rest, a phenomenon referred to as catastrophic forgetting, which is a core issue CL systems aim to address [2].", "Recently, GR-based methods, inspired by the generative nature of the hippocampus as a short-term memory system in the primate brain [3], have been widely studied to address the catastrophic forgetting problem.", "In terms of GR, we are trying to address the two open questions mentioned above.", "(Use of labels and novelty) In GR-based approaches, the quality of generated samples is crucial to keep the performance of previous tasks.", "If we use labels, we can construct a conditional generative model.", "Generally, conditioning on a generative model yields higher quality samples than unconditional one and makes it possible to generate class-balanced samples [4]; the importance of conditional generation is also described in section 6.1 in our paper.", "In this paper, we showed that discriminative regularization could make VAE possible to conduct both class conditional generation and classification with one integrated model.", "Thus, we do not need to train an additional classifier, e.g., deep CNN, which is necessary for other works, including Narayanaswamy et al. There is also classifier integrated VAE such as [6].", "The difference with [6] is the use of class-conditional priors; more details are explained at the response (Difference with CDVAE) for reviewer 3 and section 4.1 in our paper.", "(Domain translation) Even though the conditional generation improves the quality of the generated samples, there is still a big difference between real and generated images.", "Because a deep neural network is vulnerable to even single-pixel perturbation [5], the difference can seriously affect the classification performance of GR-based algorithms.", "Thus, we suggested applying the domain translation to address this issue.", "By narrowing distribution discrepancy between real and generated images using the domain translation technique, we were able to alleviate the catastrophic forgetting problem successfully (Table 2).", "(Modeling) Good point.", "Since we consider finite discrete conditions, we can directly optimize $\\mathrm{\\mu_c}$ and $\\mathrm{\\sigma_c}$ for each c as parameters without the prior network.", "However, introducing a prior network makes our model become a more general framework that can address continuous-valued conditions.", "Also, in our paper, we set the prior network as a single fully-connected layer for easy handling of conditions and simple implementation.", "Otherwise, we should keep an additional mapping table between class conditions and its $\\mathrm{\\mu_c}$ and $\\mathrm{\\sigma_c}$.", "(Experimental settings)", "Generally, CL systems assume that each task comes sequentially, and an agent can not directly access previous experience [2].", "We exactly follow the assumption.", "Also, we train DiVA sequentially for each task with one same model.", "To clarify our training process, we provide a brief summarization.", "Firstly, we train DiVA with task 1 that consists of real images and labels.", "Then, when new task 2 is coming, DiVA generates images and its labels of task 1 and learns both task 2 and the generated task 1 simultaneously.", "We added an additional figure in Figure 6 in Appendix E, for helping conceptual understanding.", "(Domains of CIFAR dataset) Since current generative models are not perfect for generating complex natural images, there is always a discrepancy between generated images and real images.", "Thus, we can define two domains: real image domain (realistic) and generated image domain (blurry).", "We used the domain translation for narrowing the gap.", "[1] Legg, Shane, and Marcus Hutter. \"Universal intelligence: A definition of machine intelligence.\" Minds and machines 17.4 (2007): 391-444.", "[2] https://sites.google.com/view/continual2018", "[3] Shin, Hanul, et al. \"Continual learning with deep generative replay.\" Advances in Neural Information Processing Systems. 2017.", "[4] Lesort, Timoth\u00e9e, et al. \"Marginal Replay vs Conditional Replay for Continual Learning.\" International Conference on Artificial Neural Networks. Springer, Cham, 2019.", "[5] Su, Jiawei, Danilo Vasconcellos Vargas, and Kouichi Sakurai. \"One pixel attack for fooling deep neural networks.\" IEEE Transactions on Evolutionary Computation (2019).", "[6] Mundt, Martin, et al. \"Unified Probabilistic Deep Continual Learning through Generative Replay and Open Set Recognition.\" arXiv preprint arXiv:1905.12019 (2019)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 181, "sentences": ["Thank you for your review and very useful comments! We\u2019re happy you found our manuscript interesting.", "To address your comments:", "1) Thank you for pointing out that we had not defined the delta.", "Here delta is the Kronecker delta defined so that \\delta_{a,b} = 1 if a = b and 0 if a != b. In the context of the variance of the multivariate normal distribution, the delta function indicates that the different neurons in each layer have zero covariance.", "We\u2019ll add an explicit discussion of this fact to the manuscript.", "2) Thanks for pointing this out, we\u2019ll correct it in the next revision.", "3) It is true that the extent to which randomized weights describe trained networks is unclear.", "However, it is true that most commonly used weight initialization schemes are random.", "For example, He initialization [1] and Xavier initialization [2] strategies are both special cases of the setup considered here.", "We therefore view our theory as a theory of neural networks at initialization.", "(There are, however, initialization schemes that are not random and that are not described by our theory).", "[1] K. He, X. Zhang, S. Ren, J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. (http://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html)", "[2] X. Glorot, Y. Bengio, Y. W. Teh, M. Titterington. Understanding the difficulty of training deep feedforward neural networks. (http://proceedings.mlr.press/v9/glorot10a.html)"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 182, "sentences": ["Thank you for your valuable feedback!", "After carefully reading it, we plan to modify the manuscript as discussed below (the planned changes are shown by **", "\u2026 *", "*", ")", ".", "We would appreciate it if you could let us know whether the proposed changes address your concerns, or whether we have misinterpreted your comments.", "\u2022", "You have raised an interesting question about how the accuracy of the GAN impacts the accuracy of the proposed method.", "In order to address this, we have developed analytical estimates for the error in the point estimates computed using the proposed approach and show that these are intimately tied to error in computing the point estimates for the prior using the GAN.", "We have also demonstrated that as the generator and the discriminator of the GAN become more expressive this error tends to zero, and the exact point estimates, for both the prior and the posterior, are recovered. ** In the revised manuscript, we will include this mathematical analysis in the Appendix and refer to it in the main text. **", "\u2022", "We note that our method of inferring the desired image from the measured image is an unsupervised method; for training we only need a set of desired images  to construct the prior.", "We are not aware of any other unsupervised learning approach for solving these types of problems with quantified uncertainty.", "In that regard, the calculation of point-wise variance (our metric of uncertainty) is possible only using our approach, and therefore a direct comparison is not possible, since other supervised methods (explained below) cannot work in this setting where only set of desired images are available. ** We will clarify this unique aspect in the revised version of the manuscript. **", "\u2022\tThere has been some work on computing the uncertainty in an inferred image within a supervised learning framework where pairs of measured and desired images are used for training the network [1, 2].", "In these articles the authors have used methods like Bayesian dropout and variational autoencoder to compute uncertainty in the inferred images.", "*", "* We will refer to these works in the revised version to better orient reader.", "*", "*", "[1]. A. Kendall and Y. Gal, \u201cWhat Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?", "\u201d, NIPS (2017).", "[2]. Kohl, S.A., Romera-Paredes, B., Meyer, C., Fauw, J.D., Ledsam, J.R., Maier-Hein, K.H., Eslami, S.M., Rezende, D.J., & Ronneberger, O. \u201cA Probabilistic U-Net for Segmentation of Ambiguous Images\u201d, NeurIPS (2018).", "Thank you for your valuable feedback! After carefully reviewing it, we have modified manuscript as discussed below (description of changes is enclosed within ** ... **).", "\"Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.", "In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.", "Hence, the effectiveness and advantage of the proposed methods are not clear.\"", "We have addressed this by responding to the specific questions below.", "\"- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc. \"", "We are not aware of any other methods for computing uncertainty in recovered images that have been used to drive an active learning task in image inpainting.", "While methods based on dropout (\\cite{Kendall2017a, Kendall2019}) or variational inference (\\cite{Kohl2018a}) could be extended to accomplish this, this has not been done thus far.", "**We have added this comment in Section 3.1**", "Another big difference between the methods mentioned above and our approach is that while they require image pairs (true and corrupted images) for training, our approach only requires uncorrupted images.", "Thus while our algorithm relies on unsupervised learning, the other algorithms fall under the category of supervised learning.", "*", "*We have also clarified this within the \"Our Contributions\" Section**", "\"- How does the estimation accuracy of GAN relate to the estimation accuracy of the proposed method? Showing a quantitative description would be nice.\"", "We thank the reviewer for raising this important question.", "*", "*We have addressed it thoroughly in Appendix A. We have provided a proof that demonstrates the weak convergence of the posterior density calculated using our method to the true posterior density as the number of weights in the discriminator and generator components of the GAN is increased.**"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_none", "rebuttal_structuring", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_none", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_none", "rebuttal_by-cr", "rebuttal_none", "rebuttal_none", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_none", "rebuttal_done", "rebuttal_structuring", "rebuttal_social", "rebuttal_none", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 183, "sentences": ["We think your suggestions are very meaningful. We respond to them one by one:", "1. We will explain anti-aliasing in our updated paper.", "Roughly, anti-aliasing is helpful for signal reconstruction.", "However, we can\u2019t provide a strict treatment of how anti-aliasing relates to classification.", "But we have intuitions: first, we believe reconstruction relates to classification (see our next response); second, frequency components are orthogonal.", "Aliasing means different components are mixed again.", "This may mislead the next layers for processing.", "2. To our knowledge, researchers haven\u2019t fully understood the whole process of image classification until now.", "Thus, we can\u2019t provide a strict treatment of how reconstruction optimality relates to classification optimality.", "But we have intuitions and empirical evidence of their relation: convolution layers are used to transform a signal which makes it easier to be classified.", "So if we accept that the feature extracted by previous convolution layers is useful, then it is best to keep it as much as possible for the current pooling layer.", "In this way, it is reasonable to assume that reconstruction optimality is consistent with classification optimality.", "On the other hand, it is difficult to directly define classification optimality for an intermediate layer.", "Moreover, several works, such as [1] have shown that using self reconstruction loss as an auxiliary is helpful for classification.", "3.", "Please refer to our general response.", "With suitable settings, the shift consistency of F-pooling is much better.", "[1] Semi-Supervised Learning with Ladder Networks, NIPS2015"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 184, "sentences": ["We sincerely appreciate your constructive comments.", "We respond to your main concerns below:", "1. The Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?", "- To demonstrate that strong generalization performance of Meta-Dropout is not the effect of using larger number of model parameters, we doubled the number of channels for the base model and report its performances (MAML(x2)).", "Models\t\t   #param.\tOmni-1shot\tOmni-5shot\tmimg-1shot\tmimg-5shot", "MAML\t\t   x1\t        \t95.23+-0.17\t98.38+-0.07\t49.58+-0.65\t64.55+-0.52", "MAML(x2)", "x4", "94.96+-0.16\t98.36+-0.08", "48.19+-0.64", "65.84+-0.52", "Meta-SGD         x2", "96.16+-0.14", "98.54+-0.07", "48.30+-0.64", "65.55+-0.56", "Meta-dropout  x2", "96.63+-0.13\t98.73+-0.06", "51.93+-0.67", "67.42+-0.52", "The number of parameters of MAML(chx2) is four times of that of MAML, while Meta-dropout is only doubled.", "Nonetheless, MAML(chx2) does not improve on MAML, demonstrating that the effectiveness of meta-dropout does not simply come from using larger number of parameters.", "Meta-SGD also doubles the number of parameters in the base MAML model, but is significantly outperformed by Meta-dropout.", "We want to emphasize that Deterministic meta-dropout is also one of our models, and that its good performance does not hurt our claim on the effectiveness of the multiplicative noise.", "This is because meta-dropout consists of two parts: meta-learned deterministic multiplicative perturbation and random noise.", "Thus the deterministic meta-dropout still \u201clearns to perturb\u201d, although not random, and is actually a core component of meta-dropout (See Table 3 in the revision).", "Please also see our response to the Reviewer #3, comment #4."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 185, "sentences": ["Thank you for your detailed review.", "On releasing a structured (parsed) form of the dataset: we agree that examining performance on structured input is a very useful exploration direction, that can give insight into what effect parsing has on ease of training.", "We feel, however, that there\u2019s no single canonical choice for the structure that may be suitable for all types of networks (e.g., tree networks, graph networks, etc), or different levels of structure that aid the network to different amounts, from completely unstructured to tree-like structures that essentially determine the required order of calculation.", "For example, in the question type of \u201cmultiple function composition\u201d, one could have a structure that lists the functions, and also the desired composition order; or one could actually have a tree structure with the functions already embedded in the correct composition order (which we suspect would be quite easy to learn models on).", "In lieu of this, we hope the released dataset source code will allow researchers to easily tailor the dataset to their specific problems and models.", "We have rewritten the section describing the neural models, with clearer terminology, and the differences between the different models made much more explicit.", "Thank you for pointing this out, and please let us know if any parts are still unclear.", "The \u201cattentional LSTM\u201d model is just the standard encoder/decoder+attention architecture prevalent in neural machine translation as introduced in \u201cNeural machine translation by jointly learning to align and translate\u201d (Bahdanau et al).", "However, we confusingly used the terms \u201cparser\u201d instead of \u201cencoder\u201d, and we have fixed the description.", "On running the decoding LSTM for a few steps before outputting the answer: we found that it was one of the few (relatively simple) architectural changes to the standard recurrent encoder/decoder setup that significantly helped performance (thus the performance on the standard architecture can be taken to be slightly worse than the numbers reported in the paper for the architecture with \u201cthinking steps\u201d), but we also realize that it is not a widespread architectural change. (Possibly the need for this is less in standard machine translation tasks.) Since your review, we have also ran experiments using the published architecture introduced in \u201cAdaptive Computation Time for Recurrent Neural Networks\u201d (Graves).", "This architecture has an adaptive number of \u201cthinking\u201d steps at every timestep dependent on the input, learnt via gradient descent.", "More specifically we investigated the use of this for both the recurrent encoder and decoder (replacing the single fixed number of \u201cthinking\u201d steps at the start of the decoder).", "After some tuning, its test performance was still around 3% worse than the same architecture without adaptive computation time.", "We\u2019ve updated the paper to mention this.", "Please refer to the updated PDF of the paper to see these changes.", "We hope that you will agree that, with your kind feedback, the changes above strengthen the paper's claims and clarity, and that you are willing to reconsider your assessment on these grounds."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_social", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 186, "sentences": ["Thank you very much for your encouraging review.", "> I read the paper and understand it, for the most part.", "The idea is to interpret some regularization techniques as a form of noisy bottleneck, where the mutual information between learned parameters and the data is limited through the injection of noise.", "While the paper is a pleasant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation.", "Perhaps other referee will have a clearer opinion.", "The main contribution of our paper is indeed to establish a connection between variational inference and regularization by observing that Gaussian mean field introduces an upper bound on the mutual information between data and model parameters.", "Reinterpreting mean field as point estimation in a noisy model allows us to quantify observed regularizing effects.", "We show links to existing regularization strategies and validate the usefulness for regularization in targeted experiments.", "While the focus of our present work lies on establishing links between existing directions of research, we believe that our information-theoretic perspective on regularization opens up plenty of avenues for future work, both in supervised and unsupervised learning.", "For example, we are interested in improving extraction of unsupervised representations by controlling the amount of extracted information.", "In particular, we aim to mitigate latent collapse, a problem reported for example in language generation [1] and autoregressive image generation [2], which is currently mitigated with ad-hoc strategies such as KL annealing.", "Intuitively, if all information can be stored in the model itself, there is little incentive to use a per-sample latent.", "This is also known as the information preference problem, as briefly discussed at the end of section 2.1.", "Therefore, limiting mutual information of the data with the model might offer a robust mitigation strategy.", "Additionally, we believe that the approach can lead to improved representations through disentanglement, as done by beta-VAE [3].", "Our formal connection to beta-VAE derived in Appendix C offers a promising information-theoretic perspective on their empirical results.", "More generally, we want to explore non-MAP inference on noise-injected models as this would allow for using highly expressive variational distributions while enjoying the information-theoretic guarantees of simpler approximate distributions, as motivated in section 3.3.", "Since these directions are rather orthogonal, we think that sharing our theoretical framework with the community in an independent piece of work is the most effective way of communicating our ideas.", "> I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)", "Reference priors are opposite to our work in the sense that they maximize the amount of information data provides about the parameters, while we aim to find models to limit it.", "Also, see [4] for the relation of Fisher information to generalization.", "References", "[1] Bowman, S. R., Vilnis, L., Vinyals, O., Dai, A. M., Jozefowicz, R. & Bengio, S. (2015). Generating sentences from a continuous space.", "arXiv preprint arXiv:1511.06349.", "[2] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D. & Courville, A. (2016). Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013.", "[3] Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G. & Lerchner, A. (2018). Understanding disentangling in beta-VAE.", "arXiv preprint arXiv:1804.03599.", "[4] Ly, A., Marsman, M., Verhagen, J., Grasman, R. P. & Wagenmakers, E. J. (2017). A tutorial on Fisher information.", "Journal of Mathematical Psychology, 80, 40-55, page 30"], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_accept-praise", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 187, "sentences": ["Thank you for the review and careful reading of our paper! We\u2019re glad that you found it of interest. On revision we will fix the typos that you identified.", "Regarding the first point, your intuition is exactly correct and a slightly simpler discussion of this phenomenon can be found in [1].", "When the network is deep enough that the covariance matrix has reached its fixed point, the distribution of the outputs of the network will be independent of the inputs.", "At this point the network becomes untrainable.", "To reconcile this with the commonsense intuition that \u201cdeeper is better\u201d, our answer is twofold.", "1) As in [1] and [2] it is often possible to find configurations or architectural modifications where the covariance matrix doesn\u2019t approach its fixed point over depths often considered in machine learning.", "When this is the case one can safely increase the depth without sacrificing accuracy.", "2) It seems that the role of depth in performance is more subtle than standard intuition would dictate.", "For example, in [3] note that although the authors were able to train a 10k hidden layer network, they did not observe any improvement in accuracy.", "In the next version of the manuscript (both in response to your review and that of referee 1) we will add a more intuitive discussion of these results which we agree are somewhat technical.", "[1] S. S. Schoenholz, J. Gilmer, S. Ganguli, J. Sohl-Dickstein.", "Deep Information Propagation (https://arxiv.org/abs/1611.01232)", "[2] G. Yang and S. S. Schoenholz. Mean Field Residual Networks (https://arxiv.org/abs/1712.08969)", "[3] L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, J. Pennington. Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks (https://arxiv.org/abs/1806.05393)"], "labels": ["rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 188, "sentences": ["Thank you very much for your review! Please see our responses below regarding your comments:", "\u201cEvaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (D\u00e9j\u00e0Vu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)\u201d", "Response: We investigated this question by running jscpd (a popular code duplication detection tool, https://github.com/kucherenko/jscpd ) on our entire data set and found that only 2.7% code is duplicated.", "Furthermore, most of these duplicates are intra-project.", "Thus, we believe that code duplication is not a severe problem in our dataset, and we will include more details about this result in any future revision.", "========", "\u201cThe hyperparameter selection regime (and the experiments used to find them) is not described\u201d", "Response: We selected hyperparameters in a standard way by tuning on a validation set as we were developing our model.", "We\u2019ll include more details about hyperparameters and hyperparameter selection in any future revision."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 189, "sentences": ["Thanks for your valuable comments and feedback.", "R: \"The authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks? \"", "A: Yes but not only.", "We propose to go beyond the classical markov hypothesis of cascade models that states that any infected node owns the same transmission probabilities whatever from whom comes the propagated content.", "We indeed do this by replacing the traditional parametric functions with  deep neural networks, which enables to consider recurrent latent states for infected nodes.", "This allows us to embed the past in node states and hence to output different future diffusion distributions regarding the past trajectory of the propagated content, which is our main contribution (a cascade model with neural network was already proposed for instance in (bourigault et al., 2016) but without past inclusion).", "While existing works on cascade models learn parameters by inferring the direct infector of every infected node (i.e., estimating $P(I_i|D_{\\leq i})$), we need to infer the whole past trajectory to compute node states (i.e., considering $P(I_i|D,I_{<i})$), which is greatly more difficult but the proposed learning approach allowed us to efficiently deal with it.", "R: \"The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works. This could have made the paper much stronger.\"", "A: To give more clues about the good behavior of the algorithm, we added results about the accuracy of the sampled trajectories on the artificial datasets (for which we have the ground truth on who infected whom).", "We report the rate of good infector choices (i.e., the rate of I_i that equal the ground truth) for our approach and the others.", "Results show that our approach actually performs better infector choices than CTIC which does not consider the history of the diffusion in its infection probabilities.", "The use of our recurrent architecture helps the process to distinguish some different diffusion contexts from the past.", "We also added a second artificial dataset to further analyze the behavior of the approaches.", "R: \"It was nice that the paper iterated and reviewed the possible inference and learning ways. There is one more way. Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.\"", "A: Thanks for the proposal and the reference that we added in the paper.", "The full computation of the posterior distributions could indeed be avoided by using an importance sampling MCMC procedure with auxiliary variables", "(such as done in [1] in the context of diffusion source detection), but in our context we think that the increased computation efficiency would be at the cost of a very higher variance in the learning process, due to the strong intrication of latent and observed variables.", "In [1], the problem is easier: they do not have to perform optimization on the diffusion parameters (since relying on a diffusion model learned a priori), the problem is to sample hidden infection times to estimate likelihoods and then identifying the most probable source of diffusion.", "R: \"The paper can benefit from a proofreading.\"", "A: Thanks, we indeed corrected serveral typos like this in the new version of the paper."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 190, "sentences": ["We appreciate your constructive feedback.", "Specifically, your comments about our derivations greatly help us to improve the quality of our paper.", "We hope you to also consider our notable experimental results as well.", "(Bounds of KL divergence) Thank you for this good comment.", "We claimed that the Equation 1 can be maximized indirectly by maximizing Equation 2 which is a lower bound of Equation 1.", "If we understand your primary concern correctly, the concern comes from the bound of KL divergence in Equation 5.", "To prove correctness of our formulation, we can rewrite the pointed term in Equation 5 by using simple bayes rule as follows:", "$$\\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\ \\mathrm{log}\\ \\frac{\\hat{q}\\mathrm{(z|c)}}{\\hat{p}\\mathrm{(c|z)}} = \\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\bigg(\\mathrm{log}\\ \\frac{\\hat{q}\\mathrm{(z|c)}}{\\hat{p}\\mathrm{(z|c)}} + \\mathrm{log}\\ \\frac{\\hat{p}\\mathrm{(z)}}{\\hat{p}\\mathrm{(c)}} \\bigg)$$", "Because the $\\hat{p}\\mathrm{(c)}$ is constant, and $\\hat{p}\\mathrm{(z)}$ is not included in our optimization, we just optimize $\\displaystyle\\sum_{\\mathrm{z}}\\hat{q}\\mathrm{(z|c)}\\mathrm{log}[\\hat{q}\\mathrm{(z|c)} / \\hat{p}\\mathrm{(z|c)}]$. Since the $\\hat{q}\\mathrm{(z|c)}$ and $\\hat{p}\\mathrm{(z|c)}$ are both normalized distributions, the $D_{KL}[\\hat{q}\\mathrm{(z|c)} || \\hat{p}\\mathrm{(z|c)}]$ is always positive.", "Then, we can conclude that Equation 2 becomes the lower bound for Equation 1.", "(lambda)", "Actually, Equation 7 consists of three terms.", "Since only the third term is proposed additional regularization, we applied weighting parameter lambda to the third term only.", "(Difference with CDVAE) To clarify the difference our DiVA with CDVAE, we write derivations for both models here.", "CDVAE: $\\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ p_{\\theta '}(\\mathrm{x|z)}] - D_{KL}[q_{\\theta}\\mathrm{(z|x)} || p\\mathrm{(z)]} + \\lambda \\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\hat{p}_{\\phi '}\\mathrm{(c|z)}]$", "DiVA: $\\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ p_{\\theta '}(\\mathrm{x|z)}] - D_{KL}[q_{\\theta}\\mathrm{(z|x)} || \\hat{q}_{\\phi}\\mathrm{(z|c)]} + \\lambda \\mathbb{E}_{q_{\\theta}\\mathrm{(z|x)}}[\\mathrm{log}\\ \\hat{p}_{\\phi '}\\mathrm{(c|z)}]$", "As we discussed in section 4.1, below the table for Algorithm 1, the key difference is that we consider class-conditional Gaussian distributions as priors for variational posteriors.", "Since CDVAE assumes the prior as unit Gaussian for all classes and optimizes classification loss simultaneously with the KL divergence, the latent space does not follow the prior exactly.", "As a result, CDVAE sometimes generates ambiguous samples (Figure 2 (c)).", "Interestingly, RtF [1] also does not consider the class-conditional priors even though they consider a classifier integrated VAE similar to CDVAE.", "In contrast, we assume class-wise specific Gaussian for each class.", "As a result, we can stably generate more realistic samples than CDVAE.", "[Additional feedback]", "(dt in Algorithm 1) dt means the domain translation explained at section 5.", "(Figure 1)", "- We corrected the typo.", "- The 3d plot conceptually represents class-specific one mode Gaussians.", "- The classification loss has implicit dependency with input conditions by minimizing the KL divergence in Equation 2.", "(heavy classifier) A classifier such as resnet.", "We used this term to distinguish the additional classifier from our integrated encoder that has discriminative power.", "(Redundant weights)", "If we extend to a more complex dataset such as ImageNet, it will become highly redundant.", "Furthermore, if we consider fully-convolutional architecture (without fully-connected layers), redundancy becomes a serious problem.", "For example, a feature map that has shape of [W x H x dim] becomes [W x H x (dim + the number of classes)].", "In contrast, using discriminative conditional distributions can keep the dimension of the feature map as [W x H x dim] regardless of the number of classes.", "(Notations) Thank you for commenting this.", "We corrected the notations of section 3 to match with later sections.", "(Complexity of encoder) We intended that the encoder network can have enough both discriminative and generative power with a powerful architecture such as a deep residual network.", "[References]", "[1] van de Ven, Gido M., and Andreas S. Tolias. \"Generative replay with feedback connections as a general strategy for continual learning.\" arXiv preprint arXiv:1809.10635 (2018)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 191, "sentences": ["Dear reviewer:", "We appreciate your comments but it appears that there is some misunderstanding regarding our contribution in this work.", "Our work is for softmax inference speedup", "while Sparse-Gated MoE (MoE) was not designed to do so.", "It was designed to increase the model expressiveness.", "It cannot achieve speedup because each expert still contains full softmax space as we mentioned in the background section (page 2 line 21st) and method section (page 2 last 4th line).", "And since it is slower than the standard softmax by definition, we chose not to compare with it in the paper.", "Our algorithm addresses speed up in softmax inference.", "This is fundamentally different from Sparse-gated MoE. We divide the output space into multiple overlapped subsets.", "To find top-k predictions, we only search a few subsets.", "While in full softmax or MoE, the complexity is linear with output dimension.", "Therefore, we did not include a comparison with Sparsely-Gated MoE in our article and only compare with full softmax.", "Just for additional reference, we tested Sparsely-Gated MoE with different experts in PTB dataset; we compared the results to DS-Softmax.", "As expected, the Sparsely-Gated MoE does not achieve speedup in terms of softmax inference.", "_____________________________________________", "_", "Method | Top 1 | Top 5 |Top 10| FLOPs|", "DS-8       | 0.257 | 0.448 | 0.530 | 2.84x |", "MoE-8    | 0.258 | 0.448 | 0.530 |  1x", "|", "DS-16     | 0.258 | 0.450 | 0.529 | 5.13x |", "MoE-16  | 0.258 | 0.449 | 0.530 | 1x", "|", "DS-32     | 0.259 | 0.449 | 0.529 | 9.43x |", "MoE-32  | 0.259 | 0.450 | 0.531 | 1x", "|", "DS-64     | 0.258 | 0.450 | 0.529 |15.99x|", "MoE-64  | 0.260 | 0.451 | 0.531 | 1x", "|", "_____________________________________________", "_", "* FLOPs means FLOPs reduction (i.e. baseline's FLOPs / target method's FLOPs)."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_none", "rebuttal_reject-criticism", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 192, "sentences": ["We thank the reviewer for the reading and suggestions of our paper.", "Q1: The exact difference between the proposed method and the ES baseline is not as clear as it could be.", "We agree and apologize for the lack of clarity in some parts of our paper.", "We renamed all the models based on the original papers and their properties.", "We refer the reviewer to general response for further details of each baseline algorithms.", "We also improved clarity in the revised version.", "Q2: The second point is that the proposed approach seems to modify a few things from the ES baseline.", "We thank the reviewer for the insightful suggestion.", "In the latest version, to test the efficacy of each submodule of NGE, the baselines now include the algorithm with the inclusion of the pruning step, and the algorithms with AF and without AF using MLP.", "More specifically, the baselines are named:", "1. ESS-Sims", "It is the baseline algorithm without the use of AF, as use by (Sims, 1994), (Cheney, 2014) and (Taylor, 2017).", "2. ESS-Sims-AF", "The modern variant of ESS-Sims with the inclusion of AF.", "3. ESS-GM-UC", "The modern variant of ESS-Sims with the inclusion of AF and graph mutation with uncertainty (pruning).", "For this baseline, we included the pruning module on top of ESS-Sims-AF.", "Similar to the original baselines available, we performed a grid search of hyperparameters and plot the average performance of the best set of hyperparameters.", "|        NGE         | ESS-Sims  |  ESS-Sims-AF  | ESS-GM-UC | ESS-BodyShare |  RGS", "fish        |", "**70.21**", "|", "38.32      |", "51.24         |", "54.40", "|", "54.97         |", "20.96", "Walker  |", "**4157.9**  |", "1804.4    |", "2486.9        |", "2458.19     |", "2185.1       |", "1777.3", ".", "On the other hand, the use of AF can greatly affect the performance.", "The previous approach ESS-Sims can only get 38.32 / 1804 average final reward for fish and walker, respectively.", "The performance of walker is even very close to random graph search with no evolution.", "With the help of AF, the performance increases from 38.32 to 51.24 and 1804.4 to 2486.9, respectively."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_other", "rebuttal_done", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_other", "rebuttal_mitigate-criticism", "rebuttal_other", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_none", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 193, "sentences": ["We thank the reviewer for mentioning the benefits of the proposed model.", "We now clarify some of the reviewer\u2019s doubts:", "**QUESTION**", "BERT-based models in the low-resource case is not very surprising", "**ANSWER**", "While this result may not look surprising, to the best of our knowledge it was not addressed before for the specific case of BERT.", "In [1], the authors claim that a large pre-trained model can be very helpful in transfer learning scenarios, and they also suggest how to best fine-tune BERT.", "We followed their guidelines and included the results to provide convincing evidence that, in this extreme scenario, our model can perform better (even without the use of rationales).", "**QUESTION**", "Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW.", "**ANSWER**", "The reviewer is correct.", "Apart from the reasons mentioned in the paper, it is possible to observe (by manual inspection) that the tweets of the HATESPEECH dataset are very noisy, short and often similar in meaning.", "This clearly helps models based on n-gram features, as we have argued in our work.", "SPOUSE and MOVIEREVIEW are different in this sense.", "SPOUSE, which is where PARCUS performs very well, contains sentences of very different nature and context, which makes it very important to focus on specific concepts (hence the use of prototypes seems appropriate).", "MOVIEREVIEW, on the other hand, contains very long reviews that need \u201cfiltering\u201d to highlight the important concepts.", "This is another context in which PARCUS can be successfully applied, as training a complex model on few data points that contain \u201clengthy\u201d sentences can be a hard task to solve."], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 194, "sentences": ["Thank you for your review!", "> Pros:", "> 1.", "The method used a latent dynamics model, which avoids reconstruction of the future images during inference.", "> 2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.", "> 3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.", "This is an accurate summary.", "We would like to highlight two additional points.", "First, the improved performance is attributed to a novel actor-critic algorithm that uses analytic multi-step gradients of predicted state-values (not Q-values).", "Second, in addition to outperforming previous latent space planning methods, the proposed algorithm also outperforms the model-free D4PG algorithm, the previous state-of-the-art on this benchmark suite.", "> 1.", "The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.", "In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.", "However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).", "Dreamer is a novel algorithm that belongs to the family of actor critic methods.", "At a high level, previous approaches can be grouped into those using Reinforce gradients with V baselines (A3C, PPO, ACER) and those using deterministic or reparameterization gradients of learned Q functions (DDPG, SAC, MVE, STEVE).", "In comparison, Dreamer uses reparameterization gradients of V functions by backpropagating the value estimates through the latent dynamics.", "Specifically, while Reinforce estimators typically learn V functions, these are only used to reduce the variance of the gradient estimate rather than directly maximizing them with respect to the actor.", "Actor-critic algorithms that use analytic gradients of Q critics differ from Dreamer in two ways.", "First, they learn a Q function rather than just a V function.", "Second, the actor only maximizes the Q value predicted for the current time step rather than maximizing multi-step value estimates.", "While MVE and STEVE learn dynamics models (from proprioceptive inputs), the dynamics are not directly used to update the policy.", "Instead, they only serve for computing multi-step Q targets for learning the Q critic.", "Thus, no gradients are backpropagated through the dynamics model for learning the actor or critic.", "Please also see the comparison in the last paragraph of Section 3, which we will extend with the present discussion.", "> 2.", "Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.", "As summarized above, Dreamer differs from previous actor-critic algorithms not just by using latent dynamics but also by using analytic multi-step gradients of a V function rather than one-step gradients Q function.", "This renders Dreamer conceptually distinct from DDPG, SAC, MVE, and STEVE.", "We have run experiments with MVE in the latent space of the same dynamics model and tuned the learning rate for actor and Q function.", "We did not find an improvement over Dreamer (MVE worked worse across tasks) in these experiments, possibly because it only updates the Q function at the initial state of the imagination rollout.", "Note that with a model, Q values can be computed by combining the dynamics with a value function, so learning Q is not necessary anymore.", "Since using V in Dreamer outperforms the state-of-the-art D4PG agent and is simpler than the Q function in DDPG and MVE and substantially simpler than STEVE (ensemble of models) and SAC (two Q functions, one V function), we argue for this design choice.", "> 3. [...] It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.", "We have run these experiments and it prevented learning completely.", "Using gradients of the action or value models to shape the dynamics allows them to \"cheat\".", "Specifically, the actions maximize value estimates; using these to update the dynamics results in overly optimistic dynamics.", "The values maximize Bellman consistency; using these to update the dynamics can encourage collapse of the latent space.", "As a result, we suggest the perspective of viewing the dynamics as a fixed MDP during imagination training.", "We will add a discussion of this to the paper.", "If we addressed your concerns satisfactorily, we would be happy if you would consider updating your score."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 195, "sentences": ["Thank you for your encouraging comments.", "We agree with your suggestions and we will revise our paper accordingly.", "We will also comment on the gap between our analysis and AGZ in the introduction to make it clearer, and discuss potential future work (e.g., considering approximation errors due to MCTS and the value function) in the conclusion."], "labels": ["rebuttal_social", "rebuttal_by-cr", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0]}
{"abstract_id": 196, "sentences": ["Thank you for your comments and suggestions.", "We will address the issues you mentioned.", "1.\tWe have added more details about sampling strategy to section 3.1 in the new version, with mathematical definition and dimensionality explicitly described.", "2.", "We did not argue the computation cost of 3D kernels in section 3.2.", "Instead, we argued that 3D kernels usually are not large enough to cover the holistic video so that Max Pooling operations are applied in most 3D CNNs to enlarge the receptive field.", "Yet this causes the loss of detailed information.", "But indeed, in order to preserve the details and increase the receptive field, simply enlarge the 3D kernels to cover the holistic video will bring enormous computation cost.", "Considering a video of size UxTxHxW, where U is number of action units, and T,H,W means temporal length, height and width of each action unit.", "In order to model the interaction of 1st frame and the (kT+1)th frame, a 3D kernel of at least (kT+1) x k x k has to be applied, which brings linear increasing of computation cost.", "Yet with our 4D kernels, a simple k x k x k x k will cover the interaction from the 1st frame to the (kT+1)th frame, because 4D convolution can go beyond space and time, making the long range interaction possible.", "For parameters, 4D kernels are k times larger than 3D kernels.", "So in order to reduce the parameters, we apply k x k x 1 x 1 kernels in most experiments, as mentioned in section 3.2 and section 4.2.", "We also propose Residual 4D Blocks to ease the optimization and preserve short-term details.", "3.", "Yes, Mini-Kinetics and Kinetics contain videos about 10 seconds.", "For Something-Something-v1, they select one frame from every 12 frames so that the original video should be around 430 frames to 860 frames, which are of about half or one minute.", "We agree that these are still too short to be called videos.", "Yet here we call our method \u201cvideo-level\u201d mainly to stress that our V4D models the holistic duration instead of a certain part.", "Additionally, we evaluated our V4D for untrimmed video classification on ActivityNet v1.3, which contains videos of 5 to 10 minutes and typically large time lapses of the videos are not related with any activity of interest.", "The very competitive result is now reported in the appendix of the second version of paper.", "Hopefully our rebuttal could stress your concerns. If there are still any possible issues, please don\u2019t hesitate to tell us and we will response as soon as possible."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 197, "sentences": ["We thank for the reviewer for their comments on our work, and we share our responses below.", "1. Novelty: To the best of our knowledge, this is the first paper presenting a simple solution to generating useful auxiliary tasks in a self-supervised manner.", "The idea indeed was inspired by other works in auxiliary learning, but only to the extent that we also use auxiliary tasks to improve performance of a principal task.", "The method is not a heuristic; it is theoretically motivated by use of the double gradient, and inspired by the success of this in meta learning (e.g. MAML [1]).", "If the reviewer thinks our method is an incremental contribution or similar to previous algorithms, please list the specific references.", "2.", "The theoretical insight in this paper comes from the recent advancements in using a double gradient, such as in MAML [1], or understanding what makes a good auxiliary data sampler [2].", "The inner gradient is based on the standard auxiliary learning loss as proposed in other works, whereas the outer gradient uses this inner gradient to actually learn the auxiliary tasks.", "The use of an outer gradient for auxiliary learning is our key novelty, and has not been used in any works before.", "3. Feature distributions of training and meta-training data (target and auxiliary data in your language) are actually not identical.", "The \"learning to generalise\" success from our method is due to closing the *existing* distribution shift in these two datasets. If the distributions are identical, then we wouldn't have any improved generalisation from our method.", "4. Both CIFAR10 and CIFAR100 are the subsets from 80 million tiny images dataset [3].", "As described in the website and paper, all images are collected from the internet and partially labelled by humans, and thus indeed present a real-world setup rather than a synthetic setup.", "Further, we show that if a harder test set with a more variety exists (CIFAR10.1v6), out method could provide even better generalisation (Figure 4).", "Thus, we hope the reviewer could better explain why you think our algorithm could fail in real-world scenarios.", "[1] Finn et al. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ICML, 2017.", "[2] Zhang et al. Fine-Grained Visual Categorization using Meta-Learning Optimization with Sample Selection of Auxiliary Data, ECCV 2018.", "[3] http://people.csail.mit.edu/torralba/tinyimages/"], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_followup", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 198, "sentences": ["We thank the reviewer for the detailed comments.", "\u201c1.[...] without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct.", "Also, the claim in Fig. 1 that the transition from \u2018\u2019high capacity\u2019\u2019 to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand (*)\u201d", "We agree raw parameter count is not a fine estimate of the capacity of the network.", "However, an information-theoretic argument shows that an upper-bound of the capacity is the raw parameter count times the size of the representation (i.e. 32 bits for float32, this argument is close to that of [A]).", "Experimentally, we show that networks with no data-augmentation (figure 1 - purple curve) stop fitting perfectly when the parameters get within 1/10 of the quantity of information in the learning set, thus we think that raw parameter count is a good first-order approximation up to that factor.", "\u201c2. Sec. 3.3, [...] capacity alone cannot explain why VGG converges faster than Resnet-18 [...]\u201d", "We observe that the rate of memorization depends on the architecture and the optimization algorithm, but predicting or explaining this rate is beyond the scope of this paper.", "\u201c(*) 3. Scenario discussed in Sec. 4 seems somewhat impractical. [...] one might also need to figure out if it is neither train nor val\u201d", "In section 4, we do not train a classifier to distinguish between a training and a validation set.", "Rather, we use a readily-available classifier (trained for e.g. image recognition) for a completely different purpose than what it was trained for, i.e. to distinguish datasets of images (section 4.1) or detect if a set of images comes from a given set (section 4.2).", "Section 4.2 shows how to use the K-S test to detect leakage, but the same test could tell if the m-set comes from neither the train nor the validation sets.", "\u201c4. Fig. 3 (right): It is not clear why the fact that the classifier is able to predict which dataset the image \u2018m\u2019 corresponds to is useful or practical, as this seems to be a property of the set \u2018m\u2019 rather than the property of the trained classification model (f_\\theta). Please clarify. [...]\u201d", "Being able to tell from the classifier output (using e.g. the confidence) if a set of images comes from the training or the validation set is a good indicator of how much the network has memorized these images.", "In our opinion, the most important outcome of the experiment of section 4.1 (figure 3, right) is to determine how many samples are needed to reliably discriminate the training set from the validation set (this corresponds to the solid curves), which is related to how much the model has memorized images from the training set.", "\u201c6. Does [section 5] use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?\u201d", "The baseline approaches make use of the loss of the model (which is not the same as the confidence).", "The proposed approach uses the lower layers of the original model, and upper layers learnt on a separate, public set (this is the \u201cpartial-layers\u201d setting).", "Table 3 reports the results of the Bayes method on top of this network with upper layers retrained, as the MAT usually gives similar results on this task (for instance, Table 4 reports 60.8% performance with Softmax + Flip, Crop on Resnet101 for the Bayes method, and the MAT gets 61.14%).", "\u201c7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images\u2019\u2019 -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen\u2019\u2019 images which it labels as the negative class, thus the negative class is also seen by the memorization model.", "(*)", "\u201d", "We feed our model an equal number of positives and negatives (chosen randomly) at each epoch.", "For n < 10K, after 300 epochs the model has seen at most 3M negatives out of 15M, and yet still generalizes to the unseen negatives.", "\u201c1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization\u2019\u2019.", "In addition, the paper would also need to show that such a model does not generalize to a validation set of images. [...]\u201d", "With the downstream application of sections 4 and 5, we are interested in \u201cmemorization\u201d in the sense of any classifier that can tell apart images marked as \u201cpositives\u201d from images marked as \u201cnegatives\u201d.", "This notion is somewhat different from", "memorization as defined in other papers, where it is related to having a good training accuracy and a a validation accuracy close to random guessing.", "With the setup used in section 3, there is no good notion of validation: our model is expected to predict \u201c0\u201d on held-out data.", "\u201c2. Figure 3: [the term CDF] is confusing\u201d", "We will update the caption to make it less ambiguous."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_future", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 199, "sentences": ["Thank you for your comments.", "We have performed additional experiments in order to address them, particularly training and evaluating HOF as in [1].", "Q1: Evaluation as in [1]", "In accordance with these recommendations in [1], we have trained HOF on the dataset provided by the authors of [1] and evaluated it according to the F1 metric that is proposed.", "We find that HOF provides competitive performance to existing methods, giving the highest average F1 score of all methods in [1].", "We will report quantitative results on this benchmark in a revised PDF submitted before the end of the discussion period.", "Q2: Input shape", "We have performed new comparison experiments to address this question.", "Varying the input shape does affect performance; sampling the surface of the 3d sphere gives worse performance than sampling the interior of the sphere (1.369 average chamfer distance for surface of sphere versus 1.247 for interior).", "In addition, we find that sampling the interior of the 4D sphere, rather than the 3D sphere gives a fairly significant improvement in performance (1.195 average chamfer distance for 4D sphere vs 1.247 for 3D).", "Since we are learning an arbitrary mapping (rather than, for example, a projection to a manifold), the mapping domain does not explicitly induce any topological constraints such as genus.", "If our mapping didn't have to be continuous, it wouldn't matter what shape we sampled.", "However, because we use neural networks with continuous activation functions (relus) to represent the mapping, it must be continuous.", "And because the experiments above indicate that the mapping domain affects the quality of the reconstructions, it is possible that the sampling domain imposes topological constraints on the set of objects.", "It's difficult to say what the \"best\" shape to sample from is; however, in future work, we would like to investigate strategies for learning the best input shape to sample from.", "Q3: Sampling the input shape", "Our input 3D points are sampled from the interior of the unit sphere, rather than the surface (we have clarified this in the revised manuscript).", "Samples are drawn uniformly at random from within the sphere in order to avoid overfitting to a particular gridding structure.", "Q4: Value of $c$ for composition networks", "In order to apply composition, the reviewer is correct that c must equal 3 (this is the case in our composition experiments).", "However, if we are not composing the reconstruction function, c could be anything greater than zero.", "For example, c=4 in our ablation experiment in which we sample input points from the 4D sphere rather than the 3D sphere.", "Q5: Clarifying values of $k$", "$k=1$", "for both HOF models in Table 1", ".", "We have edited the manuscript to clarify this point.", "The small difference in results between HOF-1 in Table 1 and HOF-1 ($k=1$) in Table 4 as well as HOF-3 in Table 1 and HOF-3 ($k=1$) in Table 4 is a matter of different initialization on a later training run.", "We have updated these tables so that the numbers are computed from the same model, rather than separate training runs.", "If we keep the number of compositions fixed while training and test with a larger value of k, we observe that the performance degrades significantly.", "On the other hand, when we use a varying number of compositions (1,2,...,k) at training time, we find that the results do generalize to higher values of k. After several additional compositions (such as k+3, k+4) however, the results start worsening similar to the trend in the fixed k evaluation.", "Finally, we have updated our pdf to reflect the additional related work and stylistic suggestion that you have brought to our attention.", "Thank you again for your feedback, and please let us know if you have any additional questions or concerns.", "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 200, "sentences": ["Thank you for your comments. Please find below our response to your questions and concerns.", "1) Technical contributions", "We are glad that the reviewer agrees that we are tackling a long standing and important problem and acknowledge the fact that neither the definition of constrained MDPs nor the application of Lagrangian relaxation to solve these problems is novel by itself.", "We should have stated our exact technical contributions more clearly and have adapted the paper to do so.", "For completeness we will list these below:", "a) We introduce pointwise, per-state constraints to learn more consistent behavior compared a single global constraint, and regress the resulting state-dependent Lagrangian multipliers using a neural network to exploit generalization across similar states.", "b) Instead of recombining the reward and cost directly on the environment side and learning a single value estimate, we train a critic network to output both return and penalty value estimates as well as the Lagrangian multipliers themselves, effectively providing more structure to the critic.", "We only combine the different terms appropriately for the actor update.", "c) We show that we can train a single, bound-conditional policy that can optimize penalty across a range of bounds and can be used to dynamically trade off reward and penalty.", "2) Comparison with the original benchmark reward", "We have extended the results on Cartpole to include the original reward as defined in the DM Control Suite (incl. bonus for low control).", "We found that compared to the original setting, our method is able to reduce the average control norm by over 50% across the entire episode, and by over 80% after the swingup phase, without significant reduction in the average return as measured without control bonus.", "3) Claims about bang-bang control in continuous RL", "The reviewer is right in that the claim of RL often leading to bang-bang control is too strongly worded.", "This is only the case when the objective function is not well-designed and one is naively optimizing for success only.", "Designing a proper objective function is however often not trivial and more of an art, requiring several iterations to achieve the desired behavior.", "This work tries to remove some of the complexities in designing such a function.", "4) State-dependent lower bound", "Defining a state-dependent bound is indeed not trivial and requires knowledge of what is feasible in the system, and as such we leave this up to future work.", "In this paper we have made the approximation that the state distribution is stationary and the discount is large enough to assume that the value is more or less constant.", "While this holds for locomotion tasks, this does not apply in e.g. the swingup phase of the cartpole task and as a result the penalty is completely ignored during this phase."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_future", "rebuttal_future", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 201, "sentences": ["Thank you very much for the constructive review.", "Summary of our response", "-------------------------------------", "We are certain that the data processing inequality is used correctly.", "As you stated, the DPI implies for any Markov chain X -> Y -> Z that I(X,Y) >= I(X,Z).", "Unlike suggested in the review, our model is defined in the form \\theta -> \\tilde{\\theta} -> D, as shown in Figure 1a.", "Following your feedback, we updated section 2.1 and 2.3 for more clarity.", "Detailed response", "-------------------------------------", "We interleave parts of the review with our detailed response for ease of reading.", "> [...]", "the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.", "As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z).", "However, based on the definition of \\theta and \\tilde{\\theta} given in the first sentence of section 2.3, the relation between \\theta, \\tilde{\\theta} and D should be: D <- \\theta -> \\tilde{\\theta} (if it is a generative model) or D -> \\theta -> \\tilde{\\theta} (if a discriminative model).", "Response: We are interested in limiting the mutual information I(\\theta, D) between our learned parameters \\theta and the dataset D.", "However, this is hard to calculate for typical deep models.", "Therefore we introduce a model that forms a Markov chain \\theta -> \\tilde{\\theta} -> D, as shown in Figure 1a.", "Hereby, \\tilde{\\theta} is a noisy version of the parameters \\theta.", "Crucially, the data D is defined to be dependent only on the noise-corrupted version \\tilde{\\theta}. By choosing a convenient noise process and prior for \\theta we can easily control I(\\tilde{\\theta}, \\theta).", "This gives us an upper bound on the mutual information I(D, \\theta) between data and parameters, according to the DPI.", "We updated section 2.3 to reflect this more clearly.", "> I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me.", "For example", "- the sentence under eq. (2)", "- the sentence \"Because the identity of the datapoint can never be learned by ...\" What is the identity of a data point?", "It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods.", "Somehow, those connections are not clear to me.", "Response: The aim of section 2.1 is to motivate limiting mutual information for the purpose of generalization.", "We link generalization problems reported in the literature to the introduced information measure.", "The information necessary to identify or distinguish between training samples is quantified by the empirical entropy, and we called it the identity of the samples.", "We updated the section to address all of your feedback."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 202, "sentences": ["Thank you for your helpful comments.", "We have improved the writing to incorporate your feedback.", "We have also performed more experiments to compare APO to manual learning rate schedules.", "Q: Please explain more how gradients w.r.t hyper-parameters are computed.", "We implemented custom versions of the optimizers we consider (SGD, RMSprop, and K-FAC) that treat the optimization hyperparameters as variables in the computation graph for an optimization step.", "We then use automatic differentiation to compute the gradient of the meta-objective with respect to the hyperparameters (e.g., the learning rate).", "Q: Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?", "Each meta-optimization step requires approximately the same amount of computation as a parameter update for the model.", "By using the default meta learning rate suggested in our updated paper, we can amortize the meta-optimization by performing 1 meta-update for every K steps of the base optimization.", "We found that K=10 works well across our settings, while reducing the computational requirements of APO to just a small fraction more than the original training procedure.", "We have added a discussion of our meta-optimization setup and the efficiency of APO in Section 5 of the updated paper.", "Q: No need to write so much decorated bounds in section 3.", "The convergence analysis is on Z, not on parameters x and hyper-parameters theta.", "So, bounds here cannot be used to explain empirical observations in Section 5.", "The convergence of the network output Z directly indicates the rate of decrease of the loss function, which is exactly what we observe in practice.", "Although the assumption of a global optimization oracle is not realistic, we believe our theoretical justification provides insight into why the method works.", "One important takeaway from the theoretical analysis is that running gradient descent on output space can potentially accelerate the optimization (since the convergence bounds have better constants).", "This directly motivates the regularization term in our meta objective to be defined as the discrepancy of network outputs instead of the network parameters, which is essential to our technique.", "Q: Could the authors compare with changing step-size?", "Thank you for the suggestion.", "We have added comparisons with custom learning rate schedules for CIFAR-10 and CIFAR-100.", "We updated our results for CIFAR-10/100 using a larger network, ResNet34, instead of the VGG11 model used in the previous version, and we used a manual learning rate decay schedule where we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training.", "We found that APO is competitive with the custom schedule, achieving similar training loss and test accuracy.", "We provide results in our response to all reviewers at the top.", "Q: How to tune lambda?", "Tuning a good lambda v.s. tuning a good step-size, which one costs more?", "We tune lambda by performing a grid search over the range {1e-1, 1e-2, 1e-3, 1e-4, 1e-5}. Because each lambda value gives rise to a learning rate schedule, tuning lambda yields significantly more value than tuning a fixed learning rate.", "Instead of trying to come up with a custom learning rate schedule, which would require deciding how frequently to decay the learning rate, and by what factor it should be decayed, all one needs to do is perform a grid search over a fixed set of lambdas to find an automated schedule that is competitive with hand-designed schedules (which are the result of years of accumulated experience in the field)."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_other", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 203, "sentences": ["(1) Multimodal setting:", "We apologize for not describing experimental settings clearly.", "In general, we believe multi-modal data is more general than simply image-text or video-text pair.", "By unifying tabular data also as multi-modal data (with each attribute as one modality), we show that VASE provides us a principled way for imputation and is capable of generalizing to more data families.", "We update additional multimodal dataset experiments in the point (3) below.", "(2) Prediction and Representation learning:", "We consider conducting these experiments during the rebuttal but none of the paper's code has been released by the authors.", "We agree deep latent variable models explicitly model the data distribution and provide a natural way for representation learning, but in our paper we evaluate the model from the perspective of imputation and generation.", "(3) Additional experiments:", "We updated additional imputation experiments on multimodal datasets (see in Appendix C.5) : CMU-MOSI/ICT-MMMO (Tsai et al. 2019), FashionMNIST/MNIST (Wu et al. 2018).", "Each dataset contains two or three modalities.", "VSAE outperforms other baselines on multimodal datasets under partially-observed setting.", "(4) Require mask during training:", "In our experiments, the binary mask is always fully-observed as is the nature of partially-observed data.", "A mask simply indicates which  modalities are observed and which are not.", "We agree that it is very interesting to design a model with partially-observed or even unobserved mask.", "However, it is beyond the scope of this work and we will consider it in future work.", "[1] Wu et al. Multimodal Generative Models for Scalable Weakly-Supervised Learning, NeurIPS 2018.", "[2] Tsai et al. Learning Factorized Multimodal Representation, ICLR 2019."], "labels": ["rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_reject-request", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 204, "sentences": ["Thank you for your thoughtful and helpful comments.", "Following the suggestions, we added additional results for the associative recall task for many network variants.", "We also report mean and variance of losses for different seeds.", "This shows that masking improves performance on this task especially when combined with improved de-allocation, while sharpness enhancements negatively affect performance in this case.", "From the variance plots it can be seen that some seeds of DNC-M and DNC-MD converge significantly faster than plain DNC.", "In our experimental section, we added requested references to methods performing better on bAbI, and point out that our goal is not to beat SOTA on bAbI, but to exhibit and overcome drawbacks of DNC.", "Comparison to Sparse DNC is an interesting idea, and we are currently running experiments in this direction. We intend to make the results available in the near future.", "We are unable to provide a fair comparison for the lowest bAbi scores, having reported 8 seeds compared to the 20 seeds reported by Graves et al. Indeed, the high variance of DNC (Table 1) suggests that it may benefit a lot from exploring additional seeds.", "We incorporated all of the smaller notes, including a comparison to the original DNC equations in Appendix A."], "labels": ["rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_done", "rebuttal_future", "rebuttal_mitigate-criticism", "rebuttal_summary"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 205, "sentences": ["Thank you for your thoughtful review.", "We have been hard at work to perform additional experiments to compare with other state of the art methods on a broader dataset.", "We summarize the changes here and will upload a revised the manuscript with complete quantitative evaluations before the revision deadline.", "Q1: More extensive evaluations", "In response to your comments, we have trained and tested our method on the dataset provided in [1], using the F1 score rather than Chamfer Distance in accordance with the recommendations in that work.", "This dataset contains more than 4 times as many classes as our original dataset.", "We find that HOF is competitive with the performance of various state of the art methods reported in [1], showing the highest average F1 score out of all methods compared in [1].", "See Section 4.1.2 for added discussion, and the Appendix Sections A7 and A8 for complete class performance breakdowns for both our original experiments as well as the new comparison with [1].", "We hope this extended comparison provides a more convincing experimental evaluation of HOF.", "Q2: Technical description and justification", "In the paper, we make the observation that codeword based approaches are equivalent to learning the biases of a fixed network, whereas the fast-weights-based HOF approach learns all of the weights.", "Therefore, we conclude that HOF is mathematically at least as general as codeword based architectures.", "We further show, with experiments, that the coding provided by HOF is more efficient than codeword-based approaches in terms of number of parameters in the decoder.", "There is similar evidence in the literature which suggests that fast-weights based approaches can be more efficient than static networks.", "However at this point, similar to our paper, the evidence is empirical and a theoretical justification of this phenomenon is missing.", "In response to your comments, in our concluding remarks, we mention this lack of theoretical analysis and note it as an important direction for future research", "Q3: Architecture of encoder/mapping network", "In addition to experiments on a new dataset, we have performed new evaluations of variants of HOF on our original dataset to demonstrate that HOF performs competitively even when we change the encoder architecture, decoder depth, decoder activation function, or input sampling for the decoder network.", "For example, using Resnet18 as the encoder network gives almost identical performance in terms of average chamfer distance on our original test set.", "The complete quantitative results of these comparisons will be included in an updated PDF before the end of the discussion period.", "Q4: Number of mapping layers", "Our original results reported in Table 1 compare two different mapping function architectures.", "HOF-1 has one hidden layer with 1024 units, HOF-3 has 3 hidden layers with 128 units each.", "We have updated the text to clarify this distinction.", "In response to your comments, we have also conducted an additional experiment with a mapping network with 6 hidden layers with 128 units each; the test performance of this architecture is almost identical to that of HOF-3 (1.2485 average Chamfer distance with 6 layers compared with 1.247 average CD for 3 layers).", "Q5: Vanishing/exploding gradients", "In all of our experiments, we address the problem of vanishing/exploding gradients by dividing by the square root of the in-degree of each neuron (as in [2]).", "Using the same initialization in the encoder network, we find that training a mapping function with 6 hidden layers (\"HOF-6\") trained easily with no modifications to our training code.", "Another advantage of HOF over deeper, fixed decoder architectures is that it admits extremely shallow decoders, which require less careful tuning of hyperparameters such as initialization scaling and normalization compared with deeper networks.", "For this work, our goal was not necessarily to find the optimal architecture for the decoder, but rather to demonstrate that the usage of the higher-order function paradigm allows for a much smaller decoder architecture than LVC methods.", "Thank you for bringing the additional literature to our attention.", "We have included it in our discussion of related work in a revised manuscript.", "We have also updated the text to more clearly explain the path-based evaluation and its motivation.", "We hope that these additional experiments better demonstrate the effectiveness of HOF as a competitive, parameter-efficient 3d reconstruction paradigm.", "Thank you again for your feedback.", "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019.", "[2] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_social", "rebuttal_social", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 206, "sentences": ["Thank you for the useful feedback.", "We\u2019ve updated our paper to take it into account -- we\u2019ve updated the model description and the notation in Section 4 to clarify our method.", "Two important additions are a high-level summary of the model, which we give at the beginning of Section 4, and a table (Table 2) that lists what each symbol represents along with its dimensions.", "We also made several updates that address your specific questions.", "1. Are e_{i,t} and lambda_{i,t} vectors?", "Scalars?", "Abstract node notations? It is not clear in the model section.", "Also, it took me a long time to figure out that \u2018i\u2019 is used to index each entity (it is mentioned later).", "The entity and location embeddings  e_{i,t} and lambda_{i,t} are d-dimensional vectors, although we also overload the symbols to refer to abstract nodes in the model\u2019s knowledge graphs.", "In the updated manuscript we state both these facts explicitly and state much earlier that \u2018i\u2019 is the index for entities.", "2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context.", "What happens if there are multiple mentions in the text? Which one does it look at?", "When there are multiple mentions of entity i, the initial representation v_i is formed by summing the representations of each mention.", "We have updated the paper to clarify this (Sec 4.1).", "3. For the LSTM in the graph update, why does it have only one input? Shouldn\u2019t it have two inputs, one for previous hidden state and the other for input?", "Good point! We\u2019ve improved the notation used to describe the model in Section 4.", "The update equation now shows clearly that the LSTM takes in the concatenation of two node inputs (entity and location embeddings) along with the previous hidden state.", "4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k.", "This is great, but could you also report the number when the full dataset is used?", "We\u2019ve completed an experiment on the full Recipes dataset and updated the paper to describe the result (this experiment did not finish in time for the initial submission).", "The model\u2019s F1 score improves from 51.64 on the partial data to 54.27 on the full data, surpassing the previous state of the art by a more significant margin.", "5. What does it mean that in training time the model \u201cupdates\u201d the location node representation with the encoding of the correct span. Do you mean you use the encoding instead?", "We meant that we perform teacher-forcing to train the model.", "During training, we extract the context encodings for the groundtruth span and use these in downstream operations  to obtain the node representations.", "At test time, we use the MRC module\u2019s predicted span rather than the groundtruth.", "6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score?", "Is it the threshold that maximizes F1?", "For ProPara task 2, our model was optimized for micro averaged F1 on the development set.", "Tandon et al. (2018) were kind enough to provide us with their evaluation script."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 207, "sentences": ["Thank you for the comments!", "To reviewer\u2019s concerns:", "- First of all, the state of interest does not have to be the object state.", "It can be the state of the robot, for example, the state of actuators.", "Maximizing the mutual information between two sets of actuator states can help the agent to learn to control itself.", "We did a new experiment in navigation environments, where train the agent to maximize the mutual information between its left wheel states and its right wheel states.", "The agent learns to run in a straight line instead of in random directions.", "The video showing experiment results is available at https://youtu.be/l5KaYJWWu70?t=134", "- Although we evaluated our method in robotic manipulation tasks, it does not mean it won\u2019t work for other tasks.", "We added additional experiments in a new navigation task, see the video at https://youtu.be/l5KaYJWWu70?t=104", "We consider our algorithm as a general-purpose skill learning algorithm in the sense that it guides the agent to learn any skills to control the states of interests.", "The states of interest could be any states, such as the robot states, the object states, or the states of the environment.", "- The state of interest is specified by the user with little domain knowledge.", "However, when there is no clear divide from the user, the agent can learn from different combinations of the states of interest and the context states.", "In the end, the user can choose skills from the learned skill sets that are useful for the task at hand.", "- The combination of our method and DIAYN enables DIAYN to learn manipulation skills efficiently, while DIAYN alone did not learn.", "Furthermore, compared to MISC, the combined method enjoys the benefits brought by DIAYN, such as learning combinable motion primitive with skill-conditioned policy for hierarchical reinforcement learning [1].", "Reference:", "[1] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=SJx63jRqFm."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 208, "sentences": ["We thank the reviewer for the comments and feedback.", "We will also include the suggested experiment that shows the plug-and-play nature of PMN.", "1. Residual modules", "- Residual modules are small neural networks (e.g., an MLP for Mvqa, Sec. 3.4, (4)) that a task module may use when other lower level modules are incapable of providing a solution to a given query.", "For example, consider the question \u201cis this person going to be happy?\u201d on an image of a person opening a present.", "Lower level modules of Mvqa may not be sufficient to solve the question.", "Therefore, Mvqa would make use of its residual module, which would essentially learn to \u201cpick up\u201d all queries that lower level modules cannot answer.", "2. Effect of fine-tuning", "- While it might be beneficial to fine-tune the modules for a specific parent task we want each module to be an expert for their own task as it facilitates a plug-and-play architecture.", "Fine-tuning may push the modules towards blindly improving parent module\u2019s performance but (i) badly affect interpretability of inputs and outputs; and (ii) may also reduce the lower module\u2019s performance on its own task.", "Most importantly, it would not scale with the number of tasks, as for each task the agent would need to keep several fine-tuned modules of the lower tasks in memory.", "3. Feeding in the ground-truth", "- Thanks for this great suggestion.", "We performed an experiment where we evaluate the benefits that the VQA model may achieve by using ground-truth captions instead of captions generated by the caption module.", "Our preliminary experiments show a gain of about 2.0% which is a relatively high gain for VQA.", "This points to important properties of the PMN allowing human-in-the-loop type of continual learning, where a human teacher can pinpoint flaws in the reasoning process and potentially help the model to fix them."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 209, "sentences": ["Dear reviewer,", "Thank you for your comment. Before we reply to the points you have made, we would kindly ask you to please add the references that are missing from your review.", "Thank you for your review of our work.", "The following are your concerns of our work:", "a. Prior distributions of hyperparameters", "b. Loss landscape plots and relation to tunability", "c. Importance of search order", "d. Details of the calibration procedure", "We address them as follows (in two parts):", "a. Prior distributions of hyperparameters:", "We envisage an optimizer not merely as update equations, but as the conjunction of the update equations, the hyperparameters, and distributions of those hyperparameters.", "Those distributions should be prescribed by the designers of the optimizer.", "This is crucial: For example, if we take Adam with LR between $10^1$ and $10^5$ and claim that Adam is less tunable than others, the evaluation is inherently faulty, as it doesn't capture where the mode of the distribution of LRs for which Adam is expected to work.", "These prescriptions are absent for the optimizers considered in the paper.", "Therefore, we define them from either mathematical reasoning (say learning rate is non-negative, $\\beta_1, \\beta_2$ in Adam are between (0, 1) and close to 1) or using the calibration step, where we determine those distributions by fitting on the configurations that yielded reasonably good results.", "We choose simple priors for their ease of estimation, though given enough computation, arbitrarily complex priors can be computed and used.", "We fail to see the explicit relationship between our work and the papers you have referenced.", "Specifically [1] only proposes that there is an optimal batch size that is dependent on the momentum parameter.", "We do not consider tuning the batch size, as we do not consider it a hyperparameter of the optimizer itself.", "[2] shows that instead of using LR decay schedule, increasing batch size has a similar effects on training, but results in faster training.", "[3] talks about the existence of an effective learning rate as a function of learning rate and the norm of the weights, and proposes that the optimal learning rate is inversely proportional to the weight decay parameter.", "This doesn\u2019t, however, trivially lend itself to modeling priors.", "In summary, these papers show a complex interplay between the parameters giving rise to other notions, but not provide any methods to jointly model these hyperparameters.", "In the absence of such knowledge, we use our calibration procedure.", "The distributions we use are justified in section 3.2 in the paper.", "However, we accept the fact that a more complex distribution that might model the interaction between these hyperparameters might exist, and using that to sample for an HPO would be better.", "b. Loss landscape plots and relation to tunability:", "We show in figures 1.a, 1.b, as you rightly pointed out, the landscapes of loss function of the HPO objective as a function of the hypothetical hyperparameter $\\theta$. There seems to be a misunderstanding of the purpose of figures 1.a, and 1.b.: These figures do not show what we try to measure, but they merely illustrate by example what properties we would like a tunability metric to have.", "We describe this in the beginning of Section 2.", "In Section 5, we explain why existing measures of tunability are unable to make the distinction between the cases in Figure 1a.", "We would like to emphasize that the point of Figures 1a, 1b is to illustrate the necessary properties that a proposed metric for tunability - it is not our intention to create such plots for our actual experiments.", "If you are interested in these nonetheless, a very recent publication by Asi & Duchi (2019) shows the plot of lr vs performance.", "In summary, they show that the sensitivity of SGD to stepsize choices, which converges only for a small range of stepsizes.", "AdamLR exhibits better robustness when tested on CIFAR10."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 210, "sentences": ["We thank the reviewer for insightful feedback and for noting that our\u200b experiments \u200bare\u200b \u200bsolid\u200b and our setup and analyses are sound.", "The reviewer asks great questions, and we provide the answers below.", "RE: Total running time", "The reviewer raises an interesting point about total training time, which includes the time to pre-train a GNN and the time to fine-tune it on a downstream task.", "To address this point, below, we give the results of the total training time as well as the amortized total time over different downstream tasks.", "We will include detailed results and a discussion in the final version of the paper.", "We note that although pre-training does take some time, it is a one-time-effort only.", "That is, we pre-train a GNN model only once and then reuse it many times by fine-tuning the model on any number of downstream prediction tasks.", "Overall, we find that GNNs, once pre-trained, tend to converge much faster on downstream tasks.", "Most importantly, we find (details below) that validation set performance converges 5-12 times more quickly when GNNs are pre-trained.", "We emphasize that this cannot be achieved by mere training of (non-pre-trained) GNNs longer.", "The following summarizes training time for chemistry and biology datasets.", "1) Chemistry dataset (single GPU implementation)", "**", "Pre-training**", "\u2014 Self-supervised pre-training: 24 hours", "\u2014 Supervised pre-training: 11 hours", "**Fine-tuning on MUV dataset** [Time to achieve the best validation set AUC]", "\u2014 From random initialization (i.e., no pre-training):", "1 hour; 74.9% AUC", "\u2014 From a pre-trained GNN:", "5 minutes; 85.3% AUC", "2) Biology dataset", "**", "Pre-training**", "\u2014 Self-supervised pre-training:  3.8 hours", "\u2014 Supervised pre-training: 2.5 hours", "**Fine-tuning** [Time to achieve the best validation set AUC]", "\u2014 From random initialization (i.e., no pre-training):", "50 minutes; 84.8% AUC", "\u2014 From a pre-trained GNN:", "10 minutes; 88.8% AUC", "On chemistry dataset, we see that fine-tuning a pre-trained GNN on the MUV required only 5 min.", "This is in sharp contrast with training a GNN from scratch, which required 12x more time, yet it gave a worse performance.", "We can reach similar conclusions on the biology dataset.", "We thus recommend using pre-trained models whenever possible as they can give better performance and can be reused for any number of downstream tasks.", "We shall add these results and explanations to the final version of the paper.", "RE: Analysis of different pre-training strategies", "Thank you for bringing up this valuable point.", "We agree that it is important to understand why some pre-training strategies work better over others.", "Our key insight backed up with extensive empirical evidence is that a combination of graph-level and node-level methods (Figure 1) is important because it allows the model to capture both local and global semantics of graphs.", "Further, we find that our structure-based node-level methods (Context Prediction and Attribute Masking) are preferred over position-based node-level methods (Edge Prediction, Deep Graph Infomax). As future work, we plan to further investigate what graph-level and node-level methods are most useful in different domains, and understand what domain-specific knowledge has been learned by the pre-trained models."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 211, "sentences": ["We thank AnonReviewer1 for their positive comments about the interesting-ness of our proposed abductive reasoning tasks (inference and generation) and the associated benchmark dataset.", "We address specific concerns individually below:", "Discussion about e-SNLI:", "A key distinction between e-SNLI and Abductive-NLI is that the explanations in e-SNLI serve the purpose of justifying model decisions.", "In contrast, the goal of Abductive-NLI and Abductive-NLG is to select or generate explanatory hypotheses for given observations.", "Indeed, analogous to e-SNLI for SNLI, Abductive-NLI can be extended to \u201ce-Abductive-NLI\u201d by providing explanations that justify the selected hypothesis.", "Consider the following example that BERT fails to predict correctly:", "O1: Chad loves Barry Bonds.", "H1: Chad got to meet Barry Bonds online, chatting.", "H2: Chad waited after a game and met Barry.", "O2: Chad ensured that he took a picture to remember the event.", "The e-Abductive-NLI task would require models to generate an explanation for selecting H2.", "For the above example, a possible explanation for selecting H2 could be: \u201cPeople need to be physically co-located to take a picture with someone. Meeting online does not mean two people are physically co-located\u201d.", "We think generating such justifications is a great next step and hope that our work will foster such interesting future research.", "Re. somewhat limited contribution:", "We appreciate the opportunity to briefly restate our contributions and to discuss its significance.", "Abductive Commonsense Reasoning, a critical capability in human reasoning, is relatively less studied in NLP research.", "To support this line of research, our work introduces a dataset that focuses explicitly on this important reasoning capability.", "Furthermore, several recent works [1,2,3,4] have shown the presence of annotation artifacts in crowdsourced datasets -- which poses a significant challenge for dataset curation.", "Our work makes the following contributions:", "i) proposes and formalizes two novel tasks of Abductive Inference and Abductive Generation,", "ii) presents a new dataset in support of these tasks collected through careful crowdsourcing design and an adversarial filtering algorithm,", "iii) establishes strong baselines on the task proving the difficulty of the tasks and", "iv) analyses the types of commonsense reasoning that current state of the art models fall short on.", "Re. limited form of Abductive Reasoning:", "The simplifying assumptions, mentioned in the paper, allow us to i) formulate the tasks concretely and ii) curate the dataset and evaluate models viably.", "We show that in spite of the assumptions, our dataset presents significant challenges for current models.", "We totally agree that in its most general form, there should be any number of observations and models should be required to generate explanatory hypotheses in natural language (as in the alpha-NLG task).", "We hope our work will lead to this future line of research.", "Re. the title:", "Thanks for the suggestion. We will update the title to reflect that this work is aimed at language-based abductive reasoning.", "Table 7 vs Table2:", "Thanks for catching that. We\u2019ve updated the paper with the fix.", "[1] Gururangan et al. Annotation artifacts in natural language inference data.", "[2] Poliak et al. Hypothesis only baselines in natural language inference.", "[3] Tsuchiya e. al. Performance impact caused by hidden bias of training data for recognizing textual entailment.", "[4] Sakaguchi et al. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_future", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_done", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 212, "sentences": ["Thank you for the detailed review.", "We appreciate your comments on the contributions of our work and invaluable suggestions to improving our paper.", "Before delving into the details and providing a more detailed rebuttal, here are some of our initial thoughts.", "Regarding comparison with MINE on fMRI dataset, MINE could be applied to fMRI data for MI analysis.", "After all, our DEMINE-vr is simply MINE under cross-validation.", "Our first guess is that MINE will not be able to identify significant dependency in Table.1 due to large confidence intervals, i.e, the rows and cols for MINE will be all 0s.", "For segment classification, MINE-f-ES uses the same model architecture and training process as DEMINE-vr and uses hyper parameters from DEMINE-vr hyperparameter search, and will provide identical segment classification accuracy as DEMINE-vr.", "Thanks again for pointing out the typos, we'll post an updated version shortly."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 213, "sentences": ["We thank for the reviewer for their positive comments on our work, and we share our responses below.", "The purpose of our work is not to achieve state-of-the-art performance simply by incorporating the latest network architectures and optimisers.", "Instead, we provide a novel general framework for automating generalisation, and show that when used with standard classification networks across all baselines, our method performs the best.", "Furthermore, as we also explained in Reviewer #3, the hyper-parameters for defining a hierarchy is not critical, and we can choose an arbitrary hierarchy whilst still achieving better performance than baselines.", "In the future work, we would like to explore how to find the optimal hierarchy in an automatic manner, or provide an alternative solution on building a general type of auxiliary tasks (such as regression).", "However, this is the first work to present a double-gradient method for auxiliary task generation, and we believe that it is important to present the success of this initial method now given how simple and general it is, and then fine-tune other aspects in future work."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_future", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 214, "sentences": ["Thanks for your positive feedback.", "(1).", "Lemma 2.4, Point 1: The gradient in your example is indeed perpendicular to w which can be seen as follows.", "w\u2019 * \\nabla L(w) = w\u2019 * (2/w\u2019*w)(Aw - L(w)*w) =  (2/w\u2019*w)(w\u2019Aw - L(w)*(w\u2019*w)) =  (2/w\u2019*w)(w\u2019Aw - w\u2019Aw) = 0.", "In case of one variable vector, our proof is to take the derivative of c on both sides of F(w) = F(cw), which is the definition of scale-invariance.", "Then the left-hand side becomes 0 and the right-hand side becomes w\u2019 * \\nabla F(cw)", "by chain rule.", "Taking c = 1, we can conclude", "that w\u2019 * \\nabla F(w)  = 0.", "(2)", ".", "Theorem 2.5: Sorry G_t should be G_t^{(i)}. We will correct this typo in the next revision of this paper.", "For t = 0, G_t^{(i)} are all initialized to some value.", "The recursion formula for G_t^{(i)} is shown in equation (9)."], "labels": ["rebuttal_social", "rebuttal_none", "rebuttal_answer", "rebuttal_answer", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_refute-question", "rebuttal_none", "rebuttal_none", "rebuttal_done", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 215, "sentences": ["Dear Reviewer,", "Thank you for your valuable comments.", "We have revised our writing in the revision, and will further improve its clarity.", "Please find our response as follows.", "- Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.", "Mitosis training can be considered as executing Algorithm 1 for multiple times with an increasing number of experts and inherited initialization from last round by changing W^e and W^g.", "Also, training with mitosis achieves similar performance as training without it shown in Appendix B, Figure (a).", "- How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?", "The hyper-parameters related to DS-softmax (such as lambda) are tuned according to the performance on a validation dataset.", "Also, as we mentioned in the paper, only one hyper-parameter (group lasso lambda) needs to be tuned.", "The heuristic we use to tune group lasso lambda is to increase lambda, starting from a small value, until it hurts the performance.", "Also threshold and balancing lambda variables are kept fixed as (0.01 and 10).", "- Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?", "In terms of baselines, SVD-softmax (NIPS\u201917) was chosen since it is a recent method that provides a significant inference speedup for softmax.", "Other alternatives, such as D-softmax and adaptive-softmax, focus on training instead of inference speedup.", "Furthermore, as claimed in their papers, they achieve limited speedup (around 5x) in language modeling, which is much worse than ours.", "With regards to Sparsely Gated MoE, it cannot speed up inference, since they select expert with full softmax.", "We would like to emphasize that most existing methods for inference speedup focus on approximating trained softmax layer, which usually suffers a loss on performance.", "Our model allows the adaptive adjustment of the softmax layer, achieves speedup through capturing the two-level overlapped hierarchy during training, which is novel and does not suffer from the performance loss."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 216, "sentences": ["Thank you very much for the comments.", "We believe that this response can help the Reviewer to be more convinced about the validness of our experiments; in particular, the validness of our retraining methodology.", "Q1. The paper is not very self-contained, and I have to constantly go back to [1] and [2] in order to read through the paper.", "In the original manuscript, we had to limit the detailed information of the previous work due to the page limit.", "Based on the Reviewer\u2019s comments, we added more description about the schemes we adopted from [1] and [2] in Appendix A.1 and A.2 of the revised manuscript.", "Q2. The input and output types of each block in Figure 1. should be clearly stated, and the figures are almost useless because the captions contain very little information.", "We tried to add more information to the figures in the revision.", "First, in Figure 1, we added the more mathematically precise description of input and output of each block to show how the exact weight representation is changed at each process.", "We also added additional explanation for 'D' of Figure 2 in its caption.", "For the Figure 4, we added the description of the underlined numbers.", "Q3.", "Optimizing compression rates should be done on the training set with a separate development set.", "The test set should not used before the best compression scheme is selected.", "Both the results on the development set and on the test set should be reported for the validity of the experiments.", "Thanks for pointing this out.", "We believe that this is the Reviewer 1's core question so would like to justify our results more in detail in this response and try to convince the Reviewer.", "We agree that optimizing compression rates should not use the test set before the best compression scheme is selected.", "In fact, in case of PTB and Wikitext-2 corpus, we already used the provided validation set and measured the test PPW only once after training (Table 2) in the original manuscript.", "From the Table 2, we can see that our proposed scheme maintains the accuracy of the uncompressed baseline network.", "On the other hand, the CIFAR-10 dataset does not include a separate validation set, so we had to use the test set in the retraining process.", "To avoid using the test set in the retraining process as the Reviewer pointed out, we randomly selected 5K validation images among the original 50K training images in CIFAR-10 dataset, and applied our scheme.", "Then, we observed the training and validation accuracy at each training epoch, and measured the test accuracy once after training.", "The accuracy results are as shown in the following table.", "Note the compression rates are the same as the data in Table 3 in the original manuscript.", "----------------------------------------------------------------------------------", "Compression scheme   Validation Error (%)    Test Error (%)", "------------------------------  ---------------------------", "---------------------", "Baseline", "11.5                         12.2", "Pruning [1]                         11.4                         12.2", "VWM (Ours)", "11.4                         12.4", "----------------------------------------------------------------------------------", "The test accuracy in the above table is about 1 % less than the accuracy which we reported in the originally submitted manuscript because the number of training data was decreased as part of the data set is used as a validation set.", "However, the results show that our proposed method does not make the network be overfitted to test data as the Reviewer doubted because the difference between the accuracy for validation set and test set are consistent with the values from the previous works.", "Note that even the uncompressed baseline network exhibits similar accuracy difference between the validation error and the test error compared with the compressed networks.", "Therefore, we believe that our proposed compression method does not suffer from the concerned overfitting problem regardless of the types of neural networks or dataset.", "Reference", "[1] Dongsoo Lee, Daehyun Ahn, Taesu Kim, Pierce I. Chuang, and Jae-Joon Kim. Viterbi-based pruning for sparse matrix with fixed and high index compression ratio. International Conference on Learning Representations (ICLR), 2018.", "[2] Chen Xu, Jianqiang Yao, Zouchen Lin, Wenwu Qu, Yuanbin Cao, Zhirong Wang, and Hongbin Zha. Alternating multi-bit quantization for recurrent neural networks. International Conference on Learning Representations (ICLR), 2018."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 217, "sentences": ["We thank the reviewer for their encouraging words and will correct the errors of expression."], "labels": ["rebuttal_by-cr"], "confs": [1.0]}
{"abstract_id": 218, "sentences": ["Thank you to Reviewer 1 for noting the clarity of our presentation and reproducibility.", "We also appreciate the constructive criticism and thought that went into your review.", "We spent a considerable amount of time trying to fulfill the reviewer\u2019s request to match state of the art (SOTA) on PTB.", "To get SOTA on PTB, we need massive architectures, which considerable computing power and experimentation at the extreme limit of what is achievable for our team.", "Still, we pursued two directions.", "First, we tried to reimplement an architecture similar to  Melis et al. 2017.", "However, they did not publish their code, hyperparameters, or weights, requiring re-implementing and re-training from scratch.", "We tried this path, but soon realized we would not be done in time (especially with a hyperparameter search).", "We then tried to weave neuromodulation and differentiable plasticity into the architecture and code base of Merity et al., ICLR 2018 (also tied for SOTA).", "However, while they could simply leverage existing PyTorch implementations of LSTMs (written in extremely fast C++), we had to re-implement LSTMs \u201cby hand\u201d (i.e. as a series of connected layers) in PyTorch to introduce plasticity and neuromodulation.", "As a result, our networks thus ran considerably slower, by more than 10x (not because our method is intrinsically slower, but just for lack of engineering optimizations on our bespoke Python implementations; we confirmed this by observing that a similar \u201chand-built\u201d reimplementation of simple, non-plastic LSTMs ran similarly slower, while producing results identical to Merity et al.).", "These experiments are thus unfortunately still running.", "For these reasons (and more provided below), we thus think it more fair (and necessary) to make such experiments the subject of a future paper.", "That said, we still believe the results in the current paper demonstrate the benefits of our techniques on a sizable model, and thus it would benefit the community to allow people to know about, and build upon, these new methods and results.", "The purpose of the present paper is to introduce a novel technique and show that it can produce an advantage in realistic settings, which we believe our PTB task confirms.", "Our claim is that, all other things being equal (especially the number of parameters), a neuromodulated plastic LSTM outperformed a standard LSTM on this particular benchmark task.", "We do **not** want to claim that our results are anywhere near SOTA.", "We have modified our text to avoid possible misunderstandings (see end of next-to-last paragraph in Section 4).", "Additionally, philosophically, If SOTA results are the bar for all papers to be accepted into conferences like ICLR, then those venues will be the exclusive domain of those with either the computation or time (i.e. large-scale resources) to dedicate to such results.", "In that case, many cutting edge ideas will by necessity be excluded from the discussion, as will many research groups.", "Moreover, insisting on papers to be SOTA to be accepted also likely encourages p-hacking and shoddy science to game the results (even if unintentionally), reducing the quality of science our community tries to build on.", "Re: \"Parameters of the model\": All trainable parameters of the Hebbian synapses (alpha and w in Equation 1, plus the neuromodulation parameters) are included in this parameter count.", "To equalize the number of parameters across architectures, we reduce the number of hidden units in the plastic models in comparison to the non-plastic baseline.", "We have clarified this in the text.", "Re: \"Attention\": Non-trainable, homogenous plasticity can indeed be compared to a form of attention, i.e. \u201cattending to the recent past\u201d in the words of Ba et al. 2016.", "However, differentiable plasticity allows for the plasticity of each connection to be trained; as a result, different connections play different roles and it is not at all clear that the analogy with attention remains relevant (see e.g. the clever mechanisms automatically implemented by the trained plasticity connections in the image completion experiment of the Differentiable Plasticity paper, Miconi et al. 2018, sections 4.3 and S.3, which can hardly be described as simply \u201cattention\u201d)", "Re: \"Style (font)\": We used the template and do not see the discrepancy. Can you clarify? We are happy to fix it."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_reject-request", "rebuttal_followup"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 219, "sentences": ["Thank you very much for the highly constructive review.", "> I think this is a very interesting direction, but the present paper is somewhat unclear. In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have \"training algorithms that are exactly equivalent.\" I think this example needs to be clarified.", "We realized that the naming was very confusing and consequently, we renamed \\tilde\\theta to \\tilde\\mu in the noise-injected model.", "Now,", "- the original, noise-free model p has the structure \\theta -> D (no bottleneck) while", "- the adapted, noise-injected model p\u2019 has the structure \\mu -> \\tilde\\mu -> D (containing a bottleneck).", "Hereby, \\tilde\\mu is a noise-corrupted version of the new parameters \\mu, and we obtain a limit on the mutual information between \\mu and D. We simplified Figure 2 and 8 to make this more clear.", "To better characterize Gaussian mean field inference on the original model, we aim to find an inference procedure on p\u2019 so that both algorithms result in exactly the same outcome, e. g. the same calculations are executed when running the corresponding program.", "We show that there is such an inference procedure on the noisy model, and it has the character of MAP.", "Note that only if generative and inference model are adapted simultaneously we end up with equivalence.", "Hereby, \\mu (the mean of the Gaussian q) and \\theta (the original parameter in p) correspond to \\mu (the MAP point-mass of q\u2019) and \\tilde\\mu (the noise-injected version of \\mu in p\u2019).", "> Many of the parameters here are also unclear and not properly defined/introduced.", "What is the relationship between \\theta and \\tilde\\theta exactly?", "In this example, \\theta and \\tilde\\theta never appear in the same model (they are part of p and p\u2019, respectively).", "We realized that this is confusing and have therefore renamed \\tilde\\theta to \\tilde\\mu.", "> In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?", "This is an excellent question.", "In fact, we believe that trying to construct noise-free deep models with a specific mutual information of data and parameters for the purpose of generalization would be an interesting research direction.", "Due to nonlinearities in typical deep models, it is at least not obvious how to calculate the mutual information between data and parameters.", "The main challenge here would certainly be to come up with an effective estimator.", "Relatedly, one would have to design priors and architecture to achieve a specific mutual information.", "> The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper [...]] and further exploration is desirable.", "This paper is giving an information-theoretic perspective on existing variational inference methods.", "Such a perspective is interesting, but needs to be further developed and explained.", "Specifically, how can mutual information in this context be formally linked to generalization/overfitting?", "We updated section 2.2 to relate to the references you mentioned.", "They explore the link of limiting mutual information and generalization error mostly in theory (and in particular for adaptive analysis).", "In contrast, we deploy this principle in a practical model structure that is easily applicable to many existing deep and variational learning approaches and provide empirical evidence of the validity of our framework.", ">", "Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.", "As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.", "We want to emphasize that we do use the standard definition of mutual information.", "Therefore, the bottleneck implied by Eq. 5 is purely a property of the generative model and not influenced by the approximate inference distribution q.", "Eq. 2 is only introduced to provide additional motivation for our approach as it allows to characterize overfitting in variational inference.", "The guarantee derived in section 2.2 ties this quantity back to the mutual information from Eq. 5."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 220, "sentences": ["Q1: In Theorem 4.3, the result holds for any $k$ and $M$. The authors claim that if we take a limit of $M \\to \\infty$ with fixed $k$, the practical dynamics converges to the discrete-time mean-field limit, in Section 4.", "However, to state the result of Theorem 4.3, $k$ should be bigger than $M c_\\eta$ from the dentition of $\\tilde{\\rho}_k^M$, as shown under the equation (4).", "How do we take a limit of $M \\to\\infty$ ? Does k also go $\\infty$?", "A1: Thanks for pointing this out.", "The result of this theorem holds uniformly for any $k$ (not a fixed $k$).", "Besides, we do not require $k$ bigger than $M c_\\eta$ in the definition of $\\tilde{\\rho}_k^M$. When $k$ is no more than $M c_\\eta$, $\\tilde{\\rho}_k^M$ and $\\rho_k^M$ are stochastic processes with same distribution and thus the Wasserstein distance between them is 0.", "And for any $k$ is greater than $M c_\\eta$, we have the uniform bound (w.r.t. $k$) as stated in the theorem 4.3.", "We are sorry for not stating this clearly in the theorem and we have revisited the present of the theorem. We will fix this issue in the next revision.", "We also point out that, as our system is complicated, in taking the limit of $M\\to\\infty$, we need to ensure that the number of iteration we run is larger than $Mc_\\eta$. To be specific, the asymptotic convergence would be", "$$\\lim_{k,M \\to\\infty, \\eta \\to 0^+} \\mathbb{D}_{\\text{BL}} (\\rho_k, \\rho^*)=0$$", "where the joint limit of k and M requires that $k\\eta\\to\\infty$; $\\exp(C\\alpha^{2}k\\eta)\\eta^{2}=o(1)$; $(k\\eta)/(Mc)=q(1+o(1))$ with $q>1$. Here if $q \\leq 1$, we degenerate to Langevin. But when $q>1$ (intuitively that means, when $M$ is large, the number of iterations we run is larger), our dynamics is different from Langevin, which is what we do in the practice.", "Also, we would like to remark that this seemingly strange things is in fact the \u2018artifact\u2019 caused by the using of Langevin dynamics at beginning to obtain the $M$ initial samples when we designed the practical implementation of the proposed methods.", "However, it is not really necessary to use Langevin dynamics to get $M$ initial samples, as we can simply using some other initialization distribution and get the $M$ initial samples from that distribution (and by this setting, our dynamics is simply the second phases in Eq (3)).", "All our theory can be easily generalized to this setting using almost identical argument, which can also address your concerns on this issue.", "Q2: Regarding other minor comments", "A2: Thanks for your notification! We will polish our paper and rewrite the corresponding part in the next revision."], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 221, "sentences": ["Thank you for your comments. Please find below our response to your questions and concerns.", "1) Pseudocode", "We apologise that the optimization procedure was unclear.", "We have added pseudocode of the general optimization procedure in Appendix A.", "2) Hyperparameter selection", "The reviewer is completely right that we are removing one hyperparameter by introducing another.", "However, there are two reasons why this might still be beneficial: one is that the penalty coefficient is now effectively dynamic and can change during training, ensuring higher chances of finding a good solution.", "Second, by elevating the hyperparameter one level up, we hope that the learning is indeed less sensitive to its specific setting.", "Indeed, we found in practice that we get similar results for \\beta within some orders of magnitude, which requires significantly less tuning compared to a fixed \\alpha.", "3) Relation to safe reinforcement learning", "It is indeed the case that constrained MDPs are often considered in safe RL.", "In those cases there is generally an upper bound on a penalty function that should never be exceeded, including during training itself.", "These algorithms generally restrict policy updates to remain within the constraint-satisfying regime.", "While our approach can similarly be applied to upper bounds on penalties, there\u2019s unfortunately no guarantee that the constraints will be satisfied at every moment during training, but only at convergence.", "As such it is not clear how these methods would apply to our specific experimental setups."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 222, "sentences": ["1. I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful. If that is true, I would suggest the authors to make this hidden assumption clearer in the paper", "Response: Thanks for the constructive suggestion. We agree with the reviewer and will make this assumption explicit in the revised manuscript.", "2. The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment.", "The idea has a cross-disciplinary nature and is fairly interesting to me.", "I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works.", "Response: Thanks for recognizing the strengths of the paper. We will add the appropriate references regarding the \"differential testing\" concept in software engineering.", "3. One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them.", "However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.", "Response: Thanks for pointing it out.", "We agree with the reviewer that images falling into Case III can be used to distinguish the associated two classifiers using the proposed semantic tree distance.", "We will revise the writing to make it more rigorous.", "In our current subjective assessment environment, we choose to stop labeling images in Case III because it is difficult for humans to select one among 200 classes, especially when they are unfamiliar with the class ontology."], "labels": ["rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 223, "sentences": ["We would like to thank the reviewer for the time and useful feedback.", "Our response is given below.", "- Relationship to z-conditioning strategy in BigGAN.", "Thanks for pointing out the connection to this concurrent submission.", "We will discuss the connections in the related work section.", "The main differences are as follows:", "1. BigGAN performs conditional generation, whilst we primarily focus on unconditional generation.", "BigGAN splits the latent vector z and concatenates it with the label embedding, whereas we transform z using a small MLP per layer, which is arguably more powerful.", "In the conditional case, we apply both additive and multiplicative interaction between the label and z, instead of concatenation as in BigGAN.", "2. Overall BigGAN focusses on scalability to demonstrate that one can train an impressive model for conditional generation.", "Instead, we focus on a single idea, and show that it can be applied very broadly.", "We provide a thorough empirical evaluation across critical design decisions in GANs and demonstrate that it is a robust and practically useful contribution.", "- Propagation of signal and ResNets.", "Indeed, ResNets provide a skip connection which helps signal propagation.", "Arguably, self-modulation has a similar effect.", "However, there are critical differences in these mechanisms which may explain the benefits of self-modulation in a resnet architecture:", "1. Self-modulation applies a channel-wise additive and multiplicative operation to each layer.", "In contrast, residual connections perform only an element-wise addition in the same spatial locality.", "As a result, channel-wise modulation allows trainable re-weighting of all feature maps, which is not the case for classic residual connections.", "2. The ResNet skip-connection is either an identity function or a learnable 1x1 convolution, both of which are linear.", "In self-modulation, the connection from z to each layer is a learnable non-linear function (MLP).", "- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?", "Yes, we notice more improvements on the harder, more diverse datasets.", "These datasets also have more headroom for improvement."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 224, "sentences": ["Thank you for your helpful comments!", "1. You are right that Frank-Wolfe would be advantageous over PGD when the constraints are more complicated and adversarial attack may not be such a case.", "Yet it is also well-known that Frank-Wolfe has quite different optimization behavior compared with PGD even though they have the same order of convergence rate.", "Therefore, it is interesting and important to examine the performance of Frank-Wolfe algorithm for adversarial attack, given the fact that PGD has been shown to be a very effective for adversarial attack.", "In fact, from our work, we found that Frank-Wolfe based methods are generally more efficient than PGD method.", "From another perspective, Frank-Wolfe solves the problem by calling Linear Minimization Oracle (LMO) over the constraint set at each iteration.", "This LMO shares the same intuition as FGSM, which also tries to linearize the neural network loss function to find the adversarial examples.", "In this sense, it is a quite natural attempt to revisit FGSM under the Frank-Wolfe framework.", "2. We are sorry maybe we didn\u2019t explain it very well in the paper, but this is a misunderstanding.", "We indeed compared our method with generalized I-FGSM/BIM, which is exactly the same as PGD (In [Madry et al.] they also mentioned this in Section 2.1 and they refer it as FGSM^k).", "We decide to just call it PGD in the revision to avoid confusion.", "We hope this remove your concern.", "3", ".", "Indeed, theoretically we can only prove for $\\lambda$ = 1 case.", "Yet we found that larger \\lambda brings us more speedup.", "We have added further empirical evidence (performance comparison of our method with different \\lambda in Figure 1 in the revised paper) to justify it.", "Intuitively speaking, using lambda>1 is essentially a \u201crelax and tighten\u201d step by first relax the constraint to make the problem easier, and then tighten it back to the real constraint.", "The \u201crelax and tighten\u201d idea has been widely used in constrained optimization, and in this paper we adapted this idea into Frank-Wolfe algorithm to make it even faster.", "4. [Lacoste-Julien 2016] considered the general first-order Frank-Wolfe algorithm for nonconvex smooth optimization.", "The result of Theorem 4.3 in our paper is almost the same as the result in (Lacoste-Julien 2016), except that the choices the learning rate in these two papers are different though.", "We have made it clear in the revision.", "5. We have added detailed hyperparameter settings for CW and EAD in the revision in the supplemental materials.", "6. While Theorem 4.7 is new and may be of independent interest in the optimization community,  it is not the main contribution in this paper.", "We would like to emphasize that our major contribution in this paper is a Frank-Wolfe based algorithm for adversarial attack, which is more efficient than PGD based adversarial attack algorithm and other baselines.", "7. Sorry about the confusion.", "$y$ should be replace by $y_{tar}$. It is a simplified notation we mentioned in the proof in the appendix.", "Thank you for your suggestion and we have revised the notation $f(x,y_{tar})$ to $f(x)$.", "8. Thank you for pointing out several typos. We have fixed all of them in the revision."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_done", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_summary", "rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 225, "sentences": ["Thanks for your review.", "The ION theorem is an important part of explaining sparsity, dead units, and rank as well, but perhaps our writing was not clear enough.", "We will work on the writing in the future version of this work.", "As for the mutual information related comment (#2), the results that you have mentioned are well known from information bottleneck paper or from the following information invariance paper.", "Alessandro Achille and Stefano Soatto.", "Emergence of invariance and disentangling in deep representations.", "Journal of Machine Learning Research. 2018"], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 226, "sentences": ["We thank reviewer 2 for the detailed feedback.", "We are glad that the reviewer found the VAE-GAN model to be a natural extension for the problem and that our work provides a good baseline for future work.", "We address the individual questions below.", "We changed Section 3.1 to explain that the posterior dependence on pairs of adjacent frames is to have temporally local latent variables that capture the ambiguity for only that transition, a sensible choice when using i.i.d. Gaussian priors.", "Another choice is to use temporally correlated latent variables, which would require a stronger prior (e.g. as in Denton & Fergus (2018)).", "For simplicity, we opted for the former.", "The blurriness in a VAE can indeed be attributable to a weak inference model.", "Note that our VAE variant and both SVG variants are able to predict sharp robot arms in the BAIR dataset, but often blur out the small objects being pushed.", "We tried recurrent posteriors and learned priors with our models, and the results were similar.", "We are now running additional experiments with a deeper encoder and with more filters.", "Although in principle a strong inference model could produce sharper images, an alternative approach is to use better losses, which is the approach we chose in this work.", "It is an interesting suggestion to experiment with the effect of the hyperparameters on the trade-off between realism and diversity.", "We are currently running experiments for various weightings of the KL loss and the adversarial loss, and we plan to include results that illustrate the trade-offs based on these hyperparameters.", "We also plan to include results on the trade-offs between accuracy and realism.", "In fact, a recent result [1] proves that this is a fundamental trade-off for all problems with inherent ambiguity.", "The statement that \u201cGANs prioritize matching joint distributions of pixels over per-pixel reconstruction\" is a criticism of per-pixel losses, and not of VAEs in general. We clarified in the introduction that VAEs can indeed model joint distributions of pixels.", "[1] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Conference on Vision and Pattern Recognition (CVPR), 2018. https://arxiv.org/abs/1711.06077"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 227, "sentences": ["Thanks for pointing out the pros and cons of our method.", "We address your concerns as follows:", "Q1. \u201cThe search space of the proposed method, such as the number of operations in the convolution block, is limited.\u201d", "A1: First, the size of search space is not determined by the number of operations but the number of connections.", "The search space of our method is different from exiting NAS methods in that the number of input of certain operation is not limited.", "Second, the search space without block share is even much larger than existing NAS methods.", "Third, we can trivially extend our DSO-NAS to accommodate more operations such as dilated conv like our ongoing experiments on PASCAL VOC semantic segmentation task, we extend our search space to accommodate 3x3 and 5x5 separable convolution with dilated = 2.", "The following table shows the performance of our model on the PASCAL VOC 2012 semantic segmentation task, where DSO-NAS-cls represents the architecture searched on ImageNet with block structure sharing and DSO-NAS-seg represents the architecture searched on PASCAL VOC segmentation task.", "---------------------------------------------------------------------------------------------------------------------------", "Architecture", "mIOU                     Params(M)", "FLOPS(B)", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-cls", "72.1", "6.5                               13.0", "---------------------------------------------------------------------------------------------------------------------------", "DSO-NAS-seg(more operations)", "72.7                            6.7                               13.2", "---------------------------------------------------------------------------------------------------------------------------", "We combine DSO-NAS with Deeplab v3 and search for the architecture of feature extractor with block sharing.", "All above models have been pre-trained on ImageNet classification task first.", "It\u2019s notable that the architecture searched on semantic segmentation task with additional operations achieve better performance in our preliminary experiment, indicating that our DSO-NAS is capable to incorporate additional operations.", "We will present the full experiments of semantic segmentation in the future revision.", "Q2: \u201cThe technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.\u201d", "A2: Please refer to Q1.", "Moreover, we never claim the main contribution of our work lies in augmenting the search space.", "And in fact, most existing NAS papers share the same architecture search space, the main differences between them is the search strategy.", "We believe that judging the novelty of a NAS paper solely by its architecture space is unfair."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 228, "sentences": ["Thank you for your constructive comments!", "1. We fully understand your concern and we have added detailed description in the supplemental materials to show the hyperparameters we use for baseline methods in the revision.", "2. We would like to argue that constrained optimization based formulation itself is not designed to achieve better distortion compared with regularized optimization based formulation.", "So there is no surprise that our algorithm\u2019s distortion is not the best.", "On the other hand, as mentioned by the other reviewer, distortion is usually not that essential in adversarial attacks as long as it is maintained in a reasonable range.", "We could actually remove the distortion column, instead, we chose to include it just to show that we did not trade a lot of distortions (to make problem much easier) and thus gains speedup.", "From our experimental results, you can see that our proposed method achieves significant speedup while keeping the distortion around the same level as the best baselines.", "3. Thank you for your suggestion.", "We have further added success rate vs queries plot (for black-box case) and loss vs iterations plot (for white-box case) in the revision.", "As you can see, in terms of number of iterations / queries, our method still outperforms the other baselines by a large margin.", "4. Thank you for your suggestion.", "We have further added experiments on ResNet V2 model and averaging over 500 correctly classified pictures to strengthen our result.", "Again, this additional experiments show that our method outperforms the other baselines for both white-box attack and black-box attack.", "5. Regarding poor time complexity in practice, first, as you mentioned, adversarial training currently is quite slow due to the slow adversarial attack steps.", "Better time complexity of adversarial attack could significantly speed up adversarial training algorithms.", "Second, it is worth noting that the running time complexity of adversarial attack also highly depends on the input size.", "For example, if you attack a CIFAR-10 classifier or an MNIST classifier, it could take only seconds per attack even for the slowest algorithm since the input size is only 32 by 32 (or 28 by 28).", "However, if you attack a ImageNet classifier or even higher dimensional data classifier, it could take significantly longer time (minutes).", "That is why reducing the runtime of adversarial attack is very important.", "6. We apologize for this confusion.", "Regarding \u201cgradient-based\u201d / \u201coptimization based\u201d methods and coordinate-wise black-box attacks, we have changed our description to avoid confusion.", "Thank you for pointing it out."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_social", "rebuttal_done", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 229, "sentences": ["Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers.", "Here we respond to your specific comments.", "\"What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \"", ">>> In few-shot learning, episodic paradigm proposed by Matching Networks [1] is widely adopted by current researchers (we follow the same setting to make a fair comparison).", "In each episode, a small subset of N-way K-shot Q-query examples is sampled from the training set.", "Typically, for 1-shot experiments, N=5, K=1, Q=15 and for 5-shot experiments, N=5, K=5, Q=15", ".", "Thus, the number of training examples are Nx(K+Q) (80 for 1-shot and 100 for 5-shot)", ".", "Constructing label propagation matrix W involves both support and query examples (80 or 100).", "So the dimension of W is either 80x80 or 100x100.", "Running label propagation on such small matrix is quite efficient.", "\"It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\"", ">>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module.", "After we get the per-example feature representation f_{\\varphi}(x_i) for x_i, we feed it into the graph construction module g_{\\phi}. The output of this module is a one-dimensional scalar.", "f and g are learned in an end-to-end way in our approach.", "\"solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \"", ">>> We want to answer this question from two aspects.", "On one hand, few-shot learning assumes that training examples in each class are quite small (only 1 or 5).", "In this situation, Eq (3) and the closed-form version can be efficiently solved, since the dimension of S is only 80x80 or 100x100.", "On the other hand, there is plenty of prior work on the scalability and efficiency of label propagation, such as [2], [3], [4], which can extend our work to large-scale data.", "On miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps.", "This is slightly worse than closed-form version (53.75/69.43), because of the inaccurate computation and unstable gradients caused by multiple step iterations.", "[1] Vinyals, Oriol, et al. \"Matching networks for one shot learning.\" NIPS. 2016.", "[2] Liang, De-Ming, and Yu-Feng Li. \"Lightweight Label Propagation for Large-Scale Network Data.\" IJCAI. 2018.", "[3] Fujiwara, Yasuhiro, and Go Irie. \"Efficient label propagation.\" ICML. 2014.", "[4] Weston, Jason. \"Large-Scale Semi-Supervised Learning.\""], "labels": ["rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_refute-question", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 230, "sentences": ["We thank the reviewer for their positive evaluation of our work.", "In response to their suggestion under Point 4, we have added the suggested references to sections 2.3 (Related Work) and 4 (Discussion and Conclusion)", "New text:", "\u201cSimilarly, Wah et al. (2014) show a series of adaptive displays for an anchor c_i, where the subject must partition the queries c_j, c_l, \u2026 into a set of similar and a set of dissimilar queries.", "In contrast to our work, the aforementioned studies did not use sparsity or positivity constraints, nor did they intend to evaluate the interpretability of the embedding.\u201d", "New text:", "\u201cYet another possible extension is to consider different types of similarity judgments (Veit et al. 2017), e.g. resulting from asking subjects to group objects based on a specific attribute (size, color, etc.).\u201d"], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 231, "sentences": ["Thank you for the detailed review.", "We appreciate your comments on the contributions of our work and the nature of our approach, as well as suggestion of experiments and paper writing.", "Before delving in and providing more details, we have some initial thoughts about the theoretical issues you brought up.", "Regarding theorem 1 and the sample complexity of MINE, we also had discussions on why we think they are comparable or not and discussed that on page 5 in our submission.", "The tldr is that the MINE sample complexity can not only be seen as 1) bounding best achievable MI estimation but also as 2) bounding distance from estimation to a proven MI lower bound.", "The former is a quite vacuous bound on generalization and would require advances in learning theory to improve, not MI estimation.", "Our theorem 1 is trying to improve the latter to enable practical applications.", "Improving the former to the level of practical use is a noble goal, let us know when you have an answer.", "Regarding \"false detection\" experiments.", "We really appreciate that you brought up this point.", "Our synthetic experiments on Gaussians rho=0.0 in Figure 1 do exactly this.", "Results show that MINE-f and MINE-f-ES estimates very much non-zero MI when there should have been 0 MI.", "MINE-f bar is not visible due to overshooting out of the chart.", "DEMINE approaches give estimations closer to 0.", "We often get questions about why our estimators give MI numbers lower than MINE and why are we claiming that our estimator is better.", "But in fact that's exactly because MINE gives false detection but our estimators provably don't.", "Hyper-parameter search example.", "Say we are given 3000 paired (x,y) observations.", "First divide into 1500 train, 1500 test.", "Take 1500 train and run Algorithm 1 using 3-fold crossval: use 1000 for (x,y)train and 500 for (x,y)val in each run (3 runs total).", "Get MI estimation m1,m2,m3 over 3 folds.", "Compute confidence interval v using Eq.8 using the hyperparameters and 1500 as test set size.", "Hyperparameter search DEMINE-vr maximizes mean([m1,m2,m3])-2std([m1,m2,m3]).", "DEMINE-sig maximizes mean([m1,m2,m3])-v.", "Will try to make it more clear.", "Regarding fMRI experiments, our focus is on demonstrating neural MI estimation and dependency test on fMRI data.", "We compare with pearson's correlation because it's another widely used technique that can perform both correlation analysis as well as significance test.", "We used a simple 1D CNN where convolution happens over the time dimension, not the spatial dimensions.", "Better architectures, e.g. transformers over time + graph networks over space could improve performance, but not our focus and we leave that to future work.", "Higher-order and nonlinear covariance tests may make very appealing comparisons and we are looking into it.", "A first impression is that our technique is more general and will probably give looser bounds, but may be applicable to a wider range of problems not only ones that have specifically that type of covariance, just like DEMINE vs Pearson's correlation.", "But at the same time we also have questions on how much additional insight it brings, as it's not an apples to apples comparison, so neither tight or loose estimations diminish the value of both types of approaches."], "labels": ["rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_social", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 232, "sentences": ["Thank you for your helpful comments.", "We have addressed your concern about the baseline models and learning rate schedules in our updated paper.", "Q: It is hard to say whether the results are applicable in practice; need updates to baselines and comparison with learning rate schedules.", "We have updated the baselines in our paper for CIFAR-10 and CIFAR-100, using a larger, modern network, ResNet34, in place of the VGG11 model used previously.", "We also compared APO to manual learning rate decay schedules.", "For CIFAR-10/100, we trained for 200 epochs, decaying the learning rate by a factor of 5 three times during training.", "The ResNet34 with a custom learning rate decay schedule achieves 93-94% test accuracy on CIFAR-10 and ~74% test accuracy on CIFAR-100.", "We believe that this is a strong baseline, and shows the applicability of APO in practical settings.", "The final test accuracies of the updated model using SGD/SGDm with and without APO are:", "| CIFAR-10 | CIFAR-100 |", "--------------------------+--------------+---------------+", "SGD (fixed lr)", "92.97            72.69", "SGDm (fixed lr)", "92.77            72.53", "SGD (decayed lr)", "93.29            73.45", "SGDm (decayed lr)", "93.53            73.80", "SGD-APO", "93.82            74.65", "SGDm-APO", "94.59            73.89", "The test accuracies using RMSprop and K-FAC with APO are shown in our response to all reviewers at the top.", "The results in these tables show that APO is competitive with manual schedules in terms of test accuracy.", "The updated figures in our paper show that APO is competitive with manual schedules both in terms of test accuracy and training loss.", "Q: Does the hyperparameter lambda itself benefit from some scheduling?", "In our updated paper we show that APO with a fixed lambda achieves comparable performance to manual learning rate decay schedules.", "While using a schedule for lambda can potentially further improve performance, a simple grid search over fixed lambda values already leads to strong performance, and has the advantage that it is easy to use in practice.", "Q: You mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it.", "I guess you mean", "w.r.t. the original RMSprop.", "Yes, we intended to say that on Rosenbrock, RMSprop-APO converges quickly compared to baseline RMSprop; we have updated the paper to clarify this.", "Thank you for your helpful feedback. We have incorporated your suggestions into the updated paper.", "Specifically, we have:", "* Updated the baseline model for CIFAR-10/100 from VGG11 to ResNet34.", "* Used manual learning rate decay schedules for the CIFAR-10/100 baselines.", "We obtained 93-94% test accuracy on CIFAR-10 (SGD/SGDm/RMSprop/K-FAC) and 73-74% test accuracy on CIFAR-100 (SGD/SGDm).", "All are compared to their APO variants, which performed as well or better.", "The final results are shown in the table in the response to all reviewers at the top.", "* Shown that APO is competitive with manual schedules both in terms of test accuracy and training loss with ResNet34.", "This demonstrates the practical applicability of APO for contemporary networks.", "* Updated Figure 2 on CIFAR-10 with SGD/SGDm/RMSprop, Figure 4 on CIFAR-100 with SGD/SGDm, and Figure 6 on CIFAR-10 with SGD.", "We also added Figure 3 on CIFAR-10 with K-FAC.", "Each figure compares the baseline optimizers with their APO variants.", "Thank you for having helped us improve the paper."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_social", "rebuttal_structuring", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_done", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 233, "sentences": ["Thanks for the review.", "@Wall-clock: We don\u2019t quite understand the question.", "As mentioned in the response to Reviewer 3, our NLP example does answer the natural question about end-to-end gains. Is the reviewer only concerned with the location of the plots?", "- Another note: to perform a full wall-clock comparison with algorithms that have different per-iteration costs, one must disentangle and retune various hyperparameter choices, most notably the learning rate schedule.", "Thus we decided to feature the per-iteration comparison in the main paper, as it is the cleanest one.", "@L-BFGS: On a high level, we agree that GGT develops a similar window-based approximation to the gradient Gram matrix as L-BFGS does to the approximated Hessian.", "While adaptive methods have proven effective in practice, quasi-Newton algorithms are not in general regarded as competitive for deep learning (despite recent efforts [1,2]), and that\u2019s why it is not compared to in the vast majority of deep learning papers.", "- Quasi-Newton methods are suited for deterministic problems, while stochasticity is crucial in deep learning.", "This is because they try to approximate the Hessian by finite differences, which seems unstable with stochastic gradients in practice.", "- Direct second-order methods require significant modifications to converge in the non-convex setting (see [3,4]).", "Even these have not been observed to work well in deep learning.", "- One reason for the practical success of AdaGrad-like algorithms we believe is the difference of  -1/2 vs. -1 power on the Gram matrix, which seems to change the training dynamics dramatically.", "With the gradient Gram matrix and a -1 power, meaningful end-to-end advances have only been claimed for niche tasks other than classification.", "[1] Stochastic L-BFGS: Improved Convergence Rates and Practical Acceleration Strategies.", "R. Zhao and W. Haskell and V. Tan. arXiv, 2017.", "[2] A Stochastic Quasi-Newton Method for Large-Scale Optimization. R. Byrd, S. Hansen, and J. Nocedal, and Y. Singer SIAM Journal on Optimization, 2016.", "[3] Accelerated methods for nonconvex optimization.", "Y. Carmon, J. Duchi, O. Hinder, A. Sidford. SIAM Journal on Optimization, 2018.", "[4] Finding approximate local minima faster than gradient descent.", "N. Agarwal, Z. Allen-Zhu, B. Bullins, E. Hazan, and T. Ma. STOC 2017."], "labels": ["rebuttal_social", "rebuttal_refute-question", "rebuttal_followup", "rebuttal_answer", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 234, "sentences": ["We appreciate AnonReviewer3 for encouraging comments about the importance of the proposed abductive inference and generation tasks and about the value of our proposed dataset.", "We address the main concerns individually below:", "Adversarially filtering using BERT and GPT gives deep learning models a disadvantage:", "While BERT originally achieved high performance on the originally collected dataset, several recent studies [1][2][3][4] have found the presence of annotation artifacts in crowdsourced data that inadvertently leak information about the target label.", "This subsequently leads to overestimation of the performance of AI systems on end tasks.", "Our adversarial filtering (AF) algorithm aims to address the problem of overestimation of performance.", "In spite of targeting GPT/BERT during AF, human performance on the AF resulting dataset is still high.", "The significant gap between human and BERT performance leaves scope for inventing new methods for abductive reasoning.", "Ensemble of BERT models:", "An ensemble of three BERT models achieves an accuracy of 68.9%, very close to a single model 68.6%.", "Average score of human:", "The average score of human annotations is 89.4%.", "This is directly comparable with BERT-Ft [Fully Connected] model\u2019s performance of 68.6% in Table 1.", "Re. Ground Truth:", "The ground truth is assigned based on whether a hypothesis was collected during the plausible (Appendix A1 Task1) or implausible (Appendix A1 Task2) phase of the data collection procedure.", "To measure human performance, we had three annotators select the correct hypothesis and measured human performance as the accuracy of their majority-vote.", "Please let us know if this answers your question. If not, could you please clarify your question?", "Generative task vs classification:", "We completely agree.", "While the generative task is more general and much more interesting, the challenge of evaluating generations is significant, particularly for this task.", "This is due to the fact that there could be multiple distinct plausible explanations for a given pair of hypothesis.", "Consider the following example:", "O1: Kelly and her friend wanted to take a train to the city.", "O2: They had to wait for another one.", "Plausible explanations:", "1. They read the timetable incorrectly and arrived at the station just after a train had left.", "2. The train was full.", "Both explanations are plausible, and explain the observations, but automated evaluation metrics are not reliable enough to capture this phenomenon based on their reliance on surface level similarities.", "To simultaneously make progress on the novel abductive reasoning task and due to the ease of evaluation, we additionally introduce a discriminative version of the task.", "Nonetheless, we agree that in its most general form, there could be any number of observations and models should be required to generate explanatory hypotheses in natural language (alpha-NLG task).", "[1] Gururangan et al. Annotation artifacts in natural language inference data.", "[2] Poliak et al. Hypothesis only baselines in natural language inference.", "[3] Tsuchiya et al. Performance impact caused by hidden bias of training data for recognizing textual entailment.", "[4] Sakaguchi et al. WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"], "labels": ["rebuttal_accept-praise", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_followup", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 235, "sentences": ["(1) Reconstruction from prior during training:", "The crux of the proposed model is the selective proposal distribution.", "\"Pseudo\" sampling for unobserved modalities during training provides a way to facilitate model training process.", "We evaluated the model under two training settings: (I) optimize the final ELBO without conditional log-likelihood for unobserved modalities x_u; and (II) optimize the final ELBO with  conditional log-likelihood of unobserved modalities.", "This is realized by utilizing the \"pseudo\" sampling described before (and in the paper).", "The results are comparable but the added term in setting II shows benefits on some datasets.", "While setting I is solely based on the observed modalities, the setting II incorporates the unobserved modalities along with the observed ones.", "By using the complete data, the setting II describes the complete ELBO corresponding to the partially observed multimodal data (in consideration).", "(2) Comparison with VAEAC:", "In order to establish fair comparison, we used the same backbone network structures and training criteria for all baseline models and our proposed VSAE.", "Therefore, the implementation details differ from the original VAEAC paper.", "We did our best to maintain the optimization details described in all baseline papers.", "Experiments on VAEAC with partially-observed data are also conducted.", "Results show that VAEAC under this setting can achieve comparable performance on categorical datasets: 0.245(0.002) on Phishing, 0.399(0.011) on Mushroom while the errors of VSAE are 0.237(0.001) on Phishing,  0.396(0.008) on Mushroom.", "However, on numerical and bimodal datasets, partially trained VAEAC performs worse than VSAE :", "*VSAE:", "0.455(0.003) on Yeast; 1.312(0.021) on Glass;0.1376(0.0002) on MNIST+MNIST; 0.1198(0.0001) on MNIST+SVHN;", "*VAEAC trained partially:", "0.878(0.006) on Yeast; 1.846(0.037) on Glass;0.1402(0.0001) on MNIST+MNIST; 0.2126(0.0031) on MNIST+SVHN.", "(3) Experiments under synthetic non-MCAR masking:", "As mentioned by the reviewer, we conduct experiments on non-MCAR masking following state-of-the-art non-MCAR model MIWAE [2].", "Same as MIWAE, we synthesize masks by defining some rules to specify the probability of a Bernoulli distribution.", "Please refer to Table 3 and Appendix C.4 for updated comparison results.", "VSAE outperforms MIWAE under all MCAR, MAR and NMAR masking mechanisms.", "(4) Baselines:", "All baselines considered in the paper are designed to have comparable number of parameters (same or larger than our model) to make the comparison fair.", "We have updated the baseline details in the Appendix B.3.", "Although GAN-based models show promising imputation results, they usually fail to model data distribution properly.", "Therefore, we do not consider them as our baseline models.", "It is also important to note that VSAE is not a model designed only for imputation, but a generic framework to learn from partially-observed data for both imputation and generation.", "(5) Conditional imputation:", "When performing imputation, we assume that the generation is not conditioned on the observed image, but only conditioned on the factorized latent variables.", "Input an observed image to the model, we observe a \"conditional\" distribution if we independently sample from the latent variables.", "See Figure.7 in updated Appendix C.2.", "(6) Answers to the questions:", "1. Please refer to point (2) for detailed explanation on comparison with VAEAC.", "In summary, there are multiple reasons why the performance is not identical with the original VAEAC: (I) the back-bone structures are not the same; (II) training criteria (including batch size, learning rate, etc.) are not the same; and (III)  training/validation/test split is different.", "We would like to emphasize that the aforementioned changes are necessary to establish fair comparison.", "2.", "We adopt the calculation from [1] where NRMSE is RMSE normalized by the standard deviation of each feature followed by an average over all imputed features.", "The standard deviation of ground truth features does not guarantee NRMSE < 1.", "[1] Ivanov et al.Variational Autoencoder with Arbitrary Conditioning, ICLR 2019", "[2] Mattei et al. MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets, ICML 2019"], "labels": ["rebuttal_structuring", "rebuttal_refute-question", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_summary", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 236, "sentences": ["Thank you for the review, and we really appreciate your suggestions!", "In the revision, we have added analysis in section 4.2 and section 5 on how the learned causal matrix can be used downstream, for example in RL/IL and interpretability of neural nets.", "In the discussion in section 5, we also analyze how the error may affect the tasks downstream.", "We are excited that various tasks may utilize or incorporate our algorithm, and benefit from the causal inference ability it enables.", "We have also added comparison with sparse learning/feature selection methods in the \u201crelated works\u201d section.", "In particular, we note that L1 and group L1 regularization is dependent on the model structure change and rescaling of input variables, while our learnable noise risk is invariant to both, making it suitable for causal discovery where the scale of data may span orders of magnitude and the model structure may vary."], "labels": ["rebuttal_social", "rebuttal_done", "rebuttal_done", "rebuttal_mitigate-criticism", "rebuttal_done", "rebuttal_mitigate-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 237, "sentences": ["1. Thank you for your suggestion, we have addressed this in the revision as you suggested. This is indeed a good motivation.", "2. Thank you for your suggestion.", "We have further added experiments using even stronger query limit (previously 500000, now 50000) for the additional experiments on ResNet V2 model in the supplemental material. (We did not choose to use smaller epsilon because first, we already used a quite standard choice of epsilon, second, as you said, going for extremely small distortion does not really mean anything in adversarial context.) As you can see, in this even harder setting our proposed algorithm still maintain a performance lead over other baselines.", "Also, we have revised the statement in the abstract as you suggested.", "3. You are right, it is a quite weak attack and we have removed it from the table (just mention it in the text).", "4. Yes, we could just remove the distortion column in our result.", "We choose to include it because we do not want others to think that we actually trade a lot of distortions (to make problem easy) for speedup in runtime.", "5. We have added further empirical evidence to show that in the revision.", "From an intuitive perspective, using lambda>1 is essentially a \u201crelax and tighten\u201d step by first relax the constraint to make the problem easier, and then tighten it back to the real constraint.", "The \u201crelax and tighten\u201d idea has been widely used in constrained optimization, and we adapted this idea to Frank-Wolfe algorithm to make it even faster.", "6. As mentioned in an anonymous comment, there is one paper which proposed a similar but different zeroth-order non-convex FW algorithm as well as convergence rate analysis ahead of us.", "We were not aware of this paper when we prepared our ICLR submission, since it was posted only ten days before the ICLR deadline.", "We have cited this paper and modify our claim correspondingly in the revision.", "Nevertheless, it does not affect the main contribution of our paper: a novel Frank-Wolfe based adversarial attack framework for both white-box and black-box attacks, which is much more efficient than existing white-box/black-box adversarial attacks in both query complexity and runtime.", "7. Thank you for your suggestion and we have explicitly written down the update for a better comparison in the supplemental materials (Section A) in the revision.", "8. It means it is invariant to an affine transformation of the constraint set, i.e., if we choose to re-parameterize of the constraint with some linear or affine transformation M, the original and the new optimization problem will looks the same to the Frank-Wolfe algorithm.", "Please refer to [Jaggi (2013)], [Lacoste-Julien (2016)] for more details.", "9. In white-box setting, we perform grid search / binary search for parameter epsilon (or c for CW) for all algorithms.", "This will lead to better/ closer distortions for all methods.", "In black-box setting, we care more about query complexity and thus did not perform the grid search/binary search steps to avoid extra queries in finding the best epsilon/lambda.", "10. Thank you for pointing these typos out, we have addressed it in the revision."], "labels": ["rebuttal_done", "rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_summary", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 238, "sentences": ["Thank you for your review.", "We agree that further investigation is needed for mutual information, and we are currently working on it.", "As for the layer to investigate, we have presented the higher layer results because the representation regularizers showed the most improvements when applied to the higher (or even output) layer.", "We believe the representations in the lower layers are inherently less structured and therefore representation shaping can be harmful.", "The layer dependency is further explained in the following article.", "Daeyoung Choi and Wonjong Rhee, Utilizing class information for deep network representation shaping, AAAI 2019   (https://arxiv.org/abs/1809.09307)", ">> The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?", "Response: In general, representation regularizers showed better performance than the others.", "Among the representation regularizers, cw-VR and L1R frequently achieved the best performance.", "Nonetheless, we were not able to identify any specific task condition that makes a specific regularizer consistently best performing regularizer."], "labels": ["rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 239, "sentences": ["We thank you for the  detailed and thoughtful review.", "We are glad that you found the paper well-contextualized and the presentation high-quality. Here we discuss some of your comments.", "R2: \"this 'finishes' [Pathak et al., ICML17] to its logical conclusion for game-based environments and should spur interesting conversations and further research. In terms of actual technical contributions, I believe much less significant.\"", "=> In the light of the comments on originality and significance, we would like to highlight our finding that random features perform quite well and at times as well as learned features across many environments.", "This is a novel contribution since prior works have relied on learned features as a crucial requirement for good performance [Pathak et. al. ICML17].", "We believe this investigation would allow random features to be seen as an easily reproducible and strong baseline for future investigations of feature learning in exploration.", "Indeed, since the release of our paper, there has been some follow-ups on using random features for exploration in achieving state of the art results on hard exploration games when combined with extrinsic reward (in the interest of preserving anonymity, we don't include the references here).", "R2: \"However, it isn't entirely clear if the primary contribution is showing that 'curiosity reward' is a potentially promising approach or if game environments aren't particularly good testbeds for practical RL algorithms\"", "=> We believe that both are valuable insofar as generating discussion within the community and leading to follow-up experimentation.", "In particular, we hope our paper stimulates both, an interest in trying out more realistic/stochastic environments, *and* further research on curiosity as a potential useful reward.", "In addition to that, we have shown that curiosity could be a very strong baseline to compare against in future papers.", "All these, we argue, are valuable to the progress and health of the field.", "R2: \"Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method. However, as I said previously, this is probably a discussion worth having given the popularity and visibility of game-based testbeds\"", "=> We agree that significant amounts of stochasticity would break the method we used in the paper, and it is an important issue to be addressed by future work.", "Our vivid demonstration of this issue in the maze environment has already inspired some recent papers to look into, in particular, by incentivizing episodic reachability (in the interest of preserving anonymity, we don't include references to these, but we will include them in the final version of the paper).", "R2: \"I think that more casual ML/RL researchers will find these results controversial and surprising while more experienced researchers will see curiosity-driven learning to be explainable primarily by the intuition...\"", "R2: \"Even the 'focused experiments' can be explained with the intuitive narrative that in the state/action space\"", "=> Indeed in our experience, although a few people were not surprised, most of them were very surprised at the agents being able to make progress without any any extrinsic rewards.", "This suggests that the game designers (similar to architects, urban planners, gardeners, etc.) are purposefully setting up curricula to guide agents through the task by curiosity alone [Lazzaro, 2004].", "R2: \"consider [Mirowski et al., ICLR17] with respect to auxiliary loss + RL extrinsic rewards to improve performance\"", "R2: \"RL + auxiliary loss isn't evaluated in detail\"", "=> We will add a discussion of recent works that deal with navigation tasks in maze environments [Mirowski et. al. ICLR 2017, Jaderberg et. al. ICLR 2017] in the related works section.", "In contrast to these works, we don't assume privileged access to the maze environment in the form of depth estimation or loop closure supervision.", "Auxiliary tasks are an important component of RL and exploration methods, however, in this work we chose to focus on the most generic setting with minimal assumptions about the environment: providing raw observations in response to actions.", "In environments with privileged access we expect auxiliary tasks to benefit both curiosity-driven and extrinsic-reward-driven RL methods."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_by-cr", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 240, "sentences": ["We thank Reviewer 3 (R3) for their review and for clearly articulating their concerns regarding the paper.", "In our response below, we will clarify the design and results of our experiments as well as argue why we believe that these results should be of interest and are not, indeed, that predictable.", "R3 asked why training performance of many models is 100% when they do not generalize and suggested us to perform a large number of training runs to see if occasionally the right solution is found.", "First, we agree that from the point of view of training there are many equally good solutions, and in fact, this is the main and the only challenge of SQOOP.", "We designed the task with the goal of testing which models are more likely to converge to the right solution, with which they can handle all possible combinations of objects, despite being trained only on a small subset of objects.", "We argued extensively in the introduction that such an ability to find the systematic solution despite other alternatives being available is highly desirable for language understanding approaches.", "We fully agree with R3 that in investigations of whether or not a particular model converges to the right solution repeating every experiment several times is absolutely necessary, and we would like to emphasize that we did repeat each experiment 3, 5, or 10 times (see \u201cdetails\u201d in Table 1 and the paragraph \u201cParametrization Induction\u201d on page 8).", "In most cases we saw a consistent success or consistent failure, one exception being the parametrization induction results, where 4 out of 10 runs were successful (see Table 4, row 1 for the mean and the confidence interval).", "We hope that 3 takes this fact into account, and we will furthermore improve on the current level of rigor in the upcoming revision by repeating each experiment at least 5 times.", "We are not sure if we fully understand the question \u201cCould you somehow test for if a given trained model will show systematic generalization?\u201d that R3 asked.", "We test the systematic generalization of a model by evaluating it on all SQOOP questions that were not present in the training set.", "We hope that this answers the question of R3 and we would be happy to engage in a further discussion regarding this and make edits to the paper if necessary.", "We thank R3 for the suggestion to investigate the influence of model size and regularization on systematic generalization.", "It is indeed a very appropriate question in the  context of our study, however, we note that there exists a wide variety of regularization methods and trying them all (and all their combinations) would be infeasible.", "In the upcoming update of the paper we will report results of an on-going ablation study for the MAC model, in which we vary the module size, the number of modules and experiment with weight decay.", "We would welcome any other specific experiment requests R3 may have.", "Finally, we would like to discuss the significance of our investigation and its results.", "While we agree that the results that we report may not shock the reader (although perhaps hindsight bias plays a role in what people find surprising or not after reading an article) we find them highly interesting and not at all easily predictable.", "Reading prior work on visual reasoning may lead a researcher to conclude, roughly speaking, that NMNs are a lost cause, since a variety of generic models perform comparably or better.", "In contrast, our rigorous investigation highlights their strong generalization capabilities and relates them to the specific design of NMNs.", "Notably, chain-structured NMNs were used in the literature prior to this work (e.g. in the model of Jonshon et al multiple filter_...[...] modules are often chained), so the fact that tree-structured NMNs show much stronger generalization was not obvious prior to this investigation and should be of a high interest to the research community.", "Last but not least, an important part of our investigation (which the review does not discuss) is the systematic generalization analysis of popular end-to-end NMN versions, that shows how making NMNs more end-to-end makes them more susceptible to finding spurious solutions.", "As we argued in our conclusion, these findings should be of a highest importance to researchers working on end-to-end NMNs, which is a very popular research direction nowadays.", "We conclude our response by announcing that an updated version of the paper, that among others incorporates valuable suggestions by R3, will soon be uploaded to OpenReview.", "We are currently performing a lot of additional experiments, the results of which will make our investigation even more rigorous and complete.", "We sincerely hope that R3 takes into account the arguments we have made here and the new results that we will publish soon and reevaluates our paper more positively.", "Dear Reviewer 3,", "We thank you again for your informative review that you wrote before the revision period.", "In our response and the revised version of the paper we tried our best to address your concerns.", "We would highly appreciate to get some feedback from you regarding the changes that we have made and the arguments that we have presented.", "In particular, we report that NMN-Chains (with a lot of inductive bias built-in and also used in prior work such as Johnson et al. 2017) generalize poorly compared to even generic modules, and that layout/parameterization induction often fails to converge to the correct solution.", "We believe both these findings are quite surprising.", "We also report new experiments with the MAC model, including a hyperparameter search, a comparison against end-to-end NMNs, and a qualitative exploration of the failure modes of this model.", "All these experiments are repeated at least 5 times each, like you suggested in your review, although it\u2019s worth noting that results the original version of the paper also reported results after  multiple runs.", "We would highly appreciate a response on our newest revision and suggestions on how it could be improved. If you still think that paper is uninteresting or not well executed, could you then suggest what specifically it is lacking?", "We are sincerely hoping to hear from you."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_answer", "rebuttal_social", "rebuttal_social", "rebuttal_reject-request", "rebuttal_by-cr", "rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_by-cr", "rebuttal_by-cr", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_followup", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 241, "sentences": ["We thank the reviewer for acknowledging the novelty of our work and for noting that our experiments are thorough.", "Thank you for pointing out a related preprint by Z. Hu et al. [arXiv:1905.13728].", "We note the work by Z. Hu et al. was developed independently and concurrently to our work here, and we were not aware of it at the time of writing our paper.", "We shall cite the preprint and include a discussion in our paper.", "Briefly, the key difference between our work and that of Hu et al. is that Hu et al. consider a more restrictive setting where graphs are completely unlabeled (i.e., graphs have no node features).", "Hu et al. then focus on extracting generic graph properties of unlabeled graphs by pre-training on randomly-generated graphs.", "While the approach is interesting, the limitation of such an approach is that it improves performance only marginally over ordinary supervised classification of the original attributed graphs.", "This is because it is hard for random unlabeled graphs to capture domain-specific knowledge that is useful for a specific application.", "Moreover, in practice, graphs tend to have labels together with rich node and edge attributes, but Hu et al.\u2019s approach cannot naturally leverage such attribute information, which then results in limited gains.", "In principle, we could compare our approach against Hu et al., however, right now, this would be extremely challenging because of the following reasons.", "(1) We cannot find a public implementation of Hu et al.\u2019s approach for reliable comparison.", "(2) Reimplementing their method requires knowledge of many specific implementational details and design choices (feature extraction, graph generation, etc.), which are not discussed in their preprint.", "(3) Finally, their pre-trained GNN operates on unlabeled graphs, and so it cannot be directly applied to our datasets of labeled graphs.", "Lastly, in contrast to Hu et al., our work focuses on important real-world domains, where one wants to pre-train GNNs by utilizing the abundant graph, node, and edge attributes.", "Importantly, our approach is able to learn a domain-specific data distribution that is useful for downstream prediction.", "We demonstrate on two application domains that such practical settings (i.e., labeled graphs with naturally-given node and edge attributes) are very important to consider and that our pre-training can substantially improve model performance."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 242, "sentences": ["Thanks for your valuable comments.", "It helps us to prepare the revision.", "We address all your concerns in the revision as below.", "Q1: Was the auxiliary tower used during the training of the shared weights W?", "A1: Auxiliary tower is used only in the retraining stage.", "Q2: \u201cDid the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule?\u201d", "A2:", "CIFAR: In the pretrain stage and search stage, the learning rate is fixed to 0.1 with batch size 128; In the retraining stage, we use cosine learning rate schedule.", "ImageNet: In the pretrain stage and search stage, the learning rate is fixed to 0.1 with batch 224; In the retraining stage, we use linear decay learning rate schedule.", "Q3: \u201cFigure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?\u201d", "A3: In the revision, we replace the Figure 4 with a new version which has more details.", "As show in Figure 4, all the operators in level 4 are pruned.", "Q4: \u201cThe sparse regularization of \\lambda induces great difficulties in optimization\u201d", "A4: The non-smooth regularization introduced by l1 regularization makes traditional stochastic SGD failed to yield sparse results.", "If we need exact zero, we have to use heuristic thresholding on the \\lambda learned, which has already been demonstrated in SSS [1] that is inferior.", "Besides, traditional APG method is not friendly for deep learning as extra forward-backward computation is required, also as shown by SSS.", "Q5: \u201cMissed citation: MnasNet also incorporates the cost of architectures in their search process. On ImageNet, your performance is similar to theirs. I think this will be a good comparison.\u201d", "A5: We have added the result of MnasNet [2] in Table 2.", "Indeed, MnasNet achieves similar results with us with less FLOPs.", "However, it is also need to note that MnasNet evaluates more than 8K models, which introduces much higher search cost than our method.", "Moreover, the design space of MnasNet is significant different from other existing NAS methods including ours.", "It is interesting to explore the combination of MnasNet with ours in the future work.", "Q6: \u201cThe paper has some grammatical errors.\u201d", "A6: We have fixed the typos and grammatical errors in the revision.", "Q7: About \u201cfirst NAS algorithm to perform direct search on ImageNet\u201d", "A7: We check this claim again and find methods like MnasNet [2] and one-shot architecture search [3] also have the ability to perform direct search on ImageNet, we have delete this claim in the paper.", "However, to the best of our knowledge, our method is the first method to perform directly search without block structure sharing.", "We also report preliminary results that directly search on task beyond classification (semantic segmentation).", "Please refer to Q1 of Reviewer3 for details.", "[1] Data-Driven Sparse Structure Selection for Deep Neural Networks. ECCV 2018.", "[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf", "[3] Understanding and simplifying one-shot architecture search. ICML 2018."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_structuring", "rebuttal_done", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_future", "rebuttal_structuring", "rebuttal_done", "rebuttal_structuring", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 243, "sentences": ["Thank you for pointing out the other datasets in algebraic word reasoning.", "We\u2019ve included these in an expanded discussion of related work with discussion on how they relate to the current dataset.", "Please let us know if we have missed other papers.", "Your proposal of combining multiple extant problem sets is a good idea.", "We\u2019d want to ensure the combined datasets have a common format (e.g., the same unambiguous freeform text format for reasons of transferability, etc as argued in the paper), and there are interesting problem types occurring in other datasets (such as logical entailment or boolean satisfiability) that we haven\u2019t yet included.", "We may in the future extend the dataset to include these other problem types if the current ones become solved, and of course we solicit contributions (in the form of generation code) to the dataset.", "We likely could not use workbooks etc as a source for problems without significant investment, since obtaining legal permission to redistribute copyrighted problems found in these books would probably be hard and/or expensive.", "Having said that, it is definitely important to ensure the problems remain grounded in real-life problems (thus our small list of real-life exam questions).", "This was the motivation for testing trained models against \u201creal life\u201d questions occurring in school-level examinations; these questions are not intended to be a primary benchmark (with more questions and detailed grades), but rather simply a rough indication of whether training models to answer school-level questions could be achievable.", "On the distribution of the sampled answer (and the related question of how difficulty levels are determined), these are great questions.", "For some modules with two output choices (e.g., True, False), we can simply split the answers 50-50.", "But in general, the answer distribution depends on the module, with hand-tuning to ensure the (question, answer) pair is of a reasonable difficulty level as judged by humans.", "In more detail: as mentioned in the paper, we want to achieve upper bounds on the maximum probability that any single (question, answer) is sampled; thus if we sample the answer from a set of N possible answers, then to achieve a maximum probability p of a given question, the remaining choices made in generating the question must", "be from a set of size p/N. We roughly aim to pick N (depending on p) so that conditioned on this, the question is as easy as possible; there is typically a hand-tuned sweet spot.", "On evaluating general-purpose models only, we may have phrased this badly in the paper, and have updated it.", "We are definitely interested in any models that learns to do mathematics and symbolic reasoning, which would include more sophisticated models tailored towards doing mathematics (one could imagine models with working memory, etc).", "However, we discount models that already have their mathematics knowledge inbuilt rather than learnt (for example, this includes many of the models that occur in algebraic reasoning tasks, where the model learns to map the input text to an existing equation template, that is then solved by a fixed calculator).", "We test DNC (differentiable neural computers) and RMC (relational memory core) models, which arguably are more specialized for doing mathematics, since they have a slot-based memory that may be appropriate for storing intermediate results.", "However these models obtained worse performance than the more general architectures, and we are not yet aware of models that are more tailored for doing mathematics that do not simply have their mathematics knowledge built-in and unlearnable; we hope the dataset will spur the development of new models along these lines.", "On the number of thinking steps, in our earlier analysis we trained up to 150k steps (compared with 500k for final performance reported in paper), and observed the following interpolation test performances by number of steps: 39% (0 steps), 46% (1 step), 48% (2), 49% (4), 50% (8), 51% (16).", "We are re-running experiments now to confirm the final performances, which we can include in the final paper."], "labels": ["rebuttal_structuring", "rebuttal_done", "rebuttal_social", "rebuttal_social", "rebuttal_mitigate-criticism", "rebuttal_future", "rebuttal_reject-request", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 244, "sentences": ["Thank you very much for your encouraging review and helpful comments.", "We will make revisions to address the several points you have raised in your review.", "Below we first address the main concerns.", "Q1: \u201cAlt-az\u201d rotation is not a group.", "A1: Thank you for pointing this out.", "You are correct.", "The Alt-az rotation, according to our definition, is not a group.", "SO(3)  is a group which can be parametrized by a 3-sphere .", "But when we reduce one parameter from it, it is not a group anymore mathematically; the composition of two alt-az rotations becomes a general rotation in SO(3).", "In the new revision, we will use the term alt-az rotation in \u201cquotient SO(3)/SO(2)\u201d  instead of alt-az rotation group.", "Moreover, the quotient SO(3)/SO(2) is isomorphic to $S^2$ and to avoid the ill-definition on the two poles (the two degenerate points), we will add a constraint to the alt-az rotation, i.e. $\\phi=0, if \\theta=0 or \\theta=\\pi$. This is because, when the altitude rotation is zero or PI, the azimuth rotation is meaningless in a alt-az rotation and is therefore fixed as zero.", "If $\\theta=0 or \\theta=pi, and \u201c\\phi \\neq 0$, this rotation belongs to the azimuthal rotation in SO(2) group.", "(Q2) Equivariance property of the Alt-az convolution", "We think we can still have the equivariance property but only for single alt-az rotation.", "Notice the definition of alt-az convolution do not use any composite rotation.", "Here is our tentative proof:", "Under the definition of alt-azimuth anisotropic convolution and using the unitary property (5) of rotation operators, we have (assume the number of channels K=1 for simplicity, assume Q and R be both alt-az rotations):", "************************************************", "\\begin{equation}", "\\begin{aligned}", "& (h \\star D_{Q} f) (R)", "\\\\", "&", "= \\int_{S^2}(D_Rh)(\\hat{u})f(Q^{-1}\\hat{u})ds(\\hat{u}) \\\\", "&", "=\\int_{S^2}h(R^{-1}\\hat{u})f(Q^{-1}\\hat{u})ds(\\hat{u}) \\\\", "&", "=\\int_{S^2}h(R^{-1}Q\\hat{u})f(\\hat{u})ds(\\hat{u}) \\\\", "&", "=\\int_{S^2}h((Q^{-1}R)^{-1}\\hat{u})f(\\hat{u})ds(\\hat{u}) \\\\", "&", "=(h \\star f)(Q^{-1}R) = D_{Q}( h \\star f)(R)", "\\end{aligned}", "\\end{equation}", "**************************************************", "This means that for a single alt-az rotation of input spherical image; the output of a convolution layer will rotate in the same way.", "Although the property doesn\u2019t hold if one performs multiple alt-az rotations to the input spherical image, it is still valuable because we assume the different SO(3) orientation of an input 3D shape is from a composite of an azimuthal rotation and an alt-az rotation, the azimuthal rotation is treated by data augmentation and the single alt-az rotation is treated by the network equivariance and invariance.", "Q3: alt-az convolution is not well defined on the south pole", "A3: Yes, we agree that our original definition of alt-az convolution is not well defined on both north and south poles.", "Therefore, in the new revision, we will add the constraints to the definition of alt-az rotation and make it one-to-one corresponds to the set points on $S^2$. See A1.", "Q4: The paragraph motivating the alt-az convolution on page 4 is not very clear.", "A4: Thanks for the comments, as you suggested, we will rewrite this paragraph in the new version, and acknowledge the importance and effectiveness of the recent work on the group equivariance and rotation invariant networks.", "Q5: It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2)", "A5: The two related papers (Cohen et al 2018 for general SO3 and Esteves, and Esteves et al 2018 for isotropic S2) both use lat-lon grid and Fourier domain convolution, while ours uses a icosahedron-sphere grid and direct spherical domain convolution.", "The use of different sampling in the input spherical image, and the use of filters are totally different.", "We think a direct comparison should be done in one of the following ways : (a) perform the three types of spherical convolution all using icosahedron-sphere grid and then convolve in the spherical domain.", "(b) perform the three types of spherical convolution all using lat-lon grid and convolve in the Fourier domain.", "For the first type of direct comparison, to implement isotropic spherical convolution (Type II), we should make the geodesic disc filter share an identical weight along the angular direction.", "To implement a general SO(3) spherical convolution, we should add a rotation degree of freedom into our disc filter.", "We are conducting this experiment and if the time and paper page limit are allowed, we will report the comparison result in the revised version.", "Otherwise, we will put it into our future work.", "For the second type of direct comparison, we need to conduct alt-az spherical convolution in the Fourier domain, this is possible by determining the spherical harmonic coefficient, $<g_0, Y_l^m> $ for the alt-az convolution in terms of the spherical harmonic coefficient of input spherical signal $f$ and the filter $h$. This comparison needs re-designing of our network and we can not finish it within the rebuttal period, we\u2019ll leave it for future work.", "Q6: Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1.", "I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned.", "Some more explanation / discussion would be good.", "A6: Theoretically, our method will be rotation invariant with only AZ rotation, it will be full rotation invariant with SO(2) rotation augmentation about an arbitrary axis .", "In table I, we believe the reason alt-az augmentation performs better because it contains more training data.", "SO(3) augmentation underperforms the AZ augmentation because several random SO(3) rotation augmentation might not be able to cover all the relative rotation wrt the filter's orientation (see appendix).", "Q7: It would be nice to explain the spherical parameterization in more detail.", "Is this operation itself rotation equivariant?", "A7: Due to the page limit of the conference paper, we could not explain the spherical parameterization method in detail.", "This operation is theoretically rotation equivariant.", "Spherical parameterization establishes a map that transforms the points of a closed surface into the points on the unit sphere.", "A good spherical mapping for a closed surface should satisfy the following properties:bijective mapping and least distortion.", "Bijective mapping is the most important but most difficult in this process which implies that the resulting map is one-to-one, fold-free, and therefore feature preserving (information lossless).", "Least distortion seeks a good sampling rate such that interesting features of the model receive enough real estate on the sphere in order to be accurately sampled.", "We achieved the bijective mapping by adapting a coarse-to-fine strategy with minor modifications (See http://hhoppe.com/proj/sphereparam/).", "The minimizing of the map distortion is obtained using the authalic parameterization proposed in Sinha et al 2016.", "This process is rotation equivariant because the initial bijective mapping is depends on the object orientation and the authalic remeshing does not change the orientation of the spherical embeddings.", "Spherical parameterization is a good way to retain geometric and topological information of original shapes (compared to the spherical projection method), but currently it works only for genus-0 closed object, extending it to 3d shapes with arbitrary topology is still an unsolved problem, that is why we could not adapt this method for dataset such as ModelNET and Shrec\u201917, they contain 3D objects with arbitrary topology.", "(Q8) Other typos and minor issues", "We will correct all the typos and other minor issues in the revised paper, thank you again for the detailed review. WE really appreciate your help."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_mitigate-criticism", "rebuttal_mitigate-criticism", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_other", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_contradict-assertion", "rebuttal_other", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_answer", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_future", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 245, "sentences": ["We appreciate the reviewer 1 for his/her feedback on our paper.", "The reviewer mentioned: \u201cIn general it is very unlikely that you will be able to choose every variation of out-distribution cases\u201d:", "Actually, for training A-CNN (Augmented CNN), we did not train it on every variation of out-distribution cases, rather, we recognize a single representative out-distribution set among the available ones according to our measurement.", "Then using it for training A-CNN with the aim of effectively controlling over-generalization.", "The reviewer mentioned: \u201c Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax\u201d:", "We would be appreciated if the reviewer could provide us with the references that showing using only sigmoid could control such a challenging problem of adversaries.", "Please note we did not aim to devise a method that is able to reject all adversaries.", "Rather, we attempted to show that a CNN with less over-generalization is able to reject some of the adversaries while correctly classifies many of the remainder, particularly non-transferable attacks."], "labels": ["rebuttal_social", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_followup", "rebuttal_reject-criticism", "rebuttal_reject-criticism"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 246, "sentences": ["Thanks for the review.", "@Update overhead: We argue that per-iteration performance is a worthwhile objective in itself, which is less significant in some scenarios (e.g. costly function evaluation, like in RL, or expensive backprops, like in RNNs).", "That said, we were indeed not able to demonstrate end-to-end gains in vision.", "Please note that in the NLP benchmark our algorithm finds a better solution and wins in wall-clock time.", "@Switching: This is a good suggestion, and we indeed do cite one of the papers attempting to approach optimizer-switching in a principled way.", "We found that we could squeeze out some wall-clock gains by applying the expensive update more sparingly, but the value of including this in the paper was unclear (effectively adding a host of hyperparameters orthogonal to the central idea)."], "labels": ["rebuttal_social", "rebuttal_reject-criticism", "rebuttal_concede-criticism", "rebuttal_other", "rebuttal_done", "rebuttal_answer"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 247, "sentences": ["Overall:", "We thank you for your valuable suggestions in helping us avoid potential inefficiencies in our work, and suggesting ways to avoid misunderstandings.", "We have incorporated your comments to significantly improve our work, and hope our revised draft is able to convince you towards a favorable outcome.", "Concern 1: Concerns with title \u201cMeta Domain Adaptation\u201d", "\u201c\u2026unlike as advertised, the paper does not address", "\u2026 \u201c", "It appears that our choice of the title may have resulted in the reviewer qualifying our paper as a form of false advertising.", "We acknowledge this problem and agree with you about a possible misinterpretation.", "However, we feel this maybe a bit harsh, as in the technical content of the paper (Abstract, Introduction, etc.), we have been very clear about the motivation and the problem setting, and do not think we did any form of false advertising.", "We also do think that the problem setting we have proposed is an important problem that deserves attention, and has not been studied in the meta-learning paradigm.", "We are glad that you also agree that setting makes sense (\u201c... the combination \u2026 is fair\u201d).", "Overall, we think that we have made an important contribution to Meta-Learning literature, by identifying its limitation for few-shot learning under domain shift, and proposed a solution to tackle this problem.", "We have tried to revise the draft with appropriate renaming of the method to avoid potential misunderstandings.", "In fact, we have mostly changed the name from \u201cMeta Domain Adaptation\u201d to \u201cMeta Learning with Domain Adaptation\u201d, and the rest of the paper is almost identical, which we believe addresses the concerns of false advertising.", "Concern 2: Experiments", "Domain Adaptation Baselines + Other datasets", "Being a new problem setting, designing appropriate baselines can be challenging.", "We considered the traditional meta-learning for few-shot learning approaches, and combined meta-learning with a popular domain adaptation baseline.", "We are grateful for your suggestions on the domain adaptation baselines, and fully agree that it is reasonable.", "It is something we should have done on our own.", "Accordingly, based on your suggestions, and suggestions from other reviewers, we have tried to expand the baselines substantially (specifically, we include three state of the art Domain Adaptation methods as baselines \u2013 RevGrad [1], ADDA [2] and CyCADA [3]), and our proposed methods outperform them.", "For the other dataset suggested (VisDA), for synthetic-real adaptation, it is difficult to match the training paradigm of meta-learning.", "Typically, we desire several classes for meta-train, and several classes for meta-test, so that a variety of (e.g.) 5-way tasks can be crawn.", "With just 12 classes, the dataset is not very suitable for such settings.", "[1] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\" The Journal of Machine Learning Research 17.1 (2016): 2096-2030", "[2] Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\" Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017", "[3] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K. & Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. ICML 2018", "We thank you for considering our rebuttal and updating the score.", "We are grateful for your time and advice, and would appreciate if we could further extend the discussion.", "We appreciate the concern in the updated comments, but would like to point out that the novelty in our work should be viewed from two angles: the need to study this problem (i.e., the problem setting), and the proposed solution.", "We have identified a novel problem setting, which is closer to the real world setting, than what has been studied so far under the meta-learning paradigm.", "Existing solutions are not effective in this setting, restricting their use in the real world.", "Addressing this setting in our framework gives us a direction to improve the practical utility of meta-learning solutions for few-shot learning.", "Specifically, we identify that the principle of image-to-image translation is very suitable for this setting, and apply those concepts to boost the performance of few-shot learning under domain shift.", "As a combination of problem setting and proposed solution, we do believe we have addressed an important problem, and made a novel contribution.", "As regards the experiments: \u201cfairly small datasets", "\u2026 feature extractor backbone\u201d", "Most domain adaptation experiments use MNIST, USPS, SVHN, which are comparable in size to our Omniglot experiments.", "The other popular benchmark is using the Office-dataset, which also we have used (although a more recent version of a similar dataset, i.e., office-home \u2013 more suitable for meta-learning evaluation, as it has larger number of classes).", "See for example some of the recent domain adaptation papers [1, 2, 3].", "While a feature extractor backbone network may have some influence, we would like to highlight three points.", "First, when networks are trained in one domain, and evaluated in another, regardless of the backbone network, it is the domain-shift that dominates the performance.", "For example, no matter how large the network is, if it is trained to recognize black and white digits, it will still struggle to recognize colored digits.", "Second, any benefit of a larger backbone network will likely also enhance the performance of our model.", "Third, we just wanted to clarify (if there was a misunderstanding), unlike domain adaptation papers, we do not use a pretrained network \u2013 we train the full network from scratch (following traditional meta-training settings).", "[1] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\" The Journal of Machine Learning Research 17.1 (2016): 2096-2030", "[2] Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\" Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017", "[3] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K. & Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. ICML 2018"], "labels": ["rebuttal_structuring", "rebuttal_social", "rebuttal_summary", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_mitigate-criticism", "rebuttal_answer", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_reject-request", "rebuttal_other", "rebuttal_other", "rebuttal_other", "rebuttal_social", "rebuttal_social", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_reject-criticism", "rebuttal_reject-criticism", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_other", "rebuttal_other", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 248, "sentences": ["We thank the reviewer for the valuable feedback!", "The suggestion comments were very helpful and led to a clear improvement of our manuscript.", "We reply to the answers and comments in the order they were raised:", "(1) If one uses the same matrix-variate normal distribution that we use for p(\\theta | x) as approximate posterior p(\\theta) of a BNN in conjunction with the ELBO objective, one arrives at a BNN proposed by Louizos and Welling (2016) [1], i.e. the Variational Matrix Gaussian (VMG).", "We found that VMG\u2019s results (obtained from their original code https://github.com/AMLab-Amsterdam/SEVDL_MGP) are not as good as that for the CDN, as shown in Figure 8 in the appendix.", "This is further discussed in the newly added section 6.4.", "(2) Thank you for this valuable suggestion!", "We have added a new section (Sec. 4) to discuss the differences between the objective used for CDN, when performing variational inference for BNNs, and in the variational information bottleneck (VIB) framework.", "Furthermore, we present an experimental investigation of these different objectives (Sec. 6.4).", "We found that the CDN objective leads to superior results, especially in the adversarial examples experiment.", "(3) We observed that as \\lambda increases, in the validation set, the uncertainty is increasing, while the accuracy is decreasing.", "So", ", a simple heuristic that we use is to choose the highest \\lambda that allow high validation accuracy (e.g. > 0.97 on MNIST).", "We found that this heuristic works very well in our experiments (the results have updated to reflect on this heuristic).", "We have made this procedure clear in the revised manuscript.", "Detailed comments about experiments:", "(a) Thanks for catching this. Indeed this was due to a bug in the toy regression experiment which we have fixed now.", "(b) We have revised the baselines so that they either use \\lambda = 1 or the settings that the original authors recommended.", "We detail this in Appendix F.", "References:", "[1] Louizos, Christos, and Max Welling. \"Structured and efficient variational deep learning with matrix gaussian posteriors.\" International Conference on Machine Learning. 2016."], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_structuring", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_answer", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_done", "rebuttal_done", "rebuttal_structuring", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 249, "sentences": ["Thank you for your review and comments.", "We\u2019ve made a number of additions and improvements to address them in the updated version of the paper, which we will submit before the end of the discussion period.", "First, we have performed a new set of experiments on the larger dataset in [1].", "HOF shows greater average reconstruction accuracy than the methods compared in [1].", "Second, we also perform ablation experiments to demonstrate that HOF performs competitively even when we vary the encoder architecture, decoder depth, decoder activation function, or input sampling for the decoder network.", "For example, using Resnet18 as the encoder architecture or using a decoder network with twice as many hidden layers showed nearly identical performance in terms of average Chamfer distance on the test set.", "The complete quantitative results will be included in an updated PDF before the end of the discussion period.", "\"The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples.", "The function composition doesn't capture that.", "I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).", "But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc. I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands  reasonably on its own without that.\"", "We agree that the current formulation of composition is not equivalent to a generative model.", "In our work, function composition primarily serves the purpose of demonstrating that the model learns a meaningful subspace of objects (rather than memorizing the training set, as you mentioned).", "We have revised the abstract to clarify this point.", "\"In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?\"", "We have clarified in the manuscript that our comparisons are between architectures, rather than training objectives/output representations.", "Thus our DeepSDF, FoldingNet, and HOF architectures all output point clouds, which can be compared directly.", "\"For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)? I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable).\"", "Additional techniques for promoting learning of composable representations such as skip connections and layer dropout are an exciting direction for future research.", "One way function composition might allow for R^3 -> R mappings by composing a mapping from R^3 -> R^3 and taking the only first dimension of each element in the final output.", "Thank you again for your feedback.", "[1] M. Tatarchenko, S. R. Richter, R. Ranftl, Z. Li, V. Koltun, and T. Brox, \u201cWhat do single-view 3d reconstruction networks learn?,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3405\u2013 3414, 2019."], "labels": ["rebuttal_social", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_summary", "rebuttal_by-cr", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_structuring", "rebuttal_concede-criticism", "rebuttal_reject-criticism", "rebuttal_done", "rebuttal_structuring", "rebuttal_done", "rebuttal_answer", "rebuttal_structuring", "rebuttal_future", "rebuttal_answer", "rebuttal_social", "rebuttal_other"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}
{"abstract_id": 250, "sentences": ["We thank the reviewer for acknowledging the technical aspects of the paper and for noting that our\u200b \u200bresults\u200b \u200bare\u200b \u200bsolid\u200b \u200band\u200b \u200bour\u200b \u200banalysis\u200b \u200bis\u200b \u200bthorough.", "RE: Source code", "The reviewer makes an important point about the availability of the source code.", "To address this point, in the link privately shared with the reviewers, we have provided all of our code, datasets together with their train/test splits, as well as our pre-trained models, to help with the reproducibility of our results.", "We note that we will share PyTorch implementations of all pre-training methods and datasets with the community upon publication.", "Please feel free to ask any further questions regarding our code and implementation.", "RE: Linear time complexity in Appendix F", "We acknowledge that the time complexity of our pre-training methods was not well explained in Appendix F. In Figure 2 (a) we show that we only sample one node per graph.", "We then use breadth-first search to extract a K-hop neighborhood of the node, which takes at most linear time with respect to the number of edges in the graph.", "As a result, pre-training via context prediction has linear time complexity.", "We will edit Appendix F to include more detailed information and cover this important point.", "Please let us know if you have any further questions or comments!"], "labels": ["rebuttal_social", "rebuttal_social", "rebuttal_answer", "rebuttal_done", "rebuttal_by-cr", "rebuttal_social", "rebuttal_social", "rebuttal_concede-criticism", "rebuttal_answer", "rebuttal_answer", "rebuttal_by-cr", "rebuttal_social"], "confs": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}